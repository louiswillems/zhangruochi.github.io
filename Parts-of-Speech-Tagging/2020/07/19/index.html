<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="Assignment 2: Parts-of-Speech Tagging (POS)Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) t">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Parts-of-Speech Tagging">
<meta property="og:url" content="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Assignment 2: Parts-of-Speech Tagging (POS)Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) t">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/DataSources1.PNG">
<meta property="og:image" content="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/pos.png">
<meta property="og:image" content="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/Initialize4.PNG">
<meta property="og:image" content="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/Forward4.PNG">
<meta property="og:image" content="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/Backwards5.PNG">
<meta property="og:updated_time" content="2020-07-18T17:42:08.653Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Parts-of-Speech Tagging">
<meta name="twitter:description" content="Assignment 2: Parts-of-Speech Tagging (POS)Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) t">
<meta name="twitter:image" content="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/DataSources1.PNG">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/">





  <title>Parts-of-Speech Tagging | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Parts-of-Speech Tagging</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-07-19T01:39:21+08:00">
                2020-07-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Parts-of-Speech-Tagging/2020/07/19/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Parts-of-Speech-Tagging/2020/07/19/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Assignment-2-Parts-of-Speech-Tagging-POS"><a href="#Assignment-2-Parts-of-Speech-Tagging-POS" class="headerlink" title="Assignment 2: Parts-of-Speech Tagging (POS)"></a>Assignment 2: Parts-of-Speech Tagging (POS)</h1><p>Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective…) to each word in an input text.  Tagging is difficult because some words can represent more than one part of speech at different times. They are  <strong>Ambiguous</strong>. Let’s look at the following example: </p>
<ul>
<li>The whole team played <strong>well</strong>. [adverb]</li>
<li>You are doing <strong>well</strong> for yourself. [adjective]</li>
<li><strong>Well</strong>, this assignment took me forever to complete. [interjection]</li>
<li>The <strong>well</strong> is dry. [noun]</li>
<li>Tears were beginning to <strong>well</strong> in her eyes. [verb]</li>
</ul>
<p>Distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will: </p>
<ul>
<li>Learn how parts-of-speech tagging works</li>
<li>Compute the transition matrix A in a Hidden Markov Model</li>
<li>Compute the transition matrix B in a Hidden Markov Model</li>
<li>Compute the Viterbi algorithm </li>
<li>Compute the accuracy of your own model </li>
</ul>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#0">0 Data Sources</a></li>
<li><a href="#1">1 POS Tagging</a><ul>
<li><a href="#1.1">1.1 Training</a><ul>
<li><a href="#ex-01">Exercise 01</a></li>
</ul>
</li>
<li><a href="#1.2">1.2 Testing</a><ul>
<li><a href="#ex-02">Exercise 02</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">2 Hidden Markov Models</a><ul>
<li><a href="#2.1">2.1 Generating Matrices</a><ul>
<li><a href="#ex-03">Exercise 03</a></li>
<li><a href="#ex-04">Exercise 04</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3">3 Viterbi Algorithm</a><ul>
<li><a href="#3.1">3.1 Initialization</a><ul>
<li><a href="#ex-05">Exercise 05</a></li>
</ul>
</li>
<li><a href="#3.2">3.2 Viterbi Forward</a><ul>
<li><a href="#ex-06">Exercise 06</a></li>
</ul>
</li>
<li><a href="#3.3">3.3 Viterbi Backward</a><ul>
<li><a href="#ex-07">Exercise 07</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4">4 Predicting on a data set</a><ul>
<li><a href="#ex-08">Exercise 08</a></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Importing packages and loading in the data set </span></span><br><span class="line"><span class="keyword">from</span> utils_pos <span class="keyword">import</span> get_word_tag, preprocess  </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p><a name="0"></a></p>
<h2 id="Part-0-Data-Sources"><a href="#Part-0-Data-Sources" class="headerlink" title="Part 0: Data Sources"></a>Part 0: Data Sources</h2><p>This assignment will use two tagged data sets collected from the <strong>Wall Street Journal (WSJ)</strong>. </p>
<p><a href="http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html" target="_blank" rel="noopener">Here</a> is an example ‘tag-set’ or Part of Speech designation describing the two or three letter tag and their meaning. </p>
<ul>
<li>One data set (<strong>WSJ-2_21.pos</strong>) will be used for <strong>training</strong>.</li>
<li>The other (<strong>WSJ-24.pos</strong>) for <strong>testing</strong>. </li>
<li>The tagged training data has been preprocessed to form a vocabulary (<strong>hmm_vocab.txt</strong>). </li>
<li>The words in the vocabulary are words from the training set that were used two or more times. </li>
<li>The vocabulary is augmented with a set of ‘unknown word tokens’, described below. </li>
</ul>
<p>The training set will be used to create the emission, transmission and tag counts. </p>
<p>The test set (WSJ-24.pos) is read in to create <code>y</code>. </p>
<ul>
<li>This contains both the test text and the true tag. </li>
<li>The test set has also been preprocessed to remove the tags to form <strong>test_words.txt</strong>. </li>
<li>This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in <strong>utils_pos.py</strong>. </li>
<li>This forms the list <code>prep</code>, the preprocessed text used to test our  POS taggers.</li>
</ul>
<p>A POS tagger will necessarily encounter words that are not in its datasets. </p>
<ul>
<li>To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. </li>
<li>For example, the suffix ‘ize’ is a hint that the word is a verb, as in ‘final-ize’ or ‘character-ize’. </li>
<li>A set of unknown-tokens, such as ‘—unk-verb—‘ or ‘—unk-noun—‘ will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.</li>
</ul>
<p><img src="DataSources1.PNG"></p>
<p>Implementation note: </p>
<ul>
<li>For python 3.6 and beyond, dictionaries retain the insertion order. </li>
<li>Furthermore, their hash-based lookup makes them suitable for rapid membership tests. <ul>
<li>If _di_ is a dictionary, <code>key in di</code> will return <code>True</code> if _di_ has a key _key_, else <code>False</code>. </li>
</ul>
</li>
</ul>
<p>The dictionary <code>vocab</code> will utilize these features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the training corpus</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"WSJ_02-21.pos"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    training_corpus = f.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"A few items of the training corpus list"</span>)</span><br><span class="line">print(training_corpus[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>A few items of the training corpus list
[&#39;In\tIN\n&#39;, &#39;an\tDT\n&#39;, &#39;Oct.\tNNP\n&#39;, &#39;19\tCD\n&#39;, &#39;review\tNN\n&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read the vocabulary data, split by each line of text, and save the list</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"hmm_vocab.txt"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    voc_l = f.read().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"A few items of the vocabulary list"</span>)</span><br><span class="line">print(voc_l[<span class="number">0</span>:<span class="number">50</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"A few items at the end of the vocabulary list"</span>)</span><br><span class="line">print(voc_l[<span class="number">-50</span>:])</span><br></pre></td></tr></table></figure>
<pre><code>A few items of the vocabulary list
[&#39;!&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;, &#39;&amp;&#39;, &quot;&#39;&quot;, &quot;&#39;&#39;&quot;, &quot;&#39;40s&quot;, &quot;&#39;60s&quot;, &quot;&#39;70s&quot;, &quot;&#39;80s&quot;, &quot;&#39;86&quot;, &quot;&#39;90s&quot;, &quot;&#39;N&quot;, &quot;&#39;S&quot;, &quot;&#39;d&quot;, &quot;&#39;em&quot;, &quot;&#39;ll&quot;, &quot;&#39;m&quot;, &quot;&#39;n&#39;&quot;, &quot;&#39;re&quot;, &quot;&#39;s&quot;, &quot;&#39;til&quot;, &quot;&#39;ve&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;-&#39;, &#39;--&#39;, &#39;--n--&#39;, &#39;--unk--&#39;, &#39;--unk_adj--&#39;, &#39;--unk_adv--&#39;, &#39;--unk_digit--&#39;, &#39;--unk_noun--&#39;, &#39;--unk_punct--&#39;, &#39;--unk_upper--&#39;, &#39;--unk_verb--&#39;, &#39;.&#39;, &#39;...&#39;, &#39;0.01&#39;, &#39;0.0108&#39;, &#39;0.02&#39;, &#39;0.03&#39;, &#39;0.05&#39;, &#39;0.1&#39;, &#39;0.10&#39;, &#39;0.12&#39;, &#39;0.13&#39;, &#39;0.15&#39;]

A few items at the end of the vocabulary list
[&#39;yards&#39;, &#39;yardstick&#39;, &#39;year&#39;, &#39;year-ago&#39;, &#39;year-before&#39;, &#39;year-earlier&#39;, &#39;year-end&#39;, &#39;year-on-year&#39;, &#39;year-round&#39;, &#39;year-to-date&#39;, &#39;year-to-year&#39;, &#39;yearlong&#39;, &#39;yearly&#39;, &#39;years&#39;, &#39;yeast&#39;, &#39;yelled&#39;, &#39;yelling&#39;, &#39;yellow&#39;, &#39;yen&#39;, &#39;yes&#39;, &#39;yesterday&#39;, &#39;yet&#39;, &#39;yield&#39;, &#39;yielded&#39;, &#39;yielding&#39;, &#39;yields&#39;, &#39;you&#39;, &#39;young&#39;, &#39;younger&#39;, &#39;youngest&#39;, &#39;youngsters&#39;, &#39;your&#39;, &#39;yourself&#39;, &#39;youth&#39;, &#39;youthful&#39;, &#39;yuppie&#39;, &#39;yuppies&#39;, &#39;zero&#39;, &#39;zero-coupon&#39;, &#39;zeroing&#39;, &#39;zeros&#39;, &#39;zinc&#39;, &#39;zip&#39;, &#39;zombie&#39;, &#39;zone&#39;, &#39;zones&#39;, &#39;zoning&#39;, &#39;{&#39;, &#39;}&#39;, &#39;&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocab: dictionary that has the index of the corresponding words</span></span><br><span class="line">vocab = &#123;&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the index of the corresponding words. </span></span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sorted(voc_l)): </span><br><span class="line">    vocab[word] = i       </span><br><span class="line">    </span><br><span class="line">print(<span class="string">"Vocabulary dictionary, key is the word, value is a unique integer"</span>)</span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> vocab.items():</span><br><span class="line">    print(<span class="string">f"<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;v&#125;</span>"</span>)</span><br><span class="line">    cnt += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> cnt &gt; <span class="number">20</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>Vocabulary dictionary, key is the word, value is a unique integer
:0
!:1
#:2
$:3
%:4
&amp;:5
&#39;:6
&#39;&#39;:7
&#39;40s:8
&#39;60s:9
&#39;70s:10
&#39;80s:11
&#39;86:12
&#39;90s:13
&#39;N:14
&#39;S:15
&#39;d:16
&#39;em:17
&#39;ll:18
&#39;m:19
&#39;n&#39;:20
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the test corpus</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"WSJ_24.pos"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    y = f.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"A sample of the test corpus"</span>)</span><br><span class="line">print(y[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<pre><code>A sample of the test corpus
[&#39;The\tDT\n&#39;, &#39;economy\tNN\n&#39;, &quot;&#39;s\tPOS\n&quot;, &#39;temperature\tNN\n&#39;, &#39;will\tMD\n&#39;, &#39;be\tVB\n&#39;, &#39;taken\tVBN\n&#39;, &#39;from\tIN\n&#39;, &#39;several\tJJ\n&#39;, &#39;vantage\tNN\n&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#corpus without tags, preprocessed</span></span><br><span class="line">_, prep = preprocess(vocab, <span class="string">"test.words"</span>)     </span><br><span class="line"></span><br><span class="line">print(<span class="string">'The length of the preprocessed test corpus: '</span>, len(prep))</span><br><span class="line">print(<span class="string">'This is a sample of the test_corpus: '</span>)</span><br><span class="line">print(prep[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<pre><code>The length of the preprocessed test corpus:  34199
This is a sample of the test_corpus: 
[&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;, &#39;from&#39;, &#39;several&#39;, &#39;--unk--&#39;]
</code></pre><p><a name="1"></a></p>
<h1 id="Part-1-Parts-of-speech-tagging"><a href="#Part-1-Parts-of-speech-tagging" class="headerlink" title="Part 1: Parts-of-speech tagging"></a>Part 1: Parts-of-speech tagging</h1><p><a name="1.1"></a></p>
<h2 id="Part-1-1-Training"><a href="#Part-1-1-Training" class="headerlink" title="Part 1.1 - Training"></a>Part 1.1 - Training</h2><p>You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. </p>
<p>In this section, you will find the words that are not ambiguous. </p>
<ul>
<li>For example, the word <code>is</code> is a verb and it is not ambiguous. </li>
<li>In the <code>WSJ</code> corpus, $86$% of the token are unambiguous (meaning they have only one tag) </li>
<li>About $14\%$ are ambiguous (meaning that they have more than one tag)</li>
</ul>
<p><img src="pos.png" style="width:400px;height:250px;"></p>
<p>Before you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables. </p>
<h4 id="Transition-counts"><a href="#Transition-counts" class="headerlink" title="Transition counts"></a>Transition counts</h4><ul>
<li>The first dictionary is the <code>transition_counts</code> dictionary which computes the number of times each tag happened next to another tag. </li>
</ul>
<p>This dictionary will be used to compute: </p>
<script type="math/tex; mode=display">P(t_i |t_{i-1}) \tag{1}</script><p>This is the probability of a tag at position $i$ given the tag at position $i-1$.</p>
<p>In order for you to compute equation 1, you will create a <code>transition_counts</code> dictionary where </p>
<ul>
<li>The keys are <code>(prev_tag, tag)</code></li>
<li>The values are the number of times those two tags appeared in that order. </li>
</ul>
<h4 id="Emission-counts"><a href="#Emission-counts" class="headerlink" title="Emission counts"></a>Emission counts</h4><p>The second dictionary you will compute is the <code>emission_counts</code> dictionary. This dictionary will be used to compute:</p>
<script type="math/tex; mode=display">P(w_i|t_i)\tag{2}</script><p>In other words, you will use it to compute the probability of a word given its tag. </p>
<p>In order for you to compute equation 2, you will create an <code>emission_counts</code> dictionary where </p>
<ul>
<li>The keys are <code>(tag, word)</code> </li>
<li>The values are the number of times that pair showed up in your training set. </li>
</ul>
<h4 id="Tag-counts"><a href="#Tag-counts" class="headerlink" title="Tag counts"></a>Tag counts</h4><p>The last dictionary you will compute is the <code>tag_counts</code> dictionary. </p>
<ul>
<li>The key is the tag </li>
<li>The value is the number of times each tag appeared.</li>
</ul>
<p><a name="ex-01"></a></p>
<h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Write a program that takes in the <code>training_corpus</code> and returns the three dictionaries mentioned above <code>transition_counts</code>, <code>emission_counts</code>, and <code>tag_counts</code>. </p>
<ul>
<li><code>emission_counts</code>: maps (tag, word) to the number of times it happened. </li>
<li><code>transition_counts</code>: maps (prev_tag, tag) to the number of times it has appeared. </li>
<li><code>tag_counts</code>: maps (tag) to the number of times it has occured. </li>
</ul>
<p>Implementation note: This routine utilises <em>defaultdict</em>, which is a subclass of <em>dict</em>. </p>
<ul>
<li>A standard Python dictionary throws a <em>KeyError</em> if you try to access an item with a key that is not currently in the dictionary. </li>
<li>In contrast, the <em>defaultdict</em> will create an item of the type of the argument, in this case an integer with the default value of 0. </li>
<li>See <a href="https://docs.python.org/3.3/library/collections.html#defaultdict-objects" target="_blank" rel="noopener">defaultdict</a>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_dictionaries</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dictionaries</span><span class="params">(training_corpus, vocab)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        training_corpus: a corpus where each line has a word followed by its tag.</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary where the keys are the tags and the values are the counts</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the dictionaries using defaultdict</span></span><br><span class="line">    emission_counts = defaultdict(int)</span><br><span class="line">    transition_counts = defaultdict(int)</span><br><span class="line">    tag_counts = defaultdict(int)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "prev_tag" (previous tag) with the start state, denoted by '--s--'</span></span><br><span class="line">    prev_tag = <span class="string">'--s--'</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use 'i' to track the line number in the corpus</span></span><br><span class="line">    i = <span class="number">0</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Each item in the training corpus contains a word and its POS tag</span></span><br><span class="line">    <span class="comment"># Go through each word and its tag in the training corpus</span></span><br><span class="line">    <span class="keyword">for</span> word_tag <span class="keyword">in</span> training_corpus:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the word_tag count</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Every 50,000 words, print the word count</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"word count = <span class="subst">&#123;i&#125;</span>"</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># get the word and tag using the get_word_tag helper function (imported from utils_pos.py)</span></span><br><span class="line">        word, tag = get_word_tag(word_tag,vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the transition count for the previous word and tag</span></span><br><span class="line">        transition_counts[(prev_tag, tag)] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the emission count for the tag and word</span></span><br><span class="line">        emission_counts[(tag, word)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Increment the tag count</span></span><br><span class="line">        tag_counts[tag] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set the previous tag to this tag (for the next iteration of the loop)</span></span><br><span class="line">        prev_tag = tag</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> emission_counts, transition_counts, tag_counts</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)</span><br></pre></td></tr></table></figure>
<pre><code>word count = 50000
word count = 100000
word count = 150000
word count = 200000
word count = 250000
word count = 300000
word count = 350000
word count = 400000
word count = 450000
word count = 500000
word count = 550000
word count = 600000
word count = 650000
word count = 700000
word count = 750000
word count = 800000
word count = 850000
word count = 900000
word count = 950000
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get all the POS states</span></span><br><span class="line">states = sorted(tag_counts.keys())</span><br><span class="line">print(<span class="string">f"Number of POS tags (number of 'states'): <span class="subst">&#123;len(states)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">"View these POS tags (states)"</span>)</span><br><span class="line">print(states)</span><br></pre></td></tr></table></figure>
<pre><code>Number of POS tags (number of &#39;states&#39;): 46
View these POS tags (states)
[&#39;#&#39;, &#39;$&#39;, &quot;&#39;&#39;&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;--s--&#39;, &#39;.&#39;, &#39;:&#39;, &#39;CC&#39;, &#39;CD&#39;, &#39;DT&#39;, &#39;EX&#39;, &#39;FW&#39;, &#39;IN&#39;, &#39;JJ&#39;, &#39;JJR&#39;, &#39;JJS&#39;, &#39;LS&#39;, &#39;MD&#39;, &#39;NN&#39;, &#39;NNP&#39;, &#39;NNPS&#39;, &#39;NNS&#39;, &#39;PDT&#39;, &#39;POS&#39;, &#39;PRP&#39;, &#39;PRP$&#39;, &#39;RB&#39;, &#39;RBR&#39;, &#39;RBS&#39;, &#39;RP&#39;, &#39;SYM&#39;, &#39;TO&#39;, &#39;UH&#39;, &#39;VB&#39;, &#39;VBD&#39;, &#39;VBG&#39;, &#39;VBN&#39;, &#39;VBP&#39;, &#39;VBZ&#39;, &#39;WDT&#39;, &#39;WP&#39;, &#39;WP$&#39;, &#39;WRB&#39;, &#39;``&#39;]
</code></pre><h5 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Number of POS tags (number of 'states'46</span><br><span class="line">View these states</span><br><span class="line">['#', '$', "''", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']</span><br></pre></td></tr></table></figure>
<p>The ‘states’ are the Parts-of-speech designations found in the training data. They will also be referred to as ‘tags’ or POS in this assignment. </p>
<ul>
<li>“NN” is noun, singular, </li>
<li>‘NNS’ is noun, plural. </li>
<li>In addition, there are helpful tags like ‘—s—‘ which indicate a start of a sentence.</li>
<li>You can get a more complete description at <a href="https://www.clips.uantwerpen.be/pages/mbsp-tags" target="_blank" rel="noopener">Penn Treebank II tag set</a>. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"transition examples: "</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> list(transition_counts.items())[:<span class="number">3</span>]:</span><br><span class="line">    print(ex)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"emission examples: "</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> list(emission_counts.items())[<span class="number">200</span>:<span class="number">203</span>]:</span><br><span class="line">    <span class="keyword">print</span> (ex)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"ambiguous word example: "</span>)</span><br><span class="line"><span class="keyword">for</span> tup,cnt <span class="keyword">in</span> emission_counts.items():</span><br><span class="line">    <span class="keyword">if</span> tup[<span class="number">1</span>] == <span class="string">'back'</span>: <span class="keyword">print</span> (tup, cnt)</span><br></pre></td></tr></table></figure>
<pre><code>transition examples: 
((&#39;--s--&#39;, &#39;IN&#39;), 5050)
((&#39;IN&#39;, &#39;DT&#39;), 32364)
((&#39;DT&#39;, &#39;NNP&#39;), 9044)

emission examples: 
((&#39;DT&#39;, &#39;any&#39;), 721)
((&#39;NN&#39;, &#39;decrease&#39;), 7)
((&#39;NN&#39;, &#39;insider-trading&#39;), 5)

ambiguous word example: 
(&#39;RB&#39;, &#39;back&#39;) 304
(&#39;VB&#39;, &#39;back&#39;) 20
(&#39;RP&#39;, &#39;back&#39;) 84
(&#39;JJ&#39;, &#39;back&#39;) 25
(&#39;NN&#39;, &#39;back&#39;) 29
(&#39;VBP&#39;, &#39;back&#39;) 4
</code></pre><h5 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">transition examples: </span><br><span class="line">(('--s--', 'IN'), 5050)</span><br><span class="line">(('IN', 'DT'), 32364)</span><br><span class="line">(('DT', 'NNP'), 9044)</span><br><span class="line"></span><br><span class="line">emission examples: </span><br><span class="line">(('DT', 'any'), 721)</span><br><span class="line">(('NN', 'decrease'), 7)</span><br><span class="line">(('NN', 'insider-trading'), 5)</span><br><span class="line"></span><br><span class="line">ambiguous word example: </span><br><span class="line">('RB', 'back') 304</span><br><span class="line">('VB', 'back') 20</span><br><span class="line">('RP', 'back') 84</span><br><span class="line">('JJ', 'back') 25</span><br><span class="line">('NN', 'back') 29</span><br><span class="line">('VBP', 'back') 4</span><br></pre></td></tr></table></figure>
<p><a name="1.2"></a></p>
<h3 id="Part-1-2-Testing"><a href="#Part-1-2-Testing" class="headerlink" title="Part 1.2 - Testing"></a>Part 1.2 - Testing</h3><p>Now you will test the accuracy of your parts-of-speech tagger using your <code>emission_counts</code> dictionary. </p>
<ul>
<li>Given your preprocessed test corpus <code>prep</code>, you will assign a parts-of-speech tag to every word in that corpus. </li>
<li>Using the original tagged test corpus <code>y</code>, you will then compute what percent of the tags you got correct. </li>
</ul>
<p><a name="ex-02"></a></p>
<h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement <code>predict_pos</code> that computes the accuracy of your model. </p>
<ul>
<li>This is a warm up exercise. </li>
<li>To assign a part of speech to a word, assign the most frequent POS for that word in the training set. </li>
<li>Then evaluate how well this approach works.  Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same.  If so, the prediction was correct!</li>
<li>Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: predict_pos</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_pos</span><span class="params">(prep, y, emission_counts, vocab, states)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.</span></span><br><span class="line"><span class="string">        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">        states: a sorted list of all possible tags for this assignment</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: Number of times you classified a word correctly</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the number of correct predictions to zero</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the (tag, word) tuples, stored as a set</span></span><br><span class="line">    all_words = set(emission_counts.keys())  <span class="comment"># (tag, word)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of (word, POS) tuples in the corpus 'y'</span></span><br><span class="line">    total = len(y)</span><br><span class="line">    <span class="keyword">for</span> word, y_tup <span class="keyword">in</span> zip(prep, y): </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the (word, POS) string into a list of two items</span></span><br><span class="line">        y_tup_l = y_tup.split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Verify that y_tup contain both word and POS</span></span><br><span class="line">        <span class="keyword">if</span> len(y_tup_l) == <span class="number">2</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the true POS label for this word</span></span><br><span class="line">            true_label = y_tup_l[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># If the y_tup didn't contain word and POS, go to next word</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">        count_final = <span class="number">0</span></span><br><span class="line">        pos_final = <span class="string">''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the word is in the vocabulary...</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> states:</span><br><span class="line"></span><br><span class="line">            <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># define the key as the tuple containing the POS and word</span></span><br><span class="line">                key = (pos, word)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if the (pos, word) key exists in the emission_counts dictionary</span></span><br><span class="line">                <span class="keyword">if</span> key <span class="keyword">in</span> emission_counts: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># get the emission count of the (pos,word) tuple </span></span><br><span class="line">                    count = emission_counts[key]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># keep track of the POS with the largest count</span></span><br><span class="line">                    <span class="keyword">if</span> count &gt; count_final: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final count (largest count)</span></span><br><span class="line">                        count_final = count</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final POS</span></span><br><span class="line">                        pos_final = pos</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the final POS (with the largest count) matches the true POS:</span></span><br><span class="line">            <span class="keyword">if</span> pos_final == true_label: <span class="comment"># complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Update the number of correct predictions</span></span><br><span class="line">                num_correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    accuracy = num_correct / total</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)</span><br><span class="line">print(<span class="string">f"Accuracy of prediction using predict_pos is <span class="subst">&#123;accuracy_predict_pos:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of prediction using predict_pos is 0.8889
</code></pre><h5 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of prediction <span class="keyword">using</span> predict_pos is <span class="number">0.8889</span></span><br></pre></td></tr></table></figure>
<p>88.9% is really good for this warm up exercise. With hidden markov models, you should be able to get <strong>95% accuracy.</strong></p>
<p><a name="2"></a></p>
<h1 id="Part-2-Hidden-Markov-Models-for-POS"><a href="#Part-2-Hidden-Markov-Models-for-POS" class="headerlink" title="Part 2: Hidden Markov Models for POS"></a>Part 2: Hidden Markov Models for POS</h1><p>Now you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder</p>
<ul>
<li>The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization. </li>
<li>In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. </li>
<li>By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in Part 1.</li>
</ul>
<p>The Markov Model contains a number of states and the probability of transition between those states. </p>
<ul>
<li>In this case, the states are the parts-of-speech. </li>
<li>A Markov Model utilizes a transition matrix, <code>A</code>. </li>
<li>A Hidden Markov Model adds an observation or emission matrix <code>B</code> which describes the probability of a visible observation when we are in a particular state. </li>
<li>In this case, the emissions are the words in the corpus</li>
<li>The state, which is hidden, is the POS tag of that word.</li>
</ul>
<p><a name="2.1"></a></p>
<h2 id="Part-2-1-Generating-Matrices"><a href="#Part-2-1-Generating-Matrices" class="headerlink" title="Part 2.1 Generating Matrices"></a>Part 2.1 Generating Matrices</h2><h3 id="Creating-the-‘A’-transition-probabilities-matrix"><a href="#Creating-the-‘A’-transition-probabilities-matrix" class="headerlink" title="Creating the ‘A’ transition probabilities matrix"></a>Creating the ‘A’ transition probabilities matrix</h3><p>Now that you have your <code>emission_counts</code>, <code>transition_counts</code>, and <code>tag_counts</code>, you will start implementing the Hidden Markov Model. </p>
<p>This will allow you to quickly construct the </p>
<ul>
<li><code>A</code> transition probabilities matrix.</li>
<li>and the <code>B</code> emission probabilities matrix. </li>
</ul>
<p>You will also use some smoothing when computing these matrices. </p>
<p>Here is an example of what the <code>A</code> transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>A</strong></th>
<th>…</th>
<th>RBS</th>
<th>RP</th>
<th>SYM</th>
<th>TO</th>
<th>UH</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RBS</strong></td>
<td>…</td>
<td>2.217069e-06</td>
<td>2.217069e-06</td>
<td>2.217069e-06</td>
<td>0.008870</td>
<td>2.217069e-06</td>
<td>…</td>
</tr>
<tr>
<td><strong>RP</strong></td>
<td>…</td>
<td>3.756509e-07</td>
<td>7.516775e-04</td>
<td>3.756509e-07</td>
<td>0.051089</td>
<td>3.756509e-07</td>
<td>…</td>
</tr>
<tr>
<td><strong>SYM</strong></td>
<td>…</td>
<td>1.722772e-05</td>
<td>1.722772e-05</td>
<td>1.722772e-05</td>
<td>0.000017</td>
<td>1.722772e-05</td>
<td>…</td>
</tr>
<tr>
<td><strong>TO</strong></td>
<td>…</td>
<td>4.477336e-05</td>
<td>4.472863e-08</td>
<td>4.472863e-08</td>
<td>0.000090</td>
<td>4.477336e-05</td>
<td>…</td>
</tr>
<tr>
<td><strong>UH</strong></td>
<td>…</td>
<td>1.030439e-05</td>
<td>1.030439e-05</td>
<td>1.030439e-05</td>
<td>0.061837</td>
<td>3.092348e-02</td>
<td>…</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
<p>Note that the matrix above was computed with smoothing. </p>
<p>Each cell gives you the probability to go from one part of speech to another. </p>
<ul>
<li>In other words, there is a 4.47e-8 chance of going from parts-of-speech <code>TO</code> to <code>RP</code>. </li>
<li>The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.</li>
</ul>
<p>The smoothing was done as follows: </p>
<script type="math/tex; mode=display">P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_{i}) + \alpha }{C(t_{i-1}) +\alpha * N}\tag{3}</script><ul>
<li>$N$ is the total number of tags</li>
<li>$C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in <code>transition_counts</code> dictionary.</li>
<li>$C(t_{i-1})$ is the count of the previous POS in the <code>tag_counts</code> dictionary.</li>
<li>$\alpha$ is a smoothing parameter.</li>
</ul>
<p><a name="ex-03"></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the <code>create_transition_matrix</code> below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix <code>A</code>. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_transition_matrix</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_transition_matrix</span><span class="params">(alpha, tag_counts, transition_counts)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: number used for smoothing</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        transition_counts: transition count for the previous word and tag</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        A: matrix of dimension (num_tags,num_tags)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get a sorted list of unique POS tags</span></span><br><span class="line">    all_tags = sorted(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Count the number of unique POS tags</span></span><br><span class="line">    num_tags = len(all_tags)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the transition matrix 'A'</span></span><br><span class="line">    A = np.zeros((num_tags,num_tags))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the unique transition tuples (previous POS, current POS)</span></span><br><span class="line">    trans_keys = set(transition_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Return instances of 'None' with your code) ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row of the transition matrix A</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column of the transition matrix A</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_tags):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the count of the (prev POS, current POS) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># Define the tuple (prev POS, current POS)</span></span><br><span class="line">            <span class="comment"># Get the tag at position i and tag at position j (from the all_tags list)</span></span><br><span class="line">            key = (all_tags[i],all_tags[j])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check if the (prev POS, current POS) tuple </span></span><br><span class="line">            <span class="comment"># exists in the transition counts dictionaory</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> trans_keys: <span class="comment">#complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get count from the transition_counts dictionary </span></span><br><span class="line">                <span class="comment"># for the (prev POS, current POS) tuple</span></span><br><span class="line">                count = transition_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the previous tag (index position i) from tag_counts</span></span><br><span class="line">            count_prev_tag = tag_counts[all_tags[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Apply smoothing using count of the tuple, alpha, </span></span><br><span class="line">            <span class="comment"># count of previous tag, alpha, and number of total tags</span></span><br><span class="line">            A[i,j] = (count + alpha ) / ( count_prev_tag + alpha * num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line">A = create_transition_matrix(alpha, tag_counts, transition_counts)</span><br><span class="line"><span class="comment"># Testing your function</span></span><br><span class="line">print(<span class="string">f"A at row 0, col 0: <span class="subst">&#123;A[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"A at row 3, col 1: <span class="subst">&#123;A[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"View a subset of transition matrix A"</span>)</span><br><span class="line">A_sub = pd.DataFrame(A[<span class="number">30</span>:<span class="number">35</span>,<span class="number">30</span>:<span class="number">35</span>], index=states[<span class="number">30</span>:<span class="number">35</span>], columns = states[<span class="number">30</span>:<span class="number">35</span>] )</span><br><span class="line">print(A_sub)</span><br></pre></td></tr></table></figure>
<pre><code>A at row 0, col 0: 0.000007040
A at row 3, col 1: 0.1691
View a subset of transition matrix A
              RBS            RP           SYM        TO            UH
RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06
RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07
SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05
TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05
UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02
</code></pre><h5 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A at row <span class="number">0</span>, col <span class="number">0</span>: <span class="number">0.000007040</span></span><br><span class="line">A at row <span class="number">3</span>, col <span class="number">1</span>: <span class="number">0.1691</span></span><br><span class="line">View a subset of transition matrix A</span><br><span class="line">              RBS            RP           SYM        TO            UH</span><br><span class="line">RBS  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">0.008870</span>  <span class="number">2.217069e-06</span></span><br><span class="line">RP   <span class="number">3.756509e-07</span>  <span class="number">7.516775e-04</span>  <span class="number">3.756509e-07</span>  <span class="number">0.051089</span>  <span class="number">3.756509e-07</span></span><br><span class="line">SYM  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">0.000017</span>  <span class="number">1.722772e-05</span></span><br><span class="line">TO   <span class="number">4.477336e-05</span>  <span class="number">4.472863e-08</span>  <span class="number">4.472863e-08</span>  <span class="number">0.000090</span>  <span class="number">4.477336e-05</span></span><br><span class="line">UH   <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">0.061837</span>  <span class="number">3.092348e-02</span></span><br></pre></td></tr></table></figure>
<h3 id="Create-the-‘B’-emission-probabilities-matrix"><a href="#Create-the-‘B’-emission-probabilities-matrix" class="headerlink" title="Create the ‘B’ emission probabilities matrix"></a>Create the ‘B’ emission probabilities matrix</h3><p>Now you will create the <code>B</code> transition matrix which computes the emission probability. </p>
<p>You will use smoothing as defined below: </p>
<script type="math/tex; mode=display">P(w_i | t_i) = \frac{C(t_i, word_i)+ \alpha}{C(t_{i}) +\alpha * N}\tag{4}</script><ul>
<li>$C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in <code>emission_counts</code> dictionary).</li>
<li>$C(t_i)$ is the number of times $tag_i$ was in the training data (stored in <code>tag_counts</code> dictionary).</li>
<li>$N$ is the number of words in the vocabulary</li>
<li>$\alpha$ is a smoothing parameter. </li>
</ul>
<p>The matrix <code>B</code> is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. </p>
<p>Here is an example of the matrix, only a subset of tags and words are shown: </p>
<p style="text-align: center;"> <b>B Emissions Probability Matrix (subset)</b>  </p>

<div class="table-container">
<table>
<thead>
<tr>
<th><strong>B</strong></th>
<th>…</th>
<th>725</th>
<th>adroitly</th>
<th>engineers</th>
<th>promoted</th>
<th>synergy</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CD</strong></td>
<td>…</td>
<td><strong>8.201296e-05</strong></td>
<td>2.732854e-08</td>
<td>2.732854e-08</td>
<td>2.732854e-08</td>
<td>2.732854e-08</td>
<td>…</td>
</tr>
<tr>
<td><strong>NN</strong></td>
<td>…</td>
<td>7.521128e-09</td>
<td>7.521128e-09</td>
<td>7.521128e-09</td>
<td>7.521128e-09</td>
<td><strong>2.257091e-05</strong></td>
<td>…</td>
</tr>
<tr>
<td><strong>NNS</strong></td>
<td>…</td>
<td>1.670013e-08</td>
<td>1.670013e-08</td>
<td><strong>4.676203e-04</strong></td>
<td>1.670013e-08</td>
<td>1.670013e-08</td>
<td>…</td>
</tr>
<tr>
<td><strong>VB</strong></td>
<td>…</td>
<td>3.779036e-08</td>
<td>3.779036e-08</td>
<td>3.779036e-08</td>
<td>3.779036e-08</td>
<td>3.779036e-08</td>
<td>…</td>
</tr>
<tr>
<td><strong>RB</strong></td>
<td>…</td>
<td>3.226454e-08</td>
<td><strong>6.456135e-05</strong></td>
<td>3.226454e-08</td>
<td>3.226454e-08</td>
<td>3.226454e-08</td>
<td>…</td>
</tr>
<tr>
<td><strong>RP</strong></td>
<td>…</td>
<td>3.723317e-07</td>
<td>3.723317e-07</td>
<td>3.723317e-07</td>
<td><strong>3.723317e-07</strong></td>
<td>3.723317e-07</td>
<td>…</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
<p><a name="ex-04"></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement the <code>create_emission_matrix</code> below that computes the <code>B</code> emission probabilities matrix. Your function takes in $\alpha$, the smoothing parameter, <code>tag_counts</code>, which is a dictionary mapping each tag to its respective count, the <code>emission_counts</code> dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix <code>B</code>. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_emission_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_emission_matrix</span><span class="params">(alpha, tag_counts, emission_counts, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: tuning parameter used in smoothing </span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        B: a matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get the number of POS tag</span></span><br><span class="line">    num_tags = len(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a list of all POS tags</span></span><br><span class="line">    all_tags = sorted(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the total number of unique words in the vocabulary</span></span><br><span class="line">    num_words = len(vocab)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the emission matrix B with places for</span></span><br><span class="line">    <span class="comment"># tags in the rows and words in the columns</span></span><br><span class="line">    B = np.zeros((num_tags, num_words))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a set of all (POS, word) tuples </span></span><br><span class="line">    <span class="comment"># from the keys of the emission_counts dictionary</span></span><br><span class="line">    emis_keys = set(list(emission_counts.keys()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row (POS tags)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column (words)</span></span><br><span class="line">        <span class="keyword">for</span> j,word <span class="keyword">in</span> enumerate(vocab): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the emission count for the (POS tag, word) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># Define the (POS tag, word) tuple for this row and column</span></span><br><span class="line">            key =  (all_tags[i], word)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the (POS tag, word) tuple exists as a key in emission counts</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> emis_keys: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">                <span class="comment"># Get the count of (POS tag, word) from the emission_counts d</span></span><br><span class="line">                count = emission_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the POS tag</span></span><br><span class="line">            count_tag = tag_counts[all_tags[i]]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Apply smoothing and store the smoothed value </span></span><br><span class="line">            <span class="comment"># into the emission matrix B for this row and column</span></span><br><span class="line">            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> B</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># creating your emission probability matrix. this takes a few minutes to run. </span></span><br><span class="line">B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"View Matrix position at row 0, column 0: <span class="subst">&#123;B[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"View Matrix position at row 3, column 1: <span class="subst">&#123;B[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Try viewing emissions for a few words in a sample dataframe</span></span><br><span class="line">cidx  = [<span class="string">'725'</span>,<span class="string">'adroitly'</span>,<span class="string">'engineers'</span>, <span class="string">'promoted'</span>, <span class="string">'synergy'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the integer ID for each word</span></span><br><span class="line">cols = [vocab[a] <span class="keyword">for</span> a <span class="keyword">in</span> cidx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose POS tags to show in a sample dataframe</span></span><br><span class="line">rvals =[<span class="string">'CD'</span>,<span class="string">'NN'</span>,<span class="string">'NNS'</span>, <span class="string">'VB'</span>,<span class="string">'RB'</span>,<span class="string">'RP'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># For each POS tag, get the row number from the 'states' list</span></span><br><span class="line">rows = [states.index(a) <span class="keyword">for</span> a <span class="keyword">in</span> rvals]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the emissions for the sample of words, and the sample of POS tags</span></span><br><span class="line">B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )</span><br><span class="line">print(B_sub)</span><br></pre></td></tr></table></figure>
<pre><code>View Matrix position at row 0, column 0: 0.000006032
View Matrix position at row 3, column 1: 0.000000720
              725      adroitly     engineers      promoted       synergy
CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08
NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05
NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08
VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08
RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08
RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07
</code></pre><h5 id="Expected-Output-4"><a href="#Expected-Output-4" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">View Matrix position at row <span class="number">0</span>, column <span class="number">0</span>: <span class="number">0.000006032</span></span><br><span class="line">View Matrix position at row <span class="number">3</span>, column <span class="number">1</span>: <span class="number">0.000000720</span></span><br><span class="line">              <span class="number">725</span>      adroitly     engineers      promoted       synergy</span><br><span class="line">CD   <span class="number">8.201296e-05</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span></span><br><span class="line">NN   <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">2.257091e-05</span></span><br><span class="line">NNS  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span>  <span class="number">4.676203e-04</span>  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span></span><br><span class="line">VB   <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span></span><br><span class="line">RB   <span class="number">3.226454e-08</span>  <span class="number">6.456135e-05</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span></span><br><span class="line">RP   <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span></span><br></pre></td></tr></table></figure>
<p><a name="3"></a></p>
<h1 id="Part-3-Viterbi-Algorithm-and-Dynamic-Programming"><a href="#Part-3-Viterbi-Algorithm-and-Dynamic-Programming" class="headerlink" title="Part 3: Viterbi Algorithm and Dynamic Programming"></a>Part 3: Viterbi Algorithm and Dynamic Programming</h1><p>In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, <code>A</code> and <code>B</code> to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. </p>
<ul>
<li><strong>Initialization</strong> - In this part you initialize the <code>best_paths</code> and <code>best_probabilities</code> matrices that you will be populating in <code>feed_forward</code>.</li>
<li><strong>Feed forward</strong> - At each step, you calculate the probability of each path happening and the best paths up to that point. </li>
<li><strong>Feed backward</strong>: This allows you to find the best path with the highest probabilities. </li>
</ul>
<p><a name="3.1"></a></p>
<h2 id="Part-3-1-Initialization"><a href="#Part-3-1-Initialization" class="headerlink" title="Part 3.1:  Initialization"></a>Part 3.1:  Initialization</h2><p>You will start by initializing two matrices of the same dimension. </p>
<ul>
<li><p>best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.</p>
</li>
<li><p>best_paths: A matrix that helps you trace through the best possible path in the corpus. </p>
</li>
</ul>
<p><a name="ex-05"></a></p>
<h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions</strong>:<br>Write a program below that initializes the <code>best_probs</code> and the <code>best_paths</code> matrix. </p>
<p>Both matrices will be initialized to zero except for column zero of <code>best_probs</code>.  </p>
<ul>
<li>Column zero of <code>best_probs</code> is initialized with the assumption that the first word of the corpus was preceded by a start token (“—s—“). </li>
<li>This allows you to reference the <strong>A</strong> matrix for the transition probability</li>
</ul>
<p>Here is how to initialize column 0 of <code>best_probs</code>:</p>
<ul>
<li>The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\textrm{best_probs}[s_{idx}, i]$.</li>
<li>This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\mathbf{B}[i, vocab[corpus[0]]]$.</li>
<li>Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). </li>
<li><strong>vocab</strong> is a dictionary that returns the unique integer that refers to that particular word.</li>
</ul>
<p>Conceptually, it looks like this:<br>$\textrm{best_probs}[s_{idx}, i] = \mathbf{A}[s_{idx}, i] \times \mathbf{B}[i, corpus[0] ]$</p>
<p>In order to avoid multiplying and storing small values on the computer, we’ll take the log of the product, which becomes the sum of two logs:</p>
<p>$best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$</p>
<p>Also, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set $best_probs[i,0] = float(‘-inf’)$ when $A[s_{idx}, i] == 0$</p>
<p>So the implementation to initialize $best_probs$ looks like this:</p>
<p>$ if A[s_{idx}, i] &lt;&gt; 0 : best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])$</p>
<p>$ if A[s_{idx}, i] == 0 : best_probs[i,0] = float(‘-inf’)$</p>
<p>Please use <a href="https://docs.python.org/3/library/math.html" target="_blank" rel="noopener">math.log</a> to compute the natural logarithm.</p>
<p>The example below shows the initialization assuming the corpus starts with the phrase “Loss tracks upward”.</p>
<p><img src="Initialize4.PNG"></p>
<p>Represent infinity and negative infinity like this:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">float('inf')</span><br><span class="line">float('-inf')</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: initialize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(states, tag_counts, A, B, corpus, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        states: a list of all possible parts-of-speech</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        A: Transition Matrix of dimension (num_tags, num_tags)</span></span><br><span class="line"><span class="string">        B: Emission Matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">        corpus: a sequence of words whose POS is to be identified in a list </span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        best_probs: matrix of dimension (num_tags, len(corpus)) of floats</span></span><br><span class="line"><span class="string">        best_paths: matrix of dimension (num_tags, len(corpus)) of integers</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the total number of unique POS tags</span></span><br><span class="line">    num_tags = len(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_probs matrix </span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as the columns</span></span><br><span class="line">    best_probs = np.zeros((num_tags, len(corpus)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_paths matrix</span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as columns</span></span><br><span class="line">    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the start token</span></span><br><span class="line">    s_idx = states.index(<span class="string">"--s--"</span>)</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each of the POS tags</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags) : <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Handle the special case when the transition from start token to POS tag i is zero</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] == <span class="number">0</span>: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag 'i', column 0, to negative infinity</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = float(<span class="string">"-inf"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># For all other cases when transition from start token to POS tag i is non-zero:</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag 'i', column 0</span></span><br><span class="line">            <span class="comment"># Check the formula in the instructions above</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = math.log(A[s_idx,i])  +  math.log(B[i,vocab[corpus[<span class="number">0</span>]]])</span><br><span class="line">            </span><br><span class="line">                         </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the function</span></span><br><span class="line">print(<span class="string">f"best_probs[0,0]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>"</span>) </span><br><span class="line">print(<span class="string">f"best_paths[2,3]: <span class="subst">&#123;best_paths[<span class="number">2</span>,<span class="number">3</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>best_probs[0,0]: -22.6098
best_paths[2,3]: 0.0000
</code></pre><h5 id="Expected-Output-5"><a href="#Expected-Output-5" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">0</span>]: <span class="number">-22.6098</span></span><br><span class="line">best_paths[<span class="number">2</span>,<span class="number">3</span>]: <span class="number">0.0000</span></span><br></pre></td></tr></table></figure>
<p><a name="3.2"></a></p>
<h2 id="Part-3-2-Viterbi-Forward"><a href="#Part-3-2-Viterbi-Forward" class="headerlink" title="Part 3.2 Viterbi Forward"></a>Part 3.2 Viterbi Forward</h2><p>In this part of the assignment, you will implement the <code>viterbi_forward</code> segment. In other words, you will populate your <code>best_probs</code> and <code>best_paths</code> matrices.</p>
<ul>
<li>Walk forward through the corpus.</li>
<li>For each word, compute a probability for each possible tag. </li>
<li>Unlike the previous algorithm <code>predict_pos</code> (the ‘warm-up’ exercise), this will include the path up to that (word,tag) combination. </li>
</ul>
<p>Here is an example with a three-word corpus “Loss tracks upward”:</p>
<ul>
<li>Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. </li>
<li>In the diagram below, the first word “Loss” is already initialized. </li>
<li>The algorithm will compute a probability for each of the potential tags in the second and future words. </li>
</ul>
<p>Compute the probability that the tag of the second work (‘tracks’) is a verb, 3rd person singular present (VBZ).  </p>
<ul>
<li>In the <code>best_probs</code> matrix, go to the column of the second word (‘tracks’), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.</li>
<li>Examine each of the paths from the tags of the first word (‘Loss’) and choose the most likely path.  </li>
<li>An example of the calculation for <strong>one</strong> of those paths is the path from (‘Loss’, NN) to (‘tracks’, VBZ).</li>
<li>The log of the probability of the path up to and including the first word ‘Loss’ having POS tag NN is $-14.32$.  The <code>best_probs</code> matrix contains this value -14.32 in the column for ‘Loss’ and row for ‘NN’.</li>
<li>Find the probability that NN transitions to VBZ.  To find this probability, go to the <code>A</code> transition matrix, and go to the row for ‘NN’ and the column for ‘VBZ’.  The value is $4.37e-02$, which is circled in the diagram, so add $-14.32 + log(4.37e-02)$. </li>
<li>Find the log of the probability that the tag VBS would ‘emit’ the word ‘tracks’.  To find this, look at the ‘B’ emission matrix in row ‘VBZ’ and the column for the word ‘tracks’.  The value $4.61e-04$ is circled in the diagram below.  So add $-14.32 + log(4.37e-02) + log(4.61e-04)$.</li>
<li>The sum of $-14.32 + log(4.37e-02) + log(4.61e-04)$ is $-25.13$. Store $-25.13$ in the <code>best_probs</code> matrix at row ‘VBZ’ and column ‘tracks’ (as seen in the cell that is highlighted in light orange in the diagram).</li>
<li>All other paths in best_probs are calculated.  Notice that $-25.13$ is greater than all of the other values in column ‘tracks’ of matrix <code>best_probs</code>, and so the most likely path to ‘VBZ’ is from ‘NN’.  ‘NN’ is in row 20 of the <code>best_probs</code> matrix, so $20$ is the most likely path.</li>
<li>Store the most likely path $20$ in the <code>best_paths</code> table.  This is highlighted in light orange in the diagram below.</li>
</ul>
<p>The formula to compute the probability and path for the $i^{th}$ word in the $corpus$, the prior word $i-1$ in the corpus, current POS tag $j$, and previous POS tag $k$ is:</p>
<p>$\mathrm{prob} = \mathbf{best_prob}_{k, i-1} + \mathrm{log}(\mathbf{A}_{k, j}) + \mathrm{log}(\mathbf{B}_{j, vocab(corpus_{i})})$</p>
<p>where $corpus_{i}$ is the word in the corpus at index $i$, and $vocab$ is the dictionary that gets the unique integer that represents a given word.</p>
<p>$\mathrm{path} = k$</p>
<p>where $k$ is the integer representing the previous POS tag.</p>
<p><a name="ex-06"></a></p>
<h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p>Instructions: Implement the <code>viterbi_forward</code> algorithm and store the best_path and best_prob for every possible tag for each word in the matrices <code>best_probs</code> and <code>best_tags</code> using the pseudo code below.</p>
<p>`for each word in the corpus</p>
<pre><code>for each POS tag type that this word may be

    for POS tag type that the previous word could be

        compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.

        retain the highest probability computed for the current word

        set best_probs to this highest probability

        set best_paths to the index &#39;k&#39;, representing the POS tag of the previous word which produced the highest probability `
</code></pre><p>Please use <a href="https://docs.python.org/3/library/math.html" target="_blank" rel="noopener">math.log</a> to compute the natural logarithm.</p>
<p><img src="Forward4.PNG"></p>
<h2 id><a href="#" class="headerlink" title></a><details></details></h2><p><summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary></p>
<p>
<ul>
    <li>Remember that when accessing emission matrix B, the column index is the unique integer ID associated with the word.  It can be accessed by using the 'vocab' dictionary, where the key is the word, and the value is the unique integer ID for that word.</li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_forward</span><span class="params">(A, B, test_corpus, best_probs, best_paths, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        A, B: The transiton and emission matrices respectively</span></span><br><span class="line"><span class="string">        test_corpus: a list containing a preprocessed corpus</span></span><br><span class="line"><span class="string">        best_probs: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        best_probs: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the number of unique POS tags (which is the num of rows in best_probs)</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through every word in the corpus starting from word 1</span></span><br><span class="line">    <span class="comment"># Recall that word 0 was initialized in `initialize()`</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(test_corpus)): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print number of words processed, every 5000 words</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">5000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Words processed: &#123;:&gt;8&#125;"</span>.format(i))</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###</span></span><br><span class="line">        <span class="comment"># For each unique POS tag that the current word can be</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_prob for word i to negative infinity</span></span><br><span class="line">            best_prob_i = float(<span class="string">"-inf"</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_path for current word i to None</span></span><br><span class="line">            best_path_i = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># For each POS tag that the previous word can be:</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Calculate the probability = </span></span><br><span class="line">                <span class="comment"># best probs of POS tag k, previous word i-1 + </span></span><br><span class="line">                <span class="comment"># log(prob of transition from POS k to POS j) + </span></span><br><span class="line">                <span class="comment"># log(prob that emission of POS j is word i)</span></span><br><span class="line">                prob = best_probs[k, i<span class="number">-1</span>] + math.log(A[k,j]) + math.log(B[j, vocab[test_corpus[i]]])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if this path's probability is greater than</span></span><br><span class="line">                <span class="comment"># the best probability up to and before this point</span></span><br><span class="line">                <span class="keyword">if</span> prob &gt; best_prob_i: <span class="comment"># complete this line</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Keep track of the best probability</span></span><br><span class="line">                    best_prob_i = prob</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># keep track of the POS tag of the previous word</span></span><br><span class="line">                    <span class="comment"># that is part of the best path.  </span></span><br><span class="line">                    <span class="comment"># Save the index (integer) associated with </span></span><br><span class="line">                    <span class="comment"># that previous word's POS tag</span></span><br><span class="line">                    best_path_i = k</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Save the best probability for the </span></span><br><span class="line">            <span class="comment"># given current word's POS tag</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus</span></span><br><span class="line">            best_probs[j,i] = best_prob_i</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the unique integer ID of the previous POS tag</span></span><br><span class="line">            <span class="comment"># into best_paths matrix, for the POS tag of the current word</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus.</span></span><br><span class="line">            best_paths[j,i] = best_path_i</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure>
<p>Run the <code>viterbi_forward</code> function to fill in the <code>best_probs</code> and <code>best_paths</code> matrices.</p>
<p><strong>Note</strong> that this will take a few minutes to run.  There are about 30,000 words to process.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this will take a few minutes to run =&gt; processes ~ 30,000 words</span></span><br><span class="line">best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)</span><br></pre></td></tr></table></figure>
<pre><code>Words processed:     5000
Words processed:    10000
Words processed:    15000
Words processed:    20000
Words processed:    25000
Words processed:    30000
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test this function </span></span><br><span class="line">print(<span class="string">f"best_probs[0,1]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>) </span><br><span class="line">print(<span class="string">f"best_probs[0,4]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">4</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>best_probs[0,1]: -24.7822
best_probs[0,4]: -49.5601
</code></pre><h5 id="Expected-Output-6"><a href="#Expected-Output-6" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">1</span>]: <span class="number">-24.7822</span></span><br><span class="line">best_probs[<span class="number">0</span>,<span class="number">4</span>]: <span class="number">-49.5601</span></span><br></pre></td></tr></table></figure>
<p><a name="3.3"></a></p>
<h2 id="Part-3-3-Viterbi-backward"><a href="#Part-3-3-Viterbi-backward" class="headerlink" title="Part 3.3 Viterbi backward"></a>Part 3.3 Viterbi backward</h2><p>Now you will implement the Viterbi backward algorithm.</p>
<ul>
<li>The Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the <code>best_paths</code> and the <code>best_probs</code> matrices.</li>
</ul>
<p>The example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: “Loss tracks upward”.</p>
<p>POS tag for ‘upward’ is <code>RB</code></p>
<ul>
<li>Select the the most likely POS tag for the last word in the corpus, ‘upward’ in the <code>best_prob</code> table.</li>
<li>Look for the row in the column for ‘upward’ that has the largest probability.</li>
<li>Notice that in row 28 of <code>best_probs</code>, the estimated probability is -34.99, which is larger than the other values in the column.  So the most likely POS tag for ‘upward’ is <code>RB</code> an adverb, at row 28 of <code>best_prob</code>. </li>
<li>The variable <code>z</code> is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus.  In array z, at position 2, store the value 28 to indicate that the word ‘upward’ (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is <code>RB</code>).</li>
<li>The variable <code>pred</code> contains the POS tags in string form.  So <code>pred</code> at index 2 stores the string <code>RB</code>.</li>
</ul>
<p>POS tag for ‘tracks’ is <code>VBZ</code></p>
<ul>
<li>The next step is to go backward one word in the corpus (‘tracks’).  Since the most likely POS tag for ‘upward’ is <code>RB</code>, which is uniquely identified by integer ID 28, go to the <code>best_paths</code> matrix in column 2, row 28.  The value stored in <code>best_paths</code>, column 2, row 28 indicates the unique ID of the POS tag of the previous word.  In this case, the value stored here is 40, which is the unique ID for POS tag <code>VBZ</code> (verb, 3rd person singular present).</li>
<li>So the previous word at index 1 of the corpus (‘tracks’), most likely has the POS tag with unique ID 40, which is <code>VBZ</code>.</li>
<li>In array <code>z</code>, store the value 40 at position 1, and for array <code>pred</code>, store the string <code>VBZ</code> to indicate that the word ‘tracks’ most likely has POS tag <code>VBZ</code>.</li>
</ul>
<p>POS tag for ‘Loss’ is <code>NN</code></p>
<ul>
<li>In <code>best_paths</code> at column 1, the unique ID stored at row 40 is 20.  20 is the unique ID for POS tag <code>NN</code>.</li>
<li>In array <code>z</code> at position 0, store 20.  In array <code>pred</code> at position 0, store <code>NN</code>.</li>
</ul>
<p><img src="Backwards5.PNG"></p>
<p><a name="ex-07"></a></p>
<h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Implement the <code>viterbi_backward</code> algorithm, which returns a list of predicted POS tags for each word in the corpus.</p>
<ul>
<li>Note that the numbering of the index positions starts at 0 and not 1. </li>
<li><code>m</code> is the number of words in the corpus.  <ul>
<li>So the indexing into the corpus goes from <code>0</code> to <code>m - 1</code>.</li>
<li>Also, the columns in <code>best_probs</code> and <code>best_paths</code> are indexed from <code>0</code> to <code>m - 1</code></li>
</ul>
</li>
</ul>
<p><strong>In Step 1:</strong><br>Loop through all the rows (POS tags) in the last entry of <code>best_probs</code> and find the row (POS tag) with the maximum value.<br>Convert the unique integer ID to a tag (a string representation) using the dictionary <code>states</code>.  </p>
<p>Referring to the three-word corpus described above:</p>
<ul>
<li><code>z[2] = 28</code>: For the word ‘upward’ at position 2 in the corpus, the POS tag ID is 28.  Store 28 in <code>z</code> at position 2.</li>
<li>states(28) is ‘RB’: The POS tag ID 28 refers to the POS tag ‘RB’.</li>
<li><code>pred[2] = &#39;RB&#39;</code>: In array <code>pred</code>, store the POS tag for the word ‘upward’.</li>
</ul>
<p><strong>In Step 2:</strong>  </p>
<ul>
<li>Starting at the last column of best_paths, use <code>best_probs</code> to find the most likely POS tag for the last word in the corpus.</li>
<li>Then use <code>best_paths</code> to find the most likely POS tag for the previous word. </li>
<li>Update the POS tag for each word in <code>z</code> and in <code>preds</code>.</li>
</ul>
<p>Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.<br><code>z[1] = best_paths[z[2],2]</code>  </p>
<p>The small test following the routine prints the last few words of the corpus and their states to aid in debug.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_backward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_backward</span><span class="params">(best_probs, best_paths, corpus, states)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function returns the best path.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the number of words in the corpus</span></span><br><span class="line">    <span class="comment"># which is also the number of columns in best_probs, best_paths</span></span><br><span class="line">    m = best_paths.shape[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize array z, same length as the corpus</span></span><br><span class="line">    z = [<span class="keyword">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of unique POS tags</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the best probability for the last word</span></span><br><span class="line">    best_prob_for_last_word = float(<span class="string">'-inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize pred array, same length as corpus</span></span><br><span class="line">    pred = [<span class="keyword">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment">## Step 1 ##</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each POS tag for the last word (last column of best_probs)</span></span><br><span class="line">    <span class="comment"># in order to find the row (POS tag integer ID) </span></span><br><span class="line">    <span class="comment"># with highest probability for the last word</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the probability of POS tag at row k </span></span><br><span class="line">        <span class="comment"># is better than the previosly best probability for the last word:</span></span><br><span class="line">        <span class="keyword">if</span> best_probs[k,<span class="number">-1</span>] &gt; best_prob_for_last_word: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Store the new best probability for the lsat word</span></span><br><span class="line">            best_prob_for_last_word = best_probs[k,<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># Store the unique integer ID of the POS tag</span></span><br><span class="line">            <span class="comment"># which is also the row number in best_probs</span></span><br><span class="line">            z[m - <span class="number">1</span>] = k</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># Convert the last word's predicted POS tag</span></span><br><span class="line">    <span class="comment"># from its unique integer ID into the string representation</span></span><br><span class="line">    <span class="comment"># using the 'states' dictionary</span></span><br><span class="line">    <span class="comment"># store this in the 'pred' array for the last word</span></span><br><span class="line">    pred[m - <span class="number">1</span>] = states[k]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2 ##</span></span><br><span class="line">    <span class="comment"># Find the best POS tags by walking backward through the best_paths</span></span><br><span class="line">    <span class="comment"># From the last word in the corpus to the 0th word in the corpus</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the unique integer ID of</span></span><br><span class="line">        <span class="comment"># the POS tag for the word at position 'i' in the corpus</span></span><br><span class="line">        pos_tag_for_word_i = best_paths[z[i], i]</span><br><span class="line"><span class="comment">#         print(z[i])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># In best_paths, go to the row representing the POS tag of word i</span></span><br><span class="line">        <span class="comment"># and the column representing the word's position in the corpus</span></span><br><span class="line">        <span class="comment"># to retrieve the predicted POS for the word at position i-1 in the corpus</span></span><br><span class="line">        z[i - <span class="number">1</span>] = pos_tag_for_word_i</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the previous word's POS tag in string form</span></span><br><span class="line">        <span class="comment"># Use the 'states' dictionary, </span></span><br><span class="line">        <span class="comment"># where the key is the unique integer ID of the POS tag,</span></span><br><span class="line">        <span class="comment"># and the value is the string representation of that POS tag</span></span><br><span class="line">        pred[i - <span class="number">1</span>] = states[pos_tag_for_word_i]        </span><br><span class="line">     <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run and test your function</span></span><br><span class="line">pred = viterbi_backward(best_probs, best_paths, prep, states)</span><br><span class="line">m=len(pred)</span><br><span class="line">print(<span class="string">'The prediction for pred[-7:m-1] is: \n'</span>, prep[<span class="number">-7</span>:m<span class="number">-1</span>], <span class="string">"\n"</span>, pred[<span class="number">-7</span>:m<span class="number">-1</span>], <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">'The prediction for pred[0:8] is: \n'</span>, pred[<span class="number">0</span>:<span class="number">7</span>], <span class="string">"\n"</span>, prep[<span class="number">0</span>:<span class="number">7</span>])</span><br></pre></td></tr></table></figure>
<pre><code>The prediction for pred[-7:m-1] is: 
 [&#39;see&#39;, &#39;them&#39;, &#39;here&#39;, &#39;with&#39;, &#39;us&#39;, &#39;.&#39;] 
 [&#39;VB&#39;, &#39;PRP&#39;, &#39;RB&#39;, &#39;IN&#39;, &#39;PRP&#39;, &#39;.&#39;] 

The prediction for pred[0:8] is: 
 [&#39;DT&#39;, &#39;NN&#39;, &#39;POS&#39;, &#39;NN&#39;, &#39;MD&#39;, &#39;VB&#39;, &#39;VBN&#39;] 
 [&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;]
</code></pre><p><strong>Expected Output:</strong>   </p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The prediction <span class="keyword">for</span> prep[<span class="number">-7</span>:m<span class="number">-1</span>] is:  </span><br><span class="line"> ['see', 'them', 'here', 'with', 'us', '.']  </span><br><span class="line"> ['VB', 'PRP', 'RB', 'IN', 'PRP', '.']   </span><br><span class="line">The prediction <span class="keyword">for</span> pred[<span class="number">0</span>:<span class="number">8</span>] is:    </span><br><span class="line"> ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN']   </span><br><span class="line"> ['The', 'economy', "'s", 'temperature', 'will', 'be', 'taken']</span><br></pre></td></tr></table></figure>
<p>Now you just have to compare the predicted labels to the true labels to evaluate your model on the accuracy metric!</p>
<p><a name="4"></a></p>
<h1 id="Part-4-Predicting-on-a-data-set"><a href="#Part-4-Predicting-on-a-data-set" class="headerlink" title="Part 4: Predicting on a data set"></a>Part 4: Predicting on a data set</h1><p>Compute the accuracy of your prediction by comparing it with the true <code>y</code> labels. </p>
<ul>
<li><code>pred</code> is a list of predicted POS tags corresponding to the words of the <code>test_corpus</code>. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The third word is:'</span>, prep[<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'Your prediction is:'</span>, pred[<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'Your corresponding label y is: '</span>, y[<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>The third word is: temperature
Your prediction is: NN
Your corresponding label y is:  temperature NN
</code></pre><p><a name="ex-08"></a></p>
<h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p>Implement a function to compute the accuracy of the viterbi algorithm’s POS tag predictions.</p>
<ul>
<li>To split y into the word and its tag you can use <code>y.split()</code>. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        pred: a list of the predicted parts-of-speech </span></span><br><span class="line"><span class="string">        y: a list of lines where each word is separated by a '\t' (i.e. word \t tag)</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zip together the prediction and the labels</span></span><br><span class="line">    <span class="keyword">for</span> prediction, y <span class="keyword">in</span> zip(pred, y):</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># Split the label into the word and the POS tag</span></span><br><span class="line">        word_tag_tuple = y.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check that there is actually a word and a tag</span></span><br><span class="line">        <span class="comment"># no more and no less than 2 items</span></span><br><span class="line">        <span class="keyword">if</span> len(word_tag_tuple) &lt; <span class="number">2</span>: <span class="comment"># complete this line</span></span><br><span class="line">            <span class="keyword">continue</span> </span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># store the word and tag separately</span></span><br><span class="line">        word, tag = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> word_tag_tuple]</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(tag, prediction)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if the POS tag label matches the prediction</span></span><br><span class="line">        <span class="keyword">if</span> tag == prediction: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># count the number of times that the prediction</span></span><br><span class="line">            <span class="comment"># and label match</span></span><br><span class="line">            num_correct += <span class="number">1.0</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># keep track of the total number of examples (that have valid labels)</span></span><br><span class="line">        total += <span class="number">1.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> num_correct/total</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Accuracy of the Viterbi algorithm is <span class="subst">&#123;compute_accuracy(pred, y):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of the Viterbi algorithm is 0.9531
</code></pre><h5 id="Expected-Output-7"><a href="#Expected-Output-7" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of the Viterbi algorithm is <span class="number">0.9531</span></span><br></pre></td></tr></table></figure>
<p>Congratulations you were able to classify the parts-of-speech with 95% accuracy. </p>
<h3 id="Key-Points-and-overview"><a href="#Key-Points-and-overview" class="headerlink" title="Key Points and overview"></a>Key Points and overview</h3><p>In this assignment you learned about parts-of-speech tagging. </p>
<ul>
<li>In this assignment, you predicted POS tags by walking forward through a corpus and knowing the previous word.</li>
<li>There are other implementations that use bidirectional POS tagging.</li>
<li>Bidirectional POS tagging requires knowing the previous word and the next word in the corpus when predicting the current word’s POS tag.</li>
<li>Bidirectional POS tagging would tell you more about the POS instead of just knowing the previous word. </li>
<li>Since you have learned to implement the unidirectional approach, you have the foundation to implement other POS taggers used in industry.</li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">“Speech and Language Processing”, Dan Jurafsky and James H. Martin</a></li>
<li>We would like to thank Melanie Tosik for her help and inspiration</li>
</ul>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/ML-Interview-Computer-Vision/2020/05/29/" rel="next" title="ML-Interview-Computer-Vision">
                <i class="fa fa-chevron-left"></i> ML-Interview-Computer-Vision
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate article here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Ruochi Zhang WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    

    
      <div id="bitcoin" style="display: inline-block">
        <img id="vemo_qr" src="/images/venmo.png" alt="Ruochi Zhang Bitcoin">
        <p>Venmo(last 4 digits 1570)</p>
      </div>
    

  </div>
</div>

      </div>
    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">235</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Assignment-2-Parts-of-Speech-Tagging-POS"><span class="nav-text">Assignment 2: Parts-of-Speech Tagging (POS)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-0-Data-Sources"><span class="nav-text">Part 0: Data Sources</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-1-Parts-of-speech-tagging"><span class="nav-text">Part 1: Parts-of-speech tagging</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-1-1-Training"><span class="nav-text">Part 1.1 - Training</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Transition-counts"><span class="nav-text">Transition counts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Emission-counts"><span class="nav-text">Emission counts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tag-counts"><span class="nav-text">Tag counts</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-01"><span class="nav-text">Exercise 01</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output"><span class="nav-text">Expected Output</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output-1"><span class="nav-text">Expected Output</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Part-1-2-Testing"><span class="nav-text">Part 1.2 - Testing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-02"><span class="nav-text">Exercise 02</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output-2"><span class="nav-text">Expected Output</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-2-Hidden-Markov-Models-for-POS"><span class="nav-text">Part 2: Hidden Markov Models for POS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-2-1-Generating-Matrices"><span class="nav-text">Part 2.1 Generating Matrices</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Creating-the-‘A’-transition-probabilities-matrix"><span class="nav-text">Creating the ‘A’ transition probabilities matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-03"><span class="nav-text">Exercise 03</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output-3"><span class="nav-text">Expected Output</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Create-the-‘B’-emission-probabilities-matrix"><span class="nav-text">Create the ‘B’ emission probabilities matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-04"><span class="nav-text">Exercise 04</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output-4"><span class="nav-text">Expected Output</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-3-Viterbi-Algorithm-and-Dynamic-Programming"><span class="nav-text">Part 3: Viterbi Algorithm and Dynamic Programming</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-3-1-Initialization"><span class="nav-text">Part 3.1:  Initialization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-05"><span class="nav-text">Exercise 05</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output-5"><span class="nav-text">Expected Output</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-3-2-Viterbi-Forward"><span class="nav-text">Part 3.2 Viterbi Forward</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-06"><span class="nav-text">Exercise 06</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output-6"><span class="nav-text">Expected Output</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-3-3-Viterbi-backward"><span class="nav-text">Part 3.3 Viterbi backward</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-07"><span class="nav-text">Exercise 07</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-4-Predicting-on-a-data-set"><span class="nav-text">Part 4: Predicting on a data set</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-08"><span class="nav-text">Exercise 08</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output-7"><span class="nav-text">Expected Output</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Key-Points-and-overview"><span class="nav-text">Key Points and overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
