<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Medicine,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="Risk Models Using Tree-based ModelsWelcome to the second assignment of Course 2! Outline 1. Import Packages 2. Load the Dataset 3. Explore the Dataset 4. Dealing with Missing Data Exercise 1   5. Deci">
<meta name="keywords" content="Medicine">
<meta property="og:type" content="article">
<meta property="og:title" content="Risk Models Using Tree-based Models">
<meta property="og:url" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Risk Models Using Tree-based ModelsWelcome to the second assignment of Course 2! Outline 1. Import Packages 2. Load the Dataset 3. Explore the Dataset 4. Dealing with Missing Data Exercise 1   5. Deci">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_17_0.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_17_1.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_38_0.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_0.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_1.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_2.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_3.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_4.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_5.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_6.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_7.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_8.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_9.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_10.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_11.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_12.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_13.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_14.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_15.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_54_16.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_83_0.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_87_0.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_89_0.png">
<meta property="og:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_91_0.png">
<meta property="og:updated_time" content="2020-04-19T15:09:51.850Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Risk Models Using Tree-based Models">
<meta name="twitter:description" content="Risk Models Using Tree-based ModelsWelcome to the second assignment of Course 2! Outline 1. Import Packages 2. Load the Dataset 3. Explore the Dataset 4. Dealing with Missing Data Exercise 1   5. Deci">
<meta name="twitter:image" content="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/output_17_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/">





  <title>Risk Models Using Tree-based Models | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Risk Models Using Tree-based Models</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-04-18T23:53:23+08:00">
                2020-04-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Risk-Models-Using-Tree-based-Models/2020/04/18/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Risk-Models-Using-Tree-based-Models/2020/04/18/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Risk-Models-Using-Tree-based-Models"><a href="#Risk-Models-Using-Tree-based-Models" class="headerlink" title="Risk Models Using Tree-based Models"></a>Risk Models Using Tree-based Models</h1><p>Welcome to the second assignment of Course 2!</p>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#1">1. Import Packages</a></li>
<li><a href="#2">2. Load the Dataset</a></li>
<li><a href="#3">3. Explore the Dataset</a></li>
<li><a href="#4">4. Dealing with Missing Data</a><ul>
<li><a href="#Ex-1">Exercise 1</a></li>
</ul>
</li>
<li><a href="#5">5. Decision Trees</a><ul>
<li><a href="#Ex-2">Exercise 2</a></li>
</ul>
</li>
<li><a href="#6">6. Random Forests</a><ul>
<li><a href="#Ex-3">Exercise 3</a></li>
</ul>
</li>
<li><a href="#7">7. Imputation</a></li>
<li><a href="#8">8. Error Analysis</a><ul>
<li><a href="#Ex-4">Exercise 4</a></li>
</ul>
</li>
<li><a href="#Ex-9">9. Imputation Approaches</a><ul>
<li><a href="#Ex-5">Exercise 5</a></li>
<li><a href="#Ex-6">Exercise 6</a></li>
</ul>
</li>
<li><a href="#10">10. Comparison</a></li>
<li><a href="#">11. Explanations: SHAP</a></li>
</ul>
<p>In this assignment, you’ll gain experience with tree based models by predicting the 10-year risk of death of individuals from the NHANES I epidemiology dataset (for a detailed description of this dataset you can check the <a href="https://wwwn.cdc.gov/nchs/nhanes/nhefs/default.aspx/" target="_blank" rel="noopener">CDC Website</a>). This is a challenging task and a great test bed for the machine learning methods we learned this week.</p>
<p>As you go through the assignment, you’ll learn about: </p>
<ul>
<li>Dealing with Missing Data<ul>
<li>Complete Case Analysis.</li>
<li>Imputation</li>
</ul>
</li>
<li>Decision Trees<ul>
<li>Evaluation.</li>
<li>Regularization.</li>
</ul>
</li>
<li>Random Forests <ul>
<li>Hyperparameter Tuning.</li>
</ul>
</li>
</ul>
<p><a name="1"></a></p>
<h2 id="1-Import-Packages"><a href="#1-Import-Packages" class="headerlink" title="1. Import Packages"></a>1. Import Packages</h2><p>We’ll first import all the common packages that we need for this assignment. </p>
<ul>
<li><code>shap</code> is a library that explains predictions made by machine learning models.</li>
<li><code>sklearn</code> is one of the most popular machine learning libraries.</li>
<li><code>itertools</code> allows us to conveniently manipulate iterable objects such as lists.</li>
<li><code>pydotplus</code> is used together with <code>IPython.display.Image</code> to visualize graph structures such as decision trees.</li>
<li><code>numpy</code> is a fundamental package for scientific computing in Python.</li>
<li><code>pandas</code> is what we’ll use to manipulate our data.</li>
<li><code>seaborn</code> is a plotting library which has some convenient functions for visualizing missing data.</li>
<li><code>matplotlib</code> is a plotting library.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> shap</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn.externals.six <span class="keyword">import</span> StringIO</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.experimental <span class="keyword">import</span> enable_iterative_imputer</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> IterativeImputer, SimpleImputer</span><br><span class="line"></span><br><span class="line"><span class="comment"># We'll also import some helper functions that will be useful later on.</span></span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> load_data, cindex</span><br></pre></td></tr></table></figure>
<p><a name="2"></a></p>
<h2 id="2-Load-the-Dataset"><a href="#2-Load-the-Dataset" class="headerlink" title="2. Load the Dataset"></a>2. Load the Dataset</h2><p>Run the next cell to load in the NHANES I epidemiology dataset. This dataset contains various features of hospital patients as well as their outcomes, i.e. whether or not they died within 10 years.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_dev, X_test, y_dev, y_test = load_data(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>The dataset has been split into a development set (or dev set), which we will use to develop our risk models, and a test set, which we will use to test our models.</p>
<p>We further split the dev set into a training and validation set, respectively to train and tune our models, using a 75/25 split (note that we set a random state to make this split repeatable).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=<span class="number">0.25</span>, random_state=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><a name="3"></a></p>
<h2 id="3-Explore-the-Dataset"><a href="#3-Explore-the-Dataset" class="headerlink" title="3. Explore the Dataset"></a>3. Explore the Dataset</h2><p>The first step is to familiarize yourself with the data. Run the next cell to get the size of your training set and look at a small sample. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"X_train shape: &#123;&#125;"</span>.format(X_train.shape))</span><br><span class="line">X_train.head()</span><br></pre></td></tr></table></figure>
<pre><code>X_train shape: (5147, 18)
</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Diastolic BP</th>
      <th>Poverty index</th>
      <th>Race</th>
      <th>Red blood cells</th>
      <th>Sedimentation rate</th>
      <th>Serum Albumin</th>
      <th>Serum Cholesterol</th>
      <th>Serum Iron</th>
      <th>Serum Magnesium</th>
      <th>Serum Protein</th>
      <th>Sex</th>
      <th>Systolic BP</th>
      <th>TIBC</th>
      <th>TS</th>
      <th>White blood cells</th>
      <th>BMI</th>
      <th>Pulse pressure</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1599</th>
      <td>43.0</td>
      <td>84.0</td>
      <td>637.0</td>
      <td>1.0</td>
      <td>49.3</td>
      <td>10.0</td>
      <td>5.0</td>
      <td>253.0</td>
      <td>134.0</td>
      <td>1.59</td>
      <td>7.7</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>490.0</td>
      <td>27.3</td>
      <td>9.1</td>
      <td>25.803007</td>
      <td>34.0</td>
    </tr>
    <tr>
      <th>2794</th>
      <td>72.0</td>
      <td>96.0</td>
      <td>154.0</td>
      <td>2.0</td>
      <td>43.4</td>
      <td>23.0</td>
      <td>4.3</td>
      <td>265.0</td>
      <td>106.0</td>
      <td>1.66</td>
      <td>6.8</td>
      <td>2.0</td>
      <td>208.0</td>
      <td>301.0</td>
      <td>35.2</td>
      <td>6.0</td>
      <td>33.394319</td>
      <td>112.0</td>
    </tr>
    <tr>
      <th>1182</th>
      <td>54.0</td>
      <td>78.0</td>
      <td>205.0</td>
      <td>1.0</td>
      <td>43.8</td>
      <td>12.0</td>
      <td>4.2</td>
      <td>206.0</td>
      <td>180.0</td>
      <td>1.67</td>
      <td>6.6</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>363.0</td>
      <td>49.6</td>
      <td>5.9</td>
      <td>20.278410</td>
      <td>34.0</td>
    </tr>
    <tr>
      <th>6915</th>
      <td>59.0</td>
      <td>90.0</td>
      <td>417.0</td>
      <td>1.0</td>
      <td>43.4</td>
      <td>9.0</td>
      <td>4.5</td>
      <td>327.0</td>
      <td>114.0</td>
      <td>1.65</td>
      <td>7.6</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>347.0</td>
      <td>32.9</td>
      <td>6.1</td>
      <td>32.917744</td>
      <td>78.0</td>
    </tr>
    <tr>
      <th>500</th>
      <td>34.0</td>
      <td>80.0</td>
      <td>385.0</td>
      <td>1.0</td>
      <td>77.7</td>
      <td>9.0</td>
      <td>4.1</td>
      <td>197.0</td>
      <td>64.0</td>
      <td>1.74</td>
      <td>7.3</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>376.0</td>
      <td>17.0</td>
      <td>8.2</td>
      <td>30.743489</td>
      <td>30.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>Our targets <code>y</code> will be whether or not the target died within 10 years. Run the next cell to see the target data series.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train.head(<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>1599    False
2794     True
1182    False
6915    False
500     False
1188     True
9739    False
3266    False
6681    False
8822    False
5856     True
3415    False
9366    False
7975    False
1397    False
6809    False
9461    False
9374    False
1170     True
158     False
Name: time, dtype: bool
</code></pre><p>Use the next cell to examine individual cases and familiarize yourself with the features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">10</span></span><br><span class="line">print(X_train.iloc[i,:])</span><br><span class="line">print(<span class="string">"\nDied within 10 years? &#123;&#125;"</span>.format(y_train.loc[y_train.index[i]]))</span><br></pre></td></tr></table></figure>
<pre><code>Age                    67.000000
Diastolic BP           94.000000
Poverty index         114.000000
Race                    1.000000
Red blood cells        43.800000
Sedimentation rate     12.000000
Serum Albumin           3.700000
Serum Cholesterol     178.000000
Serum Iron             73.000000
Serum Magnesium         1.850000
Serum Protein           7.000000
Sex                     1.000000
Systolic BP           140.000000
TIBC                  311.000000
TS                     23.500000
White blood cells       4.300000
BMI                    17.481227
Pulse pressure         46.000000
Name: 5856, dtype: float64

Died within 10 years? True
</code></pre><p><a name="4"></a></p>
<h2 id="4-Dealing-with-Missing-Data"><a href="#4-Dealing-with-Missing-Data" class="headerlink" title="4. Dealing with Missing Data"></a>4. Dealing with Missing Data</h2><p>Looking at our data in <code>X_train</code>, we see that some of the data is missing: some values in the output of the previous cell are marked as <code>NaN</code> (“not a number”).</p>
<p>Missing data is a common occurrence in data analysis, that can be due to a variety of reasons, such as measuring instrument malfunction, respondents not willing or not able to supply information, and errors in the data collection process.</p>
<p>Let’s examine the missing data pattern. <code>seaborn</code> is an alternative to <code>matplotlib</code> that has some convenient plotting functions for data analysis. We can use its <code>heatmap</code> function to easily visualize the missing data pattern.</p>
<p>Run the cell below to plot the missing data: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sns.heatmap(X_train.isnull(), cbar=<span class="keyword">False</span>)</span><br><span class="line">plt.title(<span class="string">"Training"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">sns.heatmap(X_val.isnull(), cbar=<span class="keyword">False</span>)</span><br><span class="line">plt.title(<span class="string">"Validation"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_17_0.png" alt="png"></p>
<p><img src="output_17_1.png" alt="png"></p>
<p>For each feature, represented as a column, values that are present are shown in black, and missing values are set in a light color.</p>
<p>From this plot, we can see that many values are missing for systolic blood pressure (<code>Systolic BP</code>).</p>
<p><a name="Ex-1"></a></p>
<h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><p>In the cell below, write a function to compute the fraction of cases with missing data. This will help us decide how we handle this missing data in the future.</p>
<p><details>    </details></p>
<p><summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary></p>
<p>
<ul>
    <li> The <code>pandas.DataFrame.isnull()</code> method is helpful in this case.</li>
    <li> Use the <code>pandas.DataFrame.any()</code> method and set the <code>axis</code> parameter.</li>
    <li> Divide the total number of rows with missing data by the total number of rows. Remember that in Python, <code>True</code> values are equal to 1.</li>
</ul>
</p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fraction_rows_missing</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return percent of rows with any missing</span></span><br><span class="line"><span class="string">    data in the dataframe. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        df (dataframe): a pandas dataframe with potentially missing data</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        frac_missing (float): fraction of rows with missing data</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE 'Pass' with your 'return' code) ###</span></span><br><span class="line">    <span class="keyword">return</span> df.isnull().any(axis = <span class="number">1</span>).sum() / df.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<p>Test your function by running the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df_test = pd.DataFrame(&#123;<span class="string">'a'</span>:[<span class="keyword">None</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="keyword">None</span>], <span class="string">'b'</span>:[<span class="number">1</span>, <span class="keyword">None</span>, <span class="number">0</span>, <span class="number">1</span>]&#125;)</span><br><span class="line">print(<span class="string">"Example dataframe:\n"</span>)</span><br><span class="line">print(df_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nComputed fraction missing: &#123;&#125;, expected: &#123;&#125;"</span>.format(fraction_rows_missing(df_test), <span class="number">0.75</span>))</span><br><span class="line">print(<span class="string">f"Fraction of rows missing from X_train: <span class="subst">&#123;fraction_rows_missing(X_train):<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Fraction of rows missing from X_val: <span class="subst">&#123;fraction_rows_missing(X_val):<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Fraction of rows missing from X_test: <span class="subst">&#123;fraction_rows_missing(X_test):<span class="number">.3</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Example dataframe:

     a    b
0  NaN  1.0
1  1.0  NaN
2  1.0  0.0
3  NaN  1.0

Computed fraction missing: 0.75, expected: 0.75
Fraction of rows missing from X_train: 0.699
Fraction of rows missing from X_val: 0.704
Fraction of rows missing from X_test: 0.000
</code></pre><p>We see that our train and validation sets have missing values, but luckily our test set has complete cases.</p>
<p>As a first pass, we will begin with a <strong>complete case analysis</strong>, dropping all of the rows with any missing data. Run the following cell to drop these rows from our train and validation sets. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train_dropped = X_train.dropna(axis=<span class="string">'rows'</span>)</span><br><span class="line">y_train_dropped = y_train.loc[X_train_dropped.index]</span><br><span class="line">X_val_dropped = X_val.dropna(axis=<span class="string">'rows'</span>)</span><br><span class="line">y_val_dropped = y_val.loc[X_val_dropped.index]</span><br></pre></td></tr></table></figure>
<p><a name="5"></a></p>
<h2 id="5-Decision-Trees"><a href="#5-Decision-Trees" class="headerlink" title="5. Decision Trees"></a>5. Decision Trees</h2><p>Having just learned about decision trees, you choose to use a decision tree classifier. Use scikit-learn to build a decision tree for the hospital dataset using the train set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dt = DecisionTreeClassifier(max_depth=<span class="keyword">None</span>, random_state=<span class="number">10</span>)</span><br><span class="line">dt.fit(X_train_dropped, y_train_dropped)</span><br></pre></td></tr></table></figure>
<pre><code>DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;,
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;,
                       random_state=10, splitter=&#39;best&#39;)
</code></pre><p>Next we will evaluate our model. We’ll use C-Index for evaluation.</p>
<blockquote>
<p>Remember from lesson 4 of week 1 that the C-Index evaluates the ability of a model to differentiate between different classes, by quantifying how often, when considering all pairs of patients (A, B), the model says that patient A has a higher risk score than patient B when, in the observed data, patient A actually died and patient B actually lived. In our case, our model is a binary classifier, where each risk score is either 1 (the model predicts that the patient will die) or 0 (the patient will live).</p>
<p>More formally, defining _permissible pairs_ of patients as pairs where the outcomes are different, _concordant pairs_ as permissible pairs where the patient that died had a higher risk score (i.e. our model predicted 1 for the patient that died and 0 for the one that lived), and _ties_ as permissible pairs where the risk scores were equal (i.e. our model predicted 1 for both patients or 0 for both patients), the C-Index is equal to:</p>
<script type="math/tex; mode=display">\text{C-Index} = \frac{\#\text{concordant pairs} + 0.5\times \#\text{ties}}{\#\text{permissible pairs}}</script></blockquote>
<p>Run the next cell to compute the C-Index on the train and validation set (we’ve given you an implementation this time).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_train_preds = dt.predict_proba(X_train_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Train C-Index: <span class="subst">&#123;cindex(y_train_dropped.values, y_train_preds)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_val_preds = dt.predict_proba(X_val_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Val C-Index: <span class="subst">&#123;cindex(y_val_dropped.values, y_val_preds)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Train C-Index: 1.0
Val C-Index: 0.5629321808510638
</code></pre><p>Unfortunately your tree seems to be overfitting: it fits the training data so closely that it doesn’t generalize well to other samples such as those from the validation set.</p>
<blockquote>
<p>The training C-index comes out to 1.0 because, when initializing <code>DecisionTreeClasifier</code>, we have left <code>max_depth</code> and <code>min_samples_split</code> unspecified. The resulting decision tree will therefore keep splitting as far as it can, which pretty much guarantees a pure fit to the training data.</p>
</blockquote>
<p>To handle this, you can change some of the hyperparameters of our tree. </p>
<p><a name="Ex-2"></a></p>
<h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><p>Try and find a set of hyperparameters that improves the generalization to the validation set and recompute the C-index. If you do it right, you should get C-index above 0.6 for the validation set. </p>
<p>You can refer to the documentation for the sklearn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" target="_blank" rel="noopener">DecisionTreeClassifier</a>.</p>
<p><details>    </details></p>
<p><summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary></p>
<p>
<ul>
    <li> Try limiting the depth of the tree (<code>'max_depth'</code>).</li>
</ul>
</p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Experiment with different hyperparameters for the DecisionTreeClassifier</span></span><br><span class="line"><span class="comment"># until you get a c-index above 0.6 for the validation set</span></span><br><span class="line">dt_hyperparams = &#123;</span><br><span class="line">    <span class="comment"># set your own hyperparameters below, such as 'min_samples_split': 1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="string">"max_depth"</span>: <span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Run the next cell to fit and evaluate the regularized tree.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line">dt_reg = DecisionTreeClassifier(**dt_hyperparams, random_state=<span class="number">10</span>)</span><br><span class="line">dt_reg.fit(X_train_dropped, y_train_dropped)</span><br><span class="line"></span><br><span class="line">y_train_preds = dt_reg.predict_proba(X_train_dropped)[:, <span class="number">1</span>]</span><br><span class="line">y_val_preds = dt_reg.predict_proba(X_val_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Train C-Index: <span class="subst">&#123;cindex(y_train_dropped.values, y_train_preds)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Val C-Index (expected &gt; 0.6): <span class="subst">&#123;cindex(y_val_dropped.values, y_val_preds)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Train C-Index: 0.688738755448391
Val C-Index (expected &gt; 0.6): 0.6302692819148936
</code></pre><p>If you used a low <code>max_depth</code> you can print the entire tree. This allows for easy interpretability. Run the next cell to print the tree splits. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dot_data = StringIO()</span><br><span class="line">export_graphviz(dt_reg, feature_names=X_train_dropped.columns, out_file=dot_data,  </span><br><span class="line">                filled=<span class="keyword">True</span>, rounded=<span class="keyword">True</span>, proportion=<span class="keyword">True</span>, special_characters=<span class="keyword">True</span>,</span><br><span class="line">                impurity=<span class="keyword">False</span>, class_names=[<span class="string">'neg'</span>, <span class="string">'pos'</span>], precision=<span class="number">2</span>)</span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  </span><br><span class="line">Image(graph.create_png())</span><br></pre></td></tr></table></figure>
<p><img src="output_38_0.png" alt="png"></p>
<blockquote>
<p><strong>Overfitting, underfitting, and the bias-variance tradeoff</strong></p>
<p>If you tested several values of <code>max_depth</code>, you may have seen that a value of <code>3</code> gives training and validation C-Indices of about <code>0.689</code> and <code>0.630</code>, and that a <code>max_depth</code> of <code>2</code> gives better agreement with values of about <code>0.653</code> and <code>0.607</code>. In the latter case, we have further reduced overfitting, at the cost of a minor loss in predictive performance.</p>
<p>Contrast this with a <code>max_depth</code> value of <code>1</code>, which results in C-Indices of about <code>0.597</code> for the training set and <code>0.598</code> for the validation set: we have eliminated overfitting but with a much stronger degradation of predictive performance.</p>
<p>Lower predictive performance on the training and validation sets is indicative of the model _underfitting_ the data: it neither learns enough from the training data nor is able to generalize to unseen data (the validation data in our case).</p>
<p>Finding a model that minimizes and acceptably balances underfitting and overfitting (e.g. selecting the model with a <code>max_depth</code> of <code>2</code> over the other values) is a common problem in machine learning that is known as the _bias-variance tradeoff_.</p>
</blockquote>
<p><a name="6"></a></p>
<h2 id="6-Random-Forests"><a href="#6-Random-Forests" class="headerlink" title="6. Random Forests"></a>6. Random Forests</h2><p>No matter how you choose hyperparameters, a single decision tree is prone to overfitting. To solve this problem, you can try <strong>random forests</strong>, which combine predictions from many different trees to create a robust classifier. </p>
<p>As before, we will use scikit-learn to build a random forest for the data. We will use the default hyperparameters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">10</span>)</span><br><span class="line">rf.fit(X_train_dropped, y_train_dropped)</span><br></pre></td></tr></table></figure>
<pre><code>RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;,
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=10, verbose=0,
                       warm_start=False)
</code></pre><p>Now compute and report the C-Index for the random forest on the training and validation set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_train_rf_preds = rf.predict_proba(X_train_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Train C-Index: <span class="subst">&#123;cindex(y_train_dropped.values, y_train_rf_preds)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_val_rf_preds = rf.predict_proba(X_val_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Val C-Index: <span class="subst">&#123;cindex(y_val_dropped.values, y_val_rf_preds)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Train C-Index: 1.0
Val C-Index: 0.6660488696808511
</code></pre><p>Training a random forest with the default hyperparameters results in a model that has better predictive performance than individual decision trees as in the previous section, but this model is overfitting.</p>
<p>We therefore need to tune (or optimize) the hyperparameters, to find a model that both has good predictive performance and minimizes overfitting.</p>
<p>The hyperparameters we choose to adjust will be:</p>
<ul>
<li><code>n_estimators</code>: the number of trees used in the forest.</li>
<li><code>max_depth</code>: the maximum depth of each tree.</li>
<li><code>min_samples_leaf</code>: the minimum number (if <code>int</code>) or proportion (if <code>float</code>) of samples in a leaf.</li>
</ul>
<p>The approach we implement to tune the hyperparameters is known as a grid search:</p>
<ul>
<li><p>We define a set of possible values for each of the target hyperparameters.</p>
</li>
<li><p>A model is trained and evaluated for every possible combination of hyperparameters.</p>
</li>
<li><p>The best performing set of hyperparameters is returned.</p>
</li>
</ul>
<p>The cell below implements a hyperparameter grid search, using the C-Index to evaluate each tested model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">holdout_grid_search</span><span class="params">(clf, X_train_hp, y_train_hp, X_val_hp, y_val_hp, hyperparams, fixed_hyperparams=&#123;&#125;)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Conduct hyperparameter grid search on hold out validation set. Use holdout validation.</span></span><br><span class="line"><span class="string">    Hyperparameters are input as a dictionary mapping each hyperparameter name to the</span></span><br><span class="line"><span class="string">    range of values they should iterate over. Use the cindex function as your evaluation</span></span><br><span class="line"><span class="string">    function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        clf: sklearn classifier</span></span><br><span class="line"><span class="string">        X_train_hp (dataframe): dataframe for training set input variables</span></span><br><span class="line"><span class="string">        y_train_hp (dataframe): dataframe for training set targets</span></span><br><span class="line"><span class="string">        X_val_hp (dataframe): dataframe for validation set input variables</span></span><br><span class="line"><span class="string">        y_val_hp (dataframe): dataframe for validation set targets</span></span><br><span class="line"><span class="string">        hyperparams (dict): hyperparameter dictionary mapping hyperparameter</span></span><br><span class="line"><span class="string">                            names to range of values for grid search</span></span><br><span class="line"><span class="string">        fixed_hyperparams (dict): dictionary of fixed hyperparameters that</span></span><br><span class="line"><span class="string">                                  are not included in the grid search</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        best_estimator (sklearn classifier): fitted sklearn classifier with best performance on</span></span><br><span class="line"><span class="string">                                             validation set</span></span><br><span class="line"><span class="string">        best_hyperparams (dict): hyperparameter dictionary mapping hyperparameter</span></span><br><span class="line"><span class="string">                                 names to values in best_estimator</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    best_estimator = <span class="keyword">None</span></span><br><span class="line">    best_hyperparams = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># hold best running score</span></span><br><span class="line">    best_score = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get list of param values</span></span><br><span class="line">    lists = hyperparams.values()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get all param combinations</span></span><br><span class="line">    param_combinations = list(itertools.product(*lists))</span><br><span class="line">    total_param_combinations = len(param_combinations)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># iterate through param combinations</span></span><br><span class="line">    <span class="keyword">for</span> i, params <span class="keyword">in</span> enumerate(param_combinations, <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># fill param dict with params</span></span><br><span class="line">        param_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> param_index, param_name <span class="keyword">in</span> enumerate(hyperparams):</span><br><span class="line">            param_dict[param_name] = params[param_index]</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># create estimator with specified params</span></span><br><span class="line">        estimator = clf(**param_dict, **fixed_hyperparams)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># fit estimator</span></span><br><span class="line">        estimator.fit(X_train_hp, y_train_hp)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get predictions on validation set</span></span><br><span class="line">        preds = estimator.predict_proba(X_val_hp)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute cindex for predictions</span></span><br><span class="line">        estimator_score = cindex(y_val_hp, preds[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        print(<span class="string">f'[<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;total_param_combinations&#125;</span>] <span class="subst">&#123;param_dict&#125;</span>'</span>)</span><br><span class="line">        print(<span class="string">f'Val C-Index: <span class="subst">&#123;estimator_score&#125;</span>\n'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if new high score, update high score, best estimator</span></span><br><span class="line">        <span class="comment"># and best params </span></span><br><span class="line">        <span class="keyword">if</span> estimator_score &gt;= best_score:</span><br><span class="line">                best_score = estimator_score</span><br><span class="line">                best_estimator = estimator</span><br><span class="line">                best_hyperparams = param_dict</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add fixed hyperparamters to best combination of variable hyperparameters</span></span><br><span class="line">    best_hyperparams.update(fixed_hyperparams)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_estimator, best_hyperparams</span><br></pre></td></tr></table></figure>
<p><a name="Ex-3"></a></p>
<h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><p>In the cell below, define the values you want to run the hyperparameter grid search on, and run the cell to find the best-performing set of hyperparameters.</p>
<p>Your objective is to get a C-Index above <code>0.6</code> on both the train and validation set.</p>
<p><details>    </details></p>
<p><summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary></p>
<p>
<ul>
    <li>n_estimators: try values greater than 100</li>
    <li>max_depth: try values in the range 1 to 100</li>
    <li>min_samples_leaf: try float values below .5 and/or int values greater than 2</li>
</ul>
</p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_forest_grid_search</span><span class="params">(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define ranges for the chosen random forest hyperparameters </span></span><br><span class="line">    hyperparams = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE array values with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># how many trees should be in the forest (int)</span></span><br><span class="line">        <span class="string">'n_estimators'</span>: [<span class="number">50</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line"></span><br><span class="line">        <span class="comment"># the maximum depth of trees in the forest (int)</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">'max_depth'</span>: [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>],</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># the minimum number of samples in a leaf as a fraction</span></span><br><span class="line">        <span class="comment"># of the total number of samples in the training set</span></span><br><span class="line">        <span class="comment"># Can be int (in which case that is the minimum number)</span></span><br><span class="line">        <span class="comment"># or float (in which case the minimum is that fraction of the</span></span><br><span class="line">        <span class="comment"># number of training set samples)</span></span><br><span class="line">        <span class="string">'min_samples_leaf'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    fixed_hyperparams = &#123;</span><br><span class="line">        <span class="string">'random_state'</span>: <span class="number">10</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    rf = RandomForestClassifier</span><br><span class="line"></span><br><span class="line">    best_rf, best_hyperparams = holdout_grid_search(rf, X_train_dropped, y_train_dropped,</span><br><span class="line">                                                    X_val_dropped, y_val_dropped, hyperparams,</span><br><span class="line">                                                    fixed_hyperparams)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"Best hyperparameters:\n<span class="subst">&#123;best_hyperparams&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    y_train_best = best_rf.predict_proba(X_train_dropped)[:, <span class="number">1</span>]</span><br><span class="line">    print(<span class="string">f"Train C-Index: <span class="subst">&#123;cindex(y_train_dropped, y_train_best)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    y_val_best = best_rf.predict_proba(X_val_dropped)[:, <span class="number">1</span>]</span><br><span class="line">    print(<span class="string">f"Val C-Index: <span class="subst">&#123;cindex(y_val_dropped, y_val_best)&#125;</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add fixed hyperparamters to best combination of variable hyperparameters</span></span><br><span class="line">    best_hyperparams.update(fixed_hyperparams)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_rf, best_hyperparams</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_rf, best_hyperparams = random_forest_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped)</span><br></pre></td></tr></table></figure>
<pre><code>[1/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.6724567819148937

[2/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6722240691489362

[3/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.6725066489361702

[4/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.6637965425531915

[5/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6655585106382979

[6/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.659624335106383

[7/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.6611535904255319

[8/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6647273936170213

[9/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.6605884308510638

[10/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.6811502659574468

[11/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6815159574468085

[12/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.6809175531914894

[13/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.6765458776595744

[14/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6750831117021276

[15/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.6745844414893617

[16/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.668467420212766

[17/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6737699468085107

[18/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.667436835106383

[19/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.6811502659574468

[20/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6815159574468085

[21/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.6809175531914894

[22/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.6765458776595744

[23/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6750831117021276

[24/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.6745844414893617

[25/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.668467420212766

[26/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.6737699468085107

[27/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.667436835106383

Best hyperparameters:
{&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2, &#39;random_state&#39;: 10}
Train C-Index: 0.7798145228600575
Val C-Index: 0.6815159574468085
</code></pre><p>Finally, evaluate the model on the test set. This is a crucial step, as trying out many combinations of hyperparameters and evaluating them on the validation set could result in a model that ends up overfitting the validation set. We therefore need to check if the model performs well on unseen data, which is the role of the test set, which we have held out until now.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line">y_test_best = best_rf.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Test C-Index: <span class="subst">&#123;cindex(y_test.values, y_test_best)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Test C-Index: 0.7013860174676174
</code></pre><p>Your C-Index on the test set should be greater than <code>0.6</code>.</p>
<p><a name="7"></a></p>
<h2 id="7-Imputation"><a href="#7-Imputation" class="headerlink" title="7. Imputation"></a>7. Imputation</h2><p>You’ve now built and optimized a random forest model on our data. However, there was still a drop in test C-Index. This might be because you threw away more than half of the data of our data because of missing values for systolic blood pressure. Instead, we can try filling in, or imputing, these values. </p>
<p>First, let’s explore to see if our data is missing at random or not. Let’s plot histograms of the dropped rows against each of the covariates (aside from systolic blood pressure) to see if there is a trend. Compare these to the histograms of the feature in the entire dataset. Try to see if one of the covariates has a signficantly different distribution in the two subsets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dropped_rows = X_train[X_train.isnull().any(axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">columns_except_Systolic_BP = [col <span class="keyword">for</span> col <span class="keyword">in</span> X_train.columns <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'Systolic BP'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> columns_except_Systolic_BP:</span><br><span class="line">    sns.distplot(X_train.loc[:, col], norm_hist=<span class="keyword">True</span>, kde=<span class="keyword">False</span>, label=<span class="string">'full data'</span>)</span><br><span class="line">    sns.distplot(dropped_rows.loc[:, col], norm_hist=<span class="keyword">True</span>, kde=<span class="keyword">False</span>, label=<span class="string">'without missing data'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_54_0.png" alt="png"></p>
<p><img src="output_54_1.png" alt="png"></p>
<p><img src="output_54_2.png" alt="png"></p>
<p><img src="output_54_3.png" alt="png"></p>
<p><img src="output_54_4.png" alt="png"></p>
<p><img src="output_54_5.png" alt="png"></p>
<p><img src="output_54_6.png" alt="png"></p>
<p><img src="output_54_7.png" alt="png"></p>
<p><img src="output_54_8.png" alt="png"></p>
<p><img src="output_54_9.png" alt="png"></p>
<p><img src="output_54_10.png" alt="png"></p>
<p><img src="output_54_11.png" alt="png"></p>
<p><img src="output_54_12.png" alt="png"></p>
<p><img src="output_54_13.png" alt="png"></p>
<p><img src="output_54_14.png" alt="png"></p>
<p><img src="output_54_15.png" alt="png"></p>
<p><img src="output_54_16.png" alt="png"></p>
<p>Most of the covariates are distributed similarly whether or not we have discarded rows with missing data. In other words missingness of the data is independent of these covariates.</p>
<p>If this had been true across <em>all</em> covariates, then the data would have been said to be <strong>missing completely at random (MCAR)</strong>.</p>
<p>But when considering the age covariate, we see that much more data tends to be missing for patients over 65. The reason could be that blood pressure was measured less frequently for old people to avoid placing additional burden on them.</p>
<p>As missingness is related to one or more covariates, the missing data is said to be <strong>missing at random (MAR)</strong>.</p>
<p>Based on the information we have, there is however no reason to believe that the _values_ of the missing data — or specifically the values of the missing systolic blood pressures — are related to the age of the patients.<br>If this was the case, then this data would be said to be <strong>missing not at random (MNAR)</strong>.</p>
<p><a name="8"></a></p>
<h2 id="8-Error-Analysis"><a href="#8-Error-Analysis" class="headerlink" title="8. Error Analysis"></a>8. Error Analysis</h2><p><a name="Ex-4"></a></p>
<h3 id="Exercise-4"><a href="#Exercise-4" class="headerlink" title="Exercise 4"></a>Exercise 4</h3><p>Using the information from the plots above, try to find a subgroup of the test data on which the model performs poorly. You should be able to easily find a subgroup of at least 250 cases on which the model has a C-Index of less than 0.69.</p>
<p><details>    </details></p>
<p><summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary></p>
<p>
<ul>
    <li> Define a mask using a feature and a threshold, e.g. patients with a BMI below 20: <code>mask = X_test['BMI'] < 20 </code>. </li>
    <li> Try to find a subgroup for which the model had little data.</li>
</ul>
</p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bad_subset</span><span class="params">(forest, X_test, y_test)</span>:</span></span><br><span class="line">    <span class="comment"># define mask to select large subset with poor performance</span></span><br><span class="line">    <span class="comment"># currently mask defines the entire set</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE the code after 'mask =' with your code) ###</span></span><br><span class="line">    mask = X_test[<span class="string">'Age'</span>] &gt; <span class="number">67</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    X_subgroup = X_test[mask]</span><br><span class="line">    y_subgroup = y_test[mask]</span><br><span class="line">    subgroup_size = len(X_subgroup)</span><br><span class="line"></span><br><span class="line">    y_subgroup_preds = forest.predict_proba(X_subgroup)[:, <span class="number">1</span>]</span><br><span class="line">    performance = cindex(y_subgroup.values, y_subgroup_preds)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> performance, subgroup_size</span><br></pre></td></tr></table></figure>
<h4 id="Test-Your-Work"><a href="#Test-Your-Work" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">performance, subgroup_size = bad_subset(best_rf, X_test, y_test)</span><br><span class="line">print(<span class="string">"Subgroup size should greater than 250, performance should be less than 0.69 "</span>)</span><br><span class="line">print(<span class="string">f"Subgroup size: <span class="subst">&#123;subgroup_size&#125;</span>, C-Index: <span class="subst">&#123;performance&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Subgroup size should greater than 250, performance should be less than 0.69 
Subgroup size: 320, C-Index: 0.670638197475522
</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h4><p>Note, your actual output will vary depending on the hyper-parameters that you chose and the mask that you chose.</p>
<ul>
<li>Make sure that the c-index is less than 0.69<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Subgroup size: <span class="number">586</span>, C-Index: <span class="number">0.6275</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Bonus</strong>: </p>
<ul>
<li>See if you can get a c-index as low as 0.53<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Subgroup size: 251, C-Index: 0.5331</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><a name="9"></a></p>
<h2 id="9-Imputation-Approaches"><a href="#9-Imputation-Approaches" class="headerlink" title="9. Imputation Approaches"></a>9. Imputation Approaches</h2><p>Seeing that our data is not missing completely at random, we can handle the missing values by replacing them with substituted values based on the other values that we have. This is known as imputation.</p>
<p>The first imputation strategy that we will use is <strong>mean substitution</strong>: we will replace the missing values for each feature with the mean of the available values. In the next cell, use the <code>SimpleImputer</code> from <code>sklearn</code> to use mean imputation for the missing values.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Impute values using the mean</span></span><br><span class="line">imputer = SimpleImputer(strategy=<span class="string">'mean'</span>)</span><br><span class="line">imputer.fit(X_train)</span><br><span class="line">X_train_mean_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)</span><br><span class="line">X_val_mean_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)</span><br></pre></td></tr></table></figure>
<p><a name="Ex-5"></a></p>
<h3 id="Exercise-5"><a href="#Exercise-5" class="headerlink" title="Exercise 5"></a>Exercise 5</h3><p>Now perform a hyperparameter grid search to find the best-performing random forest model, and report results on the test set. </p>
<p>Define the parameter ranges for the hyperparameter search in the next cell, and run the cell.</p>
<h4 id="Target-performance"><a href="#Target-performance" class="headerlink" title="Target performance"></a>Target performance</h4><p>Make your test c-index at least 0.74 or higher</p>
<p><details>    </details></p>
<p><summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary></p>
<p>
<ul>
    <li>n_estimators: try values greater than 100</li>
    <li>max_depth: try values in the range 1 to 100</li>
    <li>min_samples_leaf: try float values below .5 and/or int values greater than 2</li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define ranges for the random forest hyperparameter search </span></span><br><span class="line">hyperparams = &#123;</span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE array values with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># how many trees should be in the forest (int)</span></span><br><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">200</span>,<span class="number">500</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the maximum depth of trees in the forest (int)</span></span><br><span class="line">    <span class="string">'max_depth'</span>: [<span class="number">3</span>,<span class="number">5</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the minimum number of samples in a leaf as a fraction</span></span><br><span class="line">    <span class="comment"># of the total number of samples in the training set</span></span><br><span class="line">    <span class="comment"># Can be int (in which case that is the minimum number)</span></span><br><span class="line">    <span class="comment"># or float (in which case the minimum is that fraction of the</span></span><br><span class="line">    <span class="comment"># number of training set samples)</span></span><br><span class="line">    <span class="string">'min_samples_leaf'</span>: [<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line">rf = RandomForestClassifier</span><br><span class="line"></span><br><span class="line">rf_mean_imputed, best_hyperparams_mean_imputed = holdout_grid_search(rf, X_train_mean_imputed, y_train,</span><br><span class="line">                                                                     X_val_mean_imputed, y_val,</span><br><span class="line">                                                                     hyperparams, &#123;<span class="string">'random_state'</span>: <span class="number">10</span>&#125;)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Performance for best hyperparameters:"</span>)</span><br><span class="line"></span><br><span class="line">y_train_best = rf_mean_imputed.predict_proba(X_train_mean_imputed)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Train C-Index: <span class="subst">&#123;cindex(y_train, y_train_best):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_val_best = rf_mean_imputed.predict_proba(X_val_mean_imputed)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Val C-Index: <span class="subst">&#123;cindex(y_val, y_val_best):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_test_imp = rf_mean_imputed.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Test C-Index: <span class="subst">&#123;cindex(y_test, y_test_imp):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[1/8] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.7395345453913784

[2/8] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7397907669057344

[3/8] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.7491450235484942

[4/8] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7481646505507676

[5/8] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.740106701061148

[6/8] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7403585798379725

[7/8] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.7496932941618408

[8/8] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7494088448535303

Performance for best hyperparameters:
- Train C-Index: 0.8137
- Val C-Index: 0.7497
- Test C-Index: 0.7819
</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h4><p>Note, your actual c-index values will vary depending on the hyper-parameters that you choose.  </p>
<ul>
<li>Try to get a good Test c-index, similar these numbers below:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Performance <span class="keyword">for</span> best hyperparameters:</span><br><span class="line">- Train C-Index: <span class="number">0.8109</span></span><br><span class="line">- Val C-Index: <span class="number">0.7495</span></span><br><span class="line">- Test C-Index: <span class="number">0.7805</span></span><br></pre></td></tr></table></figure>
<p>Next, we will apply another imputation strategy, known as <strong>multivariate feature imputation</strong>, using scikit-learn’s <code>IterativeImputer</code> class (see the <a href="https://scikit-learn.org/stable/modules/impute.html#iterative-imputer" target="_blank" rel="noopener">documentation</a>).</p>
<p>With this strategy, for each feature that is missing values, a regression model is trained to predict observed values based on all of the other features, and the missing values are inferred using this model.<br>As a single iteration across all features may not be enough to impute all missing values, several iterations may be performed, hence the name of the class <code>IterativeImputer</code>.</p>
<p>In the next cell, use <code>IterativeImputer</code> to perform multivariate feature imputation.</p>
<blockquote>
<p>Note that the first time the cell is run, <code>imputer.fit(X_train)</code> may fail with the message <code>LinAlgError: SVD did not converge</code>: simply re-run the cell.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Impute using regression on other covariates</span></span><br><span class="line">imputer = IterativeImputer(random_state=<span class="number">0</span>, sample_posterior=<span class="keyword">False</span>, max_iter=<span class="number">1</span>, min_value=<span class="number">0</span>)</span><br><span class="line">imputer.fit(X_train)</span><br><span class="line">X_train_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)</span><br><span class="line">X_val_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)</span><br></pre></td></tr></table></figure>
<p><a name="Ex-6"></a></p>
<h3 id="Exercise-6"><a href="#Exercise-6" class="headerlink" title="Exercise 6"></a>Exercise 6</h3><p>Perform a hyperparameter grid search to find the best-performing random forest model, and report results on the test set. Define the parameter ranges for the hyperparameter search in the next cell, and run the cell.</p>
<h4 id="Target-performance-1"><a href="#Target-performance-1" class="headerlink" title="Target performance"></a>Target performance</h4><p>Try to get a text c-index of at least 0.74 or higher.</p>
<p><details>    </details></p>
<p><summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary></p>
<p>
<ul>
    <li>n_estimators: try values greater than 100</li>
    <li>max_depth: try values in the range 1 to 100</li>
    <li>min_samples_leaf: try float values below .5 and/or int values greater than 2</li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define ranges for the random forest hyperparameter search </span></span><br><span class="line">hyperparams = &#123;</span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE array values with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># how many trees should be in the forest (int)</span></span><br><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">200</span>,<span class="number">500</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the maximum depth of trees in the forest (int)</span></span><br><span class="line">    <span class="string">'max_depth'</span>: [<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the minimum number of samples in a leaf as a fraction</span></span><br><span class="line">    <span class="comment"># of the total number of samples in the training set</span></span><br><span class="line">    <span class="comment"># Can be int (in which case that is the minimum number)</span></span><br><span class="line">    <span class="comment"># or float (in which case the minimum is that fraction of the</span></span><br><span class="line">    <span class="comment"># number of training set samples)</span></span><br><span class="line">    <span class="string">'min_samples_leaf'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line">rf = RandomForestClassifier</span><br><span class="line"></span><br><span class="line">rf_imputed, best_hyperparams_imputed = holdout_grid_search(rf, X_train_imputed, y_train,</span><br><span class="line">                                                           X_val_imputed, y_val,</span><br><span class="line">                                                           hyperparams, &#123;<span class="string">'random_state'</span>: <span class="number">10</span>&#125;)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Performance for best hyperparameters:"</span>)</span><br><span class="line"></span><br><span class="line">y_train_best = rf_imputed.predict_proba(X_train_imputed)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Train C-Index: <span class="subst">&#123;cindex(y_train, y_train_best):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_val_best = rf_imputed.predict_proba(X_val_imputed)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Val C-Index: <span class="subst">&#123;cindex(y_val, y_val_best):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_test_imp = rf_imputed.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Test C-Index: <span class="subst">&#123;cindex(y_test, y_test_imp):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[1/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.7354751714838482

[2/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7357596207921587

[3/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.7356792801478268

[4/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.745511237919047

[5/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7446752609442414

[6/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.7453787844243376

[7/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.7486206379915707

[8/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7506139545185099

[9/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.749875689138162

[10/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.73679102095588

[11/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.737142782695928

[12/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.737118897639505

[13/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.7460540801104792

[14/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7457783162772317

[15/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.746544809451534

[16/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 1}
Val C-Index: 0.7486162952540393

[17/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 2}
Val C-Index: 0.7508050349698939

[18/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 3}
Val C-Index: 0.7501840235028955

Performance for best hyperparameters:
- Train C-Index: 0.8774
- Val C-Index: 0.7508
- Test C-Index: 0.7834
</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h4><p>Note, your actual output will vary depending on the hyper-parameters that you chose and the mask that you chose.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Performance <span class="keyword">for</span> best hyperparameters:</span><br><span class="line">- Train C-Index: <span class="number">0.8131</span></span><br><span class="line">- Val C-Index: <span class="number">0.7454</span></span><br><span class="line">- Test C-Index: <span class="number">0.7797</span></span><br></pre></td></tr></table></figure></p>
<p><a name="10"></a></p>
<h2 id="10-Comparison"><a href="#10-Comparison" class="headerlink" title="10. Comparison"></a>10. Comparison</h2><p>For good measure, retest on the subgroup from before to see if your new models do better.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">performance, subgroup_size = bad_subset(best_rf, X_test, y_test)</span><br><span class="line">print(<span class="string">f"C-Index (no imputation): <span class="subst">&#123;performance&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">performance, subgroup_size = bad_subset(rf_mean_imputed, X_test, y_test)</span><br><span class="line">print(<span class="string">f"C-Index (mean imputation): <span class="subst">&#123;performance&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">performance, subgroup_size = bad_subset(rf_imputed, X_test, y_test)</span><br><span class="line">print(<span class="string">f"C-Index (multivariate feature imputation): <span class="subst">&#123;performance&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>C-Index (no imputation): 0.670638197475522
C-Index (mean imputation): 0.6847548267862058
C-Index (multivariate feature imputation): 0.6926978884039164
</code></pre><p>We should see that avoiding complete case analysis (i.e. analysis only on observations for which there is no missing data) allows our model to generalize a bit better. Remember to examine your missing cases to judge whether they are missing at random or not!</p>
<p><a name="11"></a></p>
<h2 id="11-Explanations-SHAP"><a href="#11-Explanations-SHAP" class="headerlink" title="11. Explanations: SHAP"></a>11. Explanations: SHAP</h2><p>Using a random forest has improved results, but we’ve lost some of the natural interpretability of trees. In this section we’ll try to explain the predictions using slightly more sophisticated techniques. </p>
<p>You choose to apply <strong>SHAP (SHapley Additive exPlanations) </strong>, a cutting edge method that explains predictions made by black-box machine learning models (i.e. models which are too complex to be understandable by humans as is).</p>
<blockquote>
<p>Given a prediction made by a machine learning model, SHAP values explain the prediction by quantifying the additive importance of each feature to the prediction. SHAP values have their roots in cooperative game theory, where Shapley values are used to quantify the contribution of each player to the game.</p>
<p>Although it is computationally expensive to compute SHAP values for general black-box models, in the case of trees and forests there exists a fast polynomial-time algorithm. For more details, see the <a href="https://arxiv.org/pdf/1802.03888.pdf" target="_blank" rel="noopener">TreeShap paper</a>.</p>
</blockquote>
<p>We’ll use the <a href="https://github.com/slundberg/shap" target="_blank" rel="noopener">shap library</a> to do this for our random forest model. Run the next cell to output the most at risk individuals in the test set according to our model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_test_risk = X_test.copy(deep=<span class="keyword">True</span>)</span><br><span class="line">X_test_risk.loc[:, <span class="string">'risk'</span>] = rf_imputed.predict_proba(X_test_risk)[:, <span class="number">1</span>]</span><br><span class="line">X_test_risk = X_test_risk.sort_values(by=<span class="string">'risk'</span>, ascending=<span class="keyword">False</span>)</span><br><span class="line">X_test_risk.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Diastolic BP</th>
      <th>Poverty index</th>
      <th>Race</th>
      <th>Red blood cells</th>
      <th>Sedimentation rate</th>
      <th>Serum Albumin</th>
      <th>Serum Cholesterol</th>
      <th>Serum Iron</th>
      <th>Serum Magnesium</th>
      <th>Serum Protein</th>
      <th>Sex</th>
      <th>Systolic BP</th>
      <th>TIBC</th>
      <th>TS</th>
      <th>White blood cells</th>
      <th>BMI</th>
      <th>Pulse pressure</th>
      <th>risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5493</th>
      <td>67.0</td>
      <td>80.0</td>
      <td>30.0</td>
      <td>1.0</td>
      <td>77.7</td>
      <td>59.0</td>
      <td>3.4</td>
      <td>231.0</td>
      <td>36.0</td>
      <td>1.40</td>
      <td>6.3</td>
      <td>1.0</td>
      <td>170.0</td>
      <td>202.0</td>
      <td>17.8</td>
      <td>8.4</td>
      <td>17.029470</td>
      <td>90.0</td>
      <td>0.689064</td>
    </tr>
    <tr>
      <th>6337</th>
      <td>69.0</td>
      <td>80.0</td>
      <td>233.0</td>
      <td>1.0</td>
      <td>77.7</td>
      <td>48.0</td>
      <td>4.2</td>
      <td>159.0</td>
      <td>87.0</td>
      <td>1.81</td>
      <td>6.9</td>
      <td>1.0</td>
      <td>146.0</td>
      <td>291.0</td>
      <td>29.9</td>
      <td>15.2</td>
      <td>17.931276</td>
      <td>66.0</td>
      <td>0.639300</td>
    </tr>
    <tr>
      <th>2044</th>
      <td>74.0</td>
      <td>80.0</td>
      <td>83.0</td>
      <td>1.0</td>
      <td>47.6</td>
      <td>19.0</td>
      <td>4.2</td>
      <td>205.0</td>
      <td>72.0</td>
      <td>1.71</td>
      <td>6.9</td>
      <td>1.0</td>
      <td>180.0</td>
      <td>310.0</td>
      <td>23.2</td>
      <td>10.8</td>
      <td>20.900101</td>
      <td>100.0</td>
      <td>0.582532</td>
    </tr>
    <tr>
      <th>1017</th>
      <td>65.0</td>
      <td>98.0</td>
      <td>16.0</td>
      <td>1.0</td>
      <td>49.4</td>
      <td>30.0</td>
      <td>3.4</td>
      <td>124.0</td>
      <td>129.0</td>
      <td>1.59</td>
      <td>7.7</td>
      <td>1.0</td>
      <td>184.0</td>
      <td>293.0</td>
      <td>44.0</td>
      <td>5.9</td>
      <td>30.858853</td>
      <td>86.0</td>
      <td>0.573548</td>
    </tr>
    <tr>
      <th>6609</th>
      <td>72.0</td>
      <td>90.0</td>
      <td>75.0</td>
      <td>1.0</td>
      <td>29.3</td>
      <td>59.0</td>
      <td>3.9</td>
      <td>216.0</td>
      <td>64.0</td>
      <td>1.63</td>
      <td>7.4</td>
      <td>2.0</td>
      <td>182.0</td>
      <td>322.0</td>
      <td>19.9</td>
      <td>9.3</td>
      <td>22.281793</td>
      <td>92.0</td>
      <td>0.565259</td>
    </tr>
  </tbody>
</table>
</div>



<p>We can use SHAP values to try and understand the model output on specific individuals using force plots. Run the cell below to see a force plot on the riskiest individual. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">explainer = shap.TreeExplainer(rf_imputed)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">shap_value = explainer.shap_values(X_test.loc[X_test_risk.index[i], :])[<span class="number">1</span>]</span><br><span class="line">shap.force_plot(explainer.expected_value[<span class="number">1</span>], shap_value, feature_names=X_test.columns, matplotlib=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_83_0.png" alt="png"></p>
<p>How to read this chart:</p>
<ul>
<li>The red sections on the left are features which push the model towards the final prediction in the positive direction (i.e. a higher Age increases the predicted risk).</li>
<li>The blue sections on the right are features that push the model towards the final prediction in the negative direction (if an increase in a feature leads to a lower risk, it will be shown in blue).</li>
<li>Note that the exact output of your chart will differ depending on the hyper-parameters that you choose for your model.</li>
</ul>
<p>We can also use SHAP values to understand the model output in aggregate. Run the next cell to initialize the SHAP values (this may take a few minutes).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap_values = shap.TreeExplainer(rf_imputed).shap_values(X_test)[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>Run the next cell to see a summary plot of the SHAP values for each feature on each of the test examples. The colors indicate the value of the feature.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.summary_plot(shap_values, X_test)</span><br></pre></td></tr></table></figure>
<p><img src="output_87_0.png" alt="png"></p>
<p>Clearly we see that being a woman (<code>sex = 2.0</code>, as opposed to men for which <code>sex = 1.0</code>) has a negative SHAP value, meaning that it reduces the risk of dying within 10 years. High age and high systolic blood pressure have positive SHAP values, and are therefore related to increased mortality. </p>
<p>You can see how features interact using dependence plots. These plot the SHAP value for a given feature for each data point, and color the points in using the value for another feature. This lets us begin to explain the variation in SHAP value for a single value of the main feature.</p>
<p>Run the next cell to see the interaction between Age and Sex.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.dependence_plot(<span class="string">'Age'</span>, shap_values, X_test, interaction_index=<span class="string">'Sex'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_89_0.png" alt="png"></p>
<p>We see that while Age &gt; 50 is generally bad (positive SHAP value), being a woman generally reduces the impact of age. This makes sense since we know that women generally live longer than men.</p>
<p>Let’s now look at poverty index and age.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.dependence_plot(<span class="string">'Poverty index'</span>, shap_values, X_test, interaction_index=<span class="string">'Age'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_91_0.png" alt="png"></p>
<p>We see that the impact of poverty index drops off quickly, and for higher income individuals age begins to explain much of variation in the impact of poverty index.</p>
<p>Try some other pairs and see what other interesting relationships you can find!</p>
<h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You have completed the second assignment in Course 2. Along the way you’ve learned to fit decision trees, random forests, and deal with missing data. Now you’re ready to move on to week 3!</p>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Medicine/" rel="tag"># Medicine</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Build-and-Evaluate-a-Linear-Risk-model/2020/04/18/" rel="next" title="Build and Evaluate a Linear Risk model">
                <i class="fa fa-chevron-left"></i> Build and Evaluate a Linear Risk model
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Survival-Estimates-that-Vary-with-Time/2020/04/18/" rel="prev" title="Survival Estimates that Vary with Time">
                Survival Estimates that Vary with Time <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate article here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Ruochi Zhang WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    

    
      <div id="bitcoin" style="display: inline-block">
        <img id="vemo_qr" src="/images/venmo.png" alt="Ruochi Zhang Bitcoin">
        <p>Venmo(last 4 digits 1570)</p>
      </div>
    

  </div>
</div>

      </div>
    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">251</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Risk-Models-Using-Tree-based-Models"><span class="nav-text">Risk Models Using Tree-based Models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Import-Packages"><span class="nav-text">1. Import Packages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Load-the-Dataset"><span class="nav-text">2. Load the Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Explore-the-Dataset"><span class="nav-text">3. Explore the Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Dealing-with-Missing-Data"><span class="nav-text">4. Dealing with Missing Data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-1"><span class="nav-text">Exercise 1</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Decision-Trees"><span class="nav-text">5. Decision Trees</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-2"><span class="nav-text">Exercise 2</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Random-Forests"><span class="nav-text">6. Random Forests</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-3"><span class="nav-text">Exercise 3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Imputation"><span class="nav-text">7. Imputation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Error-Analysis"><span class="nav-text">8. Error Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-4"><span class="nav-text">Exercise 4</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Test-Your-Work"><span class="nav-text">Test Your Work</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Expected-Output"><span class="nav-text">Expected Output</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-Imputation-Approaches"><span class="nav-text">9. Imputation Approaches</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-5"><span class="nav-text">Exercise 5</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Target-performance"><span class="nav-text">Target performance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Expected-output"><span class="nav-text">Expected output</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-6"><span class="nav-text">Exercise 6</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Target-performance-1"><span class="nav-text">Target performance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Expected-Output-1"><span class="nav-text">Expected Output</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-Comparison"><span class="nav-text">10. Comparison</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-Explanations-SHAP"><span class="nav-text">11. Explanations: SHAP</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Congratulations"><span class="nav-text">Congratulations!</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
