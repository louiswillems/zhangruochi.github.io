<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP,Project,Computer Vision,Transfer Learning,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="In this project you will define and train an image-to-caption model, that can produce descriptions for real world images!">
<meta name="keywords" content="NLP,Project,Computer Vision,Transfer Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Image Captioning Project">
<meta property="og:url" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="In this project you will define and train an image-to-caption model, that can produce descriptions for real world images!">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/encoder_decoder.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/inceptionv3.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_20_0.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/encoder_decoder_explained.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/flatten_help.jpg">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_60_1.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_1.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_3.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_5.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_7.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_9.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_11.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_13.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_15.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_17.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_61_19.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_63_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_64_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_65_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_66_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_67_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_68_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_69_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_70_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_71_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_72_2.png">
<meta property="og:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/output_73_2.png">
<meta property="og:updated_time" content="2020-01-09T08:31:32.575Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Image Captioning Project">
<meta name="twitter:description" content="In this project you will define and train an image-to-caption model, that can produce descriptions for real world images!">
<meta name="twitter:image" content="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/encoder_decoder.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/">





  <title>Image Captioning Project | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Image-Captioning-Project/2019/07/11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Image Captioning Project</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-11T03:49:56+08:00">
                2019-07-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Image-Captioning-Project/2019/07/11/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Image-Captioning-Project/2019/07/11/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          
              <div class="post-description">
                  In this project you will define and train an image-to-caption model, that can produce descriptions for real world images!
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Image-Captioning-Final-Project"><a href="#Image-Captioning-Final-Project" class="headerlink" title="Image Captioning Final Project"></a>Image Captioning Final Project</h1><p>In this final project you will define and train an image-to-caption model, that can produce descriptions for real world images!</p>
<p><img src="encoder_decoder.png" style="width:70%"></p>
<p>Model architecture: CNN encoder and RNN decoder.<br>(<a href="https://research.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html" target="_blank" rel="noopener">https://research.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html</a>)</p>
<h1 id="Import-stuff"><a href="#Import-stuff" class="headerlink" title="Import stuff"></a>Import stuff</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>)</span><br><span class="line"><span class="keyword">import</span> grading</span><br><span class="line"><span class="keyword">import</span> download_utils</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">download_utils.link_all_keras_resources()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">L = keras.layers</span><br><span class="line">K = keras.backend</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> choice</span><br><span class="line"><span class="keyword">import</span> grading_utils</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> keras_utils <span class="keyword">import</span> reset_tf_session</span><br><span class="line"><span class="keyword">import</span> tqdm_utils</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><h1 id="Prepare-the-storage-for-model-checkpoints"><a href="#Prepare-the-storage-for-model-checkpoints" class="headerlink" title="Prepare the storage for model checkpoints"></a>Prepare the storage for model checkpoints</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Leave USE_GOOGLE_DRIVE = False if you're running locally!</span></span><br><span class="line"><span class="comment"># We recommend to set USE_GOOGLE_DRIVE = True in Google Colab!</span></span><br><span class="line"><span class="comment"># If set to True, we will mount Google Drive, so that you can restore your checkpoint </span></span><br><span class="line"><span class="comment"># and continue trainig even if your previous Colab session dies.</span></span><br><span class="line"><span class="comment"># If set to True, follow on-screen instructions to access Google Drive (you must have a Google account).</span></span><br><span class="line">USE_GOOGLE_DRIVE = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mount_google_drive</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">    mount_directory = <span class="string">"/content/gdrive"</span></span><br><span class="line">    drive.mount(mount_directory)</span><br><span class="line">    drive_root = mount_directory + <span class="string">"/"</span> + list(filter(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] != <span class="string">'.'</span>, os.listdir(mount_directory)))[<span class="number">0</span>] + <span class="string">"/colab"</span></span><br><span class="line">    <span class="keyword">return</span> drive_root</span><br><span class="line"></span><br><span class="line">CHECKPOINT_ROOT = <span class="string">""</span></span><br><span class="line"><span class="keyword">if</span> USE_GOOGLE_DRIVE:</span><br><span class="line">    CHECKPOINT_ROOT = mount_google_drive() + <span class="string">"/"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_checkpoint_path</span><span class="params">(epoch=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> epoch <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> os.path.abspath(CHECKPOINT_ROOT + <span class="string">"weights"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> os.path.abspath(CHECKPOINT_ROOT + <span class="string">"weights_&#123;&#125;"</span>.format(epoch))</span><br><span class="line">      </span><br><span class="line"><span class="comment"># example of checkpoint dir</span></span><br><span class="line">print(get_checkpoint_path(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<pre><code>/root/intro-to-dl/week6/weights_10
</code></pre><h1 id="Fill-in-your-Coursera-token-and-email"><a href="#Fill-in-your-Coursera-token-and-email" class="headerlink" title="Fill in your Coursera token and email"></a>Fill in your Coursera token and email</h1><p>To successfully submit your answers to our grader, please fill in your Coursera submission token and email</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grader = grading.Grader(assignment_key=<span class="string">"NEDBg6CgEee8nQ6uE8a7OA"</span>, </span><br><span class="line">                        all_parts=[<span class="string">"19Wpv"</span>, <span class="string">"uJh73"</span>, <span class="string">"yiJkt"</span>, <span class="string">"rbpnH"</span>, <span class="string">"E2OIL"</span>, <span class="string">"YJR7z"</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># token expires every 30 min</span></span><br><span class="line">COURSERA_TOKEN = <span class="string">""</span></span><br><span class="line">COURSERA_EMAIL = <span class="string">""</span></span><br></pre></td></tr></table></figure>
<h1 id="Download-data"><a href="#Download-data" class="headerlink" title="Download data"></a>Download data</h1><p>Takes 10 hours and 20 GB. We’ve downloaded necessary files for you.</p>
<p>Relevant links (just in case):</p>
<ul>
<li>train images <a href="http://msvocds.blob.core.windows.net/coco2014/train2014.zip" target="_blank" rel="noopener">http://msvocds.blob.core.windows.net/coco2014/train2014.zip</a></li>
<li>validation images <a href="http://msvocds.blob.core.windows.net/coco2014/val2014.zip" target="_blank" rel="noopener">http://msvocds.blob.core.windows.net/coco2014/val2014.zip</a></li>
<li>captions for both train and validation <a href="http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip" target="_blank" rel="noopener">http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we downloaded them for you, just link them here</span></span><br><span class="line">download_utils.link_week_6_resources()</span><br></pre></td></tr></table></figure>
<h1 id="Extract-image-features"><a href="#Extract-image-features" class="headerlink" title="Extract image features"></a>Extract image features</h1><p>We will use pre-trained InceptionV3 model for CNN encoder (<a href="https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html" target="_blank" rel="noopener">https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html</a>) and extract its last hidden layer as an embedding:</p>
<p><img src="inceptionv3.png" style="width:70%"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMG_SIZE = <span class="number">299</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we take the last hidden layer of IncetionV3 as an image embedding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cnn_encoder</span><span class="params">()</span>:</span></span><br><span class="line">    K.set_learning_phase(<span class="keyword">False</span>)</span><br><span class="line">    model = keras.applications.InceptionV3(include_top=<span class="keyword">False</span>)</span><br><span class="line">    preprocess_for_model = keras.applications.inception_v3.preprocess_input</span><br><span class="line"></span><br><span class="line">    model = keras.models.Model(model.inputs, keras.layers.GlobalAveragePooling2D()(model.output))</span><br><span class="line">    <span class="keyword">return</span> model, preprocess_for_model</span><br></pre></td></tr></table></figure>
<p>Features extraction takes too much time on CPU:</p>
<ul>
<li>Takes 16 minutes on GPU.</li>
<li>25x slower (InceptionV3) on CPU and takes 7 hours.</li>
<li>10x slower (MobileNet) on CPU and takes 3 hours.</li>
</ul>
<p>So we’ve done it for you with the following code:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load pre-trained model</span></span><br><span class="line">reset_tf_session()</span><br><span class="line">encoder, preprocess_for_model = get_cnn_encoder()</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract train features</span></span><br><span class="line">train_img_embeds, train_img_fns = utils.apply_model(</span><br><span class="line">    <span class="string">"train2014.zip"</span>, encoder, preprocess_for_model, input_shape=(IMG_SIZE, IMG_SIZE))</span><br><span class="line">utils.save_pickle(train_img_embeds, <span class="string">"train_img_embeds.pickle"</span>)</span><br><span class="line">utils.save_pickle(train_img_fns, <span class="string">"train_img_fns.pickle"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract validation features</span></span><br><span class="line">val_img_embeds, val_img_fns = utils.apply_model(</span><br><span class="line">    <span class="string">"val2014.zip"</span>, encoder, preprocess_for_model, input_shape=(IMG_SIZE, IMG_SIZE))</span><br><span class="line">utils.save_pickle(val_img_embeds, <span class="string">"val_img_embeds.pickle"</span>)</span><br><span class="line">utils.save_pickle(val_img_fns, <span class="string">"val_img_fns.pickle"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sample images for learners</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_zip</span><span class="params">(fn_in, fn_out, rate=<span class="number">0.01</span>, seed=<span class="number">42</span>)</span>:</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(fn_in) <span class="keyword">as</span> fin, zipfile.ZipFile(fn_out, <span class="string">"w"</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        sampled = filter(<span class="keyword">lambda</span> _: np.random.rand() &lt; rate, fin.filelist)</span><br><span class="line">        <span class="keyword">for</span> zInfo <span class="keyword">in</span> sampled:</span><br><span class="line">            fout.writestr(zInfo, fin.read(zInfo))</span><br><span class="line">            </span><br><span class="line">sample_zip(<span class="string">"train2014.zip"</span>, <span class="string">"train2014_sample.zip"</span>)</span><br><span class="line">sample_zip(<span class="string">"val2014.zip"</span>, <span class="string">"val2014_sample.zip"</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load prepared embeddings</span></span><br><span class="line">train_img_embeds = utils.read_pickle(<span class="string">"train_img_embeds.pickle"</span>)</span><br><span class="line">train_img_fns = utils.read_pickle(<span class="string">"train_img_fns.pickle"</span>)</span><br><span class="line">val_img_embeds = utils.read_pickle(<span class="string">"val_img_embeds.pickle"</span>)</span><br><span class="line">val_img_fns = utils.read_pickle(<span class="string">"val_img_fns.pickle"</span>)</span><br><span class="line"><span class="comment"># check shapes</span></span><br><span class="line">print(train_img_embeds.shape, len(train_img_fns))</span><br><span class="line">print(val_img_embeds.shape, len(val_img_fns))</span><br></pre></td></tr></table></figure>
<pre><code>(82783, 2048) 82783
(40504, 2048) 40504
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check prepared samples of images</span></span><br><span class="line">list(filter(<span class="keyword">lambda</span> x: x.endswith(<span class="string">"_sample.zip"</span>), os.listdir(<span class="string">"."</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;val2014_sample.zip&#39;, &#39;train2014_sample.zip&#39;]
</code></pre><h1 id="Extract-captions-for-images"><a href="#Extract-captions-for-images" class="headerlink" title="Extract captions for images"></a>Extract captions for images</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># extract captions from zip</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_captions_for_fns</span><span class="params">(fns, zip_fn, zip_json_path)</span>:</span></span><br><span class="line">    zf = zipfile.ZipFile(zip_fn)</span><br><span class="line">    j = json.loads(zf.read(zip_json_path).decode(<span class="string">"utf8"</span>))</span><br><span class="line">    id_to_fn = &#123;img[<span class="string">"id"</span>]: img[<span class="string">"file_name"</span>] <span class="keyword">for</span> img <span class="keyword">in</span> j[<span class="string">"images"</span>]&#125;</span><br><span class="line">    fn_to_caps = defaultdict(list)</span><br><span class="line">    <span class="keyword">for</span> cap <span class="keyword">in</span> j[<span class="string">'annotations'</span>]:</span><br><span class="line">        fn_to_caps[id_to_fn[cap[<span class="string">'image_id'</span>]]].append(cap[<span class="string">'caption'</span>])</span><br><span class="line">    fn_to_caps = dict(fn_to_caps)</span><br><span class="line">    <span class="keyword">return</span> list(map(<span class="keyword">lambda</span> x: fn_to_caps[x], fns))</span><br><span class="line">    </span><br><span class="line">train_captions = get_captions_for_fns(train_img_fns, <span class="string">"captions_train-val2014.zip"</span>, </span><br><span class="line">                                      <span class="string">"annotations/captions_train2014.json"</span>)</span><br><span class="line"></span><br><span class="line">val_captions = get_captions_for_fns(val_img_fns, <span class="string">"captions_train-val2014.zip"</span>, </span><br><span class="line">                                      <span class="string">"annotations/captions_val2014.json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check shape</span></span><br><span class="line">print(len(train_img_fns), len(train_captions))</span><br><span class="line">print(len(val_img_fns), len(val_captions))</span><br></pre></td></tr></table></figure>
<pre><code>82783 82783
40504 40504
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># look at training example (each has 5 captions)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trainig_example</span><span class="params">(train_img_fns, train_captions, example_idx=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    You can change example_idx and see different images</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    zf = zipfile.ZipFile(<span class="string">"train2014_sample.zip"</span>)</span><br><span class="line">    captions_by_file = dict(zip(train_img_fns, train_captions))</span><br><span class="line">    all_files = set(train_img_fns)</span><br><span class="line">    found_files = list(filter(<span class="keyword">lambda</span> x: x.filename.rsplit(<span class="string">"/"</span>)[<span class="number">-1</span>] <span class="keyword">in</span> all_files, zf.filelist))</span><br><span class="line">    example = found_files[example_idx]</span><br><span class="line">    img = utils.decode_image_from_buf(zf.read(example))</span><br><span class="line">    plt.imshow(utils.image_center_crop(img))</span><br><span class="line">    plt.title(<span class="string">"\n"</span>.join(captions_by_file[example.filename.rsplit(<span class="string">"/"</span>)[<span class="number">-1</span>]]))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">show_trainig_example(train_img_fns, train_captions, example_idx=<span class="number">142</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_20_0.png" alt="png"></p>
<h1 id="Prepare-captions-for-training"><a href="#Prepare-captions-for-training" class="headerlink" title="Prepare captions for training"></a>Prepare captions for training</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># preview captions data</span></span><br><span class="line">train_captions[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[[&#39;A long dirt road going through a forest.&#39;,
  &#39;A SCENE OF WATER AND A PATH WAY&#39;,
  &#39;A sandy path surrounded by trees leads to a beach.&#39;,
  &#39;Ocean view through a dirt road surrounded by a forested area. &#39;,
  &#39;dirt path leading beneath barren trees to open plains&#39;],
 [&#39;A group of zebra standing next to each other.&#39;,
  &#39;This is an image of of zebras drinking&#39;,
  &#39;ZEBRAS AND BIRDS SHARING THE SAME WATERING HOLE&#39;,
  &#39;Zebras that are bent over and drinking water together.&#39;,
  &#39;a number of zebras drinking water near one another&#39;]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># special tokens</span></span><br><span class="line">PAD = <span class="string">"#PAD#"</span></span><br><span class="line">UNK = <span class="string">"#UNK#"</span></span><br><span class="line">START = <span class="string">"#START#"</span></span><br><span class="line">END = <span class="string">"#END#"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># split sentence into tokens (split into lowercased words)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_sentence</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> list(filter(<span class="keyword">lambda</span> x: len(x) &gt; <span class="number">0</span>, re.split(<span class="string">'\W+'</span>, sentence.lower())))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_vocabulary</span><span class="params">(train_captions)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return &#123;token: index&#125; for all train tokens (words) that occur 5 times or more, </span></span><br><span class="line"><span class="string">        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.</span></span><br><span class="line"><span class="string">    Use `split_sentence` function to split sentence into tokens.</span></span><br><span class="line"><span class="string">    Also, add PAD (for batch padding), UNK (unknown, out of vocabulary), </span></span><br><span class="line"><span class="string">        START (start of sentence) and END (end of sentence) tokens into the vocabulary.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    counter = Counter()</span><br><span class="line">    <span class="keyword">for</span> captions <span class="keyword">in</span> train_captions:</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> captions:</span><br><span class="line">            counter.update(split_sentence(sentence))</span><br><span class="line">    vocab = &#123;item <span class="keyword">for</span> item,count <span class="keyword">in</span> counter.items() <span class="keyword">if</span> count &gt;= <span class="number">5</span>&#125;</span><br><span class="line">    vocab = vocab.union(&#123;PAD,UNK,START,END&#125;)</span><br><span class="line">    <span class="keyword">return</span> &#123;token: index <span class="keyword">for</span> index, token <span class="keyword">in</span> enumerate(sorted(vocab))&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">caption_tokens_to_indices</span><span class="params">(captions, vocab)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    `captions` argument is an array of arrays:</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">        [</span></span><br><span class="line"><span class="string">            "image1 caption1",</span></span><br><span class="line"><span class="string">            "image1 caption2",</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">        ],</span></span><br><span class="line"><span class="string">        [</span></span><br><span class="line"><span class="string">            "image2 caption1",</span></span><br><span class="line"><span class="string">            "image2 caption2",</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">        ],</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">    Use `split_sentence` function to split sentence into tokens.</span></span><br><span class="line"><span class="string">    Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).</span></span><br><span class="line"><span class="string">    Add START and END tokens to start and end of each sentence respectively.</span></span><br><span class="line"><span class="string">    For the example above you should produce the following:</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">        [</span></span><br><span class="line"><span class="string">            [vocab[START], vocab["image1"], vocab["caption1"], vocab[END]],</span></span><br><span class="line"><span class="string">            [vocab[START], vocab["image1"], vocab["caption2"], vocab[END]],</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">        ],</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    res = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> img_captions <span class="keyword">in</span> captions:</span><br><span class="line">        img_indx_cations = []</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> img_captions:</span><br><span class="line">            indx_sentence = [vocab[START]] + [vocab.get(token,vocab[UNK]) <span class="keyword">for</span> token <span class="keyword">in</span> split_sentence(sentence)] + [vocab[END]]</span><br><span class="line">            img_indx_cations.append(indx_sentence)</span><br><span class="line">        res.append(img_indx_cations)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prepare vocabulary</span></span><br><span class="line"></span><br><span class="line">vocab = generate_vocabulary(train_captions)</span><br><span class="line">vocab_inverse = &#123;idx: w <span class="keyword">for</span> w, idx <span class="keyword">in</span> vocab.items()&#125;</span><br><span class="line">print(len(vocab))</span><br></pre></td></tr></table></figure>
<pre><code>8769
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># replace tokens with indices</span></span><br><span class="line">train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)</span><br><span class="line">val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)</span><br></pre></td></tr></table></figure>
<p>Captions have different length, but we need to batch them, that’s why we will add PAD tokens so that all sentences have an equal length. </p>
<p>We will crunch LSTM through all the tokens, but we will ignore padding tokens during loss calculation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we will use this during training</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_captions_to_matrix</span><span class="params">(batch_captions, pad_idx, max_len=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    `batch_captions` is an array of arrays:</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">        [vocab[START], ..., vocab[END]],</span></span><br><span class="line"><span class="string">        [vocab[START], ..., vocab[END]],</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">    Put vocabulary indexed captions into np.array of shape (len(batch_captions), columns),</span></span><br><span class="line"><span class="string">        where "columns" is max(map(len, batch_captions)) when max_len is None</span></span><br><span class="line"><span class="string">        and "columns" = min(max_len, max(map(len, batch_captions))) otherwise.</span></span><br><span class="line"><span class="string">    Add padding with pad_idx where necessary.</span></span><br><span class="line"><span class="string">    Input example: [[1, 2, 3], [4, 5]]</span></span><br><span class="line"><span class="string">    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=None</span></span><br><span class="line"><span class="string">    Output example: np.array([[1, 2], [4, 5]]) if max_len=2</span></span><br><span class="line"><span class="string">    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=100</span></span><br><span class="line"><span class="string">    Try to use numpy, we need this function to be fast!</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cap_max = max(map(len,batch_captions))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> max_len:</span><br><span class="line">        max_len = cap_max</span><br><span class="line">    <span class="keyword">elif</span> max_len &lt; cap_max:</span><br><span class="line">        max_len = max_len</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        max_len = cap_max</span><br><span class="line">    </span><br><span class="line">    matrix = np.empty([len(batch_captions),max_len])</span><br><span class="line">    matrix.fill(pad_idx)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> index,line <span class="keyword">in</span> enumerate(batch_captions):</span><br><span class="line">        matrix[index,:len(line)] = line[<span class="number">0</span>:max_len]</span><br><span class="line">        matrix[index,len(line):] = pad_idx</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> matrix</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></span><br><span class="line"><span class="comment"># Vocabulary creation</span></span><br><span class="line">grader.set_answer(<span class="string">"19Wpv"</span>, grading_utils.test_vocab(vocab, PAD, UNK, START, END))</span><br><span class="line"><span class="comment"># Captions indexing</span></span><br><span class="line">grader.set_answer(<span class="string">"uJh73"</span>, grading_utils.test_captions_indexing(train_captions_indexed, vocab, UNK))</span><br><span class="line"><span class="comment"># Captions batching</span></span><br><span class="line">grader.set_answer(<span class="string">"yiJkt"</span>, grading_utils.test_captions_batching(batch_captions_to_matrix))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># you can make submission with answers so far to check yourself at this stage</span></span><br><span class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</span><br></pre></td></tr></table></figure>
<pre><code>Submitted to Coursera platform. See results on assignment page!
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make sure you use correct argument in caption_tokens_to_indices</span></span><br><span class="line"><span class="keyword">assert</span> len(caption_tokens_to_indices(train_captions[:<span class="number">10</span>], vocab)) == <span class="number">10</span></span><br><span class="line"><span class="keyword">assert</span> len(caption_tokens_to_indices(train_captions[:<span class="number">5</span>], vocab)) == <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Define-architecture"><a href="#Define-architecture" class="headerlink" title="Define architecture"></a>Define architecture</h2><p>Since our problem is to generate image captions, RNN text generator should be conditioned on image. The idea is to use image features as an initial state for RNN instead of zeros. </p>
<p>Remember that you should transform image feature vector to RNN hidden state size by fully-connected layer and then pass it to RNN.</p>
<p>During training we will feed ground truth tokens into the lstm to get predictions of next tokens. </p>
<p>Notice that we don’t need to feed last token (END) as input (<a href="http://cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener">http://cs.stanford.edu/people/karpathy/</a>):</p>
<p><img src="encoder_decoder_explained.png" style="width:50%"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">IMG_EMBED_SIZE = train_img_embeds.shape[<span class="number">1</span>]</span><br><span class="line">IMG_EMBED_BOTTLENECK = <span class="number">120</span></span><br><span class="line">WORD_EMBED_SIZE = <span class="number">100</span></span><br><span class="line">LSTM_UNITS = <span class="number">300</span></span><br><span class="line">LOGIT_BOTTLENECK = <span class="number">120</span></span><br><span class="line">pad_idx = vocab[PAD]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMG_EMBED_SIZE,pad_idx,LOGIT_BOTTLENECK</span><br></pre></td></tr></table></figure>
<pre><code>(2048, 1, 120)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># remember to reset your graph if you want to start building it from scratch!</span></span><br><span class="line">s = reset_tf_session()</span><br><span class="line">tf.set_random_seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<p>Here we define decoder graph.</p>
<p>We use Keras layers where possible because we can use them in functional style with weights reuse like this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dense_layer = L.Dense(<span class="number">42</span>, input_shape=(<span class="keyword">None</span>, <span class="number">100</span>) activation=<span class="string">'relu'</span>)</span><br><span class="line">a = tf.placeholder(<span class="string">'float32'</span>, [<span class="keyword">None</span>, <span class="number">100</span>])</span><br><span class="line">b = tf.placeholder(<span class="string">'float32'</span>, [<span class="keyword">None</span>, <span class="number">100</span>])</span><br><span class="line">dense_layer(a)  <span class="comment"># that's how we applied dense layer!</span></span><br><span class="line">dense_layer(b)  <span class="comment"># and again</span></span><br></pre></td></tr></table></figure></p>
<p>Here’s a figure to help you with flattening in decoder:<br><img src="flatten_help.jpg" style="width:80%"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">decoder</span>:</span></span><br><span class="line">    <span class="comment"># [batch_size, IMG_EMBED_SIZE] of CNN image features</span></span><br><span class="line">    img_embeds = tf.placeholder(<span class="string">'float32'</span>, [<span class="keyword">None</span>, IMG_EMBED_SIZE])</span><br><span class="line">    <span class="comment"># [batch_size, time steps] of word ids</span></span><br><span class="line">    sentences = tf.placeholder(<span class="string">'int32'</span>, [<span class="keyword">None</span>, <span class="keyword">None</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># we use bottleneck here to reduce the number of parameters</span></span><br><span class="line">    <span class="comment"># image embedding -&gt; bottleneck</span></span><br><span class="line">    img_embed_to_bottleneck = L.Dense(IMG_EMBED_BOTTLENECK, </span><br><span class="line">                                      input_shape=(<span class="keyword">None</span>, IMG_EMBED_SIZE), </span><br><span class="line">                                      activation=<span class="string">'elu'</span>)</span><br><span class="line">    <span class="comment"># image embedding bottleneck -&gt; lstm initial state</span></span><br><span class="line">    img_embed_bottleneck_to_h0 = L.Dense(LSTM_UNITS,</span><br><span class="line">                                         input_shape=(<span class="keyword">None</span>, IMG_EMBED_BOTTLENECK),</span><br><span class="line">                                         activation=<span class="string">'elu'</span>)</span><br><span class="line">    <span class="comment"># word -&gt; embedding</span></span><br><span class="line">    word_embed = L.Embedding(len(vocab), WORD_EMBED_SIZE)</span><br><span class="line">    <span class="comment"># lstm cell (from tensorflow)</span></span><br><span class="line">    lstm = tf.nn.rnn_cell.LSTMCell(LSTM_UNITS)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># we use bottleneck here to reduce model complexity</span></span><br><span class="line">    <span class="comment"># lstm output -&gt; logits bottleneck</span></span><br><span class="line">    token_logits_bottleneck = L.Dense(LOGIT_BOTTLENECK, </span><br><span class="line">                                      input_shape=(<span class="keyword">None</span>, LSTM_UNITS),</span><br><span class="line">                                      activation=<span class="string">"elu"</span>)</span><br><span class="line">    <span class="comment"># logits bottleneck -&gt; logits for next token prediction</span></span><br><span class="line">    token_logits = L.Dense(len(vocab),</span><br><span class="line">                           input_shape=(<span class="keyword">None</span>, LOGIT_BOTTLENECK))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initial lstm cell state of shape (None, LSTM_UNITS),</span></span><br><span class="line">    <span class="comment"># we need to condition it on `img_embeds` placeholder.</span></span><br><span class="line">    c0 = h0 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># embed all tokens but the last for lstm input,</span></span><br><span class="line">    <span class="comment"># remember that L.Embedding is callable,</span></span><br><span class="line">    <span class="comment"># use `sentences` placeholder as input.</span></span><br><span class="line">    word_embeds = word_embed(sentences[:,:<span class="number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># during training we use ground truth tokens `word_embeds` as context for next token prediction.</span></span><br><span class="line">    <span class="comment"># that means that we know all the inputs for our lstm and can get </span></span><br><span class="line">    <span class="comment"># all the hidden states with one tensorflow operation (tf.nn.dynamic_rnn).</span></span><br><span class="line">    <span class="comment"># `hidden_states` has a shape of [batch_size, time steps, LSTM_UNITS].</span></span><br><span class="line">    hidden_states, _ = tf.nn.dynamic_rnn(lstm, word_embeds,</span><br><span class="line">                                         initial_state=tf.nn.rnn_cell.LSTMStateTuple(c0, h0))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># now we need to calculate token logits for all the hidden states</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># first, we reshape `hidden_states` to [-1, LSTM_UNITS]</span></span><br><span class="line">    flat_hidden_states = tf.reshape(hidden_states, [<span class="number">-1</span>, LSTM_UNITS])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># then, we calculate logits for next tokens using `token_logits_bottleneck` and `token_logits` layers</span></span><br><span class="line">    flat_token_logits = token_logits(token_logits_bottleneck(flat_hidden_states))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># then, we flatten the ground truth token ids.</span></span><br><span class="line">    <span class="comment"># remember, that we predict next tokens for each time step,</span></span><br><span class="line">    <span class="comment"># use `sentences` placeholder.</span></span><br><span class="line">    flat_ground_truth = tf.reshape(sentences[:, <span class="number">1</span>:], [<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># we need to know where we have real tokens (not padding) in `flat_ground_truth`,</span></span><br><span class="line">    <span class="comment"># we don't want to propagate the loss for padded output tokens,</span></span><br><span class="line">    <span class="comment"># fill `flat_loss_mask` with 1.0 for real tokens (not pad_idx) and 0.0 otherwise.</span></span><br><span class="line">    flat_loss_mask = tf.not_equal(flat_ground_truth, pad_idx)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute cross-entropy between `flat_ground_truth` and `flat_token_logits` predicted by lstm</span></span><br><span class="line">    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        labels=flat_ground_truth, </span><br><span class="line">        logits=flat_token_logits</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute average `xent` over tokens with nonzero `flat_loss_mask`.</span></span><br><span class="line">    <span class="comment"># we don't want to account misclassification of PAD tokens, because that doesn't make sense,</span></span><br><span class="line">    <span class="comment"># we have PAD tokens for batching purposes only!</span></span><br><span class="line">    loss = tf.reduce_mean(tf.boolean_mask(xent, flat_loss_mask))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define optimizer operation to minimize the loss</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">train_step = optimizer.minimize(decoder.loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># will be used to save/load network weights.</span></span><br><span class="line"><span class="comment"># you need to reset your default graph and define it in the same way to be able to load the saved weights!</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="comment"># intialize all variables</span></span><br><span class="line">s.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  &quot;Converting sparse IndexedSlices to a dense Tensor of unknown shape. &quot;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></span><br><span class="line"><span class="comment"># Decoder shapes test</span></span><br><span class="line">grader.set_answer(<span class="string">"rbpnH"</span>, grading_utils.test_decoder_shapes(decoder, IMG_EMBED_SIZE, vocab, s))</span><br><span class="line"><span class="comment"># Decoder random loss test</span></span><br><span class="line">grader.set_answer(<span class="string">"E2OIL"</span>, grading_utils.test_random_decoder_loss(decoder, IMG_EMBED_SIZE, vocab, s))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># you can make submission with answers so far to check yourself at this stage</span></span><br><span class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</span><br></pre></td></tr></table></figure>
<pre><code>Submitted to Coursera platform. See results on assignment page!
</code></pre><h2 id="Training-loop"><a href="#Training-loop" class="headerlink" title="Training loop"></a>Training loop</h2><p>Evaluate train and validation metrics through training and log them. Ensure that loss decreases.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_captions_indexed = np.array(train_captions_indexed)</span><br><span class="line">val_captions_indexed = np.array(val_captions_indexed)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate batch via random sampling of images and captions for them,</span></span><br><span class="line"><span class="comment"># we use `max_len` parameter to control the length of the captions (truncating long captions)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(images_embeddings, indexed_captions, batch_size, max_len=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    `images_embeddings` is a np.array of shape [number of images, IMG_EMBED_SIZE].</span></span><br><span class="line"><span class="string">    `indexed_captions` holds 5 vocabulary indexed captions for each image:</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">        [</span></span><br><span class="line"><span class="string">            [vocab[START], vocab["image1"], vocab["caption1"], vocab[END]],</span></span><br><span class="line"><span class="string">            [vocab[START], vocab["image1"], vocab["caption2"], vocab[END]],</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">        ],</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">    Generate a random batch of size `batch_size`.</span></span><br><span class="line"><span class="string">    Take random images and choose one random caption for each image.</span></span><br><span class="line"><span class="string">    Remember to use `batch_captions_to_matrix` for padding and respect `max_len` parameter.</span></span><br><span class="line"><span class="string">    Return feed dict &#123;decoder.img_embeds: ..., decoder.sentences: ...&#125;.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    indexs = [random.randint(<span class="number">0</span>,len(images_embeddings)<span class="number">-1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size)]</span><br><span class="line">    batch_image_embeddings = images_embeddings[indexs]</span><br><span class="line">    batch_captions = [caption[np.random.randint(<span class="number">5</span>)] <span class="keyword">for</span> caption <span class="keyword">in</span> indexed_captions[indexs]]</span><br><span class="line">    </span><br><span class="line">    batch_captions_matrix = batch_captions_to_matrix(batch_captions,pad_idx, max_len)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;decoder.img_embeds: batch_image_embeddings, </span><br><span class="line">            decoder.sentences: batch_captions_matrix&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">n_epochs = <span class="number">12</span></span><br><span class="line">n_batches_per_epoch = <span class="number">1000</span></span><br><span class="line">n_validation_batches = <span class="number">100</span>  <span class="comment"># how many batches are used for validation after each epoch</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># you can load trained weights here</span></span><br><span class="line"><span class="comment"># uncomment the next line if you need to load weights</span></span><br><span class="line"><span class="comment"># saver.restore(s, get_checkpoint_path(epoch=4))</span></span><br></pre></td></tr></table></figure>
<p>Look at the training and validation loss, they should be decreasing!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_img_embeds.shape,train_captions_indexed.shape</span><br></pre></td></tr></table></figure>
<pre><code>((82783, 2048), (82783,))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># actual training loop</span></span><br><span class="line">MAX_LEN = <span class="number">20</span>  <span class="comment"># truncate long captions to speed up training</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># to make training reproducible</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    </span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    pbar = tqdm_utils.tqdm_notebook_failsafe(range(n_batches_per_epoch))</span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> pbar:</span><br><span class="line">        train_loss += s.run([decoder.loss, train_step], </span><br><span class="line">                            generate_batch(train_img_embeds, </span><br><span class="line">                                           train_captions_indexed, </span><br><span class="line">                                           batch_size, </span><br><span class="line">                                           MAX_LEN))[<span class="number">0</span>]</span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">        pbar.set_description(<span class="string">"Training loss: %f"</span> % (train_loss / counter))</span><br><span class="line">        </span><br><span class="line">    train_loss /= n_batches_per_epoch</span><br><span class="line">    </span><br><span class="line">    val_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_validation_batches):</span><br><span class="line">        val_loss += s.run(decoder.loss, generate_batch(val_img_embeds,</span><br><span class="line">                                                       val_captions_indexed, </span><br><span class="line">                                                       batch_size, </span><br><span class="line">                                                       MAX_LEN))</span><br><span class="line">    val_loss /= n_validation_batches</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'Epoch: &#123;&#125;, train loss: &#123;&#125;, val loss: &#123;&#125;'</span>.format(epoch, train_loss, val_loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># save weights after finishing epoch</span></span><br><span class="line">    saver.save(s, get_checkpoint_path(epoch))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">"Finished!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))




Epoch: 0, train loss: 3.0007614777088167, val loss: 2.9724034023284913



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 1, train loss: 2.8531791372299193, val loss: 2.9006982970237734



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 2, train loss: 2.7954050121307374, val loss: 2.8111998438835144



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 3, train loss: 2.730731366157532, val loss: 2.750483591556549



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 4, train loss: 2.6690069699287413, val loss: 2.749560286998749



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 5, train loss: 2.633123325586319, val loss: 2.7148624300956725



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 6, train loss: 2.5939396080970765, val loss: 2.6811715364456177



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 7, train loss: 2.574599018335342, val loss: 2.6403690791130066



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 8, train loss: 2.546513616323471, val loss: 2.627152864933014



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 9, train loss: 2.5285718023777006, val loss: 2.6443107414245604



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 10, train loss: 2.4949201991558074, val loss: 2.6084690499305725



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



Epoch: 11, train loss: 2.478545124053955, val loss: 2.594680278301239
Finished!
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></span><br><span class="line"><span class="comment"># Validation loss</span></span><br><span class="line">grader.set_answer(<span class="string">"YJR7z"</span>, grading_utils.test_validation_loss(</span><br><span class="line">    decoder, s, generate_batch, val_img_embeds, val_captions_indexed))</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># you can make submission with answers so far to check yourself at this stage</span></span><br><span class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</span><br></pre></td></tr></table></figure>
<pre><code>Submitted to Coursera platform. See results on assignment page!
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check that it's learnt something, outputs accuracy of next word prediction (should be around 0.5)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, log_loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_sentence</span><span class="params">(sentence_indices)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">" "</span>.join(list(map(vocab_inverse.get, sentence_indices)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_after_training</span><span class="params">(n_examples)</span>:</span></span><br><span class="line">    fd = generate_batch(train_img_embeds, train_captions_indexed, batch_size)</span><br><span class="line">    logits = decoder.flat_token_logits.eval(fd)</span><br><span class="line">    truth = decoder.flat_ground_truth.eval(fd)</span><br><span class="line">    mask = decoder.flat_loss_mask.eval(fd).astype(bool)</span><br><span class="line">    print(<span class="string">"Loss:"</span>, decoder.loss.eval(fd))</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy_score(logits.argmax(axis=<span class="number">1</span>)[mask], truth[mask]))</span><br><span class="line">    <span class="keyword">for</span> example_idx <span class="keyword">in</span> range(n_examples):</span><br><span class="line">        print(<span class="string">"Example"</span>, example_idx)</span><br><span class="line">        print(<span class="string">"Predicted:"</span>, decode_sentence(logits.argmax(axis=<span class="number">1</span>).reshape((batch_size, <span class="number">-1</span>))[example_idx]))</span><br><span class="line">        print(<span class="string">"Truth:"</span>, decode_sentence(truth.reshape((batch_size, <span class="number">-1</span>))[example_idx]))</span><br><span class="line">        print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line">check_after_training(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Loss: 2.37412
Accuracy: 0.501388888889
Example 0
Predicted: a person flying flying a kite in a building of people #END# #END# #END# #END# #END# #END# #END# #END# #END# #END#
Truth: a child is flying a kite near a group of buildings #END# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD#

Example 1
Predicted: a person of a doing a skateboard in down ramp of a ramp #END# #END# #END# #END# #END# #END# #END# #END#
Truth: a closeup of someone on a skateboard riding the edge of a ramp #END# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD#

Example 2
Predicted: a bed with a bed and a on furniture #END# a wall #END# #END# #END# #END# #END# #END# #END# #END# #END#
Truth: a bedroom with aqua walls and cutouts of rain on the wall #END# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD# #PAD#
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save last graph weights to file!</span></span><br><span class="line">saver.save(s, get_checkpoint_path())</span><br></pre></td></tr></table></figure>
<pre><code>&#39;/root/intro-to-dl/week6/weights&#39;
</code></pre><h1 id="Applying-model"><a href="#Applying-model" class="headerlink" title="Applying model"></a>Applying model</h1><p>Here we construct a graph for our final model.</p>
<p>It will work as follows:</p>
<ul>
<li>take an image as an input and embed it</li>
<li>condition lstm on that embedding</li>
<li>predict the next token given a START input token</li>
<li>use predicted token as an input at next time step</li>
<li>iterate until you predict an END token</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">final_model</span>:</span></span><br><span class="line">    <span class="comment"># CNN encoder</span></span><br><span class="line">    encoder, preprocess_for_model = get_cnn_encoder()</span><br><span class="line">    saver.restore(s, get_checkpoint_path())  <span class="comment"># keras applications corrupt our graph, so we restore trained weights</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># containers for current lstm state</span></span><br><span class="line">    lstm_c = tf.Variable(tf.zeros([<span class="number">1</span>, LSTM_UNITS]), name=<span class="string">"cell"</span>)</span><br><span class="line">    lstm_h = tf.Variable(tf.zeros([<span class="number">1</span>, LSTM_UNITS]), name=<span class="string">"hidden"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input images</span></span><br><span class="line">    input_images = tf.placeholder(<span class="string">'float32'</span>, [<span class="number">1</span>, IMG_SIZE, IMG_SIZE, <span class="number">3</span>], name=<span class="string">'images'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get image embeddings</span></span><br><span class="line">    img_embeds = encoder(input_images)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize lstm state conditioned on image</span></span><br><span class="line">    init_c = init_h = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))</span><br><span class="line">    init_lstm = tf.assign(lstm_c, init_c), tf.assign(lstm_h, init_h)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current word index</span></span><br><span class="line">    current_word = tf.placeholder(<span class="string">'int32'</span>, [<span class="number">1</span>], name=<span class="string">'current_input'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># embedding for current word</span></span><br><span class="line">    word_embed = decoder.word_embed(current_word)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply lstm cell, get new lstm states</span></span><br><span class="line">    new_c, new_h = decoder.lstm(word_embed, tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h))[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute logits for next token</span></span><br><span class="line">    new_logits = decoder.token_logits(decoder.token_logits_bottleneck(new_h))</span><br><span class="line">    <span class="comment"># compute probabilities for next token</span></span><br><span class="line">    new_probs = tf.nn.softmax(new_logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `one_step` outputs probabilities of next token and updates lstm hidden state</span></span><br><span class="line">    one_step = new_probs, tf.assign(lstm_c, new_c), tf.assign(lstm_h, new_h)</span><br></pre></td></tr></table></figure>
<pre><code>INFO:tensorflow:Restoring parameters from /root/intro-to-dl/week6/weights
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># look at how temperature works for probability distributions</span></span><br><span class="line"><span class="comment"># for high temperature we have more uniform distribution</span></span><br><span class="line">_ = np.array([<span class="number">0.5</span>, <span class="number">0.4</span>, <span class="number">0.1</span>])</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]:</span><br><span class="line">    print(<span class="string">" "</span>.join(map(str, _**(<span class="number">1</span>/t) / np.sum(_**(<span class="number">1</span>/t)))), <span class="string">"with temperature"</span>, t)</span><br></pre></td></tr></table></figure>
<pre><code>0.999999999796 2.03703597592e-10 1.26765059997e-70 with temperature 0.01
0.903037043325 0.0969628642039 9.24709932365e-08 with temperature 0.1
0.5 0.4 0.1 with temperature 1
0.353447726392 0.345648113606 0.300904160002 with temperature 10
0.335367280481 0.334619764349 0.33001295517 with temperature 100
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an actual prediction loop</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_caption</span><span class="params">(image, t=<span class="number">1</span>, sample=False, max_len=<span class="number">20</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Generate caption for given image.</span></span><br><span class="line"><span class="string">    if `sample` is True, we will sample next token from predicted probability distribution.</span></span><br><span class="line"><span class="string">    `t` is a temperature during that sampling,</span></span><br><span class="line"><span class="string">        higher `t` causes more uniform-like distribution = more chaos.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># condition lstm on the image</span></span><br><span class="line">    s.run(final_model.init_lstm, </span><br><span class="line">          &#123;final_model.input_images: [image]&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current caption</span></span><br><span class="line">    <span class="comment"># start with only START token</span></span><br><span class="line">    caption = [vocab[START]]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_len):</span><br><span class="line">        next_word_probs = s.run(final_model.one_step, </span><br><span class="line">                                &#123;final_model.current_word: [caption[<span class="number">-1</span>]]&#125;)[<span class="number">0</span>]</span><br><span class="line">        next_word_probs = next_word_probs.ravel()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># apply temperature</span></span><br><span class="line">        next_word_probs = next_word_probs**(<span class="number">1</span>/t) / np.sum(next_word_probs**(<span class="number">1</span>/t))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sample:</span><br><span class="line">            next_word = np.random.choice(range(len(vocab)), p=next_word_probs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_word = np.argmax(next_word_probs)</span><br><span class="line"></span><br><span class="line">        caption.append(next_word)</span><br><span class="line">        <span class="keyword">if</span> next_word == vocab[END]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">       </span><br><span class="line">    <span class="keyword">return</span> list(map(vocab_inverse.get, caption))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># look at validation prediction example</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_model_to_image_raw_bytes</span><span class="params">(raw)</span>:</span></span><br><span class="line">    img = utils.decode_image_from_buf(raw)</span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">    plt.grid(<span class="string">'off'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), final_model.preprocess_for_model)</span><br><span class="line">    print(<span class="string">' '</span>.join(generate_caption(img)[<span class="number">1</span>:<span class="number">-1</span>]))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_valid_example</span><span class="params">(val_img_fns, example_idx=<span class="number">0</span>)</span>:</span></span><br><span class="line">    zf = zipfile.ZipFile(<span class="string">"val2014_sample.zip"</span>)</span><br><span class="line">    all_files = set(val_img_fns)</span><br><span class="line">    found_files = list(filter(<span class="keyword">lambda</span> x: x.filename.rsplit(<span class="string">"/"</span>)[<span class="number">-1</span>] <span class="keyword">in</span> all_files, zf.filelist))</span><br><span class="line">    example = found_files[example_idx]</span><br><span class="line">    apply_model_to_image_raw_bytes(zf.read(example))</span><br><span class="line">    </span><br><span class="line">show_valid_example(val_img_fns, example_idx=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>a baseball player is swinging his bat at a ball
</code></pre><p><img src="output_60_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sample more images from validation</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> np.random.choice(range(len(zipfile.ZipFile(<span class="string">"val2014_sample.zip"</span>).filelist) - <span class="number">1</span>), <span class="number">10</span>):</span><br><span class="line">    show_valid_example(val_img_fns, example_idx=idx)</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>a bear is sitting on a rock in the water
</code></pre><p><img src="output_61_1.png" alt="png"></p>
<pre><code>a train is parked on the tracks near a fence
</code></pre><p><img src="output_61_3.png" alt="png"></p>
<pre><code>a group of people standing around a man in a room
</code></pre><p><img src="output_61_5.png" alt="png"></p>
<pre><code>a young boy in a red shirt and a white shirt and a white shirt and a white shirt
</code></pre><p><img src="output_61_7.png" alt="png"></p>
<pre><code>a city with many boats and a building
</code></pre><p><img src="output_61_9.png" alt="png"></p>
<pre><code>a baseball player is swinging at a ball
</code></pre><p><img src="output_61_11.png" alt="png"></p>
<pre><code>a baby elephant standing in a field with a tree in the background
</code></pre><p><img src="output_61_13.png" alt="png"></p>
<pre><code>a group of cars driving down a street
</code></pre><p><img src="output_61_15.png" alt="png"></p>
<pre><code>a bus is driving down the street with a bus
</code></pre><p><img src="output_61_17.png" alt="png"></p>
<pre><code>a woman sitting at a table with a laptop
</code></pre><p><img src="output_61_19.png" alt="png"></p>
<p>You can download any image from the Internet and appply your model to it!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562794330534&amp;di=1e83ed1d91d6b45c6cc1faa4f144dce1&amp;imgtype=0&amp;src=http%3A%2F%2Fs6.sinaimg.cn%2Fmiddle%2F4c271807gabeaf405af25%26690"</span>,</span><br><span class="line">    <span class="string">"dora1.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"dora1.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=21799), HTML(value=&#39;&#39;)))



a man holding a cell phone in front of a store
</code></pre><p><img src="output_63_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562794426849&amp;di=fd0de4f3f602675fdbb2ae6b557a789a&amp;imgtype=jpg&amp;src=http%3A%2F%2Fwww.gaoxiaoa.cn%2Fuploads%2F2018%2F07%2F20%2Fy0rwjpcuuor2210.jpg"</span>,</span><br><span class="line">    <span class="string">"dora2.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"dora2.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=23592), HTML(value=&#39;&#39;)))



a man holding a pair of scissors in a store
</code></pre><p><img src="output_64_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562794563144&amp;di=2bc884423f1df82010cd5339f31950ed&amp;imgtype=0&amp;src=http%3A%2F%2Fi.gtimg.cn%2Fqqlive%2Fimg%2Fjpgcache%2Ffiles%2Fqqvideo%2Fhori%2F4%2F4h2fv8pu7lmkmp2.jpg"</span>,</span><br><span class="line">    <span class="string">"dora2.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"dora2.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=31865), HTML(value=&#39;&#39;)))



a person holding a kite in a parking lot
</code></pre><p><img src="output_65_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562794944314&amp;di=f8e405f3cef51839e36119df6bee30da&amp;imgtype=0&amp;src=http%3A%2F%2Fphotocdn.sohu.com%2F20160118%2Fmp55134365_1453120177203_10.jpeg"</span>,</span><br><span class="line">    <span class="string">"li.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"li.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=56360), HTML(value=&#39;&#39;)))



a man in a white shirt and a white shirt and a white shirt and a white shirt
</code></pre><p><img src="output_66_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562795102654&amp;di=6e991c1996a6cb8c3875c737270400ba&amp;imgtype=0&amp;src=http%3A%2F%2Fhnrb.hinews.cn%2Fresfile%2F2016-03-01%2F015%2F1860655_hnrbtp1_1456751616813_b.jpg"</span>,</span><br><span class="line">    <span class="string">"li2.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"li2.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=20262), HTML(value=&#39;&#39;)))



a man in a suit and tie standing in front of a microphone
</code></pre><p><img src="output_67_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562797334376&amp;di=74df4bee2022d1b0519a759e9e6ec12d&amp;imgtype=0&amp;src=http%3A%2F%2Fn.sinaimg.cn%2Fent%2Ftransform%2F20170703%2FYGH3-fyhskrq1913341.jpg"</span>,</span><br><span class="line">    <span class="string">"test.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"test.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=24164), HTML(value=&#39;&#39;)))



a man in a white shirt and tie standing next to a man
</code></pre><p><img src="output_68_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562797392119&amp;di=81c23cda7dbc08fa561a561dd806473e&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.hinews.cn%2Fpic%2F0%2F16%2F62%2F10%2F16621075_026693.jpg"</span>,</span><br><span class="line">    <span class="string">"test2.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"test2.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=29384), HTML(value=&#39;&#39;)))



a giraffe is eating from a white plate
</code></pre><p><img src="output_69_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562797392118&amp;di=1848b6cb90b57b0fde208794abf22aa2&amp;imgtype=0&amp;src=http%3A%2F%2Fi0.sinaimg.cn%2Fdy%2Fcr%2F2014%2F0716%2F4150211456.jpg"</span>,</span><br><span class="line">    <span class="string">"test3.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"test3.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=97353), HTML(value=&#39;&#39;)))



a man is standing next to a statue of a statue
</code></pre><p><img src="output_70_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562797392116&amp;di=e68eb448b9247779f4624e10e9bf8b95&amp;imgtype=0&amp;src=http%3A%2F%2Fi0.hexunimg.cn%2F2016-08-22%2F185638157.jpg"</span>,</span><br><span class="line">    <span class="string">"test4.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"test4.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=18206), HTML(value=&#39;&#39;)))



a woman in a black jacket and a woman standing next to a woman
</code></pre><p><img src="output_71_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562797392116&amp;di=580a384f03823c0503be39144bfdd85b&amp;imgtype=0&amp;src=http%3A%2F%2Fn.sinaimg.cn%2Fent%2Ftransform%2F20161207%2FzAUH-fxyipxf7913222.jpg"</span>,</span><br><span class="line">    <span class="string">"test5.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"test5.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=40295), HTML(value=&#39;&#39;)))



a man and woman standing in a field with a kite
</code></pre><p><img src="output_72_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">download_utils.download_file(</span><br><span class="line">    <span class="string">"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562797392116&amp;di=665cab330fb52ec6883f1398ec2c167c&amp;imgtype=0&amp;src=http%3A%2F%2Fgb.cri.cn%2Fmmsource%2Fimages%2F2015%2F05%2F25%2Fex20150525008.jpg"</span>,</span><br><span class="line">    <span class="string">"test6.jpg"</span></span><br><span class="line">)</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"test6.jpg"</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=33113), HTML(value=&#39;&#39;)))



a man in a suit and tie standing in front of a building
</code></pre><p><img src="output_73_2.png" alt="png"></p>
<p>Now it’s time to find 10 examples where your model works good and 10 examples where it fails! </p>
<p>You can use images from validation set as follows:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_valid_example(val_img_fns, example_idx=...)</span><br></pre></td></tr></table></figure></p>
<p>You can use images from the Internet as follows:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">! wget ...</span><br><span class="line">apply_model_to_image_raw_bytes(open(<span class="string">"..."</span>, <span class="string">"rb"</span>).read())</span><br></pre></td></tr></table></figure></p>
<p>If you use these functions, the output will be embedded into your notebook and will be visible during peer review!</p>
<p>When you’re done, download your noteboook using “File” -&gt; “Download as” -&gt; “Notebook” and prepare that file for peer review!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### YOUR EXAMPLES HERE ###</span></span><br></pre></td></tr></table></figure>
<p>That’s it! </p>
<p>Congratulations, you’ve trained your image captioning model and now can produce captions for any picture from the  Internet!</p>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
            <a href="/tags/Project/" rel="tag"># Project</a>
          
            <a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a>
          
            <a href="/tags/Transfer-Learning/" rel="tag"># Transfer Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Docker-Command-Cheatsheet/2019/07/11/" rel="next" title="Docker Command Cheatsheet">
                <i class="fa fa-chevron-left"></i> Docker Command Cheatsheet
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Autoencoders-Project/2019/07/11/" rel="prev" title="Autoencoders Project">
                Autoencoders Project <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">272</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Image-Captioning-Final-Project"><span class="nav-text">Image Captioning Final Project</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Import-stuff"><span class="nav-text">Import stuff</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Prepare-the-storage-for-model-checkpoints"><span class="nav-text">Prepare the storage for model checkpoints</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Fill-in-your-Coursera-token-and-email"><span class="nav-text">Fill in your Coursera token and email</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Download-data"><span class="nav-text">Download data</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Extract-image-features"><span class="nav-text">Extract image features</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Extract-captions-for-images"><span class="nav-text">Extract captions for images</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Prepare-captions-for-training"><span class="nav-text">Prepare captions for training</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training"><span class="nav-text">Training</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Define-architecture"><span class="nav-text">Define architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-loop"><span class="nav-text">Training loop</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Applying-model"><span class="nav-text">Applying model</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  
  

  
  


  

  

  
</body>
</html>
