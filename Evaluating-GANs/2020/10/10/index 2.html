<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Project,Generative Adversarial Network,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="Evaluating GANsGoalsIn this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called">
<meta name="keywords" content="Project,Generative Adversarial Network">
<meta property="og:type" content="article">
<meta property="og:title" content="Evaluating GANs">
<meta property="og:url" content="https://zhangruochi.com/Evaluating-GANs/2020/10/10/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Evaluating GANsGoalsIn this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://zhangruochi.com/Evaluating-GANs/2020/10/10/output_18_0.png">
<meta property="og:image" content="https://zhangruochi.com/Evaluating-GANs/2020/10/10/output_20_0.png">
<meta property="og:image" content="https://zhangruochi.com/Evaluating-GANs/2020/10/10/output_37_1.png">
<meta property="og:updated_time" content="2020-10-13T03:21:12.720Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Evaluating GANs">
<meta name="twitter:description" content="Evaluating GANsGoalsIn this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called">
<meta name="twitter:image" content="https://zhangruochi.com/Evaluating-GANs/2020/10/10/output_18_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/Evaluating-GANs/2020/10/10/">





  <title>Evaluating GANs | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Evaluating-GANs/2020/10/10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Evaluating GANs</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-10T15:34:02+08:00">
                2020-10-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Evaluating-GANs/2020/10/10/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Evaluating-GANs/2020/10/10/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Evaluating-GANs"><a href="#Evaluating-GANs" class="headerlink" title="Evaluating GANs"></a>Evaluating GANs</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called Fréchet Inception Distance (FID).</p>
<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol>
<li>Understand the challenges associated with evaluating GANs.</li>
<li>Write code to evaluate the Fréchet Inception Distance.</li>
</ol>
<h2 id="Challenges-With-Evaluating-GANs"><a href="#Challenges-With-Evaluating-GANs" class="headerlink" title="Challenges With Evaluating GANs"></a>Challenges With Evaluating GANs</h2><h4 id="Loss-is-Uninformative-of-Performance"><a href="#Loss-is-Uninformative-of-Performance" class="headerlink" title="Loss is Uninformative of Performance"></a>Loss is Uninformative of Performance</h4><p>One aspect that makes evaluating GANs challenging is that the loss tells us little about their performance. Unlike with classifiers, where a low loss on a test set indicates superior performance, a low loss for the generator or discriminator suggests that learning has stopped. </p>
<h4 id="No-Clear-Non-human-Metric"><a href="#No-Clear-Non-human-Metric" class="headerlink" title="No Clear Non-human Metric"></a>No Clear Non-human Metric</h4><p>If you define the goal of a GAN as “generating images which look real to people” then it’s technically possible to measure this directly: <a href="https://arxiv.org/abs/1904.01121" target="_blank" rel="noopener">you can ask people to act as a discriminator</a>. However, this takes significant time and money so ideally you can use a proxy for this. There is also no “perfect” discriminator that can differentiate reals from fakes - if there were, a lot of machine learning tasks would be solved ;)</p>
<p>In this notebook, you will implement Fréchet Inception Distance, one method which aims to solve these issues. </p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>For this notebook, you will again be using <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" rel="noopener">CelebA</a>. You will start by loading a pre-trained generator which has been trained on CelebA.</p>
<p>Here, you will import some useful libraries and packages. You will also be provided with the generator and noise code from earlier assignments.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> CelebA</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for our testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (CelebA is rgb, so 3 is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_chan=<span class="number">3</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">8</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">8</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = noise.view(len(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure>
<h2 id="Loading-the-Pre-trained-Model"><a href="#Loading-the-Pre-trained-Model" class="headerlink" title="Loading the Pre-trained Model"></a>Loading the Pre-trained Model</h2><p>Now, you can set the arguments for the model and load the dataset:</p>
<ul>
<li>z_dim: the dimension of the noise vector</li>
<li>image_size: the image size of the input to Inception (more details in the following section)</li>
<li>device: the device type</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">image_size = <span class="number">299</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(image_size),</span><br><span class="line">    transforms.CenterCrop(image_size),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">in_coursera = <span class="keyword">True</span> <span class="comment"># Set this to false if you're running this outside Coursera</span></span><br><span class="line"><span class="keyword">if</span> in_coursera:</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    data = torch.Tensor(np.load(<span class="string">'fid_images_tensor.npz'</span>, allow_pickle=<span class="keyword">True</span>)[<span class="string">'arr_0'</span>])</span><br><span class="line">    dataset = torch.utils.data.TensorDataset(data, data)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dataset = CelebA(<span class="string">"."</span>, download=<span class="keyword">True</span>, transform=transform)</span><br></pre></td></tr></table></figure>
<p>Then, you can load and initialize the model with weights from a pre-trained model. This allows you to use the pre-trained model as if you trained it yourself.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen.load_state_dict(torch.load(<span class="string">f"pretrained_celeba.pth"</span>, map_location=torch.device(device))[<span class="string">"gen"</span>])</span><br><span class="line">gen = gen.eval()</span><br></pre></td></tr></table></figure>
<h2 id="Inception-v3-Network"><a href="#Inception-v3-Network" class="headerlink" title="Inception-v3 Network"></a>Inception-v3 Network</h2><p>Inception-V3 is a neural network trained on <a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a> to classify objects. You may recall from the lectures that ImageNet has over 1 million images to train on. As a result, Inception-V3 does a good job detecting features and classifying images. Here, you will load Inception-V3 as <code>inception_model</code>.</p>
<!--  
In the past, people would use a pretrained Inception network to identify the classes of the objects generated by a GAN and measure how similar the distribution of classes generated was to the true image (using KL divergence). This is known as inception score. 

However, there are many problems with this metric. Barratt and Sharma's 2018 "[A Note on the Inception Score](https://arxiv.org/pdf/1801.01973.pdf)" highlights many issues with this approach. Among them, they highlight its instability, its exploitability, and the widespread use of Inception Score on models not trained on ImageNet.  -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> inception_v3</span><br><span class="line">inception_model = inception_v3(pretrained=<span class="keyword">False</span>)</span><br><span class="line">inception_model.load_state_dict(torch.load(<span class="string">"inception_v3_google-1a9a5a14.pth"</span>))</span><br><span class="line">inception_model.to(device)</span><br><span class="line">inception_model = inception_model.eval() <span class="comment"># Evaluation mode</span></span><br></pre></td></tr></table></figure>
<h2 id="Frechet-Inception-Distance"><a href="#Frechet-Inception-Distance" class="headerlink" title="Fréchet Inception Distance"></a>Fréchet Inception Distance</h2><p>Fréchet Inception Distance (FID) was proposed as an improvement over Inception Score and still uses the Inception-v3 network as part of its calculation. However, instead of using the classification labels of the Inception-v3 network, it uses the output from an earlier layer—the layer right before the labels. This is often called the feature layer. Research has shown that deep convolutional neural networks trained on difficult tasks, like classifying many classes, build increasingly sophisticated representations of features going deeper into the network. For example, the first few layers may learn to detect different kinds of edges and curves, while the later layers may have neurons that fire in response to human faces.</p>
<p>To get the feature layer of a convolutional neural network, you can replace the final fully connected layer with an identity layer that simply returns whatever input it received, unchanged. This essentially removes the final classification layer and leaves you with the intermediate outputs from the layer before.</p>
<details>

<summary>
<font size="3" color="green">
<b>Optional hint for <code><font size="4">inception_model.fc</font></code></b>
</font>
</summary>

1.    You may find [torch.nn.Identity()](https://pytorch.org/docs/master/generated/torch.nn.Identity.html) helpful.

</details>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: inception_model.fc</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You want to replace the final fully-connected (fc) layer </span></span><br><span class="line"><span class="comment"># with an identity function layer to cut off the classification</span></span><br><span class="line"><span class="comment"># layer and get a feature extractor</span></span><br><span class="line"><span class="comment">#### START CODE HERE ####</span></span><br><span class="line">inception_model.fc = nn.Identity()</span><br><span class="line"><span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_identity_noise = torch.randn(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.equal(test_identity_noise, inception_model.fc(test_identity_noise))</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h3 id="Frechet-Distance"><a href="#Frechet-Distance" class="headerlink" title="Fréchet Distance"></a>Fréchet Distance</h3><p>Fréchet distance uses the values from the feature layer for two sets of images, say reals and fakes, and compares different statistical properties between them to see how different they are. Specifically, Fréchet distance finds the shortest distance needed to walk along two lines, or two curves, simultaneously. The most intuitive explanation of Fréchet distance is as the “minimum leash distance” between two points. Imagine yourself and your dog, both moving along two curves. If you walked on one curve and your dog, attached to a leash, walked on the other at the same pace, what is the least amount of leash that you can give your dog so that you never need to give them more slack during your walk? Using this, the Fréchet distance measures the similarity between these two curves.</p>
<p>The basic idea is similar for calculating the Fréchet distance between two probability distributions. You’ll start by seeing what this looks like in one-dimensional, also called univariate, space.</p>
<h4 id="Univariate-Frechet-Distance"><a href="#Univariate-Frechet-Distance" class="headerlink" title="Univariate Fréchet Distance"></a>Univariate Fréchet Distance</h4><p>You can calculate the distance between two normal distributions $X$ and $Y$ with means $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$, as:</p>
<script type="math/tex; mode=display">d(X,Y) = (\mu_X-\mu_Y)^2 + (\sigma_X-\sigma_Y)^2</script><p>Pretty simple, right? Now you can see how it can be converted to be used in multi-dimensional, which is also called multivariate, space.</p>
<h4 id="Multivariate-Frechet-Distance"><a href="#Multivariate-Frechet-Distance" class="headerlink" title="Multivariate Fréchet Distance"></a>Multivariate Fréchet Distance</h4><p><strong>Covariance</strong></p>
<p>To find the Fréchet distance between two multivariate normal distributions, you first need to find the covariance instead of the standard deviation. The covariance, which is the multivariate version of variance (the square of standard deviation), is represented using a square matrix where the side length is equal to the number of dimensions. Since the feature vectors you will be using have 2048 values/weights, the covariance matrix will be 2048 x 2048. But for the sake of an example, this is a covariance matrix in a two-dimensional space:</p>
<p>$\Sigma = \left(\begin{array}{cc}<br>1 &amp; 0\\<br>0 &amp; 1<br>\end{array}\right)<br>$</p>
<p>The value at location $(i, j)$ corresponds to the covariance of vector $i$ with vector $j$. Since the covariance of $i$ with $j$ and $j$ with $i$ are equivalent, the matrix will always be symmetric with respect to the diagonal. The diagonal is the covariance of that element with itself. In this example, there are zeros everywhere except the diagonal. That means that the two dimensions are independent of one another, they are completely unrelated.</p>
<p>The following code cell will visualize this matrix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import os</span></span><br><span class="line"><span class="comment">#os.environ['KMP_DUPLICATE_LIB_OK']='True'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> MultivariateNormal</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment"># This is for visualization</span></span><br><span class="line">mean = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance = torch.Tensor( <span class="comment"># This matrix shows independence - there are only non-zero values on the diagonal</span></span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">independent_dist = MultivariateNormal(mean, covariance)</span><br><span class="line">samples = independent_dist.sample((<span class="number">10000</span>,))</span><br><span class="line">res = sns.jointplot(samples[:, <span class="number">0</span>], samples[:, <span class="number">1</span>], kind=<span class="string">"kde"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_18_0.png" alt="png"></p>
<p>Now, here’s an example of a multivariate normal distribution that has covariance:</p>
<p>$\Sigma = \left(\begin{array}{cc}<br>2 &amp; -1\\<br>-1 &amp; 2<br>\end{array}\right)<br>$</p>
<p>And see how it looks:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mean = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">covariance = torch.Tensor(</span><br><span class="line">    [[<span class="number">2</span>, <span class="number">-1</span>],</span><br><span class="line">     [<span class="number">-1</span>, <span class="number">2</span>]]</span><br><span class="line">)</span><br><span class="line">covariant_dist = MultivariateNormal(mean, covariance)</span><br><span class="line">samples = covariant_dist.sample((<span class="number">10000</span>,))</span><br><span class="line">res = sns.jointplot(samples[:, <span class="number">0</span>], samples[:, <span class="number">1</span>], kind=<span class="string">"kde"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_20_0.png" alt="png"></p>
<p><strong>Formula</strong></p>
<p>Based on the paper, “<a href="https://core.ac.uk/reader/82269844" target="_blank" rel="noopener">The Fréchet distance between multivariate normal distributions</a>“ by Dowson and Landau (1982), the Fréchet distance between two multivariate normal distributions $X$ and $Y$ is:</p>
<p>$d(X, Y) = \Vert\mu_X-\mu_Y\Vert^2 + \mathrm{Tr}\left(\Sigma_X+\Sigma_Y - 2 \sqrt{\Sigma_X \Sigma_Y}\right)$</p>
<p>Similar to the formula for univariate Fréchet distance, you can calculate the distance between the means and the distance between the standard deviations. However, calculating the distance between the standard deviations changes slightly here, as it includes the matrix product and matrix square root. $\mathrm{Tr}$ refers to the trace, the sum of the diagonal elements of a matrix.</p>
<p>Now you can implement this!</p>
<details>

<summary>
<font size="3" color="green">
<b>Optional hints for <code><font size="4">frechet_distance</font></code></b>
</font>
</summary>

1.   You want to implement the above equation in code.
2.   You might find the functions `torch.norm` and `torch.trace` helpful here.
3.   A matrix_sqrt function is defined for you above -- you need to use it instead of `torch.sqrt()` which only gets the elementwise square root instead of the matrix square root.
4.   You can also use the `@` symbol for matrix multiplication.
</details>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="comment"># This is the matrix square root function you will be using</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrix_sqrt</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function that takes in a matrix and returns the square root of that matrix.</span></span><br><span class="line"><span class="string">    For an input matrix A, the output matrix B would be such that B @ B is the matrix A.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        x: a matrix</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = x.cpu().detach().numpy()</span><br><span class="line">    y = scipy.linalg.sqrtm(y)</span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(y.real, device=x.device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: frechet_distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frechet_distance</span><span class="params">(mu_x, mu_y, sigma_x, sigma_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for returning the Fréchet distance between multivariate Gaussians,</span></span><br><span class="line"><span class="string">    parameterized by their means and covariance matrices.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        mu_x: the mean of the first Gaussian, (n_features)</span></span><br><span class="line"><span class="string">        mu_y: the mean of the second Gaussian, (n_features) </span></span><br><span class="line"><span class="string">        sigma_x: the covariance matrix of the first Gaussian, (n_features, n_features)</span></span><br><span class="line"><span class="string">        sigma_y: the covariance matrix of the second Gaussian, (n_features, n_features)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(mu_x - mu_y) + torch.trace(sigma_x + sigma_y - <span class="number">2</span> * matrix_sqrt(sigma_x @ sigma_y))</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"></span><br><span class="line">mean1 = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance1 = torch.Tensor( <span class="comment"># This matrix shows independence - there are only non-zero values on the diagonal</span></span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">dist1 = MultivariateNormal(mean1, covariance1)</span><br><span class="line"></span><br><span class="line">mean2 = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance2 = torch.Tensor( <span class="comment"># This matrix shows dependence </span></span><br><span class="line">    [[<span class="number">2</span>, <span class="number">-1</span>],</span><br><span class="line">     [<span class="number">-1</span>, <span class="number">2</span>]]</span><br><span class="line">)</span><br><span class="line">dist2 = MultivariateNormal(mean2, covariance2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    frechet_distance(</span><br><span class="line">        dist1.mean, dist2.mean,</span><br><span class="line">        dist1.covariance_matrix, dist2.covariance_matrix</span><br><span class="line">    ),</span><br><span class="line">    <span class="number">4</span> - <span class="number">2</span> * torch.sqrt(torch.tensor(<span class="number">3.</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> (frechet_distance(</span><br><span class="line">        dist1.mean, dist1.mean,</span><br><span class="line">        dist1.covariance_matrix, dist1.covariance_matrix</span><br><span class="line">    ).item() == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h2 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together!"></a>Putting it all together!</h2><p>Now, you can apply FID to your generator from earlier.</p>
<p>You will start by defining a bit of helper code to preprocess the image for the Inception-v3 network:<br><!-- This isn't exactly what FID is meant for, since inception scores expect a natural image, but it should give a rough idea of the diversity and quality of your images.  [TODO: move to bottom since image net is trained on nature (cat, dog) images, fidelity (quality)] --></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = torch.nn.functional.interpolate(img, size=(<span class="number">299</span>, <span class="number">299</span>), mode=<span class="string">'bilinear'</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>
<p>Then, you’ll define a function to calculate the covariance of the features that returns a covariance matrix given a list of values:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_covariance</span><span class="params">(features)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(np.cov(features.detach().numpy(), rowvar=<span class="keyword">False</span>))</span><br></pre></td></tr></table></figure>
<p>Finally, you can use the pre-trained Inception-v3 model to compute features of the real and fake images. With these features, you can then get the covariance and means of these features across many samples. </p>
<p>First, you get the features of the real and fake images using the Inception-v3 model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">fake_features_list = []</span><br><span class="line">real_features_list = []</span><br><span class="line"></span><br><span class="line">gen.eval()</span><br><span class="line">n_samples = <span class="number">512</span> <span class="comment"># The total number of samples</span></span><br><span class="line">batch_size = <span class="number">4</span> <span class="comment"># Samples per iteration</span></span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">cur_samples = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># You don't need to calculate gradients here, so you do this to save memory</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> real_example, _ <span class="keyword">in</span> tqdm(dataloader, total=n_samples // batch_size): <span class="comment"># Go by batch</span></span><br><span class="line">            real_samples = real_example</span><br><span class="line">            real_features = inception_model(real_samples.to(device)).detach().to(<span class="string">'cpu'</span>) <span class="comment"># Move features to CPU</span></span><br><span class="line">            real_features_list.append(real_features)</span><br><span class="line"></span><br><span class="line">            fake_samples = get_noise(len(real_example), z_dim).to(device)</span><br><span class="line">            fake_samples = preprocess(gen(fake_samples))</span><br><span class="line">            fake_features = inception_model(fake_samples.to(device)).detach().to(<span class="string">'cpu'</span>)</span><br><span class="line">            fake_features_list.append(fake_features)</span><br><span class="line">            cur_samples += len(real_samples)</span><br><span class="line">            <span class="keyword">if</span> cur_samples &gt;= n_samples:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Error in loop"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=128.0), HTML(value=&#39;&#39;)))
</code></pre><p>Then, you can combine all of the values that you collected for the reals and fakes into large tensors:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Needed as is for autograding</span></span><br><span class="line">fake_features_all = torch.cat(fake_features_list)</span><br><span class="line">real_features_all = torch.cat(real_features_list)</span><br></pre></td></tr></table></figure>
<p>And calculate the covariance and means of these real and fake features:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the covariance matrix for the fake and real features</span></span><br><span class="line"><span class="comment"># and also calculate the means of the feature over the batch (for each feature dimension mean)</span></span><br><span class="line"><span class="comment">#### START CODE HERE ####</span></span><br><span class="line">mu_fake = torch.mean(fake_features_all, axis = <span class="number">0</span>)</span><br><span class="line">mu_real = torch.mean(real_features_all, axis = <span class="number">0</span>)</span><br><span class="line">sigma_fake = get_covariance(fake_features_all)</span><br><span class="line">sigma_real = get_covariance(real_features_all)</span><br><span class="line"><span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> tuple(sigma_fake.shape) == (fake_features_all.shape[<span class="number">1</span>], fake_features_all.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> torch.abs(sigma_fake[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">2.5e-2</span>) &lt; <span class="number">1e-2</span> <span class="keyword">and</span> torch.abs(sigma_fake[<span class="number">-1</span>, <span class="number">-1</span>] - <span class="number">5e-2</span>) &lt; <span class="number">1e-2</span></span><br><span class="line"><span class="keyword">assert</span> tuple(sigma_real.shape) == (real_features_all.shape[<span class="number">1</span>], real_features_all.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> torch.abs(sigma_real[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">3.5768e-2</span>) &lt; <span class="number">1e-4</span> <span class="keyword">and</span> torch.abs(sigma_real[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">5.3236e-4</span>) &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> tuple(mu_fake.shape) == (fake_features_all.shape[<span class="number">1</span>],)</span><br><span class="line"><span class="keyword">assert</span> tuple(mu_real.shape) == (real_features_all.shape[<span class="number">1</span>],)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(mu_real[<span class="number">0</span>] - <span class="number">0.3099</span>) &lt; <span class="number">0.01</span> <span class="keyword">and</span> torch.abs(mu_real[<span class="number">1</span>] - <span class="number">0.2721</span>) &lt; <span class="number">0.01</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(mu_fake[<span class="number">0</span>] - <span class="number">0.37</span>) &lt; <span class="number">0.05</span> <span class="keyword">and</span> torch.abs(mu_real[<span class="number">1</span>] - <span class="number">0.27</span>) &lt; <span class="number">0.05</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><p>At this point, you can also visualize what the pairwise multivariate distributions of the inception features look like!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">indices = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">fake_dist = MultivariateNormal(mu_fake[indices], sigma_fake[indices][:, indices])</span><br><span class="line">fake_samples = fake_dist.sample((<span class="number">5000</span>,))</span><br><span class="line">real_dist = MultivariateNormal(mu_real[indices], sigma_real[indices][:, indices])</span><br><span class="line">real_samples = real_dist.sample((<span class="number">5000</span>,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df_fake = pd.DataFrame(fake_samples.numpy(), columns=indices)</span><br><span class="line">df_real = pd.DataFrame(real_samples.numpy(), columns=indices)</span><br><span class="line">df_fake[<span class="string">"is_real"</span>] = <span class="string">"no"</span></span><br><span class="line">df_real[<span class="string">"is_real"</span>] = <span class="string">"yes"</span></span><br><span class="line">df = pd.concat([df_fake, df_real])</span><br><span class="line">sns.pairplot(df, plot_kws=&#123;<span class="string">'alpha'</span>: <span class="number">0.1</span>&#125;, hue=<span class="string">'is_real'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7fa847b2ab38&gt;
</code></pre><p><img src="output_37_1.png" alt="png"></p>
<p>Lastly, you can use your earlier <code>frechet_distance</code> function to calculate the FID and evaluate your GAN. You can see how similar/different the features of the generated images are to the features of the real images. The next cell might take five minutes or so to run in Coursera.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item())</span><br></pre></td></tr></table></figure>
<pre><code>86.48429107666016
</code></pre><p>You’ll notice this model gets a pretty high FID, likely over 30. Since lower is better, and the best models on CelebA get scores in the single-digits, there’s clearly a ways to go with this model. You can use FID to compare different models, as well as different stages of training of the same model. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Project/" rel="tag"># Project</a>
          
            <a href="/tags/Generative-Adversarial-Network/" rel="tag"># Generative Adversarial Network</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Build-a-Conditional-GAN/2020/10/09/" rel="next" title="Build a Conditional GAN">
                <i class="fa fa-chevron-left"></i> Build a Conditional GAN
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Components-of-StyleGAN/2020/10/13/" rel="prev" title="Components of StyleGAN ">
                Components of StyleGAN  <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate article here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Ruochi Zhang WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    

    
      <div id="bitcoin" style="display: inline-block">
        <img id="vemo_qr" src="/images/venmo.png" alt="Ruochi Zhang Bitcoin">
        <p>Venmo(last 4 digits 1570)</p>
      </div>
    

  </div>
</div>

      </div>
    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">267</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Evaluating-GANs"><span class="nav-text">Evaluating GANs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Goals"><span class="nav-text">Goals</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Objectives"><span class="nav-text">Learning Objectives</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Challenges-With-Evaluating-GANs"><span class="nav-text">Challenges With Evaluating GANs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-is-Uninformative-of-Performance"><span class="nav-text">Loss is Uninformative of Performance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#No-Clear-Non-human-Metric"><span class="nav-text">No Clear Non-human Metric</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#Getting-Started"><span class="nav-text">Getting Started</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loading-the-Pre-trained-Model"><span class="nav-text">Loading the Pre-trained Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inception-v3-Network"><span class="nav-text">Inception-v3 Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Frechet-Inception-Distance"><span class="nav-text">Fréchet Inception Distance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Frechet-Distance"><span class="nav-text">Fréchet Distance</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Univariate-Frechet-Distance"><span class="nav-text">Univariate Fréchet Distance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multivariate-Frechet-Distance"><span class="nav-text">Multivariate Fréchet Distance</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Putting-it-all-together"><span class="nav-text">Putting it all together!</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
