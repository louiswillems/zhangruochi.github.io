<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="Assignment 4:  Question duplicatesWelcome to the fourth assignment of course 3. In this assignment you will explore Siamese networks applied to natural language processing. You will further explore th">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Question duplicates">
<meta property="og:url" content="https://zhangruochi.com/Question-duplicates/2020/08/23/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Assignment 4:  Question duplicatesWelcome to the fourth assignment of course 3. In this assignment you will explore Siamese networks applied to natural language processing. You will further explore th">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://zhangruochi.com/Question-duplicates/2020/08/23/meme.png">
<meta property="og:image" content="https://zhangruochi.com/Question-duplicates/2020/08/23/siamese.png">
<meta property="og:image" content="https://zhangruochi.com/Question-duplicates/2020/08/23/C3_W4_triploss1.png">
<meta property="og:image" content="https://zhangruochi.com/Question-duplicates/2020/08/23/C3_W4_triploss2.png">
<meta property="og:updated_time" content="2020-08-22T18:47:47.800Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Question duplicates">
<meta name="twitter:description" content="Assignment 4:  Question duplicatesWelcome to the fourth assignment of course 3. In this assignment you will explore Siamese networks applied to natural language processing. You will further explore th">
<meta name="twitter:image" content="https://zhangruochi.com/Question-duplicates/2020/08/23/meme.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/Question-duplicates/2020/08/23/">





  <title>Question duplicates | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Question-duplicates/2020/08/23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Question duplicates</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-23T02:47:25+08:00">
                2020-08-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Question-duplicates/2020/08/23/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Question-duplicates/2020/08/23/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Assignment-4-Question-duplicates"><a href="#Assignment-4-Question-duplicates" class="headerlink" title="Assignment 4:  Question duplicates"></a>Assignment 4:  Question duplicates</h1><p>Welcome to the fourth assignment of course 3. In this assignment you will explore Siamese networks applied to natural language processing. You will further explore the fundamentals of Trax and you will be able to implement a more complicated structure using it. By completing this assignment, you will learn how to implement models with different architectures. </p>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#0">Overview</a></li>
<li><a href="#1">Part 1: Importing the Data</a><ul>
<li><a href="#1.1">1.1 Loading in the data</a></li>
<li><a href="#1.2">1.2 Converting a question to a tensor</a></li>
<li><a href="#1.3">1.3 Understanding the iterator</a><ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">Part 2: Defining the Siamese model</a><ul>
<li><a href="#2.1">2.1 Understanding Siamese Network</a><ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul>
</li>
<li><a href="#2.2">2.2 Hard  Negative Mining</a><ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3">Part 3: Training</a><ul>
<li><a href="#3.1">3.1 Training the model</a><ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4">Part 4: Evaluation</a><ul>
<li><a href="#4.1">4.1 Evaluating your siamese network</a></li>
<li><a href="#4.2">4.2 Classify</a><ul>
<li><a href="#ex05">Exercise 05</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#5">Part 5: Testing with your own questions</a><ul>
<li><a href="#ex06">Exercise 06</a></li>
</ul>
</li>
<li><a href="#6">On Siamese networks</a></li>
</ul>
<p><a name="0"></a></p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>In this assignment, concretely you will: </p>
<ul>
<li>Learn about Siamese networks</li>
<li>Understand how the triplet loss works</li>
<li>Understand how to evaluate accuracy</li>
<li>Use cosine similarity between the model’s outputted vectors</li>
<li>Use the data generator to get batches of questions</li>
<li>Predict using your own model</li>
</ul>
<p>By now, you are familiar with trax and know how to make use of classes to define your model. We will start this homework by asking you to preprocess the data the same way you did in the previous assignments. After processing the data you will build a classifier that will allow you to identify whether to questions are the same or not.<br><img src="meme.png" style="width:550px;height:300px;"></p>
<p>You will process the data first and then pad in a similar way you have done in the previous assignment. Your model will take in the two question embeddings, run them through an LSTM, and then compare the outputs of the two sub networks using cosine similarity. Before taking a deep dive into the model, start by importing the data set.</p>
<p><a name="1"></a></p>
<h1 id="Part-1-Importing-the-Data"><a href="#Part-1-Importing-the-Data" class="headerlink" title="Part 1: Importing the Data"></a>Part 1: Importing the Data</h1><p><a name="1.1"></a></p>
<h3 id="1-1-Loading-in-the-data"><a href="#1-1-Loading-in-the-data" class="headerlink" title="1.1 Loading in the data"></a>1.1 Loading in the data</h3><p>You will be using the Quora question answer dataset to build a model that could identify similar questions. This is a useful task because you don’t want to have several versions of the same question posted. Several times when teaching I end up responding to similar questions on piazza, or on other community forums. This data set has been labeled for you. Run the cell below to import some of the packages you will be using. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"><span class="keyword">from</span> trax.fastmath <span class="keyword">import</span> numpy <span class="keyword">as</span> fastnp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rnd</span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seeds</span></span><br><span class="line">trax.supervised.trainer_lib.init_random_number_generators(<span class="number">34</span>)</span><br><span class="line">rnd.seed(<span class="number">34</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Notice that for this assignment Trax’s numpy is referred to as <code>fastnp</code>, while regular numpy is referred to as <code>np</code>.</strong></p>
<p>You will now load in the data set. We have done some preprocessing for you. If you have taken the deeplearning specialization, this is a slightly different training method than the one you have seen there. If you have not, then don’t worry about it, we will explain everything. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"questions.csv"</span>)</span><br><span class="line">N=len(data)</span><br><span class="line">print(<span class="string">'Number of question pairs: '</span>, N)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<pre><code>Number of question pairs:  404351
</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>qid1</th>
      <th>qid2</th>
      <th>question1</th>
      <th>question2</th>
      <th>is_duplicate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>What is the step by step guide to invest in sh...</td>
      <td>What is the step by step guide to invest in sh...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>3</td>
      <td>4</td>
      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>
      <td>What would happen if the Indian government sto...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>5</td>
      <td>6</td>
      <td>How can I increase the speed of my internet co...</td>
      <td>How can Internet speed be increased by hacking...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>7</td>
      <td>8</td>
      <td>Why am I mentally very lonely? How can I solve...</td>
      <td>Find the remainder when [math]23^{24}[/math] i...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>9</td>
      <td>10</td>
      <td>Which one dissolve in water quikly sugar, salt...</td>
      <td>Which fish would survive in salt water?</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>We first split the data into a train and test set. The test set will be used later to evaluate our model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">N_train = <span class="number">300000</span></span><br><span class="line">N_test  = <span class="number">10</span>*<span class="number">1024</span></span><br><span class="line">data_train = data[:N_train]</span><br><span class="line">data_test  = data[N_train:N_train+N_test]</span><br><span class="line">print(<span class="string">"Train set:"</span>, len(data_train), <span class="string">"Test set:"</span>, len(data_test))</span><br><span class="line"><span class="keyword">del</span>(data) <span class="comment"># remove to free memory</span></span><br></pre></td></tr></table></figure>
<pre><code>Train set: 300000 Test set: 10240
</code></pre><p>As explained in the lectures, we select only the question pairs that are duplicate to train the model. <br><br>We build two batches as input for the Siamese network and we assume that question $q1_i$ (question $i$ in the first batch) is a duplicate of $q2_i$ (question $i$ in the second batch), but all other questions in the second batch are not duplicates of $q1_i$.<br>The test set uses the original pairs of questions and the status describing if the questions are duplicates.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">td_index = (data_train[<span class="string">'is_duplicate'</span>] == <span class="number">1</span>).to_numpy()</span><br><span class="line">td_index = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(td_index) <span class="keyword">if</span> x] </span><br><span class="line">print(<span class="string">'number of duplicate questions: '</span>, len(td_index))</span><br><span class="line">print(<span class="string">'indexes of first ten duplicate questions:'</span>, td_index[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<pre><code>number of duplicate questions:  111486
indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(data_train[<span class="string">'question1'</span>][<span class="number">5</span>])  <span class="comment">#  Example of question duplicates (first one in data)</span></span><br><span class="line">print(data_train[<span class="string">'question2'</span>][<span class="number">5</span>])</span><br><span class="line">print(<span class="string">'is_duplicate: '</span>, data_train[<span class="string">'is_duplicate'</span>][<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?
I&#39;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?
is_duplicate:  1
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Q1_train_words = np.array(data_train[<span class="string">'question1'</span>][td_index])</span><br><span class="line">Q2_train_words = np.array(data_train[<span class="string">'question2'</span>][td_index])</span><br><span class="line"></span><br><span class="line">Q1_test_words = np.array(data_test[<span class="string">'question1'</span>])</span><br><span class="line">Q2_test_words = np.array(data_test[<span class="string">'question2'</span>])</span><br><span class="line">y_test  = np.array(data_test[<span class="string">'is_duplicate'</span>])</span><br></pre></td></tr></table></figure>
<p>Above, you have seen that you only took the duplicated questions for training our model. <br>You did so on purpose, because the data generator will produce batches $([q1_1, q1_2, q1_3, …]$, $[q2_1, q2_2,q2_3, …])$  where $q1_i$ and $q2_k$ are duplicate if and only if $i = k$.</p>
<p><br>Let’s print to see what your data looks like.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'TRAINING QUESTIONS:\n'</span>)</span><br><span class="line">print(<span class="string">'Question 1: '</span>, Q1_train_words[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Question 2: '</span>, Q2_train_words[<span class="number">0</span>], <span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">'Question 1: '</span>, Q1_train_words[<span class="number">5</span>])</span><br><span class="line">print(<span class="string">'Question 2: '</span>, Q2_train_words[<span class="number">5</span>], <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'TESTING QUESTIONS:\n'</span>)</span><br><span class="line">print(<span class="string">'Question 1: '</span>, Q1_test_words[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Question 2: '</span>, Q2_test_words[<span class="number">0</span>], <span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">'is_duplicate ='</span>, y_test[<span class="number">0</span>], <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>TRAINING QUESTIONS:

Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?
Question 2:  I&#39;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? 

Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?
Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? 

TESTING QUESTIONS:

Question 1:  How do I prepare for interviews for cse?
Question 2:  What is the best way to prepare for cse? 

is_duplicate = 0 
</code></pre><p>You will now encode each word of the selected duplicate pairs with an index. <br> Given a question, you can then just encode it as a list of numbers.  </p>
<p>First you tokenize the questions using <code>nltk.word_tokenize</code>. <br><br>You need a python default dictionary which later, during inference, assigns the values $0$ to all Out Of Vocabulary (OOV) words.<br><br>Then you encode each word of the selected duplicate pairs with an index. Given a question, you can then just encode it as a list of numbers. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#create arrays</span></span><br><span class="line">Q1_train = np.empty_like(Q1_train_words)</span><br><span class="line">Q2_train = np.empty_like(Q2_train_words)</span><br><span class="line"></span><br><span class="line">Q1_test = np.empty_like(Q1_test_words)</span><br><span class="line">Q2_test = np.empty_like(Q2_test_words)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Building the vocabulary with the train set         (this might take a minute)</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">vocab = defaultdict(<span class="keyword">lambda</span>: <span class="number">0</span>)</span><br><span class="line">vocab[<span class="string">'&lt;PAD&gt;'</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(len(Q1_train_words)):</span><br><span class="line">    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])</span><br><span class="line">    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])</span><br><span class="line">    q = Q1_train[idx] + Q2_train[idx]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            vocab[word] = len(vocab) + <span class="number">1</span></span><br><span class="line">print(<span class="string">'The length of the vocabulary is: '</span>, len(vocab))</span><br></pre></td></tr></table></figure>
<pre><code>The length of the vocabulary is:  36268
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(vocab[<span class="string">'&lt;PAD&gt;'</span>])</span><br><span class="line">print(vocab[<span class="string">'Astrology'</span>])</span><br><span class="line">print(vocab[<span class="string">'Astronomy'</span>])  <span class="comment">#not in vocabulary, returns 0</span></span><br></pre></td></tr></table></figure>
<pre><code>1
2
0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(len(Q1_test_words)): </span><br><span class="line">    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])</span><br><span class="line">    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Train set has reduced to: '</span>, len(Q1_train) ) </span><br><span class="line">print(<span class="string">'Test set length: '</span>, len(Q1_test) )</span><br></pre></td></tr></table></figure>
<pre><code>Train set has reduced to:  111486
Test set length:  10240
</code></pre><p><a name="1.2"></a></p>
<h3 id="1-2-Converting-a-question-to-a-tensor"><a href="#1-2-Converting-a-question-to-a-tensor" class="headerlink" title="1.2 Converting a question to a tensor"></a>1.2 Converting a question to a tensor</h3><p>You will now convert every question to a tensor, or an array of numbers, using your vocabulary built above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Converting questions to array of integers</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(Q1_train)):</span><br><span class="line">    Q1_train[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q1_train[i]]</span><br><span class="line">    Q2_train[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q2_train[i]]</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(Q1_test)):</span><br><span class="line">    Q1_test[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q1_test[i]]</span><br><span class="line">    Q2_test[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q2_test[i]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'first question in the train set:\n'</span>)</span><br><span class="line">print(Q1_train_words[<span class="number">0</span>], <span class="string">'\n'</span>) </span><br><span class="line">print(<span class="string">'encoded version:'</span>)</span><br><span class="line">print(Q1_train[<span class="number">0</span>],<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'first question in the test set:\n'</span>)</span><br><span class="line">print(Q1_test_words[<span class="number">0</span>], <span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">'encoded version:'</span>)</span><br><span class="line">print(Q1_test[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>first question in the train set:

Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? 

encoded version:
[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] 

first question in the test set:

How do I prepare for interviews for cse? 

encoded version:
[32, 38, 4, 107, 65, 1015, 65, 11509, 21]
</code></pre><p>You will now split your train set into a training/validation set so that you can use it to train and evaluate your Siamese model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splitting the data</span></span><br><span class="line">cut_off = int(len(Q1_train)*<span class="number">.8</span>)</span><br><span class="line">train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]</span><br><span class="line">val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]</span><br><span class="line">print(<span class="string">'Number of duplicate questions: '</span>, len(Q1_train))</span><br><span class="line">print(<span class="string">"The length of the training set is:  "</span>, len(train_Q1))</span><br><span class="line">print(<span class="string">"The length of the validation set is: "</span>, len(val_Q1))</span><br></pre></td></tr></table></figure>
<pre><code>Number of duplicate questions:  111486
The length of the training set is:   89188
The length of the validation set is:  22298
</code></pre><p><a name="1.3"></a></p>
<h3 id="1-3-Understanding-the-iterator"><a href="#1-3-Understanding-the-iterator" class="headerlink" title="1.3 Understanding the iterator"></a>1.3 Understanding the iterator</h3><p>Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. If you were to use stochastic gradient descent with one example at a time, it will take you forever to build a model. In this example, we show you how you can build a data generator that takes in $Q1$ and $Q2$ and returns a batch of size <code>batch_size</code>  in the following format $([q1_1, q1_2, q1_3, …]$, $[q2_1, q2_2,q2_3, …])$. The tuple consists of two arrays and each array has <code>batch_size</code> questions. Again, $q1_i$ and $q2_i$ are duplicates, but they are not duplicates with any other elements in the batch. </p>
<p><br></p>
<p>The command <figure class="highlight plain"><figcaption><span>the next batch. This iterator returns the data in a format that you could directly use in your model when computing the feed-forward of your algorithm. This iterator returns a pair of arrays of questions. </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;a name=&apos;ex01&apos;&gt;&lt;/a&gt;</span><br><span class="line">### Exercise 01</span><br><span class="line"></span><br><span class="line">**Instructions:**  </span><br><span class="line">Implement the data generator below. Here are some things you will need. </span><br><span class="line"></span><br><span class="line">- While true loop.</span><br><span class="line">- if `index &gt;= len_Q1`, set the `idx` to $0$.</span><br><span class="line">- The generator should return shuffled batches of data. To achieve this without modifying the actual question lists, a list containing the indexes of the questions is created. This list can be shuffled and used to get random batches everytime the index is reset.</span><br><span class="line">- Append elements of $Q1$ and $Q2$ to `input1` and `input2` respectively.</span><br><span class="line">- if `len(input1) == batch_size`, determine `max_len` as the longest question in `input1` and `input2`. Ceil `max_len` to a power of $2$ (for computation purposes) using the following command:  `max_len = 2**int(np.ceil(np.log2(max_len)))`.</span><br><span class="line">- Pad every question by `vocab[&apos;&lt;PAD&gt;&apos;]` until you get the length `max_len`.</span><br><span class="line">- Use yield to return `input1, input2`. </span><br><span class="line">- Don&apos;t forget to reset `input1, input2`  to empty arrays at the end (data generator resumes from where it last left).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span><br><span class="line"># GRADED FUNCTION: data_generator</span><br><span class="line">def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):</span><br><span class="line">    &quot;&quot;&quot;Generator function that yields batches of data</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        Q1 (list): List of transformed (to tensor) questions.</span><br><span class="line">        Q2 (list): List of transformed (to tensor) questions.</span><br><span class="line">        batch_size (int): Number of elements per batch.</span><br><span class="line">        pad (int, optional): Pad character from the vocab. Defaults to 1.</span><br><span class="line">        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.</span><br><span class="line">    Yields:</span><br><span class="line">        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)</span><br><span class="line">        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates</span><br><span class="line">              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    input1 = []</span><br><span class="line">    input2 = []</span><br><span class="line">    idx = 0</span><br><span class="line">    len_q = len(Q1)</span><br><span class="line">    question_indexes = [*range(len_q)]</span><br><span class="line">    </span><br><span class="line">    if shuffle:</span><br><span class="line">        rnd.shuffle(question_indexes)</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE (Replace instances of &apos;None&apos; with your code) ###</span><br><span class="line">    while True:</span><br><span class="line">        if idx &gt;= len_q:</span><br><span class="line">            # if idx is greater than or equal to len_q, set idx accordingly </span><br><span class="line">            # (Hint: look at the instructions above)</span><br><span class="line">            idx = 0</span><br><span class="line">            # shuffle to get random batches if shuffle is set to True</span><br><span class="line">            if shuffle:</span><br><span class="line">                rnd.shuffle(question_indexes)</span><br><span class="line">        </span><br><span class="line">        # get questions at the `question_indexes[idx]` position in Q1 and Q2</span><br><span class="line">        q1 = Q1[question_indexes[idx]]</span><br><span class="line">        q2 = Q2[question_indexes[idx]]</span><br><span class="line">        </span><br><span class="line">        # increment idx by 1</span><br><span class="line">        idx += 1</span><br><span class="line">        # append q1</span><br><span class="line">        input1.append(q1)</span><br><span class="line">        # append q2</span><br><span class="line">        input2.append(q2)</span><br><span class="line">        </span><br><span class="line">        if len(input1) == batch_size:</span><br><span class="line">            # determine max_len as the longest question in input1 &amp; input 2</span><br><span class="line">            # Hint: use the `max` function. </span><br><span class="line">            # take max of input1 &amp; input2 and then max out of the two of them.</span><br><span class="line">            max_len = max(max([len(_) for _ in input1]),max([len(_) for _ in input2]))</span><br><span class="line">            # pad to power-of-2 (Hint: look at the instructions above)</span><br><span class="line">            max_len = 2**int(np.ceil(np.log2(max_len)))</span><br><span class="line">            b1 = []</span><br><span class="line">            b2 = []</span><br><span class="line">            for q1, q2 in zip(input1, input2):</span><br><span class="line">                # add [pad] to q1 until it reaches max_len</span><br><span class="line">                q1 = q1 + [pad] * (max_len - len(q1))</span><br><span class="line">                # add [pad] to q2 until it reaches max_len</span><br><span class="line">                q2 = q2 + [pad] * (max_len - len(q2))</span><br><span class="line">                # append q1</span><br><span class="line">                b1.append(q1)</span><br><span class="line">                # append q2</span><br><span class="line">                b2.append(q2)</span><br><span class="line">            # use b1 and b2</span><br><span class="line">            yield np.array(b1), np.array(b2)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">            # reset the batches</span><br><span class="line">            input1, input2 = [], []  # reset the batches</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))</span><br><span class="line">print(<span class="string">"First questions  : "</span>,<span class="string">'\n'</span>, res1, <span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">"Second questions : "</span>,<span class="string">'\n'</span>, res2)</span><br></pre></td></tr></table></figure>
<pre><code>First questions  :  
 [[  30   87   78  134 2132 1981   28   78  594   21    1    1    1    1
     1    1]
 [  30   55   78 3541 1460   28   56  253   21    1    1    1    1    1
     1    1]] 

Second questions :  
 [[  30  156   78  134 2132 9508   21    1    1    1    1    1    1    1
     1    1]
 [  30  156   78 3541 1460  131   56  253   21    1    1    1    1    1
     1    1]]
</code></pre><p><strong>Note</strong>: The following expected output is valid only if you run the above test cell <strong>_once_</strong> (first time). The output will change on each execution.</p>
<p>If you think your implementation is correct and it is not matching the output, make sure to restart the kernel and run all the cells from the top again. </p>
<p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">First questions  :  </span><br><span class="line"> [[  <span class="number">30</span>   <span class="number">87</span>   <span class="number">78</span>  <span class="number">134</span> <span class="number">2132</span> <span class="number">1981</span>   <span class="number">28</span>   <span class="number">78</span>  <span class="number">594</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]</span><br><span class="line"> [  <span class="number">30</span>   <span class="number">55</span>   <span class="number">78</span> <span class="number">3541</span> <span class="number">1460</span>   <span class="number">28</span>   <span class="number">56</span>  <span class="number">253</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]] </span><br><span class="line"></span><br><span class="line">Second questions :  </span><br><span class="line"> [[  <span class="number">30</span>  <span class="number">156</span>   <span class="number">78</span>  <span class="number">134</span> <span class="number">2132</span> <span class="number">9508</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]</span><br><span class="line"> [  <span class="number">30</span>  <span class="number">156</span>   <span class="number">78</span> <span class="number">3541</span> <span class="number">1460</span>  <span class="number">131</span>   <span class="number">56</span>  <span class="number">253</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]]</span><br></pre></td></tr></table></figure></p>
<p>Now that you have your generator, you can just call it and it will return tensors which correspond to your questions in the Quora data set.<br>Now you can go ahead and start building your neural network. </p>
<p><a name="2"></a></p>
<h1 id="Part-2-Defining-the-Siamese-model"><a href="#Part-2-Defining-the-Siamese-model" class="headerlink" title="Part 2: Defining the Siamese model"></a>Part 2: Defining the Siamese model</h1><p><a name="2.1"></a></p>
<h3 id="2-1-Understanding-Siamese-Network"><a href="#2-1-Understanding-Siamese-Network" class="headerlink" title="2.1 Understanding Siamese Network"></a>2.1 Understanding Siamese Network</h3><p>A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.The Siamese network you are about to implement looks like this:</p>
<p><img src="siamese.png" style="width:600px;height:300px;"></p>
<p>You get the question embedding, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally use a triplet loss (explained below) to get the corresponding cosine similarity for each pair of questions. As usual, you will start by importing the data set. The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. In math equations, you are trying to maximize the following.</p>
<script type="math/tex; mode=display">\mathcal{L}(A, P, N)=\max \left(\|\mathrm{f}(A)-\mathrm{f}(P)\|^{2}-\|\mathrm{f}(A)-\mathrm{f}(N)\|^{2}+\alpha, 0\right)</script><p>$A$ is the anchor input, for example $q1_1$, $P$ the duplicate input, for example, $q2_1$, and $N$ the negative input (the non duplicate question), for example $q2_2$.<br><br>$\alpha$ is a margin; you can think about it as a safety net, or by how much you want to push the duplicates from the non duplicates.<br><br></p>
<p><a name="ex02"></a></p>
<h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement the <code>Siamese</code> function below. You should be using all the objects explained below. </p>
<p>To implement this model, you will be using <code>trax</code>. Concretely, you will be using the following functions.</p>
<ul>
<li><code>tl.Serial</code>: Combinator that applies layers serially (by function composition) allows you set up the overall structure of the feedforward. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26" target="_blank" rel="noopener">source code</a><ul>
<li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas. </li>
<li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code> </li>
</ul>
</li>
</ul>
<ul>
<li><code>tl.Embedding</code>: Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113" target="_blank" rel="noopener">source code</a><ul>
<li><code>tl.Embedding(vocab_size, d_feature)</code>.</li>
<li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li>
<li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li>
</ul>
</li>
</ul>
<ul>
<li><code>tl.LSTM</code> The LSTM layer. It leverages another Trax layer called <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTMCell" target="_blank" rel="noopener"><code>LSTMCell</code></a>. The number of units should be specified and should match the number of elements in the word embedding. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87" target="_blank" rel="noopener">source code</a><ul>
<li><code>tl.LSTM(n_units)</code> Builds an LSTM layer of n_units.</li>
</ul>
</li>
</ul>
<ul>
<li><code>tl.Mean</code>: Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276" target="_blank" rel="noopener">source code</a><ul>
<li><code>tl.Mean(axis=1)</code> mean over columns.</li>
</ul>
</li>
</ul>
<ul>
<li><code>tl.Fn</code> Layer with no weights that applies the function f, which should be specified using a lambda syntax. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/70f5364dcaf6ec11aabbd918e5f5e4b0f5bfb995/trax/layers/base.py#L576" target="_blank" rel="noopener">source doce</a><ul>
<li>$x$ -&gt; This is used for cosine similarity.</li>
<li><code>tl.Fn(&#39;Normalize&#39;, lambda x: normalize(x))</code> Returns a layer with no weights that applies the function <code>f</code></li>
</ul>
</li>
</ul>
<ul>
<li><code>tl.parallel</code>: It is a combinator layer (like <code>Serial</code>) that applies a list of layers in parallel to its inputs. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/37aba571a89a8ad86be76a569d0ec4a46bdd8642/trax/layers/combinators.py#L152" target="_blank" rel="noopener">source code</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Siamese</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Siamese</span><span class="params">(vocab_size=len<span class="params">(vocab)</span>, d_model=<span class="number">128</span>, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a Siamese model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int, optional): Length of the vocabulary. Defaults to len(vocab).</span></span><br><span class="line"><span class="string">        d_model (int, optional): Depth of the model. Defaults to 128.</span></span><br><span class="line"><span class="string">        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to 'train'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Parallel: A Siamese model. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(x)</span>:</span>  <span class="comment"># normalizes the vectors to have L2 norm 1</span></span><br><span class="line">        <span class="keyword">return</span> x / fastnp.sqrt(fastnp.sum(x * x, axis=<span class="number">-1</span>, keepdims=<span class="keyword">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    q_processor = tl.Serial(  <span class="comment"># Processor will run on Q1 and Q2.</span></span><br><span class="line">        tl.Embedding(vocab_size, d_model), <span class="comment"># Embedding layer</span></span><br><span class="line">        tl.LSTM(d_model), <span class="comment"># LSTM layer</span></span><br><span class="line">        tl.Mean(axis=<span class="number">1</span>), <span class="comment"># Mean over columns</span></span><br><span class="line">        tl.Fn(<span class="string">'Normalize'</span>, <span class="keyword">lambda</span> x: normalize(x))  <span class="comment"># Apply normalize function</span></span><br><span class="line">    )  <span class="comment"># Returns one vector of shape [batch_size, d_model].</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run on Q1 and Q2 in parallel.</span></span><br><span class="line">    model = tl.Parallel(q_processor, q_processor)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>Setup the Siamese network model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check your model</span></span><br><span class="line">model = Siamese()</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<pre><code>Parallel_in2_out2[
  Serial[
    Embedding_41699_128
    LSTM_128
    Mean
    Normalize
  ]
  Serial[
    Embedding_41699_128
    LSTM_128
    Mean
    Normalize
  ]
]
</code></pre><p><strong>Expected output:</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Parallel_in2_out2[</span><br><span class="line">  Serial[</span><br><span class="line">    Embedding_41699_128</span><br><span class="line">    LSTM_128</span><br><span class="line">    Mean</span><br><span class="line">    Normalize</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Embedding_41699_128</span><br><span class="line">    LSTM_128</span><br><span class="line">    Mean</span><br><span class="line">    Normalize</span><br><span class="line">  ]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p><a name="2.2"></a></p>
<h3 id="2-2-Hard-Negative-Mining"><a href="#2-2-Hard-Negative-Mining" class="headerlink" title="2.2 Hard  Negative Mining"></a>2.2 Hard  Negative Mining</h3><p>You will now implement the <code>TripletLoss</code>.<br><br>As explained in the lecture, loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the <em>closest negative</em>. Our loss expression is then:</p>
<p>\begin{align}<br> \mathcal{Loss_1(A,P,N)} &amp;=\max \left( -cos(A,P)  + mean_{neg} +\alpha, 0\right) \\<br> \mathcal{Loss_2(A,P,N)} &amp;=\max \left( -cos(A,P)  + closest_{neg} +\alpha, 0\right) \\<br>\mathcal{Loss(A,P,N)} &amp;= mean(Loss_1 + Loss_2) \\<br>\end{align}</p>
<p>Further, two sets of instructions are provided. The first set provides a brief description of the task. If that set proves insufficient, a more detailed set can be displayed.  </p>
<p><a name="ex03"></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions (Brief):</strong> Here is a list of things you should do: <br></p>
<ul>
<li>As this will be run inside trax, use <code>fastnp.xyz</code> when using any <code>xyz</code> numpy function</li>
<li>Use <code>fastnp.dot</code> to calculate the similarity matrix $v_1v_2^T$ of dimension <code>batch_size</code> x <code>batch_size</code></li>
<li>Take the score of the duplicates on the diagonal <code>fastnp.diagonal</code></li>
<li>Use the <code>trax</code> functions <code>fastnp.eye</code> and <code>fastnp.maximum</code> for the identity matrix and the maximum.</li>
</ul>
<p><details>    </details></p>
<p><summary>
    <font size="3" color="darkgreen"><b>More Detailed Instructions </b></font>
</summary><br>We’ll describe the algorithm using a detailed example. Below, V1, V2 are the output of the normalization blocks in our model. Here we will use a batch_size of 4 and a d_model of 3. As explained in lecture, the inputs, Q1, Q2 are arranged so that corresponding inputs are duplicates while non-corresponding entries are not. The outputs will have the same pattern.<br><img src="C3_W4_triploss1.png" style="width:1021px;height:229px;"><br>This testcase arranges the outputs, v1,v2, to highlight different scenarios. Here, the first two outputs V1[0], V2[0] match exactly - so the model is generating the same vector for Q1[0] and Q2[0] inputs. The second outputs differ, circled in orange, we set, V2[1] is set to match V2[<strong>2</strong>], simulating a model which is generating very poor results. V1[3] and V2[3] match exactly again while V1[4] and V2[4] are set to be exactly wrong - 180 degrees from each other, circled in blue. </p>
<p>The first step is to compute the cosine similarity matrix or <code>score</code> in the code. As explained in lecture, this is <script type="math/tex">V_1 V_2^T</script> This is generated with <code>fastnp.dot</code>.<br><img src="C3_W4_triploss2.png" style="width:959px;height:236px;"><br>The clever arrangement of inputs creates the data needed for positive <em>and</em> negative examples without having to run all pair-wise combinations. Because Q1[n] is a duplicate of only Q2[n], other combinations are explicitly created negative examples or <em>Hard Negative</em> examples. The matrix multiplication efficiently produces the cosine similarity of all positive/negative combinations as shown above on the left side of the diagram. ‘Positive’ are the results of duplicate examples and ‘negative’ are the results of explicitly created negative examples. The results for our test case are as expected, V1[0]V2[0] match producing ‘1’ while our other ‘positive’ cases (in green) don’t match well, as was arranged. The V2[2] was set to match V1[3] producing a poor match at <code>score[2,2]</code> and an undesired ‘negative’ case of a ‘1’ shown in grey. </p>
<p>With the similarity matrix (<code>score</code>) we can begin to implement the loss equations. First, we can extract <script type="math/tex">cos(A,P)</script> by utilizing <code>fastnp.diagonal</code>. The goal is to grab all the green entries in the diagram above. This is <code>positive</code> in the code.</p>
<p>Next, we will create the <em>closest_negative</em>. This is the nonduplicate entry in V2 that is closest (has largest cosine similarity) to an entry in V1. Each row, n, of <code>score</code> represents all comparisons of the results of Q1[n] vs Q2[x] within a batch. A specific example in our testcase is row <code>score[2,:]</code>. It has the cosine similarity of V1[2] and V2[x]. The <em>closest_negative</em>, as was arranged, is V2[2] which has a score of 1. This is the maximum value of the ‘negative’ entries (blue entries in the diagram).</p>
<p>To implement this, we need to pick the maximum entry on a row of <code>score</code>, ignoring the ‘positive’/green entries. To avoid selecting the ‘positive’/green entries, we can make them larger negative numbers. Multiply <code>fastnp.eye(batch_size)</code> with 2.0 and subtract it out of <code>scores</code>. The result is <code>negative_without_positive</code>. Now we can use <code>fastnp.max</code>, row by row (axis=1), to select the maximum which is <code>closest_negative</code>.</p>
<p>Next, we’ll create <em>mean_negative</em>. As the name suggests, this is the mean of all the ‘negative’/blue values in <code>score</code> on a row by row basis. We can use <code>fastnp.eye(batch_size)</code> and a constant, this time to create a mask with zeros on the diagonal. Element-wise multiply this with <code>score</code> to get just the ‘negative values. This is <code>negative_zero_on_duplicate</code> in the code. Compute the mean by using <code>fastnp.sum</code> on <code>negative_zero_on_duplicate</code> for <code>axis=1</code> and divide it by <code>(batch_size - 1)</code> . This is <code>mean_negative</code>.</p>
<p>Now, we can compute loss using the two equations above and <code>fastnp.maximum</code>. This will form <code>triplet_loss1</code> and <code>triplet_loss2</code>. </p>
<p><code>triple_loss</code> is the <code>fastnp.mean</code> of the sum of the two individual losses.</p>
<p>Once you have this code matching the expected results, you can clip out the section between ### START CODE HERE and ### END CODE HERE it out and insert it into TripletLoss below.</p>
<p>&lt;\details&gt;  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TripletLossFn</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TripletLossFn</span><span class="params">(v1, v2, margin=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Custom Loss function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.</span></span><br><span class="line"><span class="string">        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.</span></span><br><span class="line"><span class="string">        margin (float, optional): Desired margin. Defaults to 0.25.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: Triplet Loss.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use fastnp to take the dot product of the two batches (don't forget to transpose the second argument)</span></span><br><span class="line">    scores = fastnp.dot(v1,v2.T)  <span class="comment"># pairwise cosine sim</span></span><br><span class="line">    <span class="comment"># calculate new batch size</span></span><br><span class="line">    batch_size = len(scores)</span><br><span class="line">    <span class="comment"># use fastnp to grab all postive `diagonal` entries in `scores`</span></span><br><span class="line">    positive = fastnp.diagonal(scores)  <span class="comment"># the positive ones (duplicates)</span></span><br><span class="line">    <span class="comment"># multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`</span></span><br><span class="line">    negative_without_positive = scores - fastnp.eye(batch_size) * <span class="number">2.0</span> </span><br><span class="line">    <span class="comment"># take the row by row `max` of `negative_without_positive`. </span></span><br><span class="line">    <span class="comment"># Hint: negative_without_positive.max(axis = [?])  </span></span><br><span class="line">    closest_negative = negative_without_positive.max(axis = <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`</span></span><br><span class="line">    negative_zero_on_duplicate = (<span class="number">1.0</span> - fastnp.eye(batch_size)) * scores</span><br><span class="line">    <span class="comment"># use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` </span></span><br><span class="line">    mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=<span class="number">1</span>) / (batch_size - <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># compute `fastnp.maximum` among 0.0 and `A`</span></span><br><span class="line">    <span class="comment"># A = subtract `positive` from `margin` and add `closest_negative` </span></span><br><span class="line">    triplet_loss1 = fastnp.maximum(margin - positive + closest_negative, <span class="number">0</span> )</span><br><span class="line">    <span class="comment"># compute `fastnp.maximum` among 0.0 and `B`</span></span><br><span class="line">    <span class="comment"># B = subtract `positive` from `margin` and add `mean_negative`</span></span><br><span class="line">    triplet_loss2 = fastnp.maximum(margin - positive + mean_negative, <span class="number">0</span> )</span><br><span class="line">    <span class="comment"># add the two losses together and take the `fastnp.mean` of it</span></span><br><span class="line">    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> triplet_loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v1 = np.array([[<span class="number">0.26726124</span>, <span class="number">0.53452248</span>, <span class="number">0.80178373</span>],[<span class="number">0.5178918</span> , <span class="number">0.57543534</span>, <span class="number">0.63297887</span>]])</span><br><span class="line">v2 = np.array([[ <span class="number">0.26726124</span>,  <span class="number">0.53452248</span>,  <span class="number">0.80178373</span>],[<span class="number">-0.5178918</span> , <span class="number">-0.57543534</span>, <span class="number">-0.63297887</span>]])</span><br><span class="line">TripletLossFn(v2,v1)</span><br><span class="line">print(<span class="string">"Triplet Loss:"</span>, TripletLossFn(v2,v1))</span><br></pre></td></tr></table></figure>
<pre><code>Triplet Loss: 0.5
</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Triplet Loss: <span class="number">0.5</span></span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line">To make a layer out of a function with no trainable variables, use `tl.Fn`.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">from functools <span class="keyword">import</span> partial</span><br><span class="line">def TripletLoss(margin=0.25):</span><br><span class="line">    triplet_loss_fn = partial(TripletLossFn, margin=margin)</span><br><span class="line">    return tl.Fn('TripletLoss', triplet_loss_fn)</span><br></pre></td></tr></table></figure></p>
<p><a name="3"></a></p>
<h1 id="Part-3-Training"><a href="#Part-3-Training" class="headerlink" title="Part 3: Training"></a>Part 3: Training</h1><p>Now you are going to train your model. As usual, you have to define the cost function and the optimizer. You also have to feed in the built model. Before, going into the training, we will use a special data set up. We will define the inputs using the data generator we built above. The lambda function acts as a seed to remember the last batch that was given. Run the cell below to get the question pairs inputs. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab[<span class="string">'&lt;PAD&gt;'</span>])</span><br><span class="line">val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab[<span class="string">'&lt;PAD&gt;'</span>])</span><br><span class="line">print(<span class="string">'train_Q1.shape '</span>, train_Q1.shape)</span><br><span class="line">print(<span class="string">'val_Q1.shape   '</span>, val_Q1.shape)</span><br></pre></td></tr></table></figure>
<pre><code>train_Q1.shape  (89188,)
val_Q1.shape    (22298,)
</code></pre><p><a name="3.1"></a></p>
<h3 id="3-1-Training-the-model"><a href="#3-1-Training-the-model" class="headerlink" title="3.1 Training the model"></a>3.1 Training the model</h3><p>You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set; each iteration is defined as an <code>epoch</code>. For each epoch, you have to go over all the data, using your training iterator.</p>
<p><a name="ex04"></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement the <code>train_model</code> below to train the neural network above. Here is a list of things you should do, as already shown in lecture 7: </p>
<ul>
<li>Create <code>TrainTask</code> and <code>EvalTask</code></li>
<li>Create the training loop <code>trax.supervised.training.Loop</code></li>
<li>Pass in the following depending on the context (train_task or eval_task):<ul>
<li><code>labeled_data=generator</code></li>
<li><code>metrics=[TripletLoss()]</code>,</li>
<li><code>loss_layer=TripletLoss()</code></li>
<li><code>optimizer=trax.optimizers.Adam</code> with learning rate of 0.01</li>
<li><code>lr_schedule=lr_schedule</code>,</li>
<li><code>output_dir=output_dir</code></li>
</ul>
</li>
</ul>
<p>You will be using your triplet loss function with Adam optimizer. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam" target="_blank" rel="noopener">trax</a> documentation to get a full understanding. </p>
<p>This function should return a <code>training.Loop</code> object. To read more about this check the <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop" target="_blank" rel="noopener">docs</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">lr_schedule = trax.lr.warmup_and_rsqrt_decay(<span class="number">400</span>, <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir=<span class="string">'model/'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Training the Siamese Model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Siamese (function): Function that returns the Siamese model.</span></span><br><span class="line"><span class="string">        TripletLoss (function): Function that defines the TripletLoss loss function.</span></span><br><span class="line"><span class="string">        lr_schedule (function): Trax multifactor schedule function.</span></span><br><span class="line"><span class="string">        train_generator (generator, optional): Training generator. Defaults to train_generator.</span></span><br><span class="line"><span class="string">        val_generator (generator, optional): Validation generator. Defaults to val_generator.</span></span><br><span class="line"><span class="string">        output_dir (str, optional): Path to save model to. Defaults to 'model/'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop for the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    output_dir = os.path.expanduser(output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    train_task = training.TrainTask(</span><br><span class="line">        labeled_data=train_generator,       <span class="comment"># Use generator (train)</span></span><br><span class="line">        loss_layer=TripletLoss(),         <span class="comment"># Use triplet loss. Don't forget to instantiate this object</span></span><br><span class="line">        optimizer=trax.optimizers.Adam(learning_rate = <span class="number">0.01</span>),          <span class="comment"># Don't forget to add the learning rate parameter</span></span><br><span class="line">        lr_schedule=lr_schedule, <span class="comment"># Use Trax multifactor schedule function</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask(</span><br><span class="line">        labeled_data=val_generator,       <span class="comment"># Use generator (val)</span></span><br><span class="line">        metrics=[TripletLoss()],          <span class="comment"># Use triplet loss. Don't forget to instantiate this object</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    training_loop = training.Loop(Siamese(),</span><br><span class="line">                                  train_task,</span><br><span class="line">                                  eval_task=eval_task,</span><br><span class="line">                                  output_dir=output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> training_loop</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_steps = <span class="number">5</span></span><br><span class="line">training_loop = train_model(Siamese, TripletLoss, lr_schedule)</span><br><span class="line">training_loop.run(train_steps)</span><br></pre></td></tr></table></figure>
<pre><code>Step      1: train TripletLoss |  0.49954823
Step      1: eval  TripletLoss |  0.49950948
</code></pre><p>The model was only trained for 5 steps due to the constraints of this environment. For the rest of the assignment you will be using a pretrained model but now you should understand how the training can be done using Trax.</p>
<p><a name="4"></a></p>
<h1 id="Part-4-Evaluation"><a href="#Part-4-Evaluation" class="headerlink" title="Part 4:  Evaluation"></a>Part 4:  Evaluation</h1><p><a name="4.1"></a></p>
<h3 id="4-1-Evaluating-your-siamese-network"><a href="#4-1-Evaluating-your-siamese-network" class="headerlink" title="4.1 Evaluating your siamese network"></a>4.1 Evaluating your siamese network</h3><p>In this section you will learn how to evaluate a Siamese network. You will first start by loading a pretrained model and then you will use it to predict. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading in the saved model</span></span><br><span class="line">model = Siamese()</span><br><span class="line">model.init_from_file(<span class="string">'model.pkl.gz'</span>)</span><br></pre></td></tr></table></figure>
<p><a name="4.2"></a></p>
<h3 id="4-2-Classify"><a href="#4-2-Classify" class="headerlink" title="4.2 Classify"></a>4.2 Classify</h3><p>To determine the accuracy of the model, we will utilize the test set that was configured earlier. While in training we used only positive examples, the test data, Q1_test, Q2_test and y_test, is setup as pairs of questions, some of which are duplicates some are not.<br>This routine will run all the test question pairs through the model, compute the cosine simlarity of each pair, threshold it and compare the result to  y_test - the correct response from the data set. The results are accumulated to produce an accuracy.</p>
<p><a name="ex05"></a></p>
<h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions</strong>  </p>
<ul>
<li>Loop through the incoming data in batch_size chunks</li>
<li>Use the data generator to load q1, q2 a batch at a time. <strong>Don’t forget to set shuffle=False!</strong></li>
<li>copy a batch_size chunk of y into y_test</li>
<li>compute v1, v2 using the model</li>
<li>for each element of the batch<pre><code> - compute the cos similarity of each pair of entries, v1[j],v2[j]
 - determine if d &gt; threshold
 - increment accuracy if that result matches the expected results (y_test[j])
</code></pre></li>
<li>compute the final accuracy and return</li>
</ul>
<p>Due to some limitations of this environment, running classify multiple times may result in the kernel failing. If that happens <em>Restart Kernal &amp; clear output</em> and then run from the top. During development, consider using a smaller set of data to reduce the number of calls to model(). </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: classify</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=<span class="number">64</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Function to test the accuracy of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        test_Q1 (numpy.ndarray): Array of Q1 questions.</span></span><br><span class="line"><span class="string">        test_Q2 (numpy.ndarray): Array of Q2 questions.</span></span><br><span class="line"><span class="string">        y (numpy.ndarray): Array of actual target.</span></span><br><span class="line"><span class="string">        threshold (float): Desired threshold.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Parallel): The Siamese model.</span></span><br><span class="line"><span class="string">        vocab (collections.defaultdict): The vocabulary used.</span></span><br><span class="line"><span class="string">        data_generator (function): Data generator function. Defaults to data_generator.</span></span><br><span class="line"><span class="string">        batch_size (int, optional): Size of the batches. Defaults to 64.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float: Accuracy of the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    accuracy = <span class="number">0</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(test_Q1), batch_size):</span><br><span class="line">        <span class="comment"># Call the data generator (built in Ex 01) with shuffle=False using next()</span></span><br><span class="line">        <span class="comment"># use batch size chuncks of questions as Q1 &amp; Q2 arguments of the data generator. e.g x[i:i + batch_size]</span></span><br><span class="line">        <span class="comment"># Hint: use `vocab['&lt;PAD&gt;']` for the `pad` argument of the data generator</span></span><br><span class="line">        q1, q2 = next(data_generator(test_Q1[i: i + batch_size], test_Q2[i: i+batch_size], batch_size, pad=vocab[<span class="string">'&lt;PAD&gt;'</span>], shuffle=<span class="keyword">False</span>))</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(q1.shape)</span></span><br><span class="line"><span class="comment">#         print(q2.shape)</span></span><br><span class="line"><span class="comment">#         (512, 64)</span></span><br><span class="line"><span class="comment">#         (512, 64)</span></span><br><span class="line">        <span class="comment"># use batch size chuncks of actual output targets (same syntax as example above)</span></span><br><span class="line">        y_test = y[i: i + batch_size]</span><br><span class="line">        <span class="comment"># Call the model</span></span><br><span class="line">        v1, v2 = model((q1,q2))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="comment"># take dot product to compute cos similarity of each pair of entries, v1[j], v2[j]</span></span><br><span class="line">            <span class="comment"># don't forget to transpose the second argument</span></span><br><span class="line">            d = fastnp.dot(v1[j],v2[j].T)</span><br><span class="line">            <span class="comment"># is d greater than the threshold?</span></span><br><span class="line">            res = d &gt; threshold</span><br><span class="line">            <span class="comment"># increment accurancy if y_test is equal `res`</span></span><br><span class="line">            accuracy += (y_test[j] == res)</span><br><span class="line">    <span class="comment"># compute accuracy using accuracy and total length of test questions</span></span><br><span class="line">    accuracy = accuracy / len(test_Q1)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this takes around 1 minute</span></span><br><span class="line">accuracy = classify(Q1_test,Q2_test, y_test, <span class="number">0.7</span>, model, vocab, batch_size = <span class="number">512</span>) </span><br><span class="line">print(<span class="string">"Accuracy"</span>, accuracy)</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy 0.69091797
</code></pre><p><strong>Expected Result</strong><br>Accuracy ~0.69</p>
<p><a name="5"></a></p>
<h1 id="Part-5-Testing-with-your-own-questions"><a href="#Part-5-Testing-with-your-own-questions" class="headerlink" title="Part 5: Testing with your own questions"></a>Part 5: Testing with your own questions</h1><p>In this section you will test the model with your own questions. You will write a function <code>predict</code> which takes two questions as input and returns $1$ or $0$ depending on whether the question pair is a duplicate or not.   </p>
<p>But first, we build a reverse vocabulary that allows to map encoded questions back to words: </p>
<p>Write a function <code>predict</code>that takes in two questions, the model, and the vocabulary and returns whether the questions are duplicates ($1$) or not duplicates ($0$) given a similarity threshold. </p>
<p><a name="ex06"></a></p>
<h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> </p>
<ul>
<li>Tokenize your question using <code>nltk.word_tokenize</code> </li>
<li>Create Q1,Q2 by encoding your questions as a list of numbers using vocab</li>
<li>pad Q1,Q2 with next(data_generator([Q1], [Q2],1,vocab[‘<pad>‘]))</pad></li>
<li>use model() to create v1, v2</li>
<li>compute the cosine similarity (dot product) of v1, v2</li>
<li>compute res by comparing d to the threshold</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">"""Function for predicting if two questions are duplicates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        question1 (str): First question.</span></span><br><span class="line"><span class="string">        question2 (str): Second question.</span></span><br><span class="line"><span class="string">        threshold (float): Desired threshold.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Parallel): The Siamese model.</span></span><br><span class="line"><span class="string">        vocab (collections.defaultdict): The vocabulary used.</span></span><br><span class="line"><span class="string">        data_generator (function): Data generator function. Defaults to data_generator.</span></span><br><span class="line"><span class="string">        verbose (bool, optional): If the results should be printed out. Defaults to False.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bool: True if the questions are duplicates, False otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># use `nltk` word tokenize function to tokenize</span></span><br><span class="line">    q1 = nltk.word_tokenize(question1)  <span class="comment"># tokenize</span></span><br><span class="line">    q2 = nltk.word_tokenize(question2)  <span class="comment"># tokenize</span></span><br><span class="line">    Q1, Q2 = [], []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q1:  <span class="comment"># encode q1</span></span><br><span class="line">        <span class="comment"># increment by checking the 'word' index in `vocab`</span></span><br><span class="line">        Q1 += [vocab[word]]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q2:  <span class="comment"># encode q2</span></span><br><span class="line">        <span class="comment"># increment by checking the 'word' index in `vocab`</span></span><br><span class="line">        Q2 += [vocab[word]]</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Call the data generator (built in Ex 01) using next()</span></span><br><span class="line">    <span class="comment"># pass [Q1] &amp; [Q2] as Q1 &amp; Q2 arguments of the data generator. Set batch size as 1</span></span><br><span class="line">    <span class="comment"># Hint: use `vocab['&lt;PAD&gt;']` for the `pad` argument of the data generator</span></span><br><span class="line">    Q1, Q2 = next(data_generator([Q1], [Q2], <span class="number">1</span>, vocab[<span class="string">'&lt;PAD&gt;'</span>]))</span><br><span class="line">    <span class="comment"># Call the model</span></span><br><span class="line">    v1, v2 = model((Q1,Q2))</span><br><span class="line">    <span class="comment"># take dot product to compute cos similarity of each pair of entries, v1, v2</span></span><br><span class="line">    <span class="comment"># don't forget to transpose the second argument</span></span><br><span class="line">    d = fastnp.dot(v1, v2.T)</span><br><span class="line">    <span class="comment"># is d greater than the threshold?</span></span><br><span class="line">    res = d &gt; threshold</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(verbose):</span><br><span class="line">        print(<span class="string">"Q1  = "</span>, Q1, <span class="string">"\nQ2  = "</span>, Q2)</span><br><span class="line">        print(<span class="string">"d   = "</span>, d)</span><br><span class="line">        print(<span class="string">"res = "</span>, res)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feel free to try with your own questions</span></span><br><span class="line">question1 = <span class="string">"When will I see you?"</span></span><br><span class="line">question2 = <span class="string">"When can I see you again?"</span></span><br><span class="line"><span class="comment"># 1 means it is duplicated, 0 otherwise</span></span><br><span class="line">predict(question1 , question2, <span class="number">0.7</span>, model, vocab, verbose = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Q1  =  [[585  76   4  46  53  21   1   1]] 
Q2  =  [[ 585   33    4   46   53 7280   21    1]]
d   =  [[0.8811324]]
res =  [[ True]]





DeviceArray([[ True]], dtype=bool)
</code></pre><h5 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h5><p>If input is:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">question1 = <span class="string">"When will I see you?"</span></span><br><span class="line">question2 = <span class="string">"When can I see you again?"</span></span><br></pre></td></tr></table></figure></p>
<p>Output is (d may vary a bit):<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q1  =  [[<span class="number">585</span>  <span class="number">76</span>   <span class="number">4</span>  <span class="number">46</span>  <span class="number">53</span>  <span class="number">21</span>   <span class="number">1</span>   <span class="number">1</span>]] </span><br><span class="line">Q2  =  [[ <span class="number">585</span>   <span class="number">33</span>    <span class="number">4</span>   <span class="number">46</span>   <span class="number">53</span> <span class="number">7280</span>   <span class="number">21</span>    <span class="number">1</span>]]</span><br><span class="line">d   =  <span class="number">0.88113236</span></span><br><span class="line">res =  True</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feel free to try with your own questions</span></span><br><span class="line">question1 = <span class="string">"Do they enjoy eating the dessert?"</span></span><br><span class="line">question2 = <span class="string">"Do they like hiking in the desert?"</span></span><br><span class="line"><span class="comment"># 1 means it is duplicated, 0 otherwise</span></span><br><span class="line">predict(question1 , question2, <span class="number">0.7</span>, model, vocab, verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Q1  =  [[  443  1145  3159  1169    78 29017    21     1]] 
Q2  =  [[  443  1145    60 15302    28    78  7431    21]]
d   =  [[0.477536]]
res =  [[False]]





DeviceArray([[False]], dtype=bool)
</code></pre><h5 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h5><p>If input is:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">question1 = <span class="string">"Do they enjoy eating the dessert?"</span></span><br><span class="line">question2 = <span class="string">"Do they like hiking in the desert?"</span></span><br></pre></td></tr></table></figure></p>
<p>Output  (d may vary a bit):</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q1  =  [[  <span class="number">443</span>  <span class="number">1145</span>  <span class="number">3159</span>  <span class="number">1169</span>    <span class="number">78</span> <span class="number">29017</span>    <span class="number">21</span>     <span class="number">1</span>]] </span><br><span class="line">Q2  =  [[  <span class="number">443</span>  <span class="number">1145</span>    <span class="number">60</span> <span class="number">15302</span>    <span class="number">28</span>    <span class="number">78</span>  <span class="number">7431</span>    <span class="number">21</span>]]</span><br><span class="line">d   =  <span class="number">0.477536</span></span><br><span class="line">res =  False</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<p>You can see that the Siamese network is capable of catching complicated structures. Concretely it can identify question duplicates although the questions do not have many words in common. </p>
<p><a name="6"></a></p>
<h3 id="On-Siamese-networks"><a href="#On-Siamese-networks" class="headerlink" title=" On Siamese networks "></a><span style="color:blue"> On Siamese networks </span></h3><p>Siamese networks are important and useful. Many times there are several questions that are already asked in quora, or other platforms and you can use Siamese networks to avoid question duplicates. </p>
<p>Congratulations, you have now built a powerful system that can recognize question duplicates. In the next course we will use transformers for machine translation, summarization, question answering, and chatbots. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Named-Entity-Recognition-NER/2020/08/23/" rel="next" title="Named Entity Recognition (NER)">
                <i class="fa fa-chevron-left"></i> Named Entity Recognition (NER)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Bandits-and-Exploration-Exploitation/2020/09/03/" rel="prev" title="Bandits and Exploration/Exploitation">
                Bandits and Exploration/Exploitation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate article here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Ruochi Zhang WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    

    
      <div id="bitcoin" style="display: inline-block">
        <img id="vemo_qr" src="/images/venmo.png" alt="Ruochi Zhang Bitcoin">
        <p>Venmo(last 4 digits 1570)</p>
      </div>
    

  </div>
</div>

      </div>
    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">246</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Assignment-4-Question-duplicates"><span class="nav-text">Assignment 4:  Question duplicates</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline"><span class="nav-text">Outline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Overview"><span class="nav-text">Overview</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-1-Importing-the-Data"><span class="nav-text">Part 1: Importing the Data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Loading-in-the-data"><span class="nav-text">1.1 Loading in the data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Converting-a-question-to-a-tensor"><span class="nav-text">1.2 Converting a question to a tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Understanding-the-iterator"><span class="nav-text">1.3 Understanding the iterator</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-2-Defining-the-Siamese-model"><span class="nav-text">Part 2: Defining the Siamese model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Understanding-Siamese-Network"><span class="nav-text">2.1 Understanding Siamese Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-02"><span class="nav-text">Exercise 02</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Hard-Negative-Mining"><span class="nav-text">2.2 Hard  Negative Mining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-03"><span class="nav-text">Exercise 03</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-3-Training"><span class="nav-text">Part 3: Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Training-the-model"><span class="nav-text">3.1 Training the model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-04"><span class="nav-text">Exercise 04</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-4-Evaluation"><span class="nav-text">Part 4:  Evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Evaluating-your-siamese-network"><span class="nav-text">4.1 Evaluating your siamese network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Classify"><span class="nav-text">4.2 Classify</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-05"><span class="nav-text">Exercise 05</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Part-5-Testing-with-your-own-questions"><span class="nav-text">Part 5: Testing with your own questions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-06"><span class="nav-text">Exercise 06</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-Output"><span class="nav-text">Expected Output</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Expected-output"><span class="nav-text">Expected output</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-Siamese-networks"><span class="nav-text"> On Siamese networks </span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
