<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="cs231n,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier.">
<meta name="keywords" content="cs231n">
<meta property="og:type" content="article">
<meta property="og:title" content="cs231n assignment1">
<meta property="og:url" content="https://zhangruochi.com/cs231n-assignment1/2019/10/01/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier.">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-10-04T21:15:39.342Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="cs231n assignment1">
<meta name="twitter:description" content="In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/cs231n-assignment1/2019/10/01/">





  <title>cs231n assignment1 | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/cs231n-assignment1/2019/10/01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">cs231n assignment1</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-01T00:16:15+08:00">
                2019-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/cs231n-assignment1/2019/10/01/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/cs231n-assignment1/2019/10/01/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          
              <div class="post-description">
                  In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier.
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="The-goals-of-this-assignment-are-as-follows"><a href="#The-goals-of-this-assignment-are-as-follows" class="headerlink" title="The goals of this assignment are as follows"></a>The goals of this assignment are as follows</h2><ul>
<li>understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)</li>
<li>understand the train/val/test splits and the use of validation data for hyperparameter tuning.</li>
<li>develop proficiency in writing efficient vectorized code with numpy</li>
<li>implement and apply a k-Nearest Neighbor (kNN) classifier</li>
<li>implement and apply a Multiclass Support Vector Machine (SVM) classifier</li>
<li>implement and apply a Softmax classifier</li>
<li>implement and apply a Two layer neural network classifier</li>
<li>understand the differences and tradeoffs between these classifiers</li>
<li>get a basic understanding of performance improvements from using higher-level representations than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features)</li>
</ul>
<h2 id="k-Nearest-Neighbor-classifier"><a href="#k-Nearest-Neighbor-classifier" class="headerlink" title="k-Nearest Neighbor classifier"></a>k-Nearest Neighbor classifier</h2><p>The kNN classifier consists of two stages:</p>
<ul>
<li>During training, the classifier takes the training data and simply remembers it</li>
<li>During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples</li>
<li>The value of k is cross-validated</li>
</ul>
<p>In this exercise you will implement these steps and understand the basic Image Classification pipeline, cross-validation, and gain proficiency in writing efficient, vectorized code.</p>
<h3 id="Key-point"><a href="#Key-point" class="headerlink" title="Key point"></a>Key point</h3><ol>
<li>When we compute the distances wiout loop, these is a important observation</li>
</ol>
<script type="math/tex; mode=display">(a-b)^{2} = a^{2} + b^{2} - 2ab</script><p>for example, if we want to compute the distance between $\vec A$ and $\vec B$, the l2 norm is:</p>
<script type="math/tex; mode=display">\sqrt{\sum_{i=1}^{D}(A_i - B_i)^{2}}</script><p>so we can factoring the equation above:</p>
<script type="math/tex; mode=display">\sqrt{\sum_{i=1}^{D}(A_i)^{2} + \sum_{i=1}^{D}(B_i)^{2} - 2\sum_{i=1}^{D}(A_i\cdot B_i )}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test_sum = np.sum(np.square(X),axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">train_sum = np.sum(np.square(self.X_train),axis = <span class="number">1</span>)</span><br><span class="line">cross_sum = X @ self.X_train.T</span><br><span class="line">dists = np.sqrt(train_sum + test_sum - <span class="number">2</span>*cross_sum)</span><br></pre></td></tr></table></figure>
<h3 id="KNN-code"><a href="#KNN-code" class="headerlink" title="KNN code"></a>KNN code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> range</span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> object</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" a kNN classifier with L2 distance """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Train the classifier. For k-nearest neighbors this is just</span></span><br><span class="line"><span class="string">        memorizing the training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (num_train, D) containing the training data</span></span><br><span class="line"><span class="string">          consisting of num_train samples each of dimension D.</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (N,) containing the training labels, where</span></span><br><span class="line"><span class="string">             y[i] is the label for X[i].</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.X_train = X</span><br><span class="line">        self.y_train = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Predict labels for test data using this classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (num_test, D) containing test data consisting</span></span><br><span class="line"><span class="string">             of num_test samples each of dimension D.</span></span><br><span class="line"><span class="string">        - k: The number of nearest neighbors that vote for the predicted labels.</span></span><br><span class="line"><span class="string">        - num_loops: Determines which implementation to use to compute distances</span></span><br><span class="line"><span class="string">          between training points and testing points.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">          test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> num_loops == <span class="number">0</span>:</span><br><span class="line">            dists = self.compute_distances_no_loops(X)</span><br><span class="line">        <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</span><br><span class="line">            dists = self.compute_distances_one_loop(X)</span><br><span class="line">        <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</span><br><span class="line">            dists = self.compute_distances_two_loops(X)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Invalid value %d for num_loops'</span> % num_loops)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.predict_labels(dists, k=k)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_two_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">        in self.X_train using a nested loop over both the training data and the</span></span><br><span class="line"><span class="string">        test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (num_test, D) containing test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">          is the Euclidean distance between the ith test point and the jth training</span></span><br><span class="line"><span class="string">          point.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">        dists = np.zeros((num_test, num_train))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_train):</span><br><span class="line">                <span class="comment">#####################################################################</span></span><br><span class="line">                <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">                <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">                <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">                <span class="comment"># not use a loop over dimension, nor use np.linalg.norm().          #</span></span><br><span class="line">                <span class="comment">#####################################################################</span></span><br><span class="line">                <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">                dists[i,j] = np.sqrt(np.sum(np.square(X[i] - self.X_train[j])))</span><br><span class="line"></span><br><span class="line">                <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_one_loop</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">        in self.X_train using a single loop over the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">        dists = np.zeros((num_test, num_train))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">            <span class="comment">#######################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">            <span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">            <span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">            <span class="comment"># Do not use np.linalg.norm().                                        #</span></span><br><span class="line">            <span class="comment">#######################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">            dists[i] = np.sqrt(np.sum(np.square(self.X_train-X[i]), axis = <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">        in self.X_train using no explicit loops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">        dists = np.zeros((num_test, num_train))</span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">        <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">        <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">        <span class="comment"># dists.                                                                #</span></span><br><span class="line">        <span class="comment">#                                                                       #</span></span><br><span class="line">        <span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">        <span class="comment"># in particular you should not use functions from scipy,                #</span></span><br><span class="line">        <span class="comment"># nor use np.linalg.norm().                                             #</span></span><br><span class="line">        <span class="comment">#                                                                       #</span></span><br><span class="line">        <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">        <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        test_sum = np.sum(np.square(X),axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">        train_sum = np.sum(np.square(self.X_train),axis = <span class="number">1</span>)</span><br><span class="line">        cross_sum = X @ self.X_train.T</span><br><span class="line"></span><br><span class="line">        dists = np.sqrt(train_sum + test_sum - <span class="number">2</span>*cross_sum)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_labels</span><span class="params">(self, dists, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Given a matrix of distances between test points and training points,</span></span><br><span class="line"><span class="string">        predict a label for each test point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">          gives the distance betwen the ith test point and the jth training point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">          test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">        y_pred = np.zeros(num_test)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">            <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">            <span class="comment"># the ith test point.</span></span><br><span class="line">            closest_y = []</span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">            <span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">            <span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">            <span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            closest_y = self.y_train[np.argsort(dists[i])[:k]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">            <span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">            <span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">            <span class="comment"># label.                                                                #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            y_pred[i] = np.bincount(closest_y).argmax()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<h2 id="Multiclass-Support-Vector-Machine-exercise"><a href="#Multiclass-Support-Vector-Machine-exercise" class="headerlink" title="# Multiclass Support Vector Machine exercise"></a># Multiclass Support Vector Machine exercise</h2><ul>
<li>implement a fully-vectorized <strong>loss function</strong> for the SVM</li>
<li>implement the fully-vectorized expression for its <strong>analytic gradient</strong></li>
<li><strong>check your implementation</strong> using numerical gradient</li>
<li>use a validation set to <strong>tune the learning rate and regularization</strong> strength</li>
<li><strong>optimize</strong> the loss function with <strong>SGD</strong></li>
<li><strong>visualize</strong> the final learned weights</li>
</ul>
<h3 id="Key-points"><a href="#Key-points" class="headerlink" title="Key points"></a>Key points</h3><p>Recall that for the i-th example we are given the pixels of image $x_i$,and the label $y_i$ that specifies the index of the correct class. The score function takes the pixels and computes the vector $f(x_i,w)$ of class scores, which we will abbreviate to $s$ (short for scores). For example, the score for the j-th class is the j-th element: $s_j = f(x_i, W)_j$, The Multiclass SVM loss for the i-th example is then formalized as follows:</p>
<script type="math/tex; mode=display">L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)</script><p>Note that in this particular module we are working with linear score functions ($f(x_i; W) =  W x_i$), so we can also rewrite the loss function in this equivalent form:</p>
<script type="math/tex; mode=display">L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)</script><p>add regularization term and expanding this out in its full form:</p>
<script type="math/tex; mode=display">L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2</script><p>In addition to the motivation we provided above there are many desirable properties to include the regularization penalty, many of which we will come back to in later sections. For example, it turns out that including the L2 penalty leads to the appealing max margin property in SVMs</p>
<ol>
<li>Calculate the derivative for svm</li>
</ol>
<p>\begin{equation}<br>\left\{<br>\begin{array}{lr}<br>&amp; \frac{\partial {L_{i j}}}{\partial w_j} = - x_i^{T}  \quad j = y_i \\<br>&amp; \frac{\partial {L_{i j}}}{\partial w_j} = x_i^{T}   \quad j \neq y_i<br>\end{array}<br>\right.<br>\end{equation}</p>
<ol>
<li>calculate the loss and gradient by vector</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scores = X @ W</span><br><span class="line">    <span class="comment"># print(scores.shape)</span></span><br><span class="line">    <span class="comment"># print(y)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## difference of each class for every sample</span></span><br><span class="line">    margins = np.maximum(<span class="number">0</span>,scores - scores[range(num_train),y].reshape(<span class="number">-1</span>,<span class="number">1</span>) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## true class have no loss</span></span><br><span class="line">    margins[range(num_train),y] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## calculate the loss</span></span><br><span class="line">    loss = np.sum(margins) / num_train + <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">## calcute the times we should repeate when calculate the gradient for true class</span></span><br><span class="line">    margins[margins &gt; <span class="number">0</span>] = <span class="number">1.0</span></span><br><span class="line">    row_sum = np.sum(margins, axis=<span class="number">1</span>)                  </span><br><span class="line">    margins[np.arange(num_train), y] = -row_sum   </span><br><span class="line"></span><br><span class="line">    <span class="comment">## calcute the gradient</span></span><br><span class="line">    dW += np.dot(X.T, margins)/num_train + reg * W</span><br></pre></td></tr></table></figure>
<h3 id="SVM-code"><a href="#SVM-code" class="headerlink" title="SVM code"></a>SVM code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> range</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Structured SVM loss function, naive implementation (with loops).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin</span><br><span class="line">                dW[:,j] += X[i,:].T</span><br><span class="line">                dW[:,y[i]] -= X[i,:].T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train</span><br><span class="line">    dW /= num_train</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line">    dW += reg * W</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Compute the gradient of the loss function and store it dW.                #</span></span><br><span class="line">    <span class="comment"># Rather that first computing the loss and then computing the derivative,   #</span></span><br><span class="line">    <span class="comment"># it may be simpler to compute the derivative at the same time that the     #</span></span><br><span class="line">    <span class="comment"># loss is being computed. As a result you may need to modify some of the    #</span></span><br><span class="line">    <span class="comment"># code above to compute the gradient.                                       #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as svm_loss_naive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the structured SVM loss, storing the    #</span></span><br><span class="line">    <span class="comment"># result in loss.                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    scores = X @ W</span><br><span class="line">    <span class="comment"># print(scores.shape)</span></span><br><span class="line">    <span class="comment"># print(y)</span></span><br><span class="line"></span><br><span class="line">    margins = np.maximum(<span class="number">0</span>,scores - scores[range(num_train),y].reshape(<span class="number">-1</span>,<span class="number">1</span>) + <span class="number">1</span>)</span><br><span class="line">    margins[range(num_train),y] = <span class="number">0</span></span><br><span class="line">    loss = np.sum(margins) / num_train + <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the gradient for the structured SVM     #</span></span><br><span class="line">    <span class="comment"># loss, storing the result in dW.                                           #</span></span><br><span class="line">    <span class="comment">#                                                                           #</span></span><br><span class="line">    <span class="comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span></span><br><span class="line">    <span class="comment"># to reuse some of the intermediate values that you used to compute the     #</span></span><br><span class="line">    <span class="comment"># loss.                                                                     #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    margins[margins &gt; <span class="number">0</span>] = <span class="number">1.0</span></span><br><span class="line">    row_sum = np.sum(margins, axis=<span class="number">1</span>)                  </span><br><span class="line">    margins[np.arange(num_train), y] = -row_sum        </span><br><span class="line">    dW += np.dot(X.T, margins)/num_train + reg * W  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="Implement-a-Softmax-classifier"><a href="#Implement-a-Softmax-classifier" class="headerlink" title="Implement a Softmax classifier"></a>Implement a Softmax classifier</h2><ul>
<li>implement a fully-vectorized <strong>loss function</strong> for the Softmax classifier</li>
<li>implement the fully-vectorized expression for its <strong>analytic gradient</strong></li>
<li><strong>check your implementation</strong> with numerical gradient</li>
<li>use a validation set to <strong>tune the learning rate and regularization</strong> strength</li>
<li><strong>optimize</strong> the loss function with <strong>SGD</strong></li>
<li><strong>visualize</strong> the final learned weights</li>
</ul>
<p>Unlike the SVM which treats the outputs $f(x_i,W)$ as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping $f(x_i; W) =  W x_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form: </p>
<script type="math/tex; mode=display">L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}</script><script type="math/tex; mode=display">L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \frac{1}{2} \lambda \sum_k\sum_l W_{k,l}^2 }\_\text{regularization loss}</script><p>The function $\frac{e^{z_j}}{\sum_k e^{z_k}}$, is called the softmax function: It takes a vector of arbitrary real-valued scores (in $z$) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you’re seeing it for the first time but it is relatively easy to motivate.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_examples = X.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># get unnormalized probabilities</span></span><br><span class="line">exp_scores = np.exp(scores)</span><br><span class="line"><span class="comment"># normalize them for each example</span></span><br><span class="line">probs = exp_scores / np.sum(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">correct_logprobs = -np.log(probs[range(num_examples),y])</span><br><span class="line"><span class="comment"># compute the loss: average cross-entropy loss and regularization</span></span><br><span class="line">data_loss = np.sum(correct_logprobs)/num_examples</span><br><span class="line">reg_loss = <span class="number">0.5</span>*reg*np.sum(W*W)</span><br><span class="line">loss = data_loss + reg_loss</span><br></pre></td></tr></table></figure>
<p>We have a way of evaluating the loss, and now we have to minimize it. We’ll do so with gradient descent. That is, we start with random parameters (as shown above), and evaluate the gradient of the loss function with respect to the parameters, so that we know how we should change the parameters to decrease the loss. Lets introduce the intermediate variable $P$,which is a vector of the (normalized) probabilities. The loss for one example is:</p>
<script type="math/tex; mode=display">p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} L_i =-\log\left(p_{y_i}\right)</script><p>We now wish to understand how the computed scores inside $f$ should change to decrease the loss $L_i$ that this example contributes to the full objective. In other words, we want to derive the gradient $\partial L_i / \partial f_k$. The loss $Li$ is computed from $p$ which in turn depends on $f$ It’s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out:</p>
<script type="math/tex; mode=display">\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)</script><script type="math/tex; mode=display">\frac{\partial L_i }{ \partial f_j } = p_j - \mathbb{1}(y_i = k)</script><p>Notice how elegant and simple this expression is. Suppose the probabilities we computed were <code>p = [0.2, 0.3, 0.5]</code>, and that the correct class was the middle one (with probability 0.3). According to this derivation the gradient on the scores would be <code>df = [0.2, -0.7, 0.5]</code>. Recalling what the interpretation of the gradient, we see that this result is highly intuitive: increasing the first or last element of the score vector f (the scores of the incorrect classes) leads to an increased loss (due to the positive signs +0.2 and +0.5) - and increasing the loss is bad, as expected. However, increasing the score of the correct class has negative influence on the loss. The gradient of -0.7 is telling us that increasing the correct class score would lead to a decrease of the loss $L_i$, which makes sense.</p>
<p>To get the gradient on the scores, which we call $dscores$, we proceed as follows:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dscores = probs</span><br><span class="line">dscores[range(num_examples),y] -= <span class="number">1</span></span><br><span class="line">dscores /= num_examples</span><br></pre></td></tr></table></figure></p>
<p>Lastly, we had that <script type="math/tex">scores = np.dot(X, W) + b</script>, so armed with the gradient on scores (stored in dscores), we can now backpropagate into W and b:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dW = np.dot(X.T, dscores)</span><br><span class="line">db = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">dW += reg*W <span class="comment"># don't forget the regularization gradient</span></span><br></pre></td></tr></table></figure>
<h3 id="Softmax-code"><a href="#Softmax-code" class="headerlink" title="Softmax code"></a>Softmax code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> range</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using explicit loops.     #</span></span><br><span class="line">    <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">    <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">    <span class="comment"># regularization!                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    num_trains = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_trains):</span><br><span class="line">        scores = X[i].T @ W</span><br><span class="line">        shift_scores = scores - np.max(scores)</span><br><span class="line">        loss_i = - shift_scores[y[i]] + np.log(np.sum(np.exp(shift_scores)))</span><br><span class="line">        loss += loss_i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</span><br><span class="line">            softmax_output = np.exp(shift_scores[j])/sum(np.exp(shift_scores))</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                dW[:,j] += (<span class="number">-1</span> + softmax_output) *X[i] </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                dW[:,j] += softmax_output *X[i] </span><br><span class="line"></span><br><span class="line">        loss /= num_trains</span><br><span class="line">        loss +=  <span class="number">0.5</span>* reg * np.sum(W * W)</span><br><span class="line">        dW = dW/num_trains + reg* W </span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using no explicit loops.  #</span></span><br><span class="line">    <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">    <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">    <span class="comment"># regularization!                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    scores = X.dot(W)</span><br><span class="line">    shift_scores = scores - np.max(scores, axis = <span class="number">1</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    softmax_output = np.exp(shift_scores)/np.sum(np.exp(shift_scores), axis = <span class="number">1</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    loss = -np.sum(np.log(softmax_output[range(num_train), list(y)]))</span><br><span class="line">    loss /= num_train </span><br><span class="line">    loss +=  <span class="number">0.5</span>* reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    dS = softmax_output.copy()</span><br><span class="line">    dS[range(num_train), list(y)] += <span class="number">-1</span></span><br><span class="line">    dW = (X.T).dot(dS)</span><br><span class="line">    dW = dW/num_train + reg* W </span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="Two-Layer-Neural-Network"><a href="#Two-Layer-Neural-Network" class="headerlink" title="Two-Layer Neural Network"></a>Two-Layer Neural Network</h2><p>Clearly, a linear classifier is inadequate for this dataset and we would like to use a Neural Network. One additional hidden layer will suffice for this toy data. We will now need two sets of weights and biases (for the first and second layers):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize parameters randomly</span></span><br><span class="line">h = <span class="number">100</span> <span class="comment"># size of hidden layer</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,h)</span><br><span class="line">b = np.zeros((<span class="number">1</span>,h))</span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(h,K)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>,K))</span><br></pre></td></tr></table></figure>
<p>The forward pass to compute scores now changes form:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate class scores with a 2-layer Neural Network</span></span><br><span class="line">hidden_layer = np.maximum(<span class="number">0</span>, np.dot(X, W) + b) <span class="comment"># note, ReLU activation</span></span><br><span class="line">scores = np.dot(hidden_layer, W2) + b2</span><br></pre></td></tr></table></figure>
<p>Notice that the only change from before is one <strong>extra line of code</strong>, where we first compute the hidden layer representation and then the scores based on this hidden layer. Crucially, we’ve also added a non-linearity, which in this case is simple ReLU that thresholds the activations on the hidden layer at zero.</p>
<p>Everything else remains the same. We compute the loss based on the scores exactly as before, and get the gradient for the scores dscores exactly as before. However, the way we backpropagate that gradient into the model parameters now changes form, of course. First lets backpropagate the second layer of the Neural Network. This looks identical to the code we had for the Softmax classifier, except we’re replacing X (the raw data), with the variable hidden_layer):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backpropate the gradient to the parameters</span></span><br><span class="line"><span class="comment"># first backprop into parameters W2 and b2</span></span><br><span class="line">dW2 = np.dot(hidden_layer.T, dscores)</span><br><span class="line">db2 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>However, unlike before we are not yet done, because hidden_layer is itself a function of other parameters and the data! We need to continue backpropagation through this variable. Its gradient can be computed as:</p>
<script type="math/tex; mode=display">relu = max(0,a)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dhidden = np.dot(dscores, W2.T)</span><br></pre></td></tr></table></figure>
<p>Now we have the gradient on the outputs of the hidden layer. Next, we have to backpropagate the ReLU non-linearity. This turns out to be easy because ReLU during the backward pass is effectively a switch. Since $r = max(0, x)$, we have that $\frac{dr}{dx} = 1(x &gt; 0)$, Combined with the chain rule, we see that the ReLU unit lets the gradient pass through unchanged if its input was greater than 0, but kills it if its input was less than zero during the forward pass. Hence, we can backpropagate the ReLU in place simply with:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backprop the ReLU non-linearity</span></span><br><span class="line">dhidden[hidden_layer &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment"># finally into W,b</span></span><br><span class="line">dW = np.dot(X.T, dhidden)</span><br><span class="line">db = np.sum(dhidden, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Neural-Net-code"><a href="#Neural-Net-code" class="headerlink" title="Neural Net code"></a>Neural Net code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> range</span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> object</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A two-layer fully-connected neural network. The net has an input dimension of</span></span><br><span class="line"><span class="string">    N, a hidden layer dimension of H, and performs classification over C classes.</span></span><br><span class="line"><span class="string">    We train the network with a softmax loss function and L2 regularization on the</span></span><br><span class="line"><span class="string">    weight matrices. The network uses a ReLU nonlinearity after the first fully</span></span><br><span class="line"><span class="string">    connected layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In other words, the network has the following architecture:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    input - fully connected layer - ReLU - fully connected layer - softmax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The outputs of the second fully-connected layer are the scores for each class.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size, std=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize the model. Weights are initialized to small random values and</span></span><br><span class="line"><span class="string">        biases are initialized to zero. Weights and biases are stored in the</span></span><br><span class="line"><span class="string">        variable self.params, which is a dictionary with the following keys:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        W1: First layer weights; has shape (D, H)</span></span><br><span class="line"><span class="string">        b1: First layer biases; has shape (H,)</span></span><br><span class="line"><span class="string">        W2: Second layer weights; has shape (H, C)</span></span><br><span class="line"><span class="string">        b2: Second layer biases; has shape (C,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - input_size: The dimension D of the input data.</span></span><br><span class="line"><span class="string">        - hidden_size: The number of neurons H in the hidden layer.</span></span><br><span class="line"><span class="string">        - output_size: The number of classes C.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">'W1'</span>] = std * np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = std * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.maximum(<span class="number">0</span>,x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None, reg=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">        network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">          an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">          is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">          instead return the loss and gradients.</span></span><br><span class="line"><span class="string">        - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">        the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">        - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">          samples.</span></span><br><span class="line"><span class="string">        - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">          with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</span><br><span class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">        N, D = X.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the forward pass</span></span><br><span class="line">        scores = <span class="keyword">None</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Perform the forward pass, computing the class scores for the input. #</span></span><br><span class="line">        <span class="comment"># Store the result in the scores variable, which should be an array of      #</span></span><br><span class="line">        <span class="comment"># shape (N, C).                                                             #</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (n,d) * (d,h) + (h,) = (n,h)</span></span><br><span class="line">        h_output =  self.relu(X @ W1 + b1)    </span><br><span class="line">        scores = h_output @ W2 + b2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the targets are not given then jump out, we're done</span></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the loss</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Finish the forward pass, and compute the loss. This should include  #</span></span><br><span class="line">        <span class="comment"># both the data loss and L2 regularization for W1 and W2. Store the result  #</span></span><br><span class="line">        <span class="comment"># in the variable loss, which should be a scalar. Use the Softmax           #</span></span><br><span class="line">        <span class="comment"># classifier loss.                                                          #</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        shift_scores = scores - np.max(scores, axis = <span class="number">1</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">        softmax_output = np.exp(shift_scores)/np.sum(np.exp(shift_scores), axis = <span class="number">1</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">        loss = -np.sum(np.log(softmax_output[range(N), list(y)]))</span><br><span class="line">        loss /= N</span><br><span class="line">        loss +=  <span class="number">0.5</span>* reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Compute the backward pass, computing the derivatives of the weights #</span></span><br><span class="line">        <span class="comment"># and biases. Store the results in the grads dictionary. For example,       #</span></span><br><span class="line">        <span class="comment"># grads['W1'] should store the gradient on W1, and be a matrix of same size #</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        dscores = softmax_output.copy()</span><br><span class="line">        dscores[range(N), list(y)] -= <span class="number">1</span></span><br><span class="line">        dscores /= N</span><br><span class="line">        grads[<span class="string">'W2'</span>] = h_output.T.dot(dscores) + reg * W2</span><br><span class="line">        grads[<span class="string">'b2'</span>] = np.sum(dscores, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        dh = dscores.dot(W2.T)</span><br><span class="line">        dh_ReLu = (h_output &gt; <span class="number">0</span>) * dh</span><br><span class="line">        grads[<span class="string">'W1'</span>] = X.T.dot(dh_ReLu) + reg * W1</span><br><span class="line">        grads[<span class="string">'b1'</span>] = np.sum(dh_ReLu, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, X_val, y_val,</span></span></span><br><span class="line"><span class="function"><span class="params">              learning_rate=<span class="number">1e-3</span>, learning_rate_decay=<span class="number">0.95</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              reg=<span class="number">5e-6</span>, num_iters=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Train this neural network using stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) giving training data.</span></span><br><span class="line"><span class="string">        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that</span></span><br><span class="line"><span class="string">          X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">        - X_val: A numpy array of shape (N_val, D) giving validation data.</span></span><br><span class="line"><span class="string">        - y_val: A numpy array of shape (N_val,) giving validation labels.</span></span><br><span class="line"><span class="string">        - learning_rate: Scalar giving learning rate for optimization.</span></span><br><span class="line"><span class="string">        - learning_rate_decay: Scalar giving factor used to decay the learning rate</span></span><br><span class="line"><span class="string">          after each epoch.</span></span><br><span class="line"><span class="string">        - reg: Scalar giving regularization strength.</span></span><br><span class="line"><span class="string">        - num_iters: Number of steps to take when optimizing.</span></span><br><span class="line"><span class="string">        - batch_size: Number of training examples to use per step.</span></span><br><span class="line"><span class="string">        - verbose: boolean; if true print progress during optimization.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">        iterations_per_epoch = max(num_train / batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Use SGD to optimize the parameters in self.model</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        train_acc_history = []</span><br><span class="line">        val_acc_history = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</span><br><span class="line">            X_batch = <span class="keyword">None</span></span><br><span class="line">            y_batch = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Create a random minibatch of training data and labels, storing  #</span></span><br><span class="line">            <span class="comment"># them in X_batch and y_batch respectively.                             #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            idx = np.random.choice(num_train, batch_size, replace=<span class="keyword">True</span>)</span><br><span class="line">            X_batch = X[idx]</span><br><span class="line">            y_batch = y[idx]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute loss and gradients using the current minibatch</span></span><br><span class="line">            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Use the gradients in the grads dictionary to update the         #</span></span><br><span class="line">            <span class="comment"># parameters of the network (stored in the dictionary self.params)      #</span></span><br><span class="line">            <span class="comment"># using stochastic gradient descent. You'll need to use the gradients   #</span></span><br><span class="line">            <span class="comment"># stored in the grads dictionary defined above.                         #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            self.params[<span class="string">'W2'</span>] += - learning_rate * grads[<span class="string">'W2'</span>]</span><br><span class="line">            self.params[<span class="string">'b2'</span>] += - learning_rate * grads[<span class="string">'b2'</span>]</span><br><span class="line">            self.params[<span class="string">'W1'</span>] += - learning_rate * grads[<span class="string">'W1'</span>]</span><br><span class="line">            self.params[<span class="string">'b1'</span>] += - learning_rate * grads[<span class="string">'b1'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Every epoch, check train and val accuracy and decay learning rate.</span></span><br><span class="line">            <span class="keyword">if</span> it % iterations_per_epoch == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># Check accuracy</span></span><br><span class="line">                train_acc = (self.predict(X_batch) == y_batch).mean()</span><br><span class="line">                val_acc = (self.predict(X_val) == y_val).mean()</span><br><span class="line">                train_acc_history.append(train_acc)</span><br><span class="line">                val_acc_history.append(val_acc)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay learning rate</span></span><br><span class="line">                learning_rate *= learning_rate_decay</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">          <span class="string">'loss_history'</span>: loss_history,</span><br><span class="line">          <span class="string">'train_acc_history'</span>: train_acc_history,</span><br><span class="line">          <span class="string">'val_acc_history'</span>: val_acc_history,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Use the trained weights of this two-layer network to predict labels for</span></span><br><span class="line"><span class="string">        data points. For each data point we predict scores for each of the C</span></span><br><span class="line"><span class="string">        classes, and assign each data point to the class with the highest score.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) giving N D-dimensional data points to</span></span><br><span class="line"><span class="string">          classify.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - y_pred: A numpy array of shape (N,) giving predicted labels for each of</span></span><br><span class="line"><span class="string">          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted</span></span><br><span class="line"><span class="string">          to have class c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        y_pred = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement this function; it should be VERY simple!                #</span></span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        h = np.maximum(<span class="number">0</span>, X.dot(self.params[<span class="string">'W1'</span>]) + self.params[<span class="string">'b1'</span>])</span><br><span class="line">        scores = h.dot(self.params[<span class="string">'W2'</span>]) + self.params[<span class="string">'b2'</span>]</span><br><span class="line">        y_pred = np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<h2 id="Higher-Level-Representations-Image-Features"><a href="#Higher-Level-Representations-Image-Features" class="headerlink" title="Higher Level Representations: Image Features"></a>Higher Level Representations: Image Features</h2><p>An important way to gain intuition about how an algorithm works is to visualize the mistakes that it makes. In this visualization, we show examples of images that are misclassified by our current system. The first column shows images that our system labeled as “plane” but whose true label is something other than “plane”.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">examples_per_class = <span class="number">8</span></span><br><span class="line">classes = [<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>]</span><br><span class="line"><span class="keyword">for</span> cls, cls_name <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">    idxs = np.where((y_test != cls) &amp; (y_test_pred == cls))[<span class="number">0</span>]</span><br><span class="line">    idxs = np.random.choice(idxs, examples_per_class, replace=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, idx <span class="keyword">in</span> enumerate(idxs):</span><br><span class="line">        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(X_test[idx].astype(<span class="string">'uint8'</span>))</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            plt.title(cls_name)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cs231n.classifiers.neural_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"></span><br><span class="line">input_dim = X_train_feats.shape[<span class="number">1</span>]</span><br><span class="line">hidden_dim = <span class="number">500</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#net = TwoLayerNet(input_dim, hidden_dim, num_classes)</span></span><br><span class="line">best_net = <span class="keyword">None</span></span><br><span class="line">best_val = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Train a two-layer neural network on image features. You may want to    #</span></span><br><span class="line"><span class="comment"># cross-validate various parameters as in previous sections. Store your best   #</span></span><br><span class="line"><span class="comment"># model in the best_net variable.                                              #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_random_hyperparams</span><span class="params">(lr_min, lr_max, reg_min, reg_max, h_min, h_max)</span>:</span></span><br><span class="line">    lr = <span class="number">10</span>**np.random.uniform(lr_min,lr_max)</span><br><span class="line">    reg = <span class="number">10</span>**np.random.uniform(reg_min,reg_max)</span><br><span class="line">    hidden = np.random.randint(h_min, h_max)</span><br><span class="line">    <span class="keyword">return</span> lr, reg, hidden</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use of random search for hyperparameter search</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    lr, reg, hidden_dim = generate_random_hyperparams(<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-7</span>, <span class="number">-4</span>, <span class="number">10</span>, <span class="number">500</span>)</span><br><span class="line">    <span class="comment"># Create a two-layer network</span></span><br><span class="line">    net = TwoLayerNet(input_dim, hidden_dim, num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Train the network</span></span><br><span class="line">    stats = net.train(X_train_feats, y_train, X_val_feats, y_val,</span><br><span class="line">                num_iters=<span class="number">3000</span>, batch_size=<span class="number">200</span>,</span><br><span class="line">                learning_rate=lr, learning_rate_decay=<span class="number">0.95</span>,</span><br><span class="line">                reg=reg, verbose=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict on the training set</span></span><br><span class="line">    train_accuracy = (net.predict(X_train_feats) == y_train).mean()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict on the validation set</span></span><br><span class="line">    val_accuracy = (net.predict(X_val_feats) == y_val).mean()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save best values</span></span><br><span class="line">    <span class="keyword">if</span> val_accuracy &gt; best_val:</span><br><span class="line">        best_val = val_accuracy</span><br><span class="line">        best_net = net</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Print results</span></span><br><span class="line">    print(<span class="string">'lr %e reg %e hid %d  train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, hidden_dim, train_accuracy, val_accuracy))</span><br><span class="line">print(<span class="string">'best validation accuracy achieved: %f'</span> % best_val)</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/cs231n/" rel="tag"># cs231n</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Machine-Language/2019/09/22/" rel="next" title="Machine Language">
                <i class="fa fa-chevron-left"></i> Machine Language
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Naive-Bayes/2019/10/02/" rel="prev" title="Naive Bayes">
                Naive Bayes <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">272</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-goals-of-this-assignment-are-as-follows"><span class="nav-text">The goals of this assignment are as follows</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-Nearest-Neighbor-classifier"><span class="nav-text">k-Nearest Neighbor classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Key-point"><span class="nav-text">Key point</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN-code"><span class="nav-text">KNN code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiclass-Support-Vector-Machine-exercise"><span class="nav-text"># Multiclass Support Vector Machine exercise</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Key-points"><span class="nav-text">Key points</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM-code"><span class="nav-text">SVM code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implement-a-Softmax-classifier"><span class="nav-text">Implement a Softmax classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-code"><span class="nav-text">Softmax code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Two-Layer-Neural-Network"><span class="nav-text">Two-Layer Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-Net-code"><span class="nav-text">Neural Net code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Higher-Level-Representations-Image-Features"><span class="nav-text">Higher Level Representations: Image Features</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
