<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="BLOG">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="Assignment 1 - TD with State AggregationWelcome to your Course 3 Programming Assignment 1. In this assignment, you will implement semi-gradient TD(0) with State Aggregation in an environment with a la">
<meta property="og:type" content="article">
<meta property="og:title" content="TD with State Aggregation ">
<meta property="og:url" content="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Assignment 1 - TD with State AggregationWelcome to your Course 3 Programming Assignment 1. In this assignment, you will implement semi-gradient TD(0) with State Aggregation in an environment with a la">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/randomwalk_diagram.png">
<meta property="og:image" content="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/output_36_6.png">
<meta property="og:image" content="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/output_41_0.png">
<meta property="og:image" content="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/output_41_1.png">
<meta property="og:image" content="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/output_41_2.png">
<meta property="og:updated_time" content="2020-10-13T08:07:45.508Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TD with State Aggregation ">
<meta name="twitter:description" content="Assignment 1 - TD with State AggregationWelcome to your Course 3 Programming Assignment 1. In this assignment, you will implement semi-gradient TD(0) with State Aggregation in an environment with a la">
<meta name="twitter:image" content="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/randomwalk_diagram.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/">





  <title>TD with State Aggregation  | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">TD with State Aggregation </h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-13T16:06:20+08:00">
                2020-10-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Reinforcement Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/TD-with-State-Aggregation/2020/10/13/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/TD-with-State-Aggregation/2020/10/13/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Assignment-1-TD-with-State-Aggregation"><a href="#Assignment-1-TD-with-State-Aggregation" class="headerlink" title="Assignment 1 - TD with State Aggregation"></a>Assignment 1 - TD with State Aggregation</h1><p>Welcome to your Course 3 Programming Assignment 1. In this assignment, you will implement <strong>semi-gradient TD(0) with State Aggregation</strong> in an environment with a large state space. This assignment will focus on the <strong>policy evaluation task</strong> (prediction problem) where the goal is to accurately estimate state values under a given (fixed) policy.</p>
<p><strong>In this assignment, you will:</strong></p>
<ol>
<li>Implement semi-gradient TD(0) with function approximation (state aggregation).</li>
<li>Understand how to use supervised learning approaches to approximate value functions.</li>
<li>Compare the impact of different resolutions of state aggregation, and see first hand how function approximation can speed up learning through generalization.</li>
</ol>
<p><strong>Note: You can create new cells for debugging purposes but please do not duplicate any Read-only cells. This may break the grader.</strong></p>
<h2 id="500-State-RandomWalk-Environment"><a href="#500-State-RandomWalk-Environment" class="headerlink" title="500-State RandomWalk Environment"></a>500-State RandomWalk Environment</h2><p>In this assignment, we will implement and use a smaller 500 state  version of the problem we covered in lecture  (see “State Aggregation with Monte Carlo”, and Example 9.1 in the <a href="http://www.incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">textbook</a>). The diagram below illustrates the problem.</p>
<p><img src="randomwalk_diagram.png" alt></p>
<p>There are 500 states numbered from 1 to 500, left to right, and all episodes begin with the agent located at the center, in state 250. For simplicity, we will consider state 0 and state 501 as the left and right terminal states respectively. </p>
<p>The episode terminates when the agent reaches the terminal state (state 0) on the left, or the terminal state (state 501) on the right. Termination on the left (state 0) gives the agent a reward of -1, and termination on the right (state 501) gives the agent a reward of +1.</p>
<p>The agent can take one of two actions: go left or go right. If the agent chooses the left action, then it transitions uniform randomly into one of the 100 neighboring states to its left. If the agent chooses the right action, then it transitions randomly into one of the 100 neighboring states to its right. </p>
<p>States near the edge may have fewer than 100 neighboring states on that side. In this case, all transitions that would have taken the agent past the edge result in termination. If the agent takes the left action from state 50, then it has a 0.5 chance of terminating on the left. If it takes the right action from state 499, then it has a 0.99 chance of terminating on the right.</p>
<h3 id="Your-Goal"><a href="#Your-Goal" class="headerlink" title="Your Goal"></a>Your Goal</h3><p>For this assignment, we will consider the problem of <strong>policy evaluation</strong>: estimating state-value function for a fixed policy.You will evaluate a uniform random policy in the 500-State Random Walk environment. This policy takes the right action with 0.5 probability and the left with 0.5 probability, regardless of which state it is in. </p>
<p>This environment has a relatively large number of states. Generalization can significantly speed learning as we will show in this assignment. Often in realistic environments, states are high-dimensional and continuous. For these problems, function approximation is not just useful, it is also necessary.</p>
<h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>You will use the following packages in this assignment.</p>
<ul>
<li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li>
<li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> : Library for plotting graphs in Python.</li>
<li><a href="http://www.jmlr.org/papers/v10/tanner09a.html" target="_blank" rel="noopener">RL-Glue</a> : Library for reinforcement learning experiments.</li>
<li><a href="https://alexhagen.github.io/jdc/" target="_blank" rel="noopener">jdc</a> : Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li>
<li><a href="https://tqdm.github.io/" target="_blank" rel="noopener">tqdm</a> : A package to display progress bar when running experiments</li>
<li>plot_script : custom script to plot results</li>
</ul>
<p><strong>Please do not import other libraries</strong> - this will break the autograder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> environment <span class="keyword">import</span> BaseEnvironment</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> plot_script</span><br></pre></td></tr></table></figure>
<h2 id="Section-1-Create-the-500-State-RandomWalk-Environment"><a href="#Section-1-Create-the-500-State-RandomWalk-Environment" class="headerlink" title="Section 1: Create the 500-State RandomWalk Environment"></a>Section 1: Create the 500-State RandomWalk Environment</h2><p>In this section we have provided you with the implementation of the 500-State RandomWalk Environment. It is useful to know how the environment is implemented. We will also use this environment in the next programming assignment. </p>
<p>Once the agent chooses which direction to move, the environment determines how far the agent is moved in that direction. Assume the agent passes either 0 (indicating left) or 1 (indicating right) to the environment.</p>
<p>Methods needed to implement the environment are: <code>env_init</code>, <code>env_start</code>, and <code>env_step</code>.</p>
<ul>
<li><code>env_init</code>: This method sets up the environment at the very beginning of the experiment. Relevant parameters are passed through <code>env_info</code> dictionary.</li>
<li><code>env_start</code>: This is the first method called when the experiment starts, returning the start state.</li>
<li><code>env_step</code>: This method takes in action and returns reward, next_state, and is_terminal.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomWalkEnvironment</span><span class="params">(BaseEnvironment)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_init</span><span class="params">(self, env_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Setup for the environment called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Set parameters needed to setup the 500-state random walk environment.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Assume env_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: 500 [int],</span></span><br><span class="line"><span class="string">            start_state: 250 [int],</span></span><br><span class="line"><span class="string">            left_terminal_state: 0 [int],</span></span><br><span class="line"><span class="string">            right_terminal_state: 501 [int],</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set random seed for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(env_info.get(<span class="string">"seed"</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set each class attribute</span></span><br><span class="line">        self.num_states = env_info[<span class="string">"num_states"</span>] </span><br><span class="line">        self.start_state = env_info[<span class="string">"start_state"</span>] </span><br><span class="line">        self.left_terminal_state = env_info[<span class="string">"left_terminal_state"</span>] </span><br><span class="line">        self.right_terminal_state = env_info[<span class="string">"right_terminal_state"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        The first method called when the experiment starts, called before the</span></span><br><span class="line"><span class="string">        agent starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first state from the environment.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># set self.reward_state_term tuple</span></span><br><span class="line">        reward = <span class="number">0.0</span></span><br><span class="line">        state = self.start_state</span><br><span class="line">        is_terminal = <span class="keyword">False</span></span><br><span class="line">                </span><br><span class="line">        self.reward_state_term = (reward, state, is_terminal)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># return first state from the environment</span></span><br><span class="line">        <span class="keyword">return</span> self.reward_state_term[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            action: The action taken by the agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            (float, state, Boolean): a tuple of the reward, state,</span></span><br><span class="line"><span class="string">                and boolean indicating if it's terminal.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        last_state = self.reward_state_term[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set reward, current_state, and is_terminal</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># action: specifies direction of movement - 0 (indicating left) or 1 (indicating right)  [int]</span></span><br><span class="line">        <span class="comment"># current state: next state after taking action from the last state [int]</span></span><br><span class="line">        <span class="comment"># reward: -1 if terminated left, 1 if terminated right, 0 otherwise [float]</span></span><br><span class="line">        <span class="comment"># is_terminal: indicates whether the episode terminated [boolean]</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Given action (direction of movement), determine how much to move in that direction from last_state</span></span><br><span class="line">        <span class="comment"># All transitions beyond the terminal state are absorbed into the terminal state.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> action == <span class="number">0</span>: <span class="comment"># left</span></span><br><span class="line">            current_state = max(self.left_terminal_state, last_state + self.rand_generator.choice(range(<span class="number">-100</span>,<span class="number">0</span>)))</span><br><span class="line">        <span class="keyword">elif</span> action == <span class="number">1</span>: <span class="comment"># right</span></span><br><span class="line">            current_state = min(self.right_terminal_state, last_state + self.rand_generator.choice(range(<span class="number">1</span>,<span class="number">101</span>)))</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Wrong action value"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># terminate left</span></span><br><span class="line">        <span class="keyword">if</span> current_state == self.left_terminal_state: </span><br><span class="line">            reward = <span class="number">-1.0</span></span><br><span class="line">            is_terminal = <span class="keyword">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># terminate right</span></span><br><span class="line">        <span class="keyword">elif</span> current_state == self.right_terminal_state:</span><br><span class="line">            reward = <span class="number">1.0</span></span><br><span class="line">            is_terminal = <span class="keyword">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reward = <span class="number">0.0</span></span><br><span class="line">            is_terminal = <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">        self.reward_state_term = (reward, current_state, is_terminal)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.reward_state_term</span><br></pre></td></tr></table></figure>
<h2 id="Section-2-Create-Semi-gradient-TD-0-Agent-with-State-Aggregation"><a href="#Section-2-Create-Semi-gradient-TD-0-Agent-with-State-Aggregation" class="headerlink" title="Section 2: Create Semi-gradient TD(0) Agent with State Aggregation"></a>Section 2: Create Semi-gradient TD(0) Agent with State Aggregation</h2><p>Now let’s create the Agent that interacts with the Environment.</p>
<p>You will create an Agent that learns with semi-gradient TD(0) with state aggregation.<br>For state aggregation, if the resolution (num_groups) is 10, then 500 states are partitioned into 10 groups of 50 states each (i.e., states 1-50 are one group, states 51-100 are another, and so on.)</p>
<p>Hence, 50 states would share the same feature and value estimate, and there would be 10 distinct features. The feature vector for each state is a one-hot feature vector of length 10, with a single one indicating the group for that state. (one-hot vector of length 10)</p>
<h2 id="Section-2-1-Implement-Useful-Functions"><a href="#Section-2-1-Implement-Useful-Functions" class="headerlink" title="Section 2-1: Implement Useful Functions"></a>Section 2-1: Implement Useful Functions</h2><p>Before we implement the agent, we need to define a couple of useful helper functions.</p>
<p><strong>Please note all random method calls should be called through random number generator. Also do not use random method calls unless specified. In the agent, only <code>agent_policy</code> requires random method calls.</strong></p>
<h2 id="Section-2-1a-Selecting-actions"><a href="#Section-2-1a-Selecting-actions" class="headerlink" title="Section 2-1a: Selecting actions"></a>Section 2-1a: Selecting actions</h2><p>In this part we have implemented <code>agent_policy()</code> for you.</p>
<p>This method is used in <code>agent_start()</code> and <code>agent_step()</code> to select appropriate action.<br>Normally, the agent acts differently given state, but in this environment the agent chooses randomly to move either left or right with equal probability.</p>
<p>Agent returns 0 for left, and 1 for right.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_policy</span><span class="params">(rand_generator, state)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given random number generator and state, returns an action according to the agent's policy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        rand_generator: Random number generator</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        chosen action [int]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set chosen_action as 0 or 1 with equal probability</span></span><br><span class="line">    <span class="comment"># state is unnecessary for this agent policy</span></span><br><span class="line">    chosen_action = rand_generator.choice([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> chosen_action</span><br></pre></td></tr></table></figure>
<h2 id="Section-2-1b-Processing-State-Features-with-State-Aggregation"><a href="#Section-2-1b-Processing-State-Features-with-State-Aggregation" class="headerlink" title="Section 2-1b: Processing State Features with State Aggregation"></a>Section 2-1b: Processing State Features with State Aggregation</h2><p>In this part you will implement <code>get_state_feature()</code></p>
<p>This method takes in a state and returns the aggregated feature (one-hot-vector) of that state.<br>The feature vector size is determined by <code>num_groups</code>. Use <code>state</code> and <code>num_states_in_group</code> to determine which element in the feature vector is active.</p>
<p><code>get_state_feature()</code> is necessary whenever the agent receives a state and needs to convert it to a feature for learning. The features will thus be used in <code>agent_step()</code> and <code>agent_end()</code> when the agent updates its state values.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_state_feature</span><span class="params">(num_states_in_group, num_groups, state)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given state, return the feature of that state</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_states_in_group [int]</span></span><br><span class="line"><span class="string">        num_groups [int] </span></span><br><span class="line"><span class="string">        state [int] : 1~500</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        one_hot_vector [numpy array]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### Generate state feature (2~4 lines)</span></span><br><span class="line">    <span class="comment"># Create one_hot_vector with size of the num_groups, according to state</span></span><br><span class="line">    <span class="comment"># For simplicity, assume num_states is always perfectly divisible by num_groups</span></span><br><span class="line">    <span class="comment"># Note that states start from index 1, not 0!</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Example:</span></span><br><span class="line">    <span class="comment"># If num_states = 100, num_states_in_group = 20, num_groups = 5,</span></span><br><span class="line">    <span class="comment"># one_hot_vector would be of size 5.</span></span><br><span class="line">    <span class="comment"># For states 1~20, one_hot_vector would be: [1, 0, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    <span class="comment"># one_hot_vector = ?</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    one_hot_vector = np.zeros((num_groups,))</span><br><span class="line">    index,_ = divmod(state,num_states_in_group)</span><br><span class="line">    <span class="keyword">if</span> _ == <span class="number">0</span>:</span><br><span class="line">        index -= <span class="number">1</span></span><br><span class="line">    one_hot_vector[index] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot_vector</span><br></pre></td></tr></table></figure>
<p>Run the following code to verify your <code>get_state_feature()</code> function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Given that num_states = 10 and num_groups = 5, test get_state_feature()</span></span><br><span class="line"><span class="comment"># There are states 1~10, and the state feature vector would be of size 5.</span></span><br><span class="line"><span class="comment"># Only one element would be active for any state feature vector.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get_state_feature() should support various values of num_states, num_groups, not just this example</span></span><br><span class="line"><span class="comment"># For simplicity, assume num_states will always be perfectly divisible by num_groups</span></span><br><span class="line">num_states = <span class="number">10</span></span><br><span class="line">num_groups = <span class="number">5</span></span><br><span class="line">num_states_in_group = int(num_states / num_groups)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 1st group, state = 1</span></span><br><span class="line">state = <span class="number">1</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"1st group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 2nd group, state = 3</span></span><br><span class="line">state = <span class="number">3</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"2nd group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 3rd group, state = 6</span></span><br><span class="line">state = <span class="number">6</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"3rd group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 4th group, state = 7</span></span><br><span class="line">state = <span class="number">7</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"4th group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 5th group, state = 10</span></span><br><span class="line">state = <span class="number">10</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"5th group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>1st group: [1. 0. 0. 0. 0.]
2nd group: [0. 1. 0. 0. 0.]
3rd group: [0. 0. 1. 0. 0.]
4th group: [0. 0. 0. 1. 0.]
5th group: [0. 0. 0. 0. 1.]
</code></pre><h2 id="Section-2-2-Implement-Agent-Methods"><a href="#Section-2-2-Implement-Agent-Methods" class="headerlink" title="Section 2-2: Implement Agent Methods"></a>Section 2-2: Implement Agent Methods</h2><p>Now that we have implemented all the helper functions, let’s create an agent. In this part, you will implement <code>agent_init()</code>, <code>agent_start()</code>, <code>agent_step()</code> and <code>agent_end()</code>. You will have to use <code>agent_policy()</code> that we implemented above. We will implement <code>agent_message()</code> later, when returning the learned state-values.</p>
<p>To save computation time, we precompute features for all states beforehand in <code>agent_init()</code>. The pre-computed features are saved in <code>self.all_state_features</code> numpy array. Hence, you do not  need to call <code>get_state_feature()</code> every time in <code>agent_step()</code> and <code>agent_end()</code>.</p>
<p>The shape of <code>self.all_state_features</code> numpy array is <code>(num_states, feature_size)</code>, with features of states from State 1-500. Note that index 0 stores features for State 1 (Features for State 0 does not exist). Use <code>self.all_state_features</code> to access each feature vector for a state.</p>
<p>When saving state values in the agent, recall how the state values are represented with linear function approximation.</p>
<p><strong>State Value Representation</strong>: $\hat{v}(s,\mathbf{w}) = \mathbf{w}\cdot\mathbf{x^T}$ where $\mathbf{w}$ is a weight vector and $\mathbf{x}$ is the feature vector of the state.</p>
<p>When performing TD(0) updates with Linear Function Approximation, recall how we perform semi-gradient TD(0) updates using supervised learning.</p>
<p><strong>semi-gradient TD(0) Weight Update Rule</strong>: $\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha [R_{t+1} + \gamma \hat{v}(S_{t+1},\mathbf{w}) - \hat{v}(S_t,\mathbf{w})] \nabla \hat{v}(S_t,\mathbf{w})$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create TDAgent</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.num_states = <span class="keyword">None</span></span><br><span class="line">        self.num_groups = <span class="keyword">None</span></span><br><span class="line">        self.step_size = <span class="keyword">None</span></span><br><span class="line">        self.discount_factor = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the semi-gradient TD(0) state aggregation agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: 500 [int],</span></span><br><span class="line"><span class="string">            num_groups: int, </span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            discount_factor: float,</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># set random seed for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>)) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># set class attributes</span></span><br><span class="line">        self.num_states = agent_info.get(<span class="string">"num_states"</span>)</span><br><span class="line">        self.num_groups = agent_info.get(<span class="string">"num_groups"</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">"step_size"</span>)</span><br><span class="line">        self.discount_factor = agent_info.get(<span class="string">"discount_factor"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pre-compute all observable features</span></span><br><span class="line">        num_states_in_group = int(self.num_states / self.num_groups)</span><br><span class="line">        self.all_state_features = np.array([get_state_feature(num_states_in_group, self.num_groups, state) <span class="keyword">for</span> state <span class="keyword">in</span> range(<span class="number">1</span>, self.num_states + <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># initialize all weights to zero using numpy array with correct size</span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights = np.zeros((self.num_groups,))</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        self.last_state = <span class="keyword">None</span></span><br><span class="line">        self.last_action = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            self.last_action [int] : The first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment">### select action given state (using agent_policy), and save current state and action</span></span><br><span class="line">        <span class="comment"># Use self.rand_generator for agent_policy</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_state = state</span><br><span class="line">        self.last_action = agent_policy(self.rand_generator, self.last_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward [float]: the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state [int]: the state from the environment's step, where the agent ended up after the last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            self.last_action [int] : The action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get relevant feature</span></span><br><span class="line">        current_state_feature = self.all_state_features[state<span class="number">-1</span>] </span><br><span class="line">        last_state_feature = self.all_state_features[self.last_state<span class="number">-1</span>] </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### update weights and select action</span></span><br><span class="line">        <span class="comment"># (Hint: np.dot method is useful!)</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Update weights:</span></span><br><span class="line">        <span class="comment">#     use self.weights, current_state_feature, and last_state_feature</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Select action:</span></span><br><span class="line">        <span class="comment">#     use self.rand_generator for agent_policy</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Current state and selected action should be saved to self.last_state and self.last_action at the end</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights += self.step_size * (reward + self.discount_factor * (np.dot(self.weights,current_state_feature)) - np.dot(self.weights,last_state_feature)) * last_state_feature </span><br><span class="line">        self.last_state = state</span><br><span class="line">        self.last_action = agent_policy(self.rand_generator, self.last_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get relevant feature</span></span><br><span class="line">        last_state_feature = self.all_state_features[self.last_state<span class="number">-1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### update weights</span></span><br><span class="line">        <span class="comment"># Update weights using self.weights and last_state_feature</span></span><br><span class="line">        <span class="comment"># (Hint: np.dot method is useful!)</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># Note that here you don't need to choose action since the agent has reached a terminal state</span></span><br><span class="line">        <span class="comment"># Therefore you should not update self.last_state and self.last_action</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights += self.step_size * (reward - np.dot(self.weights,last_state_feature)) * last_state_feature </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="comment"># We will implement this method later</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<p>Run the following code to verify <code>agent_init()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(agent.weights == <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> agent.weights.shape == (<span class="number">10</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check attributes</span></span><br><span class="line">print(<span class="string">"num_states: &#123;&#125;"</span>.format(agent.num_states))</span><br><span class="line">print(<span class="string">"num_groups: &#123;&#125;"</span>.format(agent.num_groups))</span><br><span class="line">print(<span class="string">"step_size: &#123;&#125;"</span>.format(agent.step_size))</span><br><span class="line">print(<span class="string">"discount_factor: &#123;&#125;"</span>.format(agent.discount_factor))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"weights shape: &#123;&#125;"</span>.format(agent.weights.shape))</span><br><span class="line">print(<span class="string">"weights init. value: &#123;&#125;"</span>.format(agent.weights))</span><br></pre></td></tr></table></figure>
<pre><code>num_states: 500
num_groups: 10
step_size: 0.1
discount_factor: 1.0
weights shape: (10,)
weights init. value: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</code></pre><p>Run the following code to verify <code>agent_start()</code>.<br>Although there is randomness due to <code>rand_generator.choice()</code> in <code>agent_policy()</code>, we control the seed so your output should match the expected output. </p>
<p>Make sure <code>rand_generator.choice()</code> is called only once per <code>agent_policy()</code> call.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose state = 250</span></span><br><span class="line">state = <span class="number">250</span></span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">action = agent.agent_start(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">250</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Agent state: &#123;&#125;"</span>.format(agent.last_state))</span><br><span class="line">print(<span class="string">"Agent selected action: &#123;&#125;"</span>.format(agent.last_action))</span><br></pre></td></tr></table></figure>
<pre><code>Agent state: 250
Agent selected action: 1
</code></pre><p>Run the following code to verify <code>agent_step()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the weights to arbitrary values to verify the correctness of weight update</span></span><br><span class="line">agent.weights = np.array([<span class="number">-1.5</span>, <span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>, <span class="number">-1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 50</span></span><br><span class="line">start_state = <span class="number">50</span></span><br><span class="line">action = agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and the next state observed was State 120</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">next_state = <span class="number">120</span></span><br><span class="line">action = agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Updated weights: &#123;&#125;"</span>.format(agent.weights))</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [<span class="number">-0.26</span>, <span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0.</span>, <span class="number">-0.5</span>, <span class="number">-1.</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">120</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"last state: &#123;&#125;"</span>.format(agent.last_state))</span><br><span class="line">print(<span class="string">"last action: &#123;&#125;"</span>.format(agent.last_action))</span><br><span class="line"></span><br><span class="line"><span class="comment"># let's do another</span></span><br><span class="line">reward = <span class="number">-22</span></span><br><span class="line">next_state = <span class="number">222</span></span><br><span class="line">action = agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [<span class="number">-0.26</span>, <span class="number">0.5</span>, <span class="number">-1.165</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0</span>, <span class="number">-0.5</span>, <span class="number">-1</span>])</span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">222</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>Updated weights: [-0.26  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]
last state: 120
last action: 1
</code></pre><p>Run the following code to verify <code>agent_end()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the weights to arbitrary values to verify the correctness of weight update</span></span><br><span class="line">agent.weights = np.array([<span class="number">-1.5</span>, <span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>, <span class="number">-1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 50</span></span><br><span class="line">start_state = <span class="number">50</span></span><br><span class="line">action = agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and reached the terminal state</span></span><br><span class="line">agent.agent_end(<span class="number">10.0</span>)</span><br><span class="line">print(<span class="string">"Updated weights: &#123;&#125;"</span>.format(agent.weights))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [<span class="number">-0.35</span>, <span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0.</span>, <span class="number">-0.5</span>, <span class="number">-1.</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Updated weights: [-0.35  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]
</code></pre><p><strong>Expected output</strong>: (Note only the 1st element was changed, and the result is different from <code>agent_step()</code> )</p>
<pre><code>Initial weights: [-1.5  0.5  1.  -0.5  1.5 -0.5  1.5  0.  -0.5 -1. ]
Updated weights: [-0.35  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]
</code></pre><h2 id="Section-2-3-Returning-Learned-State-Values"><a href="#Section-2-3-Returning-Learned-State-Values" class="headerlink" title="Section 2-3: Returning Learned State Values"></a>Section 2-3: Returning Learned State Values</h2><p>You are almost done! Now let’s implement a code block in <code>agent_message()</code> that returns the learned state values.</p>
<p>The method <code>agent_message()</code> will return the learned state_value array when <code>message == &#39;get state value&#39;</code>.</p>
<p><strong>Hint</strong>: Think about how state values are represented with linear function approximation. <code>state_value</code> array will be a 1D array with length equal to the number of states.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">'get state value'</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### return state_value</span></span><br><span class="line">        <span class="comment"># Use self.all_state_features and self.weights to return the vector of all state values</span></span><br><span class="line">        <span class="comment"># Hint: Use np.dot()</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># state_value = ?</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        state_value = np.dot(self.all_state_features, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> state_value</span><br></pre></td></tr></table></figure>
<p>Run the following code to verify <code>get_state_val()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">test_state_val = agent.agent_message(<span class="string">'get state value'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> test_state_val.shape == (<span class="number">20</span>,)</span><br><span class="line"><span class="keyword">assert</span> np.all(test_state_val == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"State value shape: &#123;&#125;"</span>.format(test_state_val.shape))</span><br><span class="line">print(<span class="string">"Initial State value for all states: &#123;&#125;"</span>.format(test_state_val))</span><br></pre></td></tr></table></figure>
<pre><code>State value shape: (20,)
Initial State value for all states: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</code></pre><p><strong>Expected Output</strong>:</p>
<pre><code>State value shape: (20,)
Initial State value for all states: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</code></pre><h2 id="Section-3-Run-Experiment"><a href="#Section-3-Run-Experiment" class="headerlink" title="Section 3: Run Experiment"></a>Section 3: Run Experiment</h2><p>Now that we’ve implemented all the components of environment and agent, let’s run an experiment! We will plot two things: (1) the learned state value function and compare it against the true state values, and (2) a learning curve depicting the error in the learned value estimates over episodes. For the learning curve, what should we plot to see if the agent is learning well?</p>
<h2 id="Section-3-1-Prediction-Objective-Root-Mean-Squared-Value-Error"><a href="#Section-3-1-Prediction-Objective-Root-Mean-Squared-Value-Error" class="headerlink" title="Section 3-1: Prediction Objective (Root Mean Squared Value Error)"></a>Section 3-1: Prediction Objective (Root Mean Squared Value Error)</h2><p>Recall that the Prediction Objective in function approximation is Mean Squared Value Error $\overline{VE}(\mathbf{w}) \doteq \sum\limits_{s \in \mathcal{S}}\mu(s)[v_\pi(s)-\hat{v}(s,\mathbf{w})]^2$</p>
<p>We will use the square root of this measure, the root $\overline{VE}$ to give a rough measure of how much the learned values differ from the true values.</p>
<p><code>calc RMSVE()</code> computes the Root Mean Squared Value Error given learned state value $\hat{v}(s, \mathbf{w})$.<br>We provide you with true state value $v_\pi(s)$ and state distribution $\mu(s)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Here we provide you with the true state value and state distribution</span></span><br><span class="line">true_state_val = np.load(<span class="string">'data/true_V.npy'</span>)    </span><br><span class="line">state_distribution = np.load(<span class="string">'data/state_distribution.npy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_RMSVE</span><span class="params">(learned_state_val)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span>(len(true_state_val) == len(learned_state_val) == len(state_distribution))</span><br><span class="line">    MSVE = np.sum(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))</span><br><span class="line">    RMSVE = np.sqrt(MSVE)</span><br><span class="line">    <span class="keyword">return</span> RMSVE</span><br></pre></td></tr></table></figure>
<h2 id="Section-3-2a-Run-Experiment-with-10-State-Aggregation"><a href="#Section-3-2a-Run-Experiment-with-10-State-Aggregation" class="headerlink" title="Section 3-2a: Run Experiment with 10-State Aggregation"></a>Section 3-2a: Run Experiment with 10-State Aggregation</h2><p>We have provided you the experiment/plot code in the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define function to run experiment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(environment, agent, environment_parameters, agent_parameters, experiment_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Sweep Agent parameters</span></span><br><span class="line">    <span class="keyword">for</span> num_agg_states <span class="keyword">in</span> agent_parameters[<span class="string">"num_groups"</span>]:</span><br><span class="line">        <span class="keyword">for</span> step_size <span class="keyword">in</span> agent_parameters[<span class="string">"step_size"</span>]:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save rmsve at the end of each evaluation episode</span></span><br><span class="line">            <span class="comment"># size: num_episode / episode_eval_frequency + 1 (includes evaluation at the beginning of training)</span></span><br><span class="line">            agent_rmsve = np.zeros(int(experiment_parameters[<span class="string">"num_episodes"</span>]/experiment_parameters[<span class="string">"episode_eval_frequency"</span>]) + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save learned state value at the end of each run</span></span><br><span class="line">            agent_state_val = np.zeros(environment_parameters[<span class="string">"num_states"</span>])</span><br><span class="line"></span><br><span class="line">            env_info = &#123;<span class="string">"num_states"</span>: environment_parameters[<span class="string">"num_states"</span>],</span><br><span class="line">                        <span class="string">"start_state"</span>: environment_parameters[<span class="string">"start_state"</span>],</span><br><span class="line">                        <span class="string">"left_terminal_state"</span>: environment_parameters[<span class="string">"left_terminal_state"</span>],</span><br><span class="line">                        <span class="string">"right_terminal_state"</span>: environment_parameters[<span class="string">"right_terminal_state"</span>]&#125;</span><br><span class="line"></span><br><span class="line">            agent_info = &#123;<span class="string">"num_states"</span>: environment_parameters[<span class="string">"num_states"</span>],</span><br><span class="line">                          <span class="string">"num_groups"</span>: num_agg_states,</span><br><span class="line">                          <span class="string">"step_size"</span>: step_size,</span><br><span class="line">                          <span class="string">"discount_factor"</span>: environment_parameters[<span class="string">"discount_factor"</span>]&#125;</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'Setting - num. agg. states: &#123;&#125;, step_size: &#123;&#125;'</span>.format(num_agg_states, step_size))</span><br><span class="line">            os.system(<span class="string">'sleep 0.2'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># one agent setting</span></span><br><span class="line">            <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_runs"</span>]+<span class="number">1</span>)):</span><br><span class="line">                env_info[<span class="string">"seed"</span>] = run</span><br><span class="line">                agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">                rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Compute initial RMSVE before training</span></span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">                agent_rmsve[<span class="number">0</span>] += calc_RMSVE(current_V)</span><br><span class="line">                    </span><br><span class="line">                <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_episodes"</span>]+<span class="number">1</span>):</span><br><span class="line">                    <span class="comment"># run episode</span></span><br><span class="line">                    rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> episode % experiment_parameters[<span class="string">"episode_eval_frequency"</span>] == <span class="number">0</span>:</span><br><span class="line">                        current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">                        agent_rmsve[int(episode/experiment_parameters[<span class="string">"episode_eval_frequency"</span>])] += calc_RMSVE(current_V)</span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># store only one run of state value</span></span><br><span class="line">                <span class="keyword">if</span> run == <span class="number">50</span>:</span><br><span class="line">                    agent_state_val = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># rmsve averaged over runs</span></span><br><span class="line">            agent_rmsve /= experiment_parameters[<span class="string">"num_runs"</span>]</span><br><span class="line">            </span><br><span class="line">            save_name = <span class="string">"&#123;&#125;_agg_states_&#123;&#125;_step_size_&#123;&#125;"</span>.format(<span class="string">'TD_agent'</span>, num_agg_states, step_size).replace(<span class="string">'.'</span>,<span class="string">''</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'results'</span>):</span><br><span class="line">                os.makedirs(<span class="string">'results'</span>)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># save avg. state value</span></span><br><span class="line">            np.save(<span class="string">"results/V_&#123;&#125;"</span>.format(save_name), agent_state_val)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># save avg. rmsve</span></span><br><span class="line">            np.save(<span class="string">"results/RMSVE_&#123;&#125;"</span>.format(save_name), agent_rmsve)</span><br></pre></td></tr></table></figure>
<p>We will first test our implementation using state aggregation with resolution of 10, with three different step sizes: {0.01, 0.05, 0.1}.</p>
<p>Note that running the experiment cell below will take <strong>_approximately 5 min_</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">50</span>,</span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">2000</span>,</span><br><span class="line">    <span class="string">"episode_eval_frequency"</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episodes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">500</span>, </span><br><span class="line">    <span class="string">"start_state"</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">"left_terminal_state"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"right_terminal_state"</span> : <span class="number">501</span>, </span><br><span class="line">    <span class="string">"discount_factor"</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line"><span class="comment"># Each element is an array because we will be later sweeping over multiple values</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_groups"</span>: [<span class="number">10</span>],</span><br><span class="line">    <span class="string">"step_size"</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = RandomWalkEnvironment</span><br><span class="line">current_agent = TDAgent</span><br><span class="line"></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_script.plot_result(agent_parameters, <span class="string">'results'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Setting - num. agg. states: 10, step_size: 0.01


100%|██████████| 50/50 [01:32&lt;00:00,  1.85s/it]


Setting - num. agg. states: 10, step_size: 0.05


100%|██████████| 50/50 [01:33&lt;00:00,  1.87s/it]


Setting - num. agg. states: 10, step_size: 0.1


100%|██████████| 50/50 [01:31&lt;00:00,  1.83s/it]
</code></pre><p><img src="output_36_6.png" alt="png"></p>
<p>Is the learned state value plot with step-size=0.01 similar to Figure 9.2 (p.208) in Sutton and Barto?</p>
<p>(Note that our environment has less states: 500 states and we have done 2000 episodes, and averaged the performance over 50 runs)</p>
<p>Look at  the plot of the learning curve. Does RMSVE decrease over time?</p>
<p>Would it be possible to reduce RMSVE to 0?</p>
<p>You should see the RMSVE decrease over time, but the error seems to plateau. It is impossible to reduce RMSVE to 0, because of function approximation (and we do not decay the step-size parameter to zero). With function approximation, the agent has limited resources and has to trade-off the accuracy of one state for another state.</p>
<p>Run the following code to verify your experimental result.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_groups"</span>: [<span class="number">10</span>],</span><br><span class="line">    <span class="string">"step_size"</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">all_correct = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">for</span> num_agg_states <span class="keyword">in</span> agent_parameters[<span class="string">"num_groups"</span>]:</span><br><span class="line">    <span class="keyword">for</span> step_size <span class="keyword">in</span> agent_parameters[<span class="string">"step_size"</span>]:</span><br><span class="line">        filename = <span class="string">'RMSVE_TD_agent_agg_states_&#123;&#125;_step_size_&#123;&#125;'</span>.format(num_agg_states, step_size).replace(<span class="string">'.'</span>,<span class="string">''</span>)</span><br><span class="line">        agent_RMSVE = np.load(<span class="string">'results/&#123;&#125;.npy'</span>.format(filename))</span><br><span class="line">        correct_RMSVE = np.load(<span class="string">'correct_npy/&#123;&#125;.npy'</span>.format(filename))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.allclose(agent_RMSVE, correct_RMSVE):</span><br><span class="line">            all_correct=<span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_correct:</span><br><span class="line">    print(<span class="string">"Your experiment results are correct!"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"Your experiment results does not match with ours. Please check if you have implemented all methods correctly."</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Your experiment results are correct!
</code></pre><h2 id="Section-3-2b-Run-Experiment-with-Different-State-Aggregation-Resolution-and-Step-Size"><a href="#Section-3-2b-Run-Experiment-with-Different-State-Aggregation-Resolution-and-Step-Size" class="headerlink" title="Section 3-2b: Run Experiment with Different State Aggregation Resolution and Step-Size"></a>Section 3-2b: Run Experiment with Different State Aggregation Resolution and Step-Size</h2><p>In this section, we will run some more experiments to see how different parameter settings affect the results!</p>
<p>In particular, we will test several values of <code>num_groups</code> and <code>step_size</code>. Parameter sweeps although necessary, can take lots of time. So now that you have verified your experiment result, here we show you the results of the parameter sweeps that you would see when running the sweeps yourself.</p>
<p>We tested several different values of <code>num_groups</code>: {10, 100, 500}, and <code>step-size</code>: {0.01, 0.05, 0.1}. As before, we performed 2000 episodes per run, and averaged the results over 50 runs for each setting.</p>
<p>Run the cell below to display the sweep results.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make sure to verify your experiment result with the test cell above.</span></span><br><span class="line"><span class="comment"># Otherwise the sweep results will not be displayed.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">50</span>,</span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">2000</span>,</span><br><span class="line">    <span class="string">"episode_eval_frequency"</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episodes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">500</span>,</span><br><span class="line">    <span class="string">"start_state"</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">"left_terminal_state"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"right_terminal_state"</span> : <span class="number">501</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line"><span class="comment"># Each element is an array because we will be sweeping over multiple values</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_groups"</span>: [<span class="number">10</span>, <span class="number">100</span>, <span class="number">500</span>],</span><br><span class="line">    <span class="string">"step_size"</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_correct:</span><br><span class="line">    plot_script.plot_result(agent_parameters, <span class="string">'correct_npy'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"Make sure your experiment result is correct! Otherwise the sweep results will not be displayed."</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_41_0.png" alt="png"></p>
<p><img src="output_41_1.png" alt="png"></p>
<p><img src="output_41_2.png" alt="png"></p>
<h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Let’s think about the results of our parameter study.</p>
<h3 id="State-Aggregation"><a href="#State-Aggregation" class="headerlink" title="State Aggregation"></a>State Aggregation</h3><ul>
<li><p>Which state aggregation resolution do you think is the best after running 2000 episodes? Which state aggregation resolution do you think would be the best if we could train for only 200 episodes? What if we could train for a million episodes?</p>
</li>
<li><p>Should we use tabular representation (state aggregation of resolution 500) whenever possible? Why might we want to use function approximation?</p>
</li>
</ul>
<p>From the plots, using 100 state aggregation with step-size 0.05 reaches the best performance: the lowest RMSVE after 2000 episodes. If the agent can only be trained for 200 episodes, then 10 state aggregation with step-size 0.05 reaches the lowest error. Increasing the resolution of state aggregation makes the function approximation closer to a  tabular representation, which would be able to learn exactly correct state values for all states. But learning will be slower. </p>
<h3 id="Step-Size"><a href="#Step-Size" class="headerlink" title="Step-Size"></a>Step-Size</h3><ul>
<li>How did different step-sizes affect learning?</li>
</ul>
<p>The best step-size is different for different state aggregation resolutions. A larger step-size allows the agent to learn faster, but might not perform as well asymptotically. A smaller step-size causes it to learn more slowly, but may perform well asymptotically.</p>
<h3 id="Congratulations-You-have-successfully-implemented-Course-3-Programming-Assignment-1"><a href="#Congratulations-You-have-successfully-implemented-Course-3-Programming-Assignment-1" class="headerlink" title="Congratulations! You have successfully implemented Course 3 Programming Assignment 1."></a><strong>Congratulations!</strong> You have successfully implemented Course 3 Programming Assignment 1.</h3><p>You have implemented <strong>semi-gradient TD(0) with State Aggregation</strong> in a 500-state Random Walk. We used an environment with a large but discrete state space, where it was possible to compute the true state values. This allowed us to compare the values learned by your agent to the true state values. The same state aggregation function approximation can also be applied to continuous state space environments, where comparison to the true values is not usually possible.</p>
<p>You also successfully applied supervised learning approaches to approximate value functions with semi-gradient TD(0). </p>
<p>Finally, we plotted the learned state values and compared with true state values. We also compared learning curves of different state aggregation resolutions and learning rates. </p>
<p>From the results, you can  see why it is often desirable to use function approximation, even when tabular learning is possible. Asymptotically, an agent with tabular representation would be able to learn the true state value function, but it would learn much more slowly compared to an agent with function approximation. On the other hand, we also want to ensure we do not reduce discrimination too far (a coarse state aggregation resolution), because it will hurt the asymptotic performance.</p>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Components-of-StyleGAN/2020/10/13/" rel="next" title="Components of StyleGAN ">
                <i class="fa fa-chevron-left"></i> Components of StyleGAN 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/" rel="prev" title="Semi Gradient TD with a Neural Network">
                Semi Gradient TD with a Neural Network <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate article here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Ruochi Zhang WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    

    
      <div id="bitcoin" style="display: inline-block">
        <img id="vemo_qr" src="/images/venmo.png" alt="Ruochi Zhang Bitcoin">
        <p>Venmo(last 4 digits 1570)</p>
      </div>
    

  </div>
</div>

      </div>
    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">270</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Assignment-1-TD-with-State-Aggregation"><span class="nav-text">Assignment 1 - TD with State Aggregation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#500-State-RandomWalk-Environment"><span class="nav-text">500-State RandomWalk Environment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Your-Goal"><span class="nav-text">Your Goal</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Packages"><span class="nav-text">Packages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-1-Create-the-500-State-RandomWalk-Environment"><span class="nav-text">Section 1: Create the 500-State RandomWalk Environment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-2-Create-Semi-gradient-TD-0-Agent-with-State-Aggregation"><span class="nav-text">Section 2: Create Semi-gradient TD(0) Agent with State Aggregation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-2-1-Implement-Useful-Functions"><span class="nav-text">Section 2-1: Implement Useful Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-2-1a-Selecting-actions"><span class="nav-text">Section 2-1a: Selecting actions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-2-1b-Processing-State-Features-with-State-Aggregation"><span class="nav-text">Section 2-1b: Processing State Features with State Aggregation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-2-2-Implement-Agent-Methods"><span class="nav-text">Section 2-2: Implement Agent Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-2-3-Returning-Learned-State-Values"><span class="nav-text">Section 2-3: Returning Learned State Values</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-3-Run-Experiment"><span class="nav-text">Section 3: Run Experiment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-3-1-Prediction-Objective-Root-Mean-Squared-Value-Error"><span class="nav-text">Section 3-1: Prediction Objective (Root Mean Squared Value Error)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-3-2a-Run-Experiment-with-10-State-Aggregation"><span class="nav-text">Section 3-2a: Run Experiment with 10-State Aggregation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-3-2b-Run-Experiment-with-Different-State-Aggregation-Resolution-and-Step-Size"><span class="nav-text">Section 3-2b: Run Experiment with Different State Aggregation Resolution and Step-Size</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Wrapping-up"><span class="nav-text">Wrapping up</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#State-Aggregation"><span class="nav-text">State Aggregation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step-Size"><span class="nav-text">Step-Size</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Congratulations-You-have-successfully-implemented-Course-3-Programming-Assignment-1"><span class="nav-text">Congratulations! You have successfully implemented Course 3 Programming Assignment 1.</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
