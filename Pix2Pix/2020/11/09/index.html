<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Project,Generative Adversarial Network,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="Pix2PixGoalsIn this notebook, you will write a generative model based on the paper Image-to-Image Translation with Conditional Adversarial Networks by Isola et al. 2017, also known as Pix2Pix. You wil">
<meta name="keywords" content="Project,Generative Adversarial Network">
<meta property="og:type" content="article">
<meta property="og:title" content="Pix2Pix">
<meta property="og:url" content="https://zhangruochi.com/Pix2Pix/2020/11/09/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Pix2PixGoalsIn this notebook, you will write a generative model based on the paper Image-to-Image Translation with Conditional Adversarial Networks by Isola et al. 2017, also known as Pix2Pix. You wil">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://drive.google.com/uc?id=1F-oVKBWcd6RaMyP3weGM8wvRtkK6y6i0">
<meta property="og:image" content="https://zhangruochi.com/Pix2Pix/2020/11/09/output_14_0.png">
<meta property="og:updated_time" content="2020-11-09T07:48:46.318Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Pix2Pix">
<meta name="twitter:description" content="Pix2PixGoalsIn this notebook, you will write a generative model based on the paper Image-to-Image Translation with Conditional Adversarial Networks by Isola et al. 2017, also known as Pix2Pix. You wil">
<meta name="twitter:image" content="https://drive.google.com/uc?id=1F-oVKBWcd6RaMyP3weGM8wvRtkK6y6i0">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/Pix2Pix/2020/11/09/">





  <title>Pix2Pix | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Pix2Pix/2020/11/09/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Pix2Pix</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-09T15:48:19+08:00">
                2020-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Pix2Pix/2020/11/09/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Pix2Pix/2020/11/09/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Pix2Pix"><a href="#Pix2Pix" class="headerlink" title="Pix2Pix"></a>Pix2Pix</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you will write a generative model based on the paper <a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="noopener"><em>Image-to-Image Translation with Conditional Adversarial Networks</em></a> by Isola et al. 2017, also known as Pix2Pix.</p>
<p>You will be training a model that can convert aerial satellite imagery (“input”) into map routes (“output”), as was done in the original paper. Since the architecture for the generator is a U-Net, which you’ve already implemented (with minor changes), the emphasis of the assignment will be on the loss function. So that you can see outputs more quickly, you’ll be able to see your model train starting from a pre-trained checkpoint - but feel free to train it from scratch on your own too.</p>
<p><img src="https://drive.google.com/uc?id=1F-oVKBWcd6RaMyP3weGM8wvRtkK6y6i0" alt="pix2pix example"></p>
<!-- You will take the segmentations that you generated in the previous assignment and produce photorealistic images. -->
<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol>
<li>Implement the loss of a Pix2Pix model that differentiates it from a supervised U-Net.</li>
<li>Observe the change in generator priorities as the Pix2Pix generator trains, changing its emphasis from reconstruction to realism.</li>
</ol>
<!-- When you're done with this assignment, you'll be able to understand much of [*Image-to-Image Translation with Conditional Adversarial Networks*](https://arxiv.org/abs/1611.07004), which introduced Pix2Pix.

You'll be using the same U-Net as in the previous assignment, but you'll write another discriminator and change the loss to make it a GAN. -->
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will start by importing libraries, defining a visualization function, and getting the pre-trained Pix2Pix checkpoint. You will also be provided with the U-Net code for the Pix2Pix generator.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_shifted = image_tensor</span><br><span class="line">    image_unflat = image_shifted.detach().cpu().view(<span class="number">-1</span>, *size)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="U-Net-Code"><a href="#U-Net-Code" class="headerlink" title="U-Net Code"></a>U-Net Code</h4><p>The U-Net code will be much like the code you wrote for the last assignment, but with optional dropout and batchnorm. The structure is changed slightly for Pix2Pix, so that the final image is closer in size to the input image. Feel free to investigate the code if you’re interested!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crop</span><span class="params">(image, new_shape)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for cropping an image tensor: Given an image tensor and the new shape,</span></span><br><span class="line"><span class="string">    crops to the center pixels.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        new_shape: a torch.Size object with the shape you want x to have</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    middle_height = image.shape[<span class="number">2</span>] // <span class="number">2</span></span><br><span class="line">    middle_width = image.shape[<span class="number">3</span>] // <span class="number">2</span></span><br><span class="line">    starting_height = middle_height - round(new_shape[<span class="number">2</span>] / <span class="number">2</span>)</span><br><span class="line">    final_height = starting_height + new_shape[<span class="number">2</span>]</span><br><span class="line">    starting_width = middle_width - round(new_shape[<span class="number">3</span>] / <span class="number">2</span>)</span><br><span class="line">    final_width = starting_width + new_shape[<span class="number">3</span>]</span><br><span class="line">    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]</span><br><span class="line">    <span class="keyword">return</span> cropped_image</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContractingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ContractingBlock Class</span></span><br><span class="line"><span class="string">    Performs two convolutions followed by a max pool operation.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, use_dropout=False, use_bn=True)</span>:</span></span><br><span class="line">        super(ContractingBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels * <span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(input_channels * <span class="number">2</span>, input_channels * <span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.activation = nn.LeakyReLU(<span class="number">0.2</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            self.batchnorm = nn.BatchNorm2d(input_channels * <span class="number">2</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        <span class="keyword">if</span> use_dropout:</span><br><span class="line">            self.dropout = nn.Dropout()</span><br><span class="line">        self.use_dropout = use_dropout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ContractingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes a contracting block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batchnorm(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batchnorm(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpandingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ExpandingBlock Class:</span></span><br><span class="line"><span class="string">    Performs an upsampling, a convolution, a concatenation of its two inputs,</span></span><br><span class="line"><span class="string">    followed by two more convolutions with optional dropout</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, use_dropout=False, use_bn=True)</span>:</span></span><br><span class="line">        super(ExpandingBlock, self).__init__()</span><br><span class="line">        self.upsample = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'bilinear'</span>, align_corners=<span class="keyword">True</span>)</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(input_channels, input_channels // <span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(input_channels // <span class="number">2</span>, input_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            self.batchnorm = nn.BatchNorm2d(input_channels // <span class="number">2</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        self.activation = nn.ReLU()</span><br><span class="line">        <span class="keyword">if</span> use_dropout:</span><br><span class="line">            self.dropout = nn.Dropout()</span><br><span class="line">        self.use_dropout = use_dropout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, skip_con_x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ExpandingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes an expanding block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">            skip_con_x: the image tensor from the contracting path (from the opposing block of x)</span></span><br><span class="line"><span class="string">                    for the skip connection</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.upsample(x)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        skip_con_x = crop(skip_con_x, x.shape)</span><br><span class="line">        x = torch.cat([x, skip_con_x], axis=<span class="number">1</span>)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batchnorm(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batchnorm(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureMapBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    FeatureMapBlock Class</span></span><br><span class="line"><span class="string">    The final layer of a U-Net - </span></span><br><span class="line"><span class="string">    maps each pixel to a pixel with the correct number of output dimensions</span></span><br><span class="line"><span class="string">    using a 1x1 convolution.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels)</span>:</span></span><br><span class="line">        super(FeatureMapBlock, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of FeatureMapBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, returns it mapped to the desired number of channels.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    UNet Class</span></span><br><span class="line"><span class="string">    A series of 4 contracting blocks followed by 4 expanding blocks to </span></span><br><span class="line"><span class="string">    transform an input image into the corresponding paired image, with an upfeature</span></span><br><span class="line"><span class="string">    layer at the start and a downfeature layer at the end.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels, hidden_channels=<span class="number">32</span>)</span>:</span></span><br><span class="line">        super(UNet, self).__init__()</span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels, use_dropout=<span class="keyword">True</span>)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>, use_dropout=<span class="keyword">True</span>)</span><br><span class="line">        self.contract3 = ContractingBlock(hidden_channels * <span class="number">4</span>, use_dropout=<span class="keyword">True</span>)</span><br><span class="line">        self.contract4 = ContractingBlock(hidden_channels * <span class="number">8</span>)</span><br><span class="line">        self.contract5 = ContractingBlock(hidden_channels * <span class="number">16</span>)</span><br><span class="line">        self.contract6 = ContractingBlock(hidden_channels * <span class="number">32</span>)</span><br><span class="line">        self.expand0 = ExpandingBlock(hidden_channels * <span class="number">64</span>)</span><br><span class="line">        self.expand1 = ExpandingBlock(hidden_channels * <span class="number">32</span>)</span><br><span class="line">        self.expand2 = ExpandingBlock(hidden_channels * <span class="number">16</span>)</span><br><span class="line">        self.expand3 = ExpandingBlock(hidden_channels * <span class="number">8</span>)</span><br><span class="line">        self.expand4 = ExpandingBlock(hidden_channels * <span class="number">4</span>)</span><br><span class="line">        self.expand5 = ExpandingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of UNet: </span></span><br><span class="line"><span class="string">        Given an image tensor, passes it through U-Net and returns the output.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.contract3(x2)</span><br><span class="line">        x4 = self.contract4(x3)</span><br><span class="line">        x5 = self.contract5(x4)</span><br><span class="line">        x6 = self.contract6(x5)</span><br><span class="line">        x7 = self.expand0(x6, x5)</span><br><span class="line">        x8 = self.expand1(x7, x4)</span><br><span class="line">        x9 = self.expand2(x8, x3)</span><br><span class="line">        x10 = self.expand3(x9, x2)</span><br><span class="line">        x11 = self.expand4(x10, x1)</span><br><span class="line">        x12 = self.expand5(x11, x0)</span><br><span class="line">        xn = self.downfeature(x12)</span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(xn)</span><br></pre></td></tr></table></figure>
<h2 id="PatchGAN-Discriminator"><a href="#PatchGAN-Discriminator" class="headerlink" title="PatchGAN Discriminator"></a>PatchGAN Discriminator</h2><p>Next, you will define a discriminator based on the contracting path of the U-Net to allow you to evaluate the realism of the generated images. Remember that the discriminator outputs a one-channel matrix of classifications instead of a single value. Your discriminator’s final layer will simply map from the final number of hidden channels to a single prediction for every pixel of the layer before it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CLASS: Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Structured like the contracting path of the U-Net, the discriminator will</span></span><br><span class="line"><span class="string">    output a matrix of values classifying corresponding portions of the image as real or fake. </span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_channels: the number of image input channels</span></span><br><span class="line"><span class="string">        hidden_channels: the initial number of discriminator convolutional filters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, hidden_channels=<span class="number">8</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels, use_bn=<span class="keyword">False</span>)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        self.contract3 = ContractingBlock(hidden_channels * <span class="number">4</span>)</span><br><span class="line">        self.contract4 = ContractingBlock(hidden_channels * <span class="number">8</span>)</span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        self.final = nn.Conv2d(hidden_channels * <span class="number">16</span>, <span class="number">1</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        x = torch.cat([x, y], axis=<span class="number">1</span>)</span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.contract3(x2)</span><br><span class="line">        x4 = self.contract4(x3)</span><br><span class="line">        xn = self.final(x4)</span><br><span class="line">        <span class="keyword">return</span> xn</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_discriminator = Discriminator(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> tuple(test_discriminator(</span><br><span class="line">    torch.randn(<span class="number">1</span>, <span class="number">5</span>, <span class="number">256</span>, <span class="number">256</span>), </span><br><span class="line">    torch.randn(<span class="number">1</span>, <span class="number">5</span>, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">).shape) == (<span class="number">1</span>, <span class="number">1</span>, <span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h2 id="Training-Preparation"><a href="#Training-Preparation" class="headerlink" title="Training Preparation"></a>Training Preparation</h2><!-- You'll be using the same U-Net as in the previous assignment, but you'll write another discriminator and change the loss to make it a GAN. -->
<p>Now you can begin putting everything together for training. You start by defining some new parameters as well as the ones you are familiar with:</p>
<ul>
<li><strong>real_dim</strong>: the number of channels of the real image and the number expected in the output image</li>
<li><strong>adv_criterion</strong>: an adversarial loss function to keep track of how well the GAN is fooling the discriminator and how well the discriminator is catching the GAN</li>
<li><strong>recon_criterion</strong>: a loss function that rewards similar images to the ground truth, which “reconstruct” the image</li>
<li><strong>lambda_recon</strong>: a parameter for how heavily the reconstruction loss should be weighed</li>
<li><strong>n_epochs</strong>: the number of times you iterate through the entire dataset when training</li>
<li><strong>input_dim</strong>: the number of channels of the input image</li>
<li><strong>display_step</strong>: how often to display/visualize the images</li>
<li><strong>batch_size</strong>: the number of images per forward/backward pass</li>
<li><strong>lr</strong>: the learning rate</li>
<li><strong>target_shape</strong>: the size of the output image (in pixels)</li>
<li><strong>device</strong>: the device type</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># New parameters</span></span><br><span class="line">adv_criterion = nn.BCEWithLogitsLoss() </span><br><span class="line">recon_criterion = nn.L1Loss() </span><br><span class="line">lambda_recon = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">20</span></span><br><span class="line">input_dim = <span class="number">3</span></span><br><span class="line">real_dim = <span class="number">3</span></span><br><span class="line">display_step = <span class="number">200</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">target_shape = <span class="number">256</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br></pre></td></tr></table></figure>
<p>You will then pre-process the images of the dataset to make sure they’re all the same size and that the size change due to U-Net layers is accounted for. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = torchvision.datasets.ImageFolder(<span class="string">"maps"</span>, transform=transform)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a,b = dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_tensor_images(a,<span class="number">1</span>,(<span class="number">600</span>,<span class="number">1200</span>))</span><br></pre></td></tr></table></figure>
<p><img src="output_14_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure>
<pre><code>0
</code></pre><p>Next, you can initialize your generator (U-Net) and discriminator, as well as their optimizers. Finally, you will also load your pre-trained model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">gen = UNet(input_dim, real_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">disc = Discriminator(input_dim + real_dim).to(device)</span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feel free to change pretrained to False if you're training the model from scratch</span></span><br><span class="line">pretrained = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">if</span> pretrained:</span><br><span class="line">    loaded_state = torch.load(<span class="string">"pix2pix_15000.pth"</span>)</span><br><span class="line">    gen.load_state_dict(loaded_state[<span class="string">"gen"</span>])</span><br><span class="line">    gen_opt.load_state_dict(loaded_state[<span class="string">"gen_opt"</span>])</span><br><span class="line">    disc.load_state_dict(loaded_state[<span class="string">"disc"</span>])</span><br><span class="line">    disc_opt.load_state_dict(loaded_state[<span class="string">"disc_opt"</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    gen = gen.apply(weights_init)</span><br><span class="line">    disc = disc.apply(weights_init)</span><br></pre></td></tr></table></figure>
<p>While there are some changes to the U-Net architecture for Pix2Pix, the most important distinguishing feature of Pix2Pix is its adversarial loss. You will be implementing that here!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CLASS: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span><span class="params">(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the generator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gen: the generator; takes the condition and returns potential images</span></span><br><span class="line"><span class="string">        disc: the discriminator; takes images and the condition and</span></span><br><span class="line"><span class="string">          returns real/fake prediction matrices</span></span><br><span class="line"><span class="string">        real: the real images (e.g. maps) to be used to evaluate the reconstruction</span></span><br><span class="line"><span class="string">        condition: the source images (e.g. satellite imagery) which are used to produce the real images</span></span><br><span class="line"><span class="string">        adv_criterion: the adversarial loss function; takes the discriminator </span></span><br><span class="line"><span class="string">                  predictions and the true labels and returns a adversarial </span></span><br><span class="line"><span class="string">                  loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">        recon_criterion: the reconstruction loss function; takes the generator </span></span><br><span class="line"><span class="string">                    outputs and the real images and returns a reconstructuion </span></span><br><span class="line"><span class="string">                    loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">        lambda_recon: the degree to which the reconstruction loss should be weighted in the sum</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Steps: 1) Generate the fake images, based on the conditions.</span></span><br><span class="line">    <span class="comment">#        2) Evaluate the fake images and the condition with the discriminator.</span></span><br><span class="line">    <span class="comment">#        3) Calculate the adversarial and reconstruction losses.</span></span><br><span class="line">    <span class="comment">#        4) Add the two losses, weighting the reconstruction loss appropriately.</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    gen_img = gen(condition)</span><br><span class="line">    out = disc(gen_img, condition)</span><br><span class="line">    adv_loss = adv_criterion(out, torch.ones_like(out))</span><br><span class="line">    recon_loss = recon_criterion(gen_img, real)</span><br><span class="line">    gen_loss = adv_loss + lambda_recon * recon_loss</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_reasonable</span><span class="params">(num_images=<span class="number">10</span>)</span>:</span></span><br><span class="line">    gen = torch.zeros_like</span><br><span class="line">    disc = <span class="keyword">lambda</span> x, y: torch.ones(len(x), <span class="number">1</span>)</span><br><span class="line">    real = <span class="keyword">None</span></span><br><span class="line">    condition = torch.ones(num_images, <span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">    adv_criterion = torch.mul</span><br><span class="line">    recon_criterion = <span class="keyword">lambda</span> x, y: torch.tensor(<span class="number">0</span>)</span><br><span class="line">    lambda_recon = <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon).sum() == num_images</span><br><span class="line"></span><br><span class="line">    disc = <span class="keyword">lambda</span> x, y: torch.zeros(len(x), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.abs(get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)).sum() == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    adv_criterion = <span class="keyword">lambda</span> x, y: torch.tensor(<span class="number">0</span>)</span><br><span class="line">    recon_criterion = <span class="keyword">lambda</span> x, y: torch.abs(x - y).max()</span><br><span class="line">    real = torch.randn(num_images, <span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">    lambda_recon = <span class="number">2</span></span><br><span class="line">    gen = <span class="keyword">lambda</span> x: real + <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> torch.abs(get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon) - <span class="number">2</span>) &lt; <span class="number">1e-4</span></span><br><span class="line"></span><br><span class="line">    adv_criterion = <span class="keyword">lambda</span> x, y: (x + y).max() + x.max()</span><br><span class="line">    <span class="keyword">assert</span> torch.abs(get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon) - <span class="number">3</span>) &lt; <span class="number">1e-4</span></span><br><span class="line">test_gen_reasonable()</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h2 id="Pix2Pix-Training"><a href="#Pix2Pix-Training" class="headerlink" title="Pix2Pix Training"></a>Pix2Pix Training</h2><p>Finally, you can train the model and see some of your maps!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> color</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(save_model=False)</span>:</span></span><br><span class="line">    mean_generator_loss = <span class="number">0</span></span><br><span class="line">    mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    cur_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">        <span class="keyword">for</span> image, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">            image_width = image.shape[<span class="number">3</span>]</span><br><span class="line">            condition = image[:, :, :, :image_width // <span class="number">2</span>]</span><br><span class="line">            condition = nn.functional.interpolate(condition, size=target_shape)</span><br><span class="line">            real = image[:, :, :, image_width // <span class="number">2</span>:]</span><br><span class="line">            real = nn.functional.interpolate(real, size=target_shape)</span><br><span class="line">            cur_batch_size = len(condition)</span><br><span class="line">            condition = condition.to(device)</span><br><span class="line">            real = real.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update discriminator ###</span></span><br><span class="line">            disc_opt.zero_grad() <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = gen(condition)</span><br><span class="line">            disc_fake_hat = disc(fake.detach(), condition) <span class="comment"># Detach generator</span></span><br><span class="line">            disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))</span><br><span class="line">            disc_real_hat = disc(real, condition)</span><br><span class="line">            disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))</span><br><span class="line">            disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line">            disc_loss.backward(retain_graph=<span class="keyword">True</span>) <span class="comment"># Update gradients</span></span><br><span class="line">            disc_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update generator ###</span></span><br><span class="line">            gen_opt.zero_grad()</span><br><span class="line">            gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)</span><br><span class="line">            gen_loss.backward() <span class="comment"># Update gradients</span></span><br><span class="line">            gen_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">            mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line">            <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">            mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Visualization code ###</span></span><br><span class="line">            <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">                    print(<span class="string">f"Epoch <span class="subst">&#123;epoch&#125;</span>: Step <span class="subst">&#123;cur_step&#125;</span>: Generator (U-Net) loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, Discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">"Pretrained initial state"</span>)</span><br><span class="line">                show_tensor_images(condition, size=(input_dim, target_shape, target_shape))</span><br><span class="line">                show_tensor_images(real, size=(real_dim, target_shape, target_shape))</span><br><span class="line">                show_tensor_images(fake, size=(real_dim, target_shape, target_shape))</span><br><span class="line">                mean_generator_loss = <span class="number">0</span></span><br><span class="line">                mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">                <span class="comment"># You can change save_model to True if you'd like to save the model</span></span><br><span class="line">                <span class="keyword">if</span> save_model:</span><br><span class="line">                    torch.save(&#123;<span class="string">'gen'</span>: gen.state_dict(),</span><br><span class="line">                        <span class="string">'gen_opt'</span>: gen_opt.state_dict(),</span><br><span class="line">                        <span class="string">'disc'</span>: disc.state_dict(),</span><br><span class="line">                        <span class="string">'disc_opt'</span>: disc_opt.state_dict()</span><br><span class="line">                    &#125;, <span class="string">f"pix2pix_<span class="subst">&#123;cur_step&#125;</span>.pth"</span>)</span><br><span class="line">            cur_step += <span class="number">1</span></span><br><span class="line">train()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Project/" rel="tag"># Project</a>
          
            <a href="/tags/Generative-Adversarial-Network/" rel="tag"># Generative Adversarial Network</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/U-Net/2020/11/09/" rel="next" title="U-Net">
                <i class="fa fa-chevron-left"></i> U-Net
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/CycleGAN/2020/11/09/" rel="prev" title="CycleGAN">
                CycleGAN <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">273</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Pix2Pix"><span class="nav-text">Pix2Pix</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Goals"><span class="nav-text">Goals</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Objectives"><span class="nav-text">Learning Objectives</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Getting-Started"><span class="nav-text">Getting Started</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#U-Net-Code"><span class="nav-text">U-Net Code</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#PatchGAN-Discriminator"><span class="nav-text">PatchGAN Discriminator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Preparation"><span class="nav-text">Training Preparation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pix2Pix-Training"><span class="nav-text">Pix2Pix Training</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
