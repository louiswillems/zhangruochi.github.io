<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="cs224n,NLP,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="Word Vectors Summary">
<meta name="keywords" content="cs224n,NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Word Vectors">
<meta property="og:url" content="https://zhangruochi.com/Word-Vectors/2019/12/04/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Word Vectors Summary">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/diagonalizable.png">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/usv.png">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/svd.png">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/Skip.png">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/Skip_2.png">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/objective.png">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/CBOW.png">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/negative_sampling.png">
<meta property="og:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/hafftree.png">
<meta property="og:updated_time" content="2019-12-19T18:10:37.008Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Word Vectors">
<meta name="twitter:description" content="Word Vectors Summary">
<meta name="twitter:image" content="https://zhangruochi.com/Word-Vectors/2019/12/04/diagonalizable.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/Word-Vectors/2019/12/04/">





  <title>Word Vectors | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Word-Vectors/2019/12/04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Word Vectors</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-04T16:14:11+08:00">
                2019-12-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Word-Vectors/2019/12/04/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Word-Vectors/2019/12/04/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          
              <div class="post-description">
                  Word Vectors Summary
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Why-we-need-Word-Vectors"><a href="#Why-we-need-Word-Vectors" class="headerlink" title="Why we need Word Vectors ?"></a>Why we need Word Vectors ?</h2><p>We want to encode word tokens each into some vector that represents a point in some sort of “word” space. This is paramount for a number of reasons but the most intuitive reason is that perhaps there actually exists some N-dimensional space (such that N &lt;&lt; 13 million) that is sufficient to encode all semantics of our language. Each dimension would encode some meaning that we transfer using speech. For instance, semantic dimensions might indicate tense (past vs. present vs. future), count (singular vs. plural), and gender (masculine vs. feminine).</p>
<h2 id="One-hot-vector"><a href="#One-hot-vector" class="headerlink" title="One-hot vector"></a>One-hot vector</h2><p>Represent every word as an $\mathbb{R}^{|v|\cdot 1}$ vector with all 0s and one 1 at the index of that word in the sorted english language. $|V|$ is the size of our vocabulary. Word vectors in this type of encoding would appear as the following:</p>
<script type="math/tex; mode=display">
W^{abandon} = \begin{bmatrix} 
1 \\
0 \\
0 \\
0 \\
\vdots \\
0 \\
\end{bmatrix}</script><p>We represent each word as a completely independent entity. This word representation <strong>does not</strong> give us directly any notion of similarity. For instance,</p>
<script type="math/tex; mode=display">(W^{hotel})^{T}W^{motel} =(W^{hotel})^{T}W^{cat} = 0</script><h2 id="SVD-Based-Methods"><a href="#SVD-Based-Methods" class="headerlink" title="SVD Based Methods"></a>SVD Based Methods</h2><p>For this class of methods to find word embeddings (otherwise known as word vectors), we first loop over a massive dataset and accumulate word co-occurrence counts in some form of a matrix X, and then perform Singular Value Decomposition on X to get a<br>$USV^{T}$ decomposition. We then use the rows of U as the word embeddings for all words in our dictionary. Let us discuss a few choices of X.</p>
<h3 id="Word-Document-Matrix"><a href="#Word-Document-Matrix" class="headerlink" title="Word-Document Matrix"></a>Word-Document Matrix</h3><p>As our first attempt, we make the bold conjecture that words thatare related will often appear in the <strong>same documents</strong>. We use this fact to build a word-document matrix, $X$ in the following manner: Loop over billions of documents and for each time word $i$ appears in document $j$, we add one to entry $X_{ij}$. This is obviously a very large matrix $\mathbb{R}^{|v|\cdot M}$ and it scales with the number of documents (M). So perhaps we can try something better.</p>
<h3 id="Window-based-Co-occurrence-Matrix"><a href="#Window-based-Co-occurrence-Matrix" class="headerlink" title="Window based Co-occurrence Matrix"></a>Window based Co-occurrence Matrix</h3><p>A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the <em>context window</em> surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \dots w_{i-1}$ and $w_{i+1} \dots w_{i+n}$. We build a <em>co-occurrence matrix</em> $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$’s window.</p>
<p><strong>Example: Co-Occurrence with Fixed Window of n=1</strong>:</p>
<ul>
<li>Document 1: “all that glitters is not gold”</li>
<li>Document 2: “all is well that ends well”</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>*</th>
<th>START</th>
<th>all</th>
<th>that</th>
<th>glitters</th>
<th>is</th>
<th>not</th>
<th>gold</th>
<th>well</th>
<th>ends</th>
<th>END</th>
</tr>
</thead>
<tbody>
<tr>
<td>START</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>all</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>that</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>glitters</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>is</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>not</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>gold</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>well</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>ends</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>END</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> In NLP, we often add START and END tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine START and END tokens encapsulating each document, e.g., “START All that glitters is not gold END”, and include these tokens in our co-occurrence counts.</p>
<p>The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run <em>dimensionality reduction</em>. In particular, we will run <em>SVD (Singular Value Decomposition)</em>, which is a kind of generalized <em>PCA (Principal Components Analysis)</em> to select the top $k$ principal components. Here’s a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is $A$ with $n$ rows corresponding to $n$ words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal $S$ matrix, and our new, shorter length-$k$ word vectors in $U_k$.</p>
<h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p><strong>Eigenvalues</strong> quantify the importance of information along the line of <strong>eigenvectors</strong>. Equipped with this information, we know what part of the information can be ignored and how to compress information (SVD, Dimension reduction &amp; PCA). It also helps us to extract features in developing machine learning models. Sometimes, it makes the model easier to train because of the reduction of tangled information. It also serves the purpose to visualize tangled raw data.</p>
<p>for Eigenvalues $\lambda$ and Eigenvector $V$, we have:</p>
<script type="math/tex; mode=display">AV = \lambda V</script><p>the dimension of A is $\mathbb{R}^{n\cdot n}$ and $V$ is a $\mathbb{R}^{n\cdot 1}$ vector.</p>
<h4 id="Diagonalizable"><a href="#Diagonalizable" class="headerlink" title="Diagonalizable"></a>Diagonalizable</h4><p>Let’s assume a matrix A has two eigenvalues and eigenvectors.</p>
<script type="math/tex; mode=display">Av_1 = \lambda_1 v_1</script><script type="math/tex; mode=display">Av_2 = \lambda_2 v_2</script><p>We can concatenate them together and rewrite the equations in the matrix form.</p>
<script type="math/tex; mode=display">
A \begin{bmatrix} v1 & v2 \end{bmatrix} = \begin{bmatrix} \lambda_1 v_1 & \lambda_2 v_2 \end{bmatrix} = \begin{bmatrix} v1 & v2 \end{bmatrix} \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}</script><p>We can generalize it into any number of eigenvectors as</p>
<script type="math/tex; mode=display">AV = V\land</script><p>A square matrix A is diagonalizable if we can convert it into a diagonal matrix, like</p>
<script type="math/tex; mode=display">V^{-1} A V = \land</script><p>An n × n square matrix is diagonalizable if it has n linearly independent eigenvectors. If a matrix is symmetric, it is diagonalizable. If a matrix does not have repeated eigenvalue, it always generates enough linearly independent eigenvectors to diagonalize a vector. If it has repeated eigenvalues, there is no guarantee we have enough eigenvectors. Some will not be diagonalizable.</p>
<p>If $A$ is a square matrix with $N$ linearly independent eigenvectors ($v_1$, $v_2$, $\cdots$, $v_n$) and corresponding eigenvalues ($\lambda_1$, $\lambda_2$, $\cdots$, $\lambda_n$), we can rearrange</p>
<script type="math/tex; mode=display">V^{-1} A V = \land</script><p>into </p>
<script type="math/tex; mode=display">A = V \land V^{-1}</script><p>For example,</p>
<p><img src="diagonalizable.png" alt></p>
<h4 id="Singular-vectors-amp-singular-values"><a href="#Singular-vectors-amp-singular-values" class="headerlink" title="Singular vectors &amp; singular values"></a>Singular vectors &amp; singular values</h4><p>However, the above method is possible only if $A$ is a square matrix and $A$ has n linearly independent eigenvectors. Now, it is time to develop a solution for all matrices using SVD.</p>
<p>The matrix $AA^{T}$ and $A^{T}A$ are very special in linear algebra. Consider any m × n matrix A, we can multiply it with $A^{T}$ to form $AA^{T}$ and $A^{T}A$ separately. These matrices are</p>
<ul>
<li>symmetrical,</li>
<li>square,</li>
<li>at least positive semidefinite (eigenvalues are zero or positive),</li>
<li>both matrices have the same positive eigenvalues, and</li>
<li>both have the same rank r as A.</li>
</ul>
<p>We name the eigenvectors for $AA^{T}$ as $u_i$ and $A^{T}A$ as $v_i$ here and call these sets of eigenvectors $u$ and $v$ the <strong>singular vectors</strong> of A. Both matrices have the same positive eigenvalues. The square roots of these eigenvalues are called <strong>singular values</strong>. We concatenate vectors $u_i$ into $U$ and $v_i$ into $V$ to form orthogonal matrices.</p>
<p><strong>SVD states that any matrix A can be factorized as</strong>:</p>
<script type="math/tex; mode=display">A_{m\cdot n} = U_{m\cdot m} S_{m\cdot n} V_{n\cdot n}^{T}</script><p>S is a diagonal matrix with r elements equal to the root of the positive eigenvalues of $AA^{T}$ or $A^{T}A$ (both matrics have the same positive eigenvalues anyway).</p>
<p><img src="usv.png" alt></p>
<h4 id="Applying-SVD-to-the-cooccurrence-matrix"><a href="#Applying-SVD-to-the-cooccurrence-matrix" class="headerlink" title="Applying SVD to the cooccurrence matrix"></a>Applying SVD to the cooccurrence matrix</h4><p><img src="svd.png" alt="Picture of an SVD"></p>
<p>This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. <em>doctor</em> and <em>hospital</em> will be closer than <em>doctor</em> and <em>dog</em>. </p>
<p>Although these methods give us word vectors that are more than sufficient to encode semantic and syntactic (part of speech) information but are associated with many other problems:</p>
<ul>
<li>The dimensions of the matrix change very often (new words are added very frequently and corpus changes in size).</li>
<li>The matrix is extremely sparse since most words do not co-occur.</li>
<li>The matrix is very high dimensional in general (≈ 10e6 × 10e6)</li>
<li>Quadratic cost to train (i.e. to perform SVD)</li>
<li>Requires the incorporation of some hacks on X to account for the drastic imbalance in word frequency</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_co_occurrence_matrix</span><span class="params">(corpus, window_size=<span class="number">4</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Compute co-occurrence matrix for the given corpus and window_size (default of 4).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller</span></span><br><span class="line"><span class="string">              number of co-occurring words.</span></span><br><span class="line"><span class="string">              </span></span><br><span class="line"><span class="string">              For example, if we take the document "START All that glitters is not gold END" with window size of 4,</span></span><br><span class="line"><span class="string">              "All" will co-occur with "START", "that", "glitters", "is", and "not".</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            corpus (list of list of strings): corpus of documents</span></span><br><span class="line"><span class="string">            window_size (int): size of context window</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): </span></span><br><span class="line"><span class="string">                Co-occurence matrix of word counts. </span></span><br><span class="line"><span class="string">                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.</span></span><br><span class="line"><span class="string">            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words, num_words = distinct_words(corpus)</span><br><span class="line">    M = <span class="keyword">None</span></span><br><span class="line">    word2Ind = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    word2Ind = &#123;word:idx <span class="keyword">for</span> word,idx <span class="keyword">in</span> zip(words, range(num_words))&#125;</span><br><span class="line">    M = np.zeros((num_words,num_words))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> corpus:</span><br><span class="line">        <span class="keyword">for</span> index, central_word <span class="keyword">in</span> enumerate(doc):</span><br><span class="line">            left = max(<span class="number">0</span>,index - window_size)</span><br><span class="line">            right = min(num_words, index + window_size+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> context_word <span class="keyword">in</span> doc[left:right]:</span><br><span class="line">                <span class="keyword">if</span> context_word == central_word:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                M[word2Ind[central_word]][word2Ind[context_word]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M, word2Ind</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_to_k_dim</span><span class="params">(M, k=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)</span></span><br><span class="line"><span class="string">        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:</span></span><br><span class="line"><span class="string">            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts</span></span><br><span class="line"><span class="string">            k (int): embedding size of each word after dimension reduction</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.</span></span><br><span class="line"><span class="string">                    In terms of the SVD from math class, this actually returns U * S</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    n_iters = <span class="number">10</span>     <span class="comment"># Use this parameter in your call to `TruncatedSVD`</span></span><br><span class="line">    M_reduced = <span class="keyword">None</span></span><br><span class="line">    print(<span class="string">"Running Truncated SVD over %i words..."</span> % (M.shape[<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line">    t_svd = TruncatedSVD(n_components=k, n_iter = n_iters)</span><br><span class="line">    M_reduced = t_svd.fit_transform(M)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ------------------</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Done."</span>)</span><br><span class="line">    <span class="keyword">return</span> M_reduced</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_embeddings</span><span class="params">(M_reduced, word2Ind, words)</span>:</span></span><br><span class="line">    <span class="string">""" Plot in a scatterplot the embeddings of the words specified in the list "words".</span></span><br><span class="line"><span class="string">        NOTE: do not plot all the words listed in M_reduced / word2Ind.</span></span><br><span class="line"><span class="string">        Include a label next to each point.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            M_reduced (numpy matrix of shape (number of unique words in the corpus , k)): matrix of k-dimensioal word embeddings</span></span><br><span class="line"><span class="string">            word2Ind (dict): dictionary that maps word to indices for matrix M</span></span><br><span class="line"><span class="string">            words (list of strings): words whose embeddings we want to visualize</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    print(M_reduced.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,word <span class="keyword">in</span> enumerate(words):</span><br><span class="line">        x = M_reduced[i][<span class="number">0</span>]</span><br><span class="line">        y = M_reduced[i][<span class="number">1</span>]</span><br><span class="line">        plt.scatter(x, y, marker=<span class="string">'x'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">        plt.text(x, y, word, fontsize=<span class="number">9</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h2><p>Instead of computing and storing global information about some huge dataset (which might be billions of sentences), we can try to create a model that will be able to learn one iteration at a time and eventually be able to encode the probability of a word given its context. The idea is to design a model whose parameters are the word vec- tors. Then, train the model on a certain objective. At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors.</p>
<p>Word2vec is a software package that actually includes :</p>
<ul>
<li><strong>2 algorithms</strong>: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.</li>
<li><strong>2 training methods</strong>: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative exam- ples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary.</li>
</ul>
<h3 id="Language-Models"><a href="#Language-Models" class="headerlink" title="Language Models"></a>Language Models</h3><p>First, we need to create such a model that will assign a probability to a sequence of tokens. Let us start with an example:</p>
<blockquote>
<p>“The cat jumped over the puddle.”</p>
</blockquote>
<p>A good language model will give this sentence a high probability because this is a completely valid sentence, syntactically and semantically. Mathematically, we can call this probability on any given sequence of n words:</p>
<script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n)</script><p>We can take the unary language model approach and break apart this probability by assuming the word occurrences are completely independent:</p>
<script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n) = \prod_{i=1}^{n} P(w_i)</script><p>However, we know this is a bit ludicrous because we know the next word is highly contingent upon the previous sequence of words. And the silly sentence example might actually score highly. So perhaps we let the probability of the sequence depend on the pairwise probability of a word in the sequence and the word next to it. We call this the bigram model and represent it as: </p>
<script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n) = \prod_{i=1}^{n} P(w_i|w_{i-1})</script><p>Again this is certainly a bit naive since we are only concerning ourselves with pairs of neighboring words rather than evaluating a whole sentence, but as we will see, this representation gets us pretty far along.</p>
<h3 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><p>One approach is to create a model such that given the center word “jumped”, the model will be able to predict or generate the surrounding words “The”, “cat”, “over”, “the”, “puddle”. Here we call the word “jumped” the context. We call this type of model a Skip-Gram model.<br><img src="Skip.png" alt><br><img src="Skip_2.png" alt></p>
<p>We breakdown the way this model works in these 6 steps:</p>
<ol>
<li>We generate our one hot input vector $x \in \mathbb{R}^{|v|}$ of the center word.</li>
<li>We get our embedded word vector for the center word<script type="math/tex; mode=display">v_c = Vx  \qquad \in \mathbb{R}^{|v|}</script></li>
<li>Generate a score vector<script type="math/tex; mode=display">z = Uv_c  \qquad \in \mathbb{R}^{|v|}</script></li>
<li>Turn the score vector into probabilities,$\hat{y} = softmax(z)$<script type="math/tex; mode=display">\hat y_{c-m}, \cdots, \hat y_{c-1}, \cdots, \hat y_{c+m}</script></li>
<li>We desire our probability vector generated to match the true prob- abilities which is the one hot vectors of the actual output.<script type="math/tex; mode=display">y_{c-m}, \cdots, y_{c-1}, \cdots, y_{c+m}</script></li>
</ol>
<h4 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function"></a>Objective function</h4><p><img src="objective.png" alt></p>
<p><strong> How to calculate $P(o|c)$? We will use two vectors per word w</strong>:</p>
<ul>
<li>$V_w$ when w is a center word</li>
<li>$U_w$ when w is a context word</li>
</ul>
<p>Then for a center word c and a context word o:</p>
<script type="math/tex; mode=display">P(o|c) = \frac{exp^{u_o^{T}v_c}}{\sum_{w\in v}exp^{u_w^{T}v_c}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipGram</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""x --&gt; batch_size x word_index</span></span><br><span class="line"><span class="string">       output --&gt; batch_size x context_predicted x vocabulary"""</span>    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size, embedding_features, context_len, padding_idx=<span class="number">0</span> )</span>:</span></span><br><span class="line">        super(SkipGram, self).__init__()</span><br><span class="line">        self.context_len = context_len</span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim=embedding_features, padding_idx=padding_idx)</span><br><span class="line">        self.fc = nn.Linear(in_features = embedding_features, out_features = vocabulary_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        context_out = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.context_len):</span><br><span class="line">            wordvec_x = self.embedding(x)</span><br><span class="line">            context_word_i = self.fc(wordvec_x)</span><br><span class="line">            context_out.append(context_word_i)</span><br><span class="line">        log_prob = F.log_softmax(torch.stack(context_out, dim=<span class="number">1</span>).squeeze(), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_prob</span><br><span class="line"></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = SkipGram()</span><br><span class="line">log_prob = model(centre_word)</span><br><span class="line">loss=<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(log_prob.shape[<span class="number">1</span>]):</span><br><span class="line">    loss_i = loss_function(log_prob[:,i,], context_word_i[:,i])</span><br><span class="line">    loss *= loss_i</span><br><span class="line">loss = loss/(i+<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag of Words Model (CBOW)"></a>Continuous Bag of Words Model (CBOW)</h3><p>Another approach is to treat {“The”, “cat”, ’over”, “the’, “puddle”} as a <strong>context</strong> and from these words, be able to predict or generate the <strong>center word</strong> “jumped”. This type of model we call a Continuous Bag of Words (CBOW) Model.</p>
<p><img src="CBOW.png" alt></p>
<p>We breakdown the way this model works in these steps:</p>
<ol>
<li>We generate our one hot word vectors for the input context of size m:<script type="math/tex; mode=display">x^{(c−m)},\cdots,x^{(c−1)},x^{(c+1)},\cdots,x^{(c+m)}\in\mathbb{R}^{|v|}</script></li>
<li>We get our embedded word vectors for the context:<script type="math/tex; mode=display">V_{c-m} = Vx^{(c−m)},V_{c-m+1} = Vx^{(c−m+1)},\cdots,V_{c+m} = Vx^{(c+m)}</script></li>
<li>Average these vectors to get <script type="math/tex; mode=display">\hat{v} = \frac{v_{c-m} + v_{c-m+1} + \cdots + v_{c+m}}{2m}</script></li>
<li>Generate a score vector <script type="math/tex; mode=display">z = U\hat{v}  \qquad \in \mathbb{R}^{|v|}</script>As dot product of similar vectors is higher, it will push similar words close to each other in order to achieve a high score.</li>
<li>Turnthescoresintoprobabilities <script type="math/tex; mode=display">\hat{y} = softmax(z)  \qquad \in \mathbb{R}^{|v|}</script></li>
<li>We desire our probabilities generated, $\hat{y} \in \mathbb{R}^{|v|}$, to match the true probabilities, $y \in \mathbb{R}^{|v|}$ which also happens to be the one hot vector of the actual word.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""input  -- &gt; batch_size x context_size</span></span><br><span class="line"><span class="string">       output --&gt; batch_size x vocabulary"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size, embedding_features, padding_idx=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""nn.Embedding holds a tensor of dimmension (vocabulary_size, feature_size)--&gt;N(0,1)"""</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim = embedding_features, padding_idx=padding_idx)</span><br><span class="line">        self.fc = nn.Linear(in_features = embedding_features, out_features = vocabulary_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context_words)</span>:</span></span><br><span class="line">        wordvecs = self.embedding(context_words)</span><br><span class="line">        mean_wordvecs = wordvecs.sum(dim=<span class="number">1</span>)/x.shape[<span class="number">1</span>] </span><br><span class="line">        log_prob = F.log_softmax(self.fc(mean_wordvecs), dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_prob</span><br><span class="line"></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model  = CBOW()</span><br><span class="line">log_prob = model(context_words)</span><br><span class="line">loss = loss_function(log_prob.squeeze(), centre_word.squeeze())</span><br></pre></td></tr></table></figure>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>Lets take a second to look at the objective function. Note that the summation over |V| is computationally huge! Any update we do or evaluation of the objective function would take O(|V|) time which if we recall is in the millions. A simple idea is we could instead just approximate it.</p>
<p><strong>For every training step, instead of looping over the entire vocabulary, we can just sample several negative examples!</strong> We “sample” from a noise distribution $P_n(w)$ whose probabilities match the ordering of the frequency of the vocabulary. Unlike the probabilistic model of Word2Vec where for each input word probability is computed from all the target words in the vocabulary, here for each input word has only few target words (few true and rest randomly selected false targets). <strong>The key difference compared to the probabilistic model is the use of sigmoid activation as final discriminator replacing softmax function in the probabilistic model.</strong></p>
<p>Given this example(We get positive example by using the same skip-grams technique, a fixed window that goes around):</p>
<blockquote>
<p>“I want a glass of orange juice to go along with my cereal”</p>
</blockquote>
<p>The sampling will look like this:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Context</th>
<th style="text-align:left">Word</th>
<th style="text-align:left">target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">orange</td>
<td style="text-align:left">juice</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">orange</td>
<td style="text-align:left">king</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">orange</td>
<td style="text-align:left">book</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">orange</td>
<td style="text-align:left">the</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">orange</td>
<td style="text-align:left">of</td>
<td style="text-align:left">0</td>
</tr>
</tbody>
</table>
</div>
<p>So the steps to generate the samples are:</p>
<ol>
<li>Pick a positive context</li>
<li>Pick a k negative contexts from the dictionary.<br>We will have a k negative examples to 1 positive ones in the data we are collecting.</li>
</ol>
<p><img src="negative_sampling.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Merge</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Reshape</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># build skip-gram architecture</span></span><br><span class="line">word_model = Sequential()</span><br><span class="line">word_model.add(Embedding(vocab_size, embed_size,</span><br><span class="line">                         embeddings_initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">                         input_length=<span class="number">1</span>))</span><br><span class="line">word_model.add(Reshape((embed_size, )))</span><br><span class="line"></span><br><span class="line">context_model = Sequential()</span><br><span class="line">context_model.add(Embedding(vocab_size, embed_size,</span><br><span class="line">                  embeddings_initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">                  input_length=<span class="number">1</span>))</span><br><span class="line">context_model.add(Reshape((embed_size,)))</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Merge([word_model, context_model], mode=<span class="string">"dot"</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">"glorot_uniform"</span>, activation=<span class="string">"sigmoid"</span>))</span><br><span class="line">model.compile(loss=<span class="string">"mean_squared_error"</span>, optimizer=<span class="string">"rmsprop"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, elem <span class="keyword">in</span> enumerate(skip_grams):</span><br><span class="line">        pair_first_elem = np.array(list(zip(*elem[<span class="number">0</span>]))[<span class="number">0</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        pair_second_elem = np.array(list(zip(*elem[<span class="number">0</span>]))[<span class="number">1</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        labels = np.array(elem[<span class="number">1</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        X = [pair_first_elem, pair_second_elem]</span><br><span class="line">        Y = labels</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Processed &#123;&#125; (skip_first, skip_second, relevance) pairs'</span>.format(i))</span><br><span class="line">        loss += model.train_on_batch(X,Y)  </span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch:'</span>, epoch, <span class="string">'Loss:'</span>, loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## get word embedding</span></span><br><span class="line">merge_layer = model.layers[<span class="number">0</span>]</span><br><span class="line">word_model = merge_layer.layers[<span class="number">0</span>]</span><br><span class="line">word_embed_layer = word_model.layers[<span class="number">0</span>]</span><br><span class="line">weights = word_embed_layer.get_weights()[<span class="number">0</span>][<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">print(weights.shape)</span><br><span class="line">pd.DataFrame(weights, index=id2word.values()).head()</span><br></pre></td></tr></table></figure>
<h3 id="hierarchical-softmax"><a href="#hierarchical-softmax" class="headerlink" title="hierarchical softmax"></a>hierarchical softmax</h3><p>hierarchical softmax is a much more efficient alternative to the normal softmax. In practice, hierarchical softmax tends to be better for infrequent words, while negative sampling works better for frequent words and lower dimensional vectors.</p>
<p>Hierarchical softmax uses a binary tree to represent all words in the vocabulary. Each leaf of the tree is a word, and there is a unique path from root to leaf. In this model, there is no output representation for words. Instead, each node of the graph (except the root and the leaves) is associated to a vector that the model is going to learn.</p>
<p>In this model, the probability of a word w given a vector $w_i$, p(w|w_i),is equal to the probability of a random walk starting in the root and ending in the leaf node corresponding to w. The main advantage in computing the probability this way is that the cost is only O(log(|V|)), corresponding to the length of the path.<br><img src="hafftree.png" alt></p>
<p>Taking $w_2$ in above figure, we must take two left edges and<br>then a right edge to reach w2 from the root, so</p>
<script type="math/tex; mode=display">p(w_2) = p(n(w_2,1),left) \cdot p(n(w_2,2),left) \cdot p(n(w_2,3),right) \\ 
= \sigma({\theta_{n(w_2,1)}}^T \cdot h) \cdot \sigma({\theta_{n(w_2,2)}}^T \cdot h) \cdot 
\sigma({-\theta_{n(w_2,3)}}^T \cdot h)</script><p>Therefore,</p>
<script type="math/tex; mode=display">p(w)=\prod_{j=1}^{L(w)-1}\sigma( sign(w,j)\cdot {\theta_{n(w,j)}}^Th )</script><script type="math/tex; mode=display">sign(w,j)= 
\begin{cases} 
1, & \text{if n(w,j+1) is the left child of n(w,j)} \\ 
-1,& \text{if n(w,j+1) is the right child of n(w,j)}
\end{cases}</script><ul>
<li>$\theta_{n(w,j)}$ is the vector representation of $n(w,j)$</li>
<li>$h$ is the output of hidden layer</li>
</ul>
<h3 id="Global-Vectors-for-Word-Representation-GloVe"><a href="#Global-Vectors-for-Word-Representation-GloVe" class="headerlink" title="Global Vectors for Word Representation (GloVe)"></a>Global Vectors for Word Representation (GloVe)</h3><p>So far, we have looked at two main classes of methods to find word embeddings. </p>
<ul>
<li>The first set are count-based and rely on matrix factor- ization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indi- cating a sub-optimal vector space structure. </li>
<li>The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW mod- els), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.</li>
</ul>
<p>In comparison, GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. It shows state-of-the-art per- formance on the word analogy task, and outperforms other current methods on several word similarity tasks.</p>
<ol>
<li>Construct co-occurrence Matrix</li>
<li>Construct relationships between word vectors and co-occurrence Matrix<ul>
<li>Let X denote the word-word co-occurrence matrix, where $X_{ij}$ indicates the number of times word j occur in the context of word i</li>
<li>$w_{i}$,$\tilde{w_{j}}$ is the word vector</li>
<li>$b_i,b_j$ is the bias term<script type="math/tex; mode=display">w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} = \log(X_{ij}) \tag{1}</script></li>
</ul>
</li>
<li>Construct loss function: Mean Square Loss<script type="math/tex; mode=display">J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2</script><script type="math/tex; mode=display">f(x)=\begin{equation} 
\begin{cases} 
(x/x_{max})^{\alpha}  & \text{if} \ x < x_{max} \\ 
1 & \text{otherwise} 
\end{cases} 
\end{equation}</script></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_func</span><span class="params">(x, x_max, alpha)</span>:</span></span><br><span class="line">    wx = (x/x_max)**alpha</span><br><span class="line">    wx = torch.min(wx, torch.ones_like(wx))</span><br><span class="line">    <span class="keyword">return</span> wx</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wmse_loss</span><span class="params">(weights, inputs, targets)</span>:</span></span><br><span class="line">    loss = weights * F.mse_loss(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.mean(loss)</span><br><span class="line"></span><br><span class="line">EMBED_DIM = <span class="number">300</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GloveModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_embeddings, embedding_dim)</span>:</span></span><br><span class="line">        super(GloveModel, self).__init__()</span><br><span class="line">        self.wi = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        self.wj = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        self.bi = nn.Embedding(num_embeddings, <span class="number">1</span>)</span><br><span class="line">        self.bj = nn.Embedding(num_embeddings, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.wi.weight.data.uniform_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        self.wj.weight.data.uniform_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        self.bi.weight.data.zero_()</span><br><span class="line">        self.bj.weight.data.zero_()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, i_indices, j_indices)</span>:</span></span><br><span class="line">        w_i = self.wi(i_indices)</span><br><span class="line">        w_j = self.wj(j_indices)</span><br><span class="line">        b_i = self.bi(i_indices).squeeze()</span><br><span class="line">        b_j = self.bj(j_indices).squeeze()</span><br><span class="line">        </span><br><span class="line">        x = torch.sum(w_i * w_j, dim=<span class="number">1</span>) + b_i + b_j</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">glove = GloveModel(dataset._vocab_len, EMBED_DIM)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">outputs = glove(i_idx, j_idx)</span><br><span class="line">weights_x = weight_func(x_ij, X_MAX, ALPHA)</span><br><span class="line">loss = wmse_loss(weights_x, outputs, torch.log(x_ij))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p>In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperforms word2vec on the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>Course note and slides of <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">cs224n</a></li>
<li><a href="https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491" target="_blank" rel="noopener">Machine Learning — Singular Value Decomposition (SVD) &amp; Principal Component Analysis (PCA)</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/42651829" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/42651829</a></li>
<li><a href="https://nlpython.com/implementing-glove-model-with-pytorch/" target="_blank" rel="noopener">https://nlpython.com/implementing-glove-model-with-pytorch/</a></li>
</ul>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/cs224n/" rel="tag"># cs224n</a>
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Composition/2019/12/01/" rel="next" title="Composition">
                <i class="fa fa-chevron-left"></i> Composition
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Computational-Graph/2019/12/06/" rel="prev" title="Computational Graph">
                Computational Graph <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate article here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Ruochi Zhang WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    

    
      <div id="bitcoin" style="display: inline-block">
        <img id="vemo_qr" src="/images/venmo.png" alt="Ruochi Zhang Bitcoin">
        <p>Venmo(last 4 digits 1570)</p>
      </div>
    

  </div>
</div>

      </div>
    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">237</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-we-need-Word-Vectors"><span class="nav-text">Why we need Word Vectors ?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#One-hot-vector"><span class="nav-text">One-hot vector</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVD-Based-Methods"><span class="nav-text">SVD Based Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Document-Matrix"><span class="nav-text">Word-Document Matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Window-based-Co-occurrence-Matrix"><span class="nav-text">Window based Co-occurrence Matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVD"><span class="nav-text">SVD</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Diagonalizable"><span class="nav-text">Diagonalizable</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Singular-vectors-amp-singular-values"><span class="nav-text">Singular vectors &amp; singular values</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Applying-SVD-to-the-cooccurrence-matrix"><span class="nav-text">Applying SVD to the cooccurrence matrix</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Iteration-Based-Methods-Word2vec"><span class="nav-text">Iteration Based Methods - Word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Language-Models"><span class="nav-text">Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Skip-gram"><span class="nav-text">Skip-gram</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Objective-function"><span class="nav-text">Objective function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Continuous-Bag-of-Words-Model-CBOW"><span class="nav-text">Continuous Bag of Words Model (CBOW)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Negative-Sampling"><span class="nav-text">Negative Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hierarchical-softmax"><span class="nav-text">hierarchical softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Global-Vectors-for-Word-Representation-GloVe"><span class="nav-text">Global Vectors for Word Representation (GloVe)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
