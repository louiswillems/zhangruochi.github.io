<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-11-09T10:10:56.251Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CycleGAN</title>
    <link href="https://zhangruochi.com/CycleGAN/2020/11/09/"/>
    <id>https://zhangruochi.com/CycleGAN/2020/11/09/</id>
    <published>2020-11-09T10:10:31.000Z</published>
    <updated>2020-11-09T10:10:56.251Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you will write a generative model based on the paper <a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener"><em>Unpaired Image-to-Image Translation<br>using Cycle-Consistent Adversarial Networks</em></a> by Zhu et al. 2017, commonly referred to as CycleGAN.</p><p>You will be training a model that can convert horses into zebras, and vice versa. Once again, the emphasis of the assignment will be on the loss functions. In order for you to see good outputs more quickly, you’ll be training your model starting from a pre-trained checkpoint. You are also welcome to train it from scratch on your own, if you so choose.</p><!-- You will take the segmentations that you generated in the previous assignment and produce photorealistic images. --><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Implement the loss functions of a CycleGAN model.</li><li>Observe the two GANs used in CycleGAN.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will start by importing libraries, defining a visualization function, and getting the pre-trained CycleGAN checkpoint.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_shifted = image_tensor</span><br><span class="line">    image_unflat = image_shifted.detach().cpu().view(<span class="number">-1</span>, *size)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inspired by https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/datasets.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transform=None, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.files_A = sorted(glob.glob(os.path.join(root, <span class="string">'%sA'</span> % mode) + <span class="string">'/*.*'</span>))</span><br><span class="line">        self.files_B = sorted(glob.glob(os.path.join(root, <span class="string">'%sB'</span> % mode) + <span class="string">'/*.*'</span>))</span><br><span class="line">        <span class="keyword">if</span> len(self.files_A) &gt; len(self.files_B):</span><br><span class="line">            self.files_A, self.files_B = self.files_B, self.files_A</span><br><span class="line">        self.new_perm()</span><br><span class="line">        <span class="keyword">assert</span> len(self.files_A) &gt; <span class="number">0</span>, <span class="string">"Make sure you downloaded the horse2zebra images!"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">new_perm</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))</span><br><span class="line">        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))</span><br><span class="line">        <span class="keyword">if</span> item_A.shape[<span class="number">0</span>] != <span class="number">3</span>: </span><br><span class="line">            item_A = item_A.repeat(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> item_B.shape[<span class="number">0</span>] != <span class="number">3</span>: </span><br><span class="line">            item_B = item_B.repeat(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> index == len(self) - <span class="number">1</span>:</span><br><span class="line">            self.new_perm()</span><br><span class="line">        <span class="comment"># Old versions of PyTorch didn't support normalization for different-channeled images</span></span><br><span class="line">        <span class="keyword">return</span> (item_A - <span class="number">0.5</span>) * <span class="number">2</span>, (item_B - <span class="number">0.5</span>) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> min(len(self.files_A), len(self.files_B))</span><br></pre></td></tr></table></figure><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>The code for a CycleGAN generator is much like Pix2Pix’s U-Net with the addition of the residual block between the encoding (contracting) and decoding (expanding) blocks.</p><p><img src="https://drive.google.com/uc?id=1K7fCr6lXzxn9LCUvKl0ozXthz2I9FZSv" alt="Diagram of a CycleGAN generator: composed of encoding blocks, residual blocks, then decoding blocks"><br><em>Diagram of a CycleGAN generator: composed of encoding blocks, residual blocks, and then decoding blocks.</em></p><h4 id="Residual-Block"><a href="#Residual-Block" class="headerlink" title="Residual Block"></a>Residual Block</h4><p>Perhaps the most notable architectural difference between the U-Net you used for Pix2Pix and the architecture you’re using for CycleGAN are the residual blocks. In CycleGAN, after the expanding blocks, there are convolutional layers where the output is ultimately added to the original input so that the network can change as little as possible on the image. You can think of this transformation as a kind of skip connection, where instead of being concatenated as new channels before the convolution which combines them, it’s added directly to the output of the convolution. In the visualization below, you can imagine the stripes being generated by the convolutions and then added to the original image of the horse to transform it into a zebra. These skip connections also allow the network to be deeper, because they help with vanishing gradients issues that come when a neural network gets too deep and the gradients multiply in backpropagation to become very small; instead, these skip connections enable more gradient flow. A deeper network is often able to learn more complex features.</p><p><img src="https://drive.google.com/uc?id=1T-oic1q-wi-Hj4yeWb4r8sl_9laq58wm&amp;" alt="Residual block explanation: shows horse going through convolutions leading to stripes, added to the original horse image to get a zebra"></p><p><em>Example of a residual block.</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ResidualBlock Class:</span></span><br><span class="line"><span class="string">    Performs two convolutions and an instance normalization, the input is added</span></span><br><span class="line"><span class="string">    to this output to form the residual block output.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, padding_mode=<span class="string">'reflect'</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(input_channels, input_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, padding_mode=<span class="string">'reflect'</span>)</span><br><span class="line">        self.instancenorm = nn.InstanceNorm2d(input_channels)</span><br><span class="line">        self.activation = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ResidualBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes a residual block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        original_x = x.clone()</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.instancenorm(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.instancenorm(x)</span><br><span class="line">        <span class="keyword">return</span> original_x + x</span><br></pre></td></tr></table></figure><h4 id="Contracting-and-Expanding-Blocks"><a href="#Contracting-and-Expanding-Blocks" class="headerlink" title="Contracting and Expanding Blocks"></a>Contracting and Expanding Blocks</h4><p>The rest of the generator code will otherwise be much like the code you wrote for the last assignment: Pix2Pix’s U-Net. The primary changes are the use of instance norm instead of batch norm (which you may recall from StyleGAN), no dropout, and a stride-2 convolution instead of max pooling. Feel free to investigate the code if you’re interested!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContractingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ContractingBlock Class</span></span><br><span class="line"><span class="string">    Performs a convolution followed by a max pool operation and an optional instance norm.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, use_bn=True, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>)</span>:</span></span><br><span class="line">        super(ContractingBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels * <span class="number">2</span>, kernel_size=kernel_size, padding=<span class="number">1</span>, stride=<span class="number">2</span>, padding_mode=<span class="string">'reflect'</span>)</span><br><span class="line">        self.activation = nn.ReLU() <span class="keyword">if</span> activation == <span class="string">'relu'</span> <span class="keyword">else</span> nn.LeakyReLU(<span class="number">0.2</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            self.instancenorm = nn.InstanceNorm2d(input_channels * <span class="number">2</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ContractingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes a contracting block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.instancenorm(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpandingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ExpandingBlock Class:</span></span><br><span class="line"><span class="string">    Performs a convolutional transpose operation in order to upsample, </span></span><br><span class="line"><span class="string">        with an optional instance norm</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, use_bn=True)</span>:</span></span><br><span class="line">        super(ExpandingBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.ConvTranspose2d(input_channels, input_channels // <span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            self.instancenorm = nn.InstanceNorm2d(input_channels // <span class="number">2</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        self.activation = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ExpandingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes an expanding block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">            skip_con_x: the image tensor from the contracting path (from the opposing block of x)</span></span><br><span class="line"><span class="string">                    for the skip connection</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.instancenorm(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureMapBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    FeatureMapBlock Class</span></span><br><span class="line"><span class="string">    The final layer of a Generator - </span></span><br><span class="line"><span class="string">    maps each the output to the desired number of output channels</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels)</span>:</span></span><br><span class="line">        super(FeatureMapBlock, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=<span class="number">7</span>, padding=<span class="number">3</span>, padding_mode=<span class="string">'reflect'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of FeatureMapBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, returns it mapped to the desired number of channels.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="CycleGAN-Generator"><a href="#CycleGAN-Generator" class="headerlink" title="CycleGAN Generator"></a>CycleGAN Generator</h4><p>Finally, you can put all the blocks together to create your CycleGAN generator.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    A series of 2 contracting blocks, 9 residual blocks, and 2 expanding blocks to </span></span><br><span class="line"><span class="string">    transform an input image into an image from the other class, with an upfeature</span></span><br><span class="line"><span class="string">    layer at the start and a downfeature layer at the end.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels, hidden_channels=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        res_mult = <span class="number">4</span></span><br><span class="line">        self.res0 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res1 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res2 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res3 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res4 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res5 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res6 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res7 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res8 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.expand2 = ExpandingBlock(hidden_channels * <span class="number">4</span>)</span><br><span class="line">        self.expand3 = ExpandingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)</span><br><span class="line">        self.tanh = torch.nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of Generator: </span></span><br><span class="line"><span class="string">        Given an image tensor, passes it through the U-Net with residual blocks</span></span><br><span class="line"><span class="string">        and returns the output.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.res0(x2)</span><br><span class="line">        x4 = self.res1(x3)</span><br><span class="line">        x5 = self.res2(x4)</span><br><span class="line">        x6 = self.res3(x5)</span><br><span class="line">        x7 = self.res4(x6)</span><br><span class="line">        x8 = self.res5(x7)</span><br><span class="line">        x9 = self.res6(x8)</span><br><span class="line">        x10 = self.res7(x9)</span><br><span class="line">        x11 = self.res8(x10)</span><br><span class="line">        x12 = self.expand2(x11)</span><br><span class="line">        x13 = self.expand3(x12)</span><br><span class="line">        xn = self.downfeature(x13)</span><br><span class="line">        <span class="keyword">return</span> self.tanh(xn)</span><br></pre></td></tr></table></figure><h2 id="PatchGAN-Discriminator"><a href="#PatchGAN-Discriminator" class="headerlink" title="PatchGAN Discriminator"></a>PatchGAN Discriminator</h2><p>Next, you will define the discriminator—a PatchGAN. It will be very similar to what you saw in Pix2Pix.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Structured like the contracting path of the U-Net, the discriminator will</span></span><br><span class="line"><span class="string">    output a matrix of values classifying corresponding portions of the image as real or fake. </span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_channels: the number of image input channels</span></span><br><span class="line"><span class="string">        hidden_channels: the initial number of discriminator convolutional filters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, hidden_channels=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels, use_bn=<span class="keyword">False</span>, kernel_size=<span class="number">4</span>, activation=<span class="string">'lrelu'</span>)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>, kernel_size=<span class="number">4</span>, activation=<span class="string">'lrelu'</span>)</span><br><span class="line">        self.contract3 = ContractingBlock(hidden_channels * <span class="number">4</span>, kernel_size=<span class="number">4</span>, activation=<span class="string">'lrelu'</span>)</span><br><span class="line">        self.final = nn.Conv2d(hidden_channels * <span class="number">8</span>, <span class="number">1</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.contract3(x2)</span><br><span class="line">        xn = self.final(x3)</span><br><span class="line">        <span class="keyword">return</span> xn</span><br></pre></td></tr></table></figure><h2 id="Training-Preparation"><a href="#Training-Preparation" class="headerlink" title="Training Preparation"></a>Training Preparation</h2><!-- You'll be using the same U-Net as in the previous assignment, but you'll write another discriminator and change the loss to make it a GAN. --><p>Now you can put everything together for training! You will start by defining your parameters:</p><ul><li>adv_criterion: an adversarial loss function to keep track of how well the GAN is fooling the discriminator and how well the discriminator is catching the GAN</li><li>recon_criterion: a loss function that rewards similar images to the ground truth, which “reconstruct” the image</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>dim_A: the number of channels of the images in pile A</li><li>dim_B: the number of channels of the images in pile B (note that in the visualization this is currently treated as equivalent to dim_A)</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>target_shape: the size of the input and output images (in pixels)</li><li>load_shape: the size for the dataset to load the images at before randomly cropping them to target_shape as a simple data augmentation</li><li>device: the device type</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">adv_criterion = nn.MSELoss() </span><br><span class="line">recon_criterion = nn.L1Loss() </span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">20</span></span><br><span class="line">dim_A = <span class="number">3</span></span><br><span class="line">dim_B = <span class="number">3</span></span><br><span class="line">display_step = <span class="number">200</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">load_shape = <span class="number">286</span></span><br><span class="line">target_shape = <span class="number">256</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br></pre></td></tr></table></figure><p>You will then load the images of the dataset while introducing some data augmentation (e.g. crops and random horizontal flips). </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(load_shape),</span><br><span class="line">    transforms.RandomCrop(target_shape),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = ImageDataset(<span class="string">"horse2zebra"</span>, transform=transform)</span><br></pre></td></tr></table></figure><p>Next, you can initialize your generators and discriminators, as well as their optimizers. For CycleGAN, you will have two generators and two discriminators since there are two GANs:</p><ul><li>Generator for horse to zebra (<code>gen_AB</code>)</li><li>Generator for zebra to horse (<code>gen_BA</code>)</li><li>Discriminator for horse (<code>disc_A</code>)</li><li>Discriminator for zebra (<code>disc_B</code>)</li></ul><p>You will also load your pre-trained model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">gen_AB = Generator(dim_A, dim_B).to(device)</span><br><span class="line">gen_BA = Generator(dim_B, dim_A).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(list(gen_AB.parameters()) + list(gen_BA.parameters()), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">disc_A = Discriminator(dim_A).to(device)</span><br><span class="line">disc_A_opt = torch.optim.Adam(disc_A.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">disc_B = Discriminator(dim_B).to(device)</span><br><span class="line">disc_B_opt = torch.optim.Adam(disc_B.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feel free to change pretrained to False if you're training the model from scratch</span></span><br><span class="line">pretrained = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">if</span> pretrained:</span><br><span class="line">    pre_dict = torch.load(<span class="string">'cycleGAN_100000.pth'</span>)</span><br><span class="line">    gen_AB.load_state_dict(pre_dict[<span class="string">'gen_AB'</span>])</span><br><span class="line">    gen_BA.load_state_dict(pre_dict[<span class="string">'gen_BA'</span>])</span><br><span class="line">    gen_opt.load_state_dict(pre_dict[<span class="string">'gen_opt'</span>])</span><br><span class="line">    disc_A.load_state_dict(pre_dict[<span class="string">'disc_A'</span>])</span><br><span class="line">    disc_A_opt.load_state_dict(pre_dict[<span class="string">'disc_A_opt'</span>])</span><br><span class="line">    disc_B.load_state_dict(pre_dict[<span class="string">'disc_B'</span>])</span><br><span class="line">    disc_B_opt.load_state_dict(pre_dict[<span class="string">'disc_B_opt'</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    gen_AB = gen_AB.apply(weights_init)</span><br><span class="line">    gen_BA = gen_BA.apply(weights_init)</span><br><span class="line">    disc_A = disc_A.apply(weights_init)</span><br><span class="line">    disc_B = disc_B.apply(weights_init)</span><br></pre></td></tr></table></figure><h2 id="Discriminator-Loss"><a href="#Discriminator-Loss" class="headerlink" title="Discriminator Loss"></a>Discriminator Loss</h2><p>First, you’re going to be implementing the discriminator loss. This is the same as in previous assignments, so it should be a breeze :) Don’t forget to detach your generator!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_disc_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_disc_loss</span><span class="params">(real_X, fake_X, disc_X, adv_criterion)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the discriminator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_X: the real images from pile X</span></span><br><span class="line"><span class="string">        fake_X: the generated images of class X</span></span><br><span class="line"><span class="string">        disc_X: the discriminator for class X; takes images and returns real/fake class X</span></span><br><span class="line"><span class="string">            prediction matrices</span></span><br><span class="line"><span class="string">        adv_criterion: the adversarial loss function; takes the discriminator </span></span><br><span class="line"><span class="string">            predictions and the target labels and returns a adversarial </span></span><br><span class="line"><span class="string">            loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    fake_X.detach_()</span><br><span class="line">    fake_pred = disc_X(fake_X)</span><br><span class="line">    fake_loss = adv_criterion(fake_pred,torch.zeros_like(fake_pred))</span><br><span class="line">    real_pred = disc_X(real_X)</span><br><span class="line">    real_loss = adv_criterion(real_pred,torch.ones_like(real_pred))</span><br><span class="line">    disc_loss = (fake_loss + real_loss ) / <span class="number">2.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> disc_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_disc_X = <span class="keyword">lambda</span> x: x * <span class="number">97</span></span><br><span class="line">test_real_X = torch.tensor(<span class="number">83.</span>)</span><br><span class="line">test_fake_X = torch.tensor(<span class="number">89.</span>)</span><br><span class="line">test_adv_criterion = <span class="keyword">lambda</span> x, y: x * <span class="number">79</span> + y * <span class="number">73</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((get_disc_loss(test_real_X, test_fake_X, test_disc_X, test_adv_criterion)) - <span class="number">659054.5000</span>) &lt; <span class="number">1e-6</span></span><br><span class="line">test_disc_X = <span class="keyword">lambda</span> x: x.mean(<span class="number">0</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">test_adv_criterion = torch.nn.BCEWithLogitsLoss()</span><br><span class="line">test_input = torch.ones(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># If this runs, it's a pass - checks that the shapes are treated correctly</span></span><br><span class="line">get_disc_loss(test_input, test_input, test_disc_X, test_adv_criterion)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Generator-Loss"><a href="#Generator-Loss" class="headerlink" title="Generator Loss"></a>Generator Loss</h2><p>While there are some changes to the CycleGAN architecture from Pix2Pix, the most important distinguishing feature of CycleGAN is its generator loss. You will be implementing that here!</p><h4 id="Adversarial-Loss"><a href="#Adversarial-Loss" class="headerlink" title="Adversarial Loss"></a>Adversarial Loss</h4><p>The first component of the generator’s loss you’re going to implement is its adversarial loss—this once again is pretty similar to the GAN loss that you’ve implemented in the past. The important thing to note is that the criterion now is based on least squares loss, rather than binary cross entropy loss or W-loss.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_adversarial_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_adversarial_loss</span><span class="params">(real_X, disc_Y, gen_XY, adv_criterion)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the adversarial loss of the generator given inputs</span></span><br><span class="line"><span class="string">    (and the generated images for testing purposes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_X: the real images from pile X</span></span><br><span class="line"><span class="string">        disc_Y: the discriminator for class Y; takes images and returns real/fake class Y</span></span><br><span class="line"><span class="string">            prediction matrices</span></span><br><span class="line"><span class="string">        gen_XY: the generator for class X to Y; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class Y</span></span><br><span class="line"><span class="string">        adv_criterion: the adversarial loss function; takes the discriminator </span></span><br><span class="line"><span class="string">                  predictions and the target labels and returns a adversarial </span></span><br><span class="line"><span class="string">                  loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    fake_Y = gen_XY(real_X)</span><br><span class="line">    pred_Y = disc_Y(fake_Y)</span><br><span class="line">    adversarial_loss = adv_criterion(pred_Y,torch.ones_like(pred_Y))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> adversarial_loss, fake_Y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_disc_Y = <span class="keyword">lambda</span> x: x * <span class="number">97</span></span><br><span class="line">test_real_X = torch.tensor(<span class="number">83.</span>)</span><br><span class="line">test_gen_XY = <span class="keyword">lambda</span> x: x * <span class="number">89</span></span><br><span class="line">test_adv_criterion = <span class="keyword">lambda</span> x, y: x * <span class="number">79</span> + y * <span class="number">73</span></span><br><span class="line">test_res = get_gen_adversarial_loss(test_real_X, test_disc_Y, test_gen_XY, test_adv_criterion)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">0</span>] - <span class="number">56606652</span>) &lt; <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">1</span>] - <span class="number">7387</span>) &lt; <span class="number">1e-6</span></span><br><span class="line">test_disc_Y = <span class="keyword">lambda</span> x: x.mean(<span class="number">0</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">test_adv_criterion = torch.nn.BCEWithLogitsLoss()</span><br><span class="line">test_input = torch.ones(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># If this runs, it's a pass - checks that the shapes are treated correctly</span></span><br><span class="line">get_gen_adversarial_loss(test_input, test_disc_Y, test_gen_XY, test_adv_criterion)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h4 id="Identity-Loss"><a href="#Identity-Loss" class="headerlink" title="Identity Loss"></a>Identity Loss</h4><p>Here you get to see some of the superbly new material! You’ll want to measure the change in an image when you pass the generator an example from the target domain instead of the input domain it’s expecting. The output should be the same as the input since it is already of the target domain class. For example, if you put a horse through a zebra -&gt; horse generator, you’d expect the output to be the same horse because nothing needed to be transformed. It’s already a horse! You don’t want your generator to be transforming it into any other thing, so you want to encourage this behavior. In encouraging this identity mapping, the authors of CycleGAN found that for some tasks, this helped properly preserve the colors of an image, even when the expected input (here, a zebra) was put in. This was particularly useful for the photos &lt;-&gt; paintings mapping and, while an optional aesthetic component, you might find it useful for your applications down the line.</p><p><img src="https://drive.google.com/uc?id=19cO4sawwlNNbnyh-jZCTWFSRd6mfrary" alt="Diagram showing a real horse image going through a zebra -&gt; horse generator and the ideal output being the same input image"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_identity_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_identity_loss</span><span class="params">(real_X, gen_YX, identity_criterion)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the identity loss of the generator given inputs</span></span><br><span class="line"><span class="string">    (and the generated images for testing purposes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_X: the real images from pile X</span></span><br><span class="line"><span class="string">        gen_YX: the generator for class Y to X; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class X</span></span><br><span class="line"><span class="string">        identity_criterion: the identity loss function; takes the real images from X and</span></span><br><span class="line"><span class="string">                        those images put through a Y-&gt;X generator and returns the identity </span></span><br><span class="line"><span class="string">                        loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    identity_X = gen_YX(real_X)</span><br><span class="line">    identity_loss = identity_criterion(real_X,identity_X)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> identity_loss, identity_X</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_real_X = torch.tensor(<span class="number">83.</span>)</span><br><span class="line">test_gen_YX = <span class="keyword">lambda</span> x: x * <span class="number">89</span></span><br><span class="line">test_identity_criterion = <span class="keyword">lambda</span> x, y: (x + y) * <span class="number">73</span></span><br><span class="line">test_res = get_identity_loss(test_real_X, test_gen_YX, test_identity_criterion)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">0</span>] - <span class="number">545310</span>) &lt; <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">1</span>] - <span class="number">7387</span>) &lt; <span class="number">1e-6</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h4 id="Cycle-Consistency-Loss"><a href="#Cycle-Consistency-Loss" class="headerlink" title="Cycle Consistency Loss"></a>Cycle Consistency Loss</h4><p>Now, you can implement the final generator loss and the part that puts the “cycle” in CycleGAN: cycle consistency loss. This is used to ensure that when you put an image through one generator, that if it is then transformed back into the input class using the opposite generator, the image is the same as the original input image.</p><p><img src="https://drive.google.com/uc?id=1bg8w_LGMLcVJEMdA4kyUr5UqxxtySTDq" alt="Diagram showing a real zebra image being transformed into a horse and then back into a zebra. The output zebra should be the same as the input zebra."></p><p>Since you’ve already generated a fake image for the adversarial part, you can pass that fake image back to produce a full cycle—this loss will encourage the cycle to preserve as much information as possible.</p><p><em>Fun fact: Cycle consistency is a broader concept that’s used outside of CycleGAN a lot too! It’s helped with data augmentation and has been used on text translation too, e.g. French -&gt; English -&gt; French should get the same phrase back.</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_cycle_consistency_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cycle_consistency_loss</span><span class="params">(real_X, fake_Y, gen_YX, cycle_criterion)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the cycle consistency loss of the generator given inputs</span></span><br><span class="line"><span class="string">    (and the generated images for testing purposes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_X: the real images from pile X</span></span><br><span class="line"><span class="string">        fake_Y: the generated images of class Y</span></span><br><span class="line"><span class="string">        gen_YX: the generator for class Y to X; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class X</span></span><br><span class="line"><span class="string">        cycle_criterion: the cycle consistency loss function; takes the real images from X and</span></span><br><span class="line"><span class="string">                        those images put through a X-&gt;Y generator and then Y-&gt;X generator</span></span><br><span class="line"><span class="string">                        and returns the cycle consistency loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    cycle_X = gen_YX(fake_Y)</span><br><span class="line">    cycle_loss = cycle_criterion(real_X,cycle_X)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> cycle_loss, cycle_X</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_real_X = torch.tensor(<span class="number">83.</span>)</span><br><span class="line">test_fake_Y = torch.tensor(<span class="number">97.</span>)</span><br><span class="line">test_gen_YX = <span class="keyword">lambda</span> x: x * <span class="number">89</span></span><br><span class="line">test_cycle_criterion = <span class="keyword">lambda</span> x, y: (x + y) * <span class="number">73</span></span><br><span class="line">test_res = get_cycle_consistency_loss(test_real_X, test_fake_Y, test_gen_YX, test_cycle_criterion)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">1</span>] - <span class="number">8633</span>) &lt; <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">0</span>] - <span class="number">636268</span>) &lt; <span class="number">1e-6</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h4 id="Generator-Loss-Total"><a href="#Generator-Loss-Total" class="headerlink" title="Generator Loss (Total)"></a>Generator Loss (Total)</h4><p>Finally, you can put it all together! There are many components, so be careful as you go through this section.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span><span class="params">(real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, identity_criterion, cycle_criterion, lambda_identity=<span class="number">0.1</span>, lambda_cycle=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the generator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_A: the real images from pile A</span></span><br><span class="line"><span class="string">        real_B: the real images from pile B</span></span><br><span class="line"><span class="string">        gen_AB: the generator for class A to B; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class B</span></span><br><span class="line"><span class="string">        gen_BA: the generator for class B to A; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class A</span></span><br><span class="line"><span class="string">        disc_A: the discriminator for class A; takes images and returns real/fake class A</span></span><br><span class="line"><span class="string">            prediction matrices</span></span><br><span class="line"><span class="string">        disc_B: the discriminator for class B; takes images and returns real/fake class B</span></span><br><span class="line"><span class="string">            prediction matrices</span></span><br><span class="line"><span class="string">        adv_criterion: the adversarial loss function; takes the discriminator </span></span><br><span class="line"><span class="string">            predictions and the true labels and returns a adversarial </span></span><br><span class="line"><span class="string">            loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">        identity_criterion: the reconstruction loss function used for identity loss</span></span><br><span class="line"><span class="string">            and cycle consistency loss; takes two sets of images and returns</span></span><br><span class="line"><span class="string">            their pixel differences (which you aim to minimize)</span></span><br><span class="line"><span class="string">        cycle_criterion: the cycle consistency loss function; takes the real images from X and</span></span><br><span class="line"><span class="string">            those images put through a X-&gt;Y generator and then Y-&gt;X generator</span></span><br><span class="line"><span class="string">            and returns the cycle consistency loss (which you aim to minimize).</span></span><br><span class="line"><span class="string">            Note that in practice, cycle_criterion == identity_criterion == L1 loss</span></span><br><span class="line"><span class="string">        lambda_identity: the weight of the identity loss</span></span><br><span class="line"><span class="string">        lambda_cycle: the weight of the cycle-consistency loss</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Hint 1: Make sure you include both directions - you can think of the generators as collaborating</span></span><br><span class="line">    <span class="comment"># Hint 2: Don't forget to use the lambdas for the identity loss and cycle loss!</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="comment"># Adversarial Loss -- get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion)</span></span><br><span class="line">    adv_loss,fake_B = get_gen_adversarial_loss(real_A, disc_B, gen_AB, adv_criterion)</span><br><span class="line">    adv_loss_2,fake_A = get_gen_adversarial_loss(real_B, disc_A, gen_BA, adv_criterion)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Identity Loss -- get_identity_loss(real_X, gen_YX, identity_criterion)</span></span><br><span class="line">    ide_loss,identity_A = get_identity_loss(real_A, gen_BA, identity_criterion)</span><br><span class="line">    ide_loss_2,identity_B = get_identity_loss(real_B, gen_AB, identity_criterion)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cycle-consistency Loss -- get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion)</span></span><br><span class="line">    cyc_loss,cycle_A = get_cycle_consistency_loss(real_A, fake_B, gen_BA, cycle_criterion)</span><br><span class="line">    cyc_loss_2,cycle_B = get_cycle_consistency_loss(real_B, fake_A, gen_AB, cycle_criterion)</span><br><span class="line">    <span class="comment"># Total loss</span></span><br><span class="line">    </span><br><span class="line">    gen_loss = adv_loss + adv_loss_2 + lambda_identity * (ide_loss + ide_loss_2) +  lambda_cycle * (cyc_loss + cyc_loss_2)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss, fake_A, fake_B</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_real_A = torch.tensor(<span class="number">97</span>)</span><br><span class="line">test_real_B = torch.tensor(<span class="number">89</span>)</span><br><span class="line">test_gen_AB = <span class="keyword">lambda</span> x: x * <span class="number">83</span></span><br><span class="line">test_gen_BA = <span class="keyword">lambda</span> x: x * <span class="number">79</span></span><br><span class="line">test_disc_A = <span class="keyword">lambda</span> x: x * <span class="number">47</span></span><br><span class="line">test_disc_B = <span class="keyword">lambda</span> x: x * <span class="number">43</span></span><br><span class="line">test_adv_criterion = <span class="keyword">lambda</span> x, y: x * <span class="number">73</span> + y * <span class="number">71</span></span><br><span class="line">test_recon_criterion = <span class="keyword">lambda</span> x, y: (x + y) * <span class="number">61</span></span><br><span class="line">test_lambda_identity = <span class="number">59</span></span><br><span class="line">test_lambda_cycle = <span class="number">53</span></span><br><span class="line">test_res = get_gen_loss(</span><br><span class="line">    test_real_A, </span><br><span class="line">    test_real_B, </span><br><span class="line">    test_gen_AB, </span><br><span class="line">    test_gen_BA, </span><br><span class="line">    test_disc_A,</span><br><span class="line">    test_disc_B,</span><br><span class="line">    test_adv_criterion, </span><br><span class="line">    test_recon_criterion, </span><br><span class="line">    test_recon_criterion, </span><br><span class="line">    test_lambda_identity, </span><br><span class="line">    test_lambda_cycle)</span><br><span class="line"><span class="keyword">assert</span> test_res[<span class="number">0</span>].item() == <span class="number">4047804560</span></span><br><span class="line"><span class="keyword">assert</span> test_res[<span class="number">1</span>].item() == <span class="number">7031</span></span><br><span class="line"><span class="keyword">assert</span> test_res[<span class="number">2</span>].item() == <span class="number">8051</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="CycleGAN-Training"><a href="#CycleGAN-Training" class="headerlink" title="CycleGAN Training"></a>CycleGAN Training</h2><p>Lastly, you can train the model and see some of your zebras, horses, and some that might not quite look like either! Note that this training will take a long time, so feel free to use the pre-trained checkpoint as an example of what a pretty-good CycleGAN does.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> color</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(save_model=False)</span>:</span></span><br><span class="line">    mean_generator_loss = <span class="number">0</span></span><br><span class="line">    mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    cur_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">        <span class="comment"># for image, _ in tqdm(dataloader):</span></span><br><span class="line">        <span class="keyword">for</span> real_A, real_B <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">            <span class="comment"># image_width = image.shape[3]</span></span><br><span class="line">            real_A = nn.functional.interpolate(real_A, size=target_shape)</span><br><span class="line">            real_B = nn.functional.interpolate(real_B, size=target_shape)</span><br><span class="line">            cur_batch_size = len(real_A)</span><br><span class="line">            real_A = real_A.to(device)</span><br><span class="line">            real_B = real_B.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update discriminator A ###</span></span><br><span class="line">            disc_A_opt.zero_grad() <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake_A = gen_BA(real_B)</span><br><span class="line">            disc_A_loss = get_disc_loss(real_A, fake_A, disc_A, adv_criterion)</span><br><span class="line">            disc_A_loss.backward(retain_graph=<span class="keyword">True</span>) <span class="comment"># Update gradients</span></span><br><span class="line">            disc_A_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update discriminator B ###</span></span><br><span class="line">            disc_B_opt.zero_grad() <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake_B = gen_AB(real_A)</span><br><span class="line">            disc_B_loss = get_disc_loss(real_B, fake_B, disc_B, adv_criterion)</span><br><span class="line">            disc_B_loss.backward(retain_graph=<span class="keyword">True</span>) <span class="comment"># Update gradients</span></span><br><span class="line">            disc_B_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update generator ###</span></span><br><span class="line">            gen_opt.zero_grad()</span><br><span class="line">            gen_loss, fake_A, fake_B = get_gen_loss(</span><br><span class="line">                real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, recon_criterion, recon_criterion</span><br><span class="line">            )</span><br><span class="line">            gen_loss.backward() <span class="comment"># Update gradients</span></span><br><span class="line">            gen_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">            mean_discriminator_loss += disc_A_loss.item() / display_step</span><br><span class="line">            <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">            mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Visualization code ###</span></span><br><span class="line">            <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">f"Epoch <span class="subst">&#123;epoch&#125;</span>: Step <span class="subst">&#123;cur_step&#125;</span>: Generator (U-Net) loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, Discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">                show_tensor_images(torch.cat([real_A, real_B]), size=(dim_A, target_shape, target_shape))</span><br><span class="line">                show_tensor_images(torch.cat([fake_B, fake_A]), size=(dim_B, target_shape, target_shape))</span><br><span class="line">                mean_generator_loss = <span class="number">0</span></span><br><span class="line">                mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">                <span class="comment"># You can change save_model to True if you'd like to save the model</span></span><br><span class="line">                <span class="keyword">if</span> save_model:</span><br><span class="line">                    torch.save(&#123;</span><br><span class="line">                        <span class="string">'gen_AB'</span>: gen_AB.state_dict(),</span><br><span class="line">                        <span class="string">'gen_BA'</span>: gen_BA.state_dict(),</span><br><span class="line">                        <span class="string">'gen_opt'</span>: gen_opt.state_dict(),</span><br><span class="line">                        <span class="string">'disc_A'</span>: disc_A.state_dict(),</span><br><span class="line">                        <span class="string">'disc_A_opt'</span>: disc_A_opt.state_dict(),</span><br><span class="line">                        <span class="string">'disc_B'</span>: disc_B.state_dict(),</span><br><span class="line">                        <span class="string">'disc_B_opt'</span>: disc_B_opt.state_dict()</span><br><span class="line">                    &#125;, <span class="string">f"cycleGAN_<span class="subst">&#123;cur_step&#125;</span>.pth"</span>)</span><br><span class="line">            cur_step += <span class="number">1</span></span><br><span class="line">train()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CycleGAN&quot;&gt;&lt;a href=&quot;#CycleGAN&quot; class=&quot;headerlink&quot; title=&quot;CycleGAN&quot;&gt;&lt;/a&gt;CycleGAN&lt;/h1&gt;&lt;h3 id=&quot;Goals&quot;&gt;&lt;a href=&quot;#Goals&quot; class=&quot;headerlink
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Pix2Pix</title>
    <link href="https://zhangruochi.com/Pix2Pix/2020/11/09/"/>
    <id>https://zhangruochi.com/Pix2Pix/2020/11/09/</id>
    <published>2020-11-09T07:48:19.000Z</published>
    <updated>2020-11-09T07:48:46.318Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pix2Pix"><a href="#Pix2Pix" class="headerlink" title="Pix2Pix"></a>Pix2Pix</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you will write a generative model based on the paper <a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="noopener"><em>Image-to-Image Translation with Conditional Adversarial Networks</em></a> by Isola et al. 2017, also known as Pix2Pix.</p><p>You will be training a model that can convert aerial satellite imagery (“input”) into map routes (“output”), as was done in the original paper. Since the architecture for the generator is a U-Net, which you’ve already implemented (with minor changes), the emphasis of the assignment will be on the loss function. So that you can see outputs more quickly, you’ll be able to see your model train starting from a pre-trained checkpoint - but feel free to train it from scratch on your own too.</p><p><img src="https://drive.google.com/uc?id=1F-oVKBWcd6RaMyP3weGM8wvRtkK6y6i0" alt="pix2pix example"></p><!-- You will take the segmentations that you generated in the previous assignment and produce photorealistic images. --><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Implement the loss of a Pix2Pix model that differentiates it from a supervised U-Net.</li><li>Observe the change in generator priorities as the Pix2Pix generator trains, changing its emphasis from reconstruction to realism.</li></ol><!-- When you're done with this assignment, you'll be able to understand much of [*Image-to-Image Translation with Conditional Adversarial Networks*](https://arxiv.org/abs/1611.07004), which introduced Pix2Pix.You'll be using the same U-Net as in the previous assignment, but you'll write another discriminator and change the loss to make it a GAN. --><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will start by importing libraries, defining a visualization function, and getting the pre-trained Pix2Pix checkpoint. You will also be provided with the U-Net code for the Pix2Pix generator.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_shifted = image_tensor</span><br><span class="line">    image_unflat = image_shifted.detach().cpu().view(<span class="number">-1</span>, *size)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="U-Net-Code"><a href="#U-Net-Code" class="headerlink" title="U-Net Code"></a>U-Net Code</h4><p>The U-Net code will be much like the code you wrote for the last assignment, but with optional dropout and batchnorm. The structure is changed slightly for Pix2Pix, so that the final image is closer in size to the input image. Feel free to investigate the code if you’re interested!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crop</span><span class="params">(image, new_shape)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for cropping an image tensor: Given an image tensor and the new shape,</span></span><br><span class="line"><span class="string">    crops to the center pixels.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        new_shape: a torch.Size object with the shape you want x to have</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    middle_height = image.shape[<span class="number">2</span>] // <span class="number">2</span></span><br><span class="line">    middle_width = image.shape[<span class="number">3</span>] // <span class="number">2</span></span><br><span class="line">    starting_height = middle_height - round(new_shape[<span class="number">2</span>] / <span class="number">2</span>)</span><br><span class="line">    final_height = starting_height + new_shape[<span class="number">2</span>]</span><br><span class="line">    starting_width = middle_width - round(new_shape[<span class="number">3</span>] / <span class="number">2</span>)</span><br><span class="line">    final_width = starting_width + new_shape[<span class="number">3</span>]</span><br><span class="line">    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]</span><br><span class="line">    <span class="keyword">return</span> cropped_image</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContractingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ContractingBlock Class</span></span><br><span class="line"><span class="string">    Performs two convolutions followed by a max pool operation.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, use_dropout=False, use_bn=True)</span>:</span></span><br><span class="line">        super(ContractingBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels * <span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(input_channels * <span class="number">2</span>, input_channels * <span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.activation = nn.LeakyReLU(<span class="number">0.2</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            self.batchnorm = nn.BatchNorm2d(input_channels * <span class="number">2</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        <span class="keyword">if</span> use_dropout:</span><br><span class="line">            self.dropout = nn.Dropout()</span><br><span class="line">        self.use_dropout = use_dropout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ContractingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes a contracting block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batchnorm(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batchnorm(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpandingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ExpandingBlock Class:</span></span><br><span class="line"><span class="string">    Performs an upsampling, a convolution, a concatenation of its two inputs,</span></span><br><span class="line"><span class="string">    followed by two more convolutions with optional dropout</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, use_dropout=False, use_bn=True)</span>:</span></span><br><span class="line">        super(ExpandingBlock, self).__init__()</span><br><span class="line">        self.upsample = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'bilinear'</span>, align_corners=<span class="keyword">True</span>)</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(input_channels, input_channels // <span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(input_channels // <span class="number">2</span>, input_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            self.batchnorm = nn.BatchNorm2d(input_channels // <span class="number">2</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        self.activation = nn.ReLU()</span><br><span class="line">        <span class="keyword">if</span> use_dropout:</span><br><span class="line">            self.dropout = nn.Dropout()</span><br><span class="line">        self.use_dropout = use_dropout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, skip_con_x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ExpandingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes an expanding block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">            skip_con_x: the image tensor from the contracting path (from the opposing block of x)</span></span><br><span class="line"><span class="string">                    for the skip connection</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.upsample(x)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        skip_con_x = crop(skip_con_x, x.shape)</span><br><span class="line">        x = torch.cat([x, skip_con_x], axis=<span class="number">1</span>)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batchnorm(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.batchnorm(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            x = self.dropout(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureMapBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    FeatureMapBlock Class</span></span><br><span class="line"><span class="string">    The final layer of a U-Net - </span></span><br><span class="line"><span class="string">    maps each pixel to a pixel with the correct number of output dimensions</span></span><br><span class="line"><span class="string">    using a 1x1 convolution.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels)</span>:</span></span><br><span class="line">        super(FeatureMapBlock, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of FeatureMapBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, returns it mapped to the desired number of channels.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    UNet Class</span></span><br><span class="line"><span class="string">    A series of 4 contracting blocks followed by 4 expanding blocks to </span></span><br><span class="line"><span class="string">    transform an input image into the corresponding paired image, with an upfeature</span></span><br><span class="line"><span class="string">    layer at the start and a downfeature layer at the end.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels, hidden_channels=<span class="number">32</span>)</span>:</span></span><br><span class="line">        super(UNet, self).__init__()</span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels, use_dropout=<span class="keyword">True</span>)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>, use_dropout=<span class="keyword">True</span>)</span><br><span class="line">        self.contract3 = ContractingBlock(hidden_channels * <span class="number">4</span>, use_dropout=<span class="keyword">True</span>)</span><br><span class="line">        self.contract4 = ContractingBlock(hidden_channels * <span class="number">8</span>)</span><br><span class="line">        self.contract5 = ContractingBlock(hidden_channels * <span class="number">16</span>)</span><br><span class="line">        self.contract6 = ContractingBlock(hidden_channels * <span class="number">32</span>)</span><br><span class="line">        self.expand0 = ExpandingBlock(hidden_channels * <span class="number">64</span>)</span><br><span class="line">        self.expand1 = ExpandingBlock(hidden_channels * <span class="number">32</span>)</span><br><span class="line">        self.expand2 = ExpandingBlock(hidden_channels * <span class="number">16</span>)</span><br><span class="line">        self.expand3 = ExpandingBlock(hidden_channels * <span class="number">8</span>)</span><br><span class="line">        self.expand4 = ExpandingBlock(hidden_channels * <span class="number">4</span>)</span><br><span class="line">        self.expand5 = ExpandingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of UNet: </span></span><br><span class="line"><span class="string">        Given an image tensor, passes it through U-Net and returns the output.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.contract3(x2)</span><br><span class="line">        x4 = self.contract4(x3)</span><br><span class="line">        x5 = self.contract5(x4)</span><br><span class="line">        x6 = self.contract6(x5)</span><br><span class="line">        x7 = self.expand0(x6, x5)</span><br><span class="line">        x8 = self.expand1(x7, x4)</span><br><span class="line">        x9 = self.expand2(x8, x3)</span><br><span class="line">        x10 = self.expand3(x9, x2)</span><br><span class="line">        x11 = self.expand4(x10, x1)</span><br><span class="line">        x12 = self.expand5(x11, x0)</span><br><span class="line">        xn = self.downfeature(x12)</span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(xn)</span><br></pre></td></tr></table></figure><h2 id="PatchGAN-Discriminator"><a href="#PatchGAN-Discriminator" class="headerlink" title="PatchGAN Discriminator"></a>PatchGAN Discriminator</h2><p>Next, you will define a discriminator based on the contracting path of the U-Net to allow you to evaluate the realism of the generated images. Remember that the discriminator outputs a one-channel matrix of classifications instead of a single value. Your discriminator’s final layer will simply map from the final number of hidden channels to a single prediction for every pixel of the layer before it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CLASS: Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Structured like the contracting path of the U-Net, the discriminator will</span></span><br><span class="line"><span class="string">    output a matrix of values classifying corresponding portions of the image as real or fake. </span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_channels: the number of image input channels</span></span><br><span class="line"><span class="string">        hidden_channels: the initial number of discriminator convolutional filters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, hidden_channels=<span class="number">8</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels, use_bn=<span class="keyword">False</span>)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        self.contract3 = ContractingBlock(hidden_channels * <span class="number">4</span>)</span><br><span class="line">        self.contract4 = ContractingBlock(hidden_channels * <span class="number">8</span>)</span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        self.final = nn.Conv2d(hidden_channels * <span class="number">16</span>, <span class="number">1</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        x = torch.cat([x, y], axis=<span class="number">1</span>)</span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.contract3(x2)</span><br><span class="line">        x4 = self.contract4(x3)</span><br><span class="line">        xn = self.final(x4)</span><br><span class="line">        <span class="keyword">return</span> xn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_discriminator = Discriminator(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> tuple(test_discriminator(</span><br><span class="line">    torch.randn(<span class="number">1</span>, <span class="number">5</span>, <span class="number">256</span>, <span class="number">256</span>), </span><br><span class="line">    torch.randn(<span class="number">1</span>, <span class="number">5</span>, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">).shape) == (<span class="number">1</span>, <span class="number">1</span>, <span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Training-Preparation"><a href="#Training-Preparation" class="headerlink" title="Training Preparation"></a>Training Preparation</h2><!-- You'll be using the same U-Net as in the previous assignment, but you'll write another discriminator and change the loss to make it a GAN. --><p>Now you can begin putting everything together for training. You start by defining some new parameters as well as the ones you are familiar with:</p><ul><li><strong>real_dim</strong>: the number of channels of the real image and the number expected in the output image</li><li><strong>adv_criterion</strong>: an adversarial loss function to keep track of how well the GAN is fooling the discriminator and how well the discriminator is catching the GAN</li><li><strong>recon_criterion</strong>: a loss function that rewards similar images to the ground truth, which “reconstruct” the image</li><li><strong>lambda_recon</strong>: a parameter for how heavily the reconstruction loss should be weighed</li><li><strong>n_epochs</strong>: the number of times you iterate through the entire dataset when training</li><li><strong>input_dim</strong>: the number of channels of the input image</li><li><strong>display_step</strong>: how often to display/visualize the images</li><li><strong>batch_size</strong>: the number of images per forward/backward pass</li><li><strong>lr</strong>: the learning rate</li><li><strong>target_shape</strong>: the size of the output image (in pixels)</li><li><strong>device</strong>: the device type</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># New parameters</span></span><br><span class="line">adv_criterion = nn.BCEWithLogitsLoss() </span><br><span class="line">recon_criterion = nn.L1Loss() </span><br><span class="line">lambda_recon = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">20</span></span><br><span class="line">input_dim = <span class="number">3</span></span><br><span class="line">real_dim = <span class="number">3</span></span><br><span class="line">display_step = <span class="number">200</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">target_shape = <span class="number">256</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br></pre></td></tr></table></figure><p>You will then pre-process the images of the dataset to make sure they’re all the same size and that the size change due to U-Net layers is accounted for. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = torchvision.datasets.ImageFolder(<span class="string">"maps"</span>, transform=transform)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a,b = dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_tensor_images(a,<span class="number">1</span>,(<span class="number">600</span>,<span class="number">1200</span>))</span><br></pre></td></tr></table></figure><p><img src="output_14_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure><pre><code>0</code></pre><p>Next, you can initialize your generator (U-Net) and discriminator, as well as their optimizers. Finally, you will also load your pre-trained model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">gen = UNet(input_dim, real_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">disc = Discriminator(input_dim + real_dim).to(device)</span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feel free to change pretrained to False if you're training the model from scratch</span></span><br><span class="line">pretrained = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">if</span> pretrained:</span><br><span class="line">    loaded_state = torch.load(<span class="string">"pix2pix_15000.pth"</span>)</span><br><span class="line">    gen.load_state_dict(loaded_state[<span class="string">"gen"</span>])</span><br><span class="line">    gen_opt.load_state_dict(loaded_state[<span class="string">"gen_opt"</span>])</span><br><span class="line">    disc.load_state_dict(loaded_state[<span class="string">"disc"</span>])</span><br><span class="line">    disc_opt.load_state_dict(loaded_state[<span class="string">"disc_opt"</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    gen = gen.apply(weights_init)</span><br><span class="line">    disc = disc.apply(weights_init)</span><br></pre></td></tr></table></figure><p>While there are some changes to the U-Net architecture for Pix2Pix, the most important distinguishing feature of Pix2Pix is its adversarial loss. You will be implementing that here!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CLASS: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span><span class="params">(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the generator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gen: the generator; takes the condition and returns potential images</span></span><br><span class="line"><span class="string">        disc: the discriminator; takes images and the condition and</span></span><br><span class="line"><span class="string">          returns real/fake prediction matrices</span></span><br><span class="line"><span class="string">        real: the real images (e.g. maps) to be used to evaluate the reconstruction</span></span><br><span class="line"><span class="string">        condition: the source images (e.g. satellite imagery) which are used to produce the real images</span></span><br><span class="line"><span class="string">        adv_criterion: the adversarial loss function; takes the discriminator </span></span><br><span class="line"><span class="string">                  predictions and the true labels and returns a adversarial </span></span><br><span class="line"><span class="string">                  loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">        recon_criterion: the reconstruction loss function; takes the generator </span></span><br><span class="line"><span class="string">                    outputs and the real images and returns a reconstructuion </span></span><br><span class="line"><span class="string">                    loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">        lambda_recon: the degree to which the reconstruction loss should be weighted in the sum</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Steps: 1) Generate the fake images, based on the conditions.</span></span><br><span class="line">    <span class="comment">#        2) Evaluate the fake images and the condition with the discriminator.</span></span><br><span class="line">    <span class="comment">#        3) Calculate the adversarial and reconstruction losses.</span></span><br><span class="line">    <span class="comment">#        4) Add the two losses, weighting the reconstruction loss appropriately.</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    gen_img = gen(condition)</span><br><span class="line">    out = disc(gen_img, condition)</span><br><span class="line">    adv_loss = adv_criterion(out, torch.ones_like(out))</span><br><span class="line">    recon_loss = recon_criterion(gen_img, real)</span><br><span class="line">    gen_loss = adv_loss + lambda_recon * recon_loss</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_reasonable</span><span class="params">(num_images=<span class="number">10</span>)</span>:</span></span><br><span class="line">    gen = torch.zeros_like</span><br><span class="line">    disc = <span class="keyword">lambda</span> x, y: torch.ones(len(x), <span class="number">1</span>)</span><br><span class="line">    real = <span class="keyword">None</span></span><br><span class="line">    condition = torch.ones(num_images, <span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">    adv_criterion = torch.mul</span><br><span class="line">    recon_criterion = <span class="keyword">lambda</span> x, y: torch.tensor(<span class="number">0</span>)</span><br><span class="line">    lambda_recon = <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon).sum() == num_images</span><br><span class="line"></span><br><span class="line">    disc = <span class="keyword">lambda</span> x, y: torch.zeros(len(x), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.abs(get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)).sum() == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    adv_criterion = <span class="keyword">lambda</span> x, y: torch.tensor(<span class="number">0</span>)</span><br><span class="line">    recon_criterion = <span class="keyword">lambda</span> x, y: torch.abs(x - y).max()</span><br><span class="line">    real = torch.randn(num_images, <span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">    lambda_recon = <span class="number">2</span></span><br><span class="line">    gen = <span class="keyword">lambda</span> x: real + <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> torch.abs(get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon) - <span class="number">2</span>) &lt; <span class="number">1e-4</span></span><br><span class="line"></span><br><span class="line">    adv_criterion = <span class="keyword">lambda</span> x, y: (x + y).max() + x.max()</span><br><span class="line">    <span class="keyword">assert</span> torch.abs(get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon) - <span class="number">3</span>) &lt; <span class="number">1e-4</span></span><br><span class="line">test_gen_reasonable()</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Pix2Pix-Training"><a href="#Pix2Pix-Training" class="headerlink" title="Pix2Pix Training"></a>Pix2Pix Training</h2><p>Finally, you can train the model and see some of your maps!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> color</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(save_model=False)</span>:</span></span><br><span class="line">    mean_generator_loss = <span class="number">0</span></span><br><span class="line">    mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    cur_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">        <span class="keyword">for</span> image, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">            image_width = image.shape[<span class="number">3</span>]</span><br><span class="line">            condition = image[:, :, :, :image_width // <span class="number">2</span>]</span><br><span class="line">            condition = nn.functional.interpolate(condition, size=target_shape)</span><br><span class="line">            real = image[:, :, :, image_width // <span class="number">2</span>:]</span><br><span class="line">            real = nn.functional.interpolate(real, size=target_shape)</span><br><span class="line">            cur_batch_size = len(condition)</span><br><span class="line">            condition = condition.to(device)</span><br><span class="line">            real = real.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update discriminator ###</span></span><br><span class="line">            disc_opt.zero_grad() <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = gen(condition)</span><br><span class="line">            disc_fake_hat = disc(fake.detach(), condition) <span class="comment"># Detach generator</span></span><br><span class="line">            disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))</span><br><span class="line">            disc_real_hat = disc(real, condition)</span><br><span class="line">            disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))</span><br><span class="line">            disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line">            disc_loss.backward(retain_graph=<span class="keyword">True</span>) <span class="comment"># Update gradients</span></span><br><span class="line">            disc_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update generator ###</span></span><br><span class="line">            gen_opt.zero_grad()</span><br><span class="line">            gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon)</span><br><span class="line">            gen_loss.backward() <span class="comment"># Update gradients</span></span><br><span class="line">            gen_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">            mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line">            <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">            mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Visualization code ###</span></span><br><span class="line">            <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">                    print(<span class="string">f"Epoch <span class="subst">&#123;epoch&#125;</span>: Step <span class="subst">&#123;cur_step&#125;</span>: Generator (U-Net) loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, Discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">"Pretrained initial state"</span>)</span><br><span class="line">                show_tensor_images(condition, size=(input_dim, target_shape, target_shape))</span><br><span class="line">                show_tensor_images(real, size=(real_dim, target_shape, target_shape))</span><br><span class="line">                show_tensor_images(fake, size=(real_dim, target_shape, target_shape))</span><br><span class="line">                mean_generator_loss = <span class="number">0</span></span><br><span class="line">                mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">                <span class="comment"># You can change save_model to True if you'd like to save the model</span></span><br><span class="line">                <span class="keyword">if</span> save_model:</span><br><span class="line">                    torch.save(&#123;<span class="string">'gen'</span>: gen.state_dict(),</span><br><span class="line">                        <span class="string">'gen_opt'</span>: gen_opt.state_dict(),</span><br><span class="line">                        <span class="string">'disc'</span>: disc.state_dict(),</span><br><span class="line">                        <span class="string">'disc_opt'</span>: disc_opt.state_dict()</span><br><span class="line">                    &#125;, <span class="string">f"pix2pix_<span class="subst">&#123;cur_step&#125;</span>.pth"</span>)</span><br><span class="line">            cur_step += <span class="number">1</span></span><br><span class="line">train()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Pix2Pix&quot;&gt;&lt;a href=&quot;#Pix2Pix&quot; class=&quot;headerlink&quot; title=&quot;Pix2Pix&quot;&gt;&lt;/a&gt;Pix2Pix&lt;/h1&gt;&lt;h3 id=&quot;Goals&quot;&gt;&lt;a href=&quot;#Goals&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>U-Net</title>
    <link href="https://zhangruochi.com/U-Net/2020/11/09/"/>
    <id>https://zhangruochi.com/U-Net/2020/11/09/</id>
    <published>2020-11-09T07:06:56.000Z</published>
    <updated>2020-11-09T07:07:28.767Z</updated>
    
    <content type="html"><![CDATA[<h1 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to implement a U-Net for a biomedical imaging segmentation task. Specifically, you’re going to be labeling neurons, so one might call this a neural neural network! ;) </p><p>Note that this is not a GAN, generative model, or unsupervised learning task. This is a supervised learning task, so there’s only one correct answer (like a classifier!) You will see how this component underlies the Generator component of Pix2Pix in the next notebook this week.</p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Implement your own U-Net.</li><li>Observe your U-Net’s performance on a challenging segmentation task.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will start by importing libraries, defining a visualization function, and getting the neural dataset that you will be using.</p><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>For this notebook, you will be using a dataset of electron microscopy<br>images and segmentation data. The information about the dataset you’ll be using can be found <a href="https://www.ini.uzh.ch/~acardona/data.html" target="_blank" rel="noopener">here</a>! </p><blockquote><p>Arganda-Carreras et al. “Crowdsourcing the creation of image<br>segmentation algorithms for connectomics”. Front. Neuroanat. 2015. <a href="https://www.frontiersin.org/articles/10.3389/fnana.2015.00142/full" target="_blank" rel="noopener">https://www.frontiersin.org/articles/10.3389/fnana.2015.00142/full</a></p></blockquote><p><img src="https://drive.google.com/uc?id=1pqqpGwj6GKwICIKLuXyknSGtLay1edJO" alt="dataset example"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># image_shifted = (image_tensor + 1) / 2</span></span><br><span class="line">    image_shifted = image_tensor</span><br><span class="line">    image_unflat = image_shifted.detach().cpu().view(<span class="number">-1</span>, *size)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">4</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="U-Net-Architecture"><a href="#U-Net-Architecture" class="headerlink" title="U-Net Architecture"></a>U-Net Architecture</h2><p>Now you can build your U-Net from its components. The figure below is from the paper, <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener"><em>U-Net: Convolutional Networks for Biomedical Image Segmentation</em></a>, by Ronneberger et al. 2015. It shows the U-Net architecture and how it contracts and then expands.</p><!-- "[i]t consists of a contracting path (left side) and an expansive path (right side)" (Renneberger, 2015) --><p><img src="https://drive.google.com/uc?export=view&amp;id=1XgJRexE2CmsetRYyTLA7L8dsEwx7aQZY" alt="Figure 1 from the paper, U-Net: Convolutional Networks for Biomedical Image Segmentation"></p><p>In other words, images are first fed through many convolutional layers which reduce height and width while increasing the channels, which the authors refer to as the “contracting path.” For example, a set of two 2 x 2 convolutions with a stride of 2, will take a 1 x 28 x 28 (channels, height, width) grayscale image and result in a 2 x 14 x 14 representation. The “expanding path” does the opposite, gradually growing the image with fewer and fewer channels.</p><h2 id="Contracting-Path"><a href="#Contracting-Path" class="headerlink" title="Contracting Path"></a>Contracting Path</h2><p>You will first implement the contracting blocks for the contracting path. This path is the encoder section of the U-Net, which has several downsampling steps as part of it. The authors give more detail of the remaining parts in the following paragraph from the paper (Renneberger, 2015):</p><blockquote><p>The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3 x 3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2 x 2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.</p></blockquote><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">ContractingBlock</font></code></b></font></summary>1.    Both convolutions should use 3 x 3 kernels.2.    The max pool should use a 2 x 2 kernel with a stride 2.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CLASS: ContractingBlock</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContractingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ContractingBlock Class</span></span><br><span class="line"><span class="string">    Performs two convolutions followed by a max pool operation.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels)</span>:</span></span><br><span class="line">        super(ContractingBlock, self).__init__()</span><br><span class="line">        <span class="comment"># You want to double the number of channels in the first convolution</span></span><br><span class="line">        <span class="comment"># and keep the same number of channels in the second.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels*<span class="number">2</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(input_channels*<span class="number">2</span>, input_channels*<span class="number">2</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.activation = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ContractingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes a contracting block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#UNIT TEST</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_contracting_block</span><span class="params">(test_samples=<span class="number">100</span>, test_channels=<span class="number">10</span>, test_size=<span class="number">50</span>)</span>:</span></span><br><span class="line">    test_block = ContractingBlock(test_channels)</span><br><span class="line">    test_in = torch.randn(test_samples, test_channels, test_size, test_size)</span><br><span class="line">    test_out_conv1 = test_block.conv1(test_in)</span><br><span class="line">    <span class="comment"># Make sure that the first convolution has the right shape</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(test_out_conv1.shape) == (test_samples, test_channels * <span class="number">2</span>, test_size - <span class="number">2</span>, test_size - <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Make sure that the right activation is used</span></span><br><span class="line">    <span class="keyword">assert</span> torch.all(test_block.activation(test_out_conv1) &gt;= <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.max(test_block.activation(test_out_conv1)) &gt;= <span class="number">1</span></span><br><span class="line">    test_out_conv2 = test_block.conv2(test_out_conv1)</span><br><span class="line">    <span class="comment"># Make sure that the second convolution has the right shape</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(test_out_conv2.shape) == (test_samples, test_channels * <span class="number">2</span>, test_size - <span class="number">4</span>, test_size - <span class="number">4</span>)</span><br><span class="line">    test_out = test_block(test_in)</span><br><span class="line">    <span class="comment"># Make sure that the pooling has the right shape</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(test_out.shape) == (test_samples, test_channels * <span class="number">2</span>, test_size // <span class="number">2</span> - <span class="number">2</span>, test_size // <span class="number">2</span> - <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">test_contracting_block()</span><br><span class="line">test_contracting_block(<span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Expanding-Path"><a href="#Expanding-Path" class="headerlink" title="Expanding Path"></a>Expanding Path</h2><p>Next, you will implement the expanding blocks for the expanding path. This is the decoding section of U-Net which has several upsampling steps as part of it. In order to do this, you’ll also need to write a crop function. This is so you can crop the image from the <em>contracting path</em> and concatenate it to the current image on the expanding path—this is to form a skip connection. Again, the details are from the paper (Renneberger, 2015):</p><blockquote><p>Every step in the expanding path consists of an upsampling of the feature map followed by a 2 x 2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3 x 3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution.</p></blockquote><!-- so that the expanding block can resize the input from the contracting block can have the same size as the input from the previous layer --><p><em>Fun fact: later models based on this architecture often use padding in the convolutions to prevent the size of the image from changing outside of the upsampling / downsampling steps!</em></p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">ExpandingBlock</font></code></b></font></summary>1.    The concatenation means the number of channels goes back to being input_channels, so you need to halve it again for the next convolution.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: crop</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crop</span><span class="params">(image, new_shape)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for cropping an image tensor: Given an image tensor and the new shape,</span></span><br><span class="line"><span class="string">    crops to the center pixels.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        new_shape: a torch.Size object with the shape you want x to have</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># There are many ways to implement this crop function, but it's what allows</span></span><br><span class="line">    <span class="comment"># the skip connection to function as intended with two differently sized images!</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    </span><br><span class="line">    _,_, h, w = new_shape</span><br><span class="line">    half_h,half_w = h // <span class="number">2</span>, w // <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    _,_, ori_h, ori_w = image.shape</span><br><span class="line">    half_ori_h, half_ori_w = ori_h // <span class="number">2</span>, ori_w // <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    cropped_image = image[:,:,half_ori_h-half_h: half_ori_h-half_h + h, half_ori_w - half_w :  half_ori_w - half_w + w]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> cropped_image</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#UNIT TEST</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_expanding_block_crop</span><span class="params">(test_samples=<span class="number">100</span>, test_channels=<span class="number">10</span>, test_size=<span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Make sure that the crop function is the right shape</span></span><br><span class="line">    skip_con_x = torch.randn(test_samples, test_channels, test_size + <span class="number">6</span>, test_size + <span class="number">6</span>)</span><br><span class="line">    x = torch.randn(test_samples, test_channels, test_size, test_size)</span><br><span class="line">    cropped = crop(skip_con_x, x.shape)</span><br><span class="line">    <span class="keyword">assert</span> tuple(cropped.shape) == (test_samples, test_channels, test_size, test_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make sure that the crop function takes the right area</span></span><br><span class="line">    test_meshgrid = torch.meshgrid([torch.arange(<span class="number">0</span>, test_size), torch.arange(<span class="number">0</span>, test_size)])</span><br><span class="line">    test_meshgrid = test_meshgrid[<span class="number">0</span>] + test_meshgrid[<span class="number">1</span>]</span><br><span class="line">    test_meshgrid = test_meshgrid[<span class="keyword">None</span>, <span class="keyword">None</span>, :, :].float()</span><br><span class="line">    cropped = crop(test_meshgrid, torch.Size([<span class="number">1</span>, <span class="number">1</span>, test_size // <span class="number">2</span>, test_size // <span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">assert</span> cropped.max() == (test_size - <span class="number">1</span>) * <span class="number">2</span> - test_size // <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> cropped.min() == test_size // <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> cropped.mean() == test_size - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    test_meshgrid = torch.meshgrid([torch.arange(<span class="number">0</span>, test_size), torch.arange(<span class="number">0</span>, test_size)])</span><br><span class="line">    test_meshgrid = test_meshgrid[<span class="number">0</span>] + test_meshgrid[<span class="number">1</span>]</span><br><span class="line">    crop_size = <span class="number">5</span></span><br><span class="line">    test_meshgrid = test_meshgrid[<span class="keyword">None</span>, <span class="keyword">None</span>, :, :].float()</span><br><span class="line">    cropped = crop(test_meshgrid, torch.Size([<span class="number">1</span>, <span class="number">1</span>, crop_size, crop_size]))</span><br><span class="line">    <span class="keyword">assert</span> cropped.max() &lt; (test_size + crop_size) <span class="keyword">and</span> cropped.max() &gt; test_size</span><br><span class="line">    <span class="keyword">assert</span> cropped.min() &gt; (test_size - crop_size) <span class="keyword">and</span> cropped.min() &lt; test_size</span><br><span class="line">    <span class="keyword">assert</span> abs(cropped.mean() - test_size) &lt;= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">test_expanding_block_crop()</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CLASS: ExpandingBlock</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpandingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ExpandingBlock Class</span></span><br><span class="line"><span class="string">    Performs an upsampling, a convolution, a concatenation of its two inputs,</span></span><br><span class="line"><span class="string">    followed by two more convolutions.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels)</span>:</span></span><br><span class="line">        super(ExpandingBlock, self).__init__()</span><br><span class="line">        <span class="comment"># "Every step in the expanding path consists of an upsampling of the feature map"</span></span><br><span class="line">        self.upsample = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'bilinear'</span>, align_corners=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># "followed by a 2x2 convolution that halves the number of feature channels"</span></span><br><span class="line">        <span class="comment"># "a concatenation with the correspondingly cropped feature map from the contracting path"</span></span><br><span class="line">        <span class="comment"># "and two 3x3 convolutions"</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels//<span class="number">2</span>, kernel_size=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(input_channels, input_channels//<span class="number">2</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(input_channels//<span class="number">2</span>, input_channels//<span class="number">2</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        self.activation = nn.ReLU() <span class="comment"># "each followed by a ReLU"</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, skip_con_x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ExpandingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes an expanding block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">            skip_con_x: the image tensor from the contracting path (from the opposing block of x)</span></span><br><span class="line"><span class="string">                    for the skip connection</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.upsample(x)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        skip_con_x = crop(skip_con_x, x.shape)</span><br><span class="line">        x = torch.cat([x, skip_con_x], axis=<span class="number">1</span>)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#UNIT TEST</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_expanding_block</span><span class="params">(test_samples=<span class="number">100</span>, test_channels=<span class="number">10</span>, test_size=<span class="number">50</span>)</span>:</span></span><br><span class="line">    test_block = ExpandingBlock(test_channels)</span><br><span class="line">    skip_con_x = torch.randn(test_samples, test_channels // <span class="number">2</span>, test_size * <span class="number">2</span> + <span class="number">6</span>, test_size * <span class="number">2</span> + <span class="number">6</span>)</span><br><span class="line">    x = torch.randn(test_samples, test_channels, test_size, test_size)</span><br><span class="line">    x = test_block.upsample(x)</span><br><span class="line">    x = test_block.conv1(x)</span><br><span class="line">    <span class="comment"># Make sure that the first convolution produces the right shape</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(x.shape) == (test_samples, test_channels // <span class="number">2</span>,  test_size * <span class="number">2</span> - <span class="number">1</span>, test_size * <span class="number">2</span> - <span class="number">1</span>)</span><br><span class="line">    orginal_x = crop(skip_con_x, x.shape)</span><br><span class="line">    x = torch.cat([x, orginal_x], axis=<span class="number">1</span>)</span><br><span class="line">    x = test_block.conv2(x)</span><br><span class="line">    <span class="comment"># Make sure that the second convolution produces the right shape</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(x.shape) == (test_samples, test_channels // <span class="number">2</span>,  test_size * <span class="number">2</span> - <span class="number">3</span>, test_size * <span class="number">2</span> - <span class="number">3</span>)</span><br><span class="line">    x = test_block.conv3(x)</span><br><span class="line">    <span class="comment"># Make sure that the final convolution produces the right shape</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(x.shape) == (test_samples, test_channels // <span class="number">2</span>,  test_size * <span class="number">2</span> - <span class="number">5</span>, test_size * <span class="number">2</span> - <span class="number">5</span>)</span><br><span class="line">    x = test_block.activation(x)</span><br><span class="line"></span><br><span class="line">test_expanding_block()</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Final-Layer"><a href="#Final-Layer" class="headerlink" title="Final Layer"></a>Final Layer</h2><p>Now you will write the final feature mapping block, which takes in a tensor with arbitrarily many tensors and produces a tensor with the same number of pixels but with the correct number of output channels. From the paper (Renneberger, 2015):</p><blockquote><p>At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CLASS: FeatureMapBlock</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureMapBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    FeatureMapBlock Class</span></span><br><span class="line"><span class="string">    The final layer of a UNet - </span></span><br><span class="line"><span class="string">    maps each pixel to a pixel with the correct number of output dimensions</span></span><br><span class="line"><span class="string">    using a 1x1 convolution.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels)</span>:</span></span><br><span class="line">        super(FeatureMapBlock, self).__init__()</span><br><span class="line">        <span class="comment"># "Every step in the expanding path consists of an upsampling of the feature map"</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of FeatureMapBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, returns it mapped to the desired number of channels.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="keyword">assert</span> tuple(FeatureMapBlock(<span class="number">10</span>, <span class="number">60</span>)(torch.randn(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>)).shape) == (<span class="number">1</span>, <span class="number">60</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="U-Net-1"><a href="#U-Net-1" class="headerlink" title="U-Net"></a>U-Net</h2><p>Now you can put it all together! Here, you’ll write a <code>UNet</code> class which will combine a series of the three kinds of blocks you’ve implemented.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CLASS: UNet</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    UNet Class</span></span><br><span class="line"><span class="string">    A series of 4 contracting blocks followed by 4 expanding blocks to </span></span><br><span class="line"><span class="string">    transform an input image into the corresponding paired image, with an upfeature</span></span><br><span class="line"><span class="string">    layer at the start and a downfeature layer at the end</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels, hidden_channels=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(UNet, self).__init__()</span><br><span class="line">        <span class="comment"># "Every step in the expanding path consists of an upsampling of the feature map"</span></span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        self.contract3 = ContractingBlock(hidden_channels * <span class="number">4</span>)</span><br><span class="line">        self.contract4 = ContractingBlock(hidden_channels * <span class="number">8</span>)</span><br><span class="line">        self.expand1 = ExpandingBlock(hidden_channels * <span class="number">16</span>)</span><br><span class="line">        self.expand2 = ExpandingBlock(hidden_channels * <span class="number">8</span>)</span><br><span class="line">        self.expand3 = ExpandingBlock(hidden_channels * <span class="number">4</span>)</span><br><span class="line">        self.expand4 = ExpandingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of UNet: </span></span><br><span class="line"><span class="string">        Given an image tensor, passes it through U-Net and returns the output.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># Keep in mind that the expand function takes two inputs, </span></span><br><span class="line">        <span class="comment"># both with the same number of channels. </span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.contract3(x2)</span><br><span class="line">        x4 = self.contract4(x3)</span><br><span class="line">        x5 = self.expand1(x4, x3)</span><br><span class="line">        x6 = self.expand2(x5, x2)</span><br><span class="line">        x7 = self.expand3(x6, x1)</span><br><span class="line">        x8 = self.expand4(x7, x0)</span><br><span class="line">        xn = self.downfeature(x8)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        <span class="keyword">return</span> xn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#UNIT TEST</span></span><br><span class="line">test_unet = UNet(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">assert</span> tuple(test_unet(torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">256</span>)).shape) == (<span class="number">1</span>, <span class="number">3</span>, <span class="number">117</span>, <span class="number">117</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Finally, you will put this into action!<br>Remember that these are your parameters:</p><ul><li>criterion: the loss function</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>input_dim: the number of channels of the input image</li><li>label_dim: the number of channels of the output image</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>initial_shape: the size of the input image (in pixels)</li><li>target_shape: the size of the output image (in pixels)</li><li>device: the device type</li></ul><p>This should take only a few minutes to train!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">n_epochs = <span class="number">200</span></span><br><span class="line">input_dim = <span class="number">1</span></span><br><span class="line">label_dim = <span class="number">1</span></span><br><span class="line">display_step = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">initial_shape = <span class="number">512</span></span><br><span class="line">target_shape = <span class="number">373</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">volumes = torch.Tensor(io.imread(<span class="string">'train-volume.tif'</span>))[:, <span class="keyword">None</span>, :, :] / <span class="number">255</span></span><br><span class="line">labels = torch.Tensor(io.imread(<span class="string">'train-labels.tif'</span>, plugin=<span class="string">"tifffile"</span>))[:, <span class="keyword">None</span>, :, :] / <span class="number">255</span></span><br><span class="line">labels = crop(labels, torch.Size([len(labels), <span class="number">1</span>, target_shape, target_shape]))</span><br><span class="line">dataset = torch.utils.data.TensorDataset(volumes, labels)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=<span class="keyword">True</span>)</span><br><span class="line">    unet = UNet(input_dim, label_dim).to(device)</span><br><span class="line">    unet_opt = torch.optim.Adam(unet.parameters(), lr=lr)</span><br><span class="line">    cur_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> real, labels <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">            cur_batch_size = len(real)</span><br><span class="line">            <span class="comment"># Flatten the image</span></span><br><span class="line">            real = real.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update U-Net ###</span></span><br><span class="line">            unet_opt.zero_grad()</span><br><span class="line">            pred = unet(real)</span><br><span class="line">            unet_loss = criterion(pred, labels)</span><br><span class="line">            unet_loss.backward()</span><br><span class="line">            unet_opt.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">f"Epoch <span class="subst">&#123;epoch&#125;</span>: Step <span class="subst">&#123;cur_step&#125;</span>: U-Net loss: <span class="subst">&#123;unet_loss.item()&#125;</span>"</span>)</span><br><span class="line">                show_tensor_images(</span><br><span class="line">                    crop(real, torch.Size([len(real), <span class="number">1</span>, target_shape, target_shape])), </span><br><span class="line">                    size=(input_dim, target_shape, target_shape)</span><br><span class="line">                )</span><br><span class="line">                show_tensor_images(labels, size=(label_dim, target_shape, target_shape))</span><br><span class="line">                show_tensor_images(torch.sigmoid(pred), size=(label_dim, target_shape, target_shape))</span><br><span class="line">            cur_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">train()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;U-Net&quot;&gt;&lt;a href=&quot;#U-Net&quot; class=&quot;headerlink&quot; title=&quot;U-Net&quot;&gt;&lt;/a&gt;U-Net&lt;/h1&gt;&lt;h3 id=&quot;Goals&quot;&gt;&lt;a href=&quot;#Goals&quot; class=&quot;headerlink&quot; title=&quot;Goa
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>GAN for Data Augmentation</title>
    <link href="https://zhangruochi.com/GAN-for-Data-Augmentation/2020/11/09/"/>
    <id>https://zhangruochi.com/GAN-for-Data-Augmentation/2020/11/09/</id>
    <published>2020-11-09T03:21:47.000Z</published>
    <updated>2020-11-09T03:26:12.211Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GAN-for-Data-Augmentation"><a href="#GAN-for-Data-Augmentation" class="headerlink" title="GAN for Data Augmentation"></a>GAN for Data Augmentation</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook you’re going to build a generator that can be used to help create data to train a classifier. There are many cases where this might be useful. If you are interested in any of these topics, you are welcome to explore the linked papers and articles! </p><ul><li>With smaller datasets, GANs can provide useful data augmentation that substantially <a href="https://arxiv.org/abs/1711.04340" target="_blank" rel="noopener">improve classifier performance</a>. </li><li>You have one type of data already labeled and would like to make predictions on <a href="https://www.nature.com/articles/s41598-019-52737-x" target="_blank" rel="noopener">another related dataset for which you have no labels</a>. (You’ll learn about the techniques for this use case in future notebooks!)</li><li>You want to protect the privacy of the people who provided their information so you can provide access to a <a href="https://www.ahajournals.org/doi/full/10.1161/CIRCOUTCOMES.118.005122" target="_blank" rel="noopener">generator instead of real data</a>. </li><li>You have <a href="https://arxiv.org/abs/1806.02920" target="_blank" rel="noopener">input data with many missing values</a>, where the input dimensions are correlated and you would like to train a model on complete inputs. </li><li>You would like to be able to identify a real-world abnormal feature in an image <a href="https://link.springer.com/chapter/10.1007/978-3-030-00946-5_11" target="_blank" rel="noopener">for the purpose of diagnosis</a>, but have limited access to real examples of the condition. </li></ul><p>In this assignment, you’re going to be acting as a bug enthusiast — more on that later. </p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Understand some use cases for data augmentation and why GANs suit this task.</li><li>Implement a classifier that takes a mixed dataset of reals/fakes and analyze its accuracy.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><p>Before you implement GAN-based data augmentation, you should know a bit about data augmentation in general, specifically for image datasets. It is <a href="https://arxiv.org/abs/1712.04621" target="_blank" rel="noopener">very common practice</a> to augment image-based datasets in ways that are appropriate for a given dataset. This may include having your dataloader randomly flipping images across their vertical axis, randomly cropping your image to a particular size, randomly adding a bit of noise or color to an image in ways that are true-to-life. </p><p>In general, data augmentation helps to stop your model from overfitting to the data, and allows you to make small datasets many times larger. However, a sufficiently powerful classifier often still overfits to the original examples which is why GANs are particularly useful here. They can generate new images instead of simply modifying existing ones.</p><h3 id="CIFAR"><a href="#CIFAR" class="headerlink" title="CIFAR"></a>CIFAR</h3><p>The <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" target="_blank" rel="noopener">CIFAR-10 and CIFAR-100</a> datasets are extremely widely used within machine learning — they contain many thousands of “tiny” 32x32 color images of different classes representing relatively common real-world objects like airplanes and dogs, with 10 classes in CIFAR-10 and 100 classes in CIFAR-100. In CIFAR-100, there are 20 “superclasses” which each contain five classes. For example, the “fish” superclass contains “aquarium fish, flatfish, ray, shark, trout”. For the purposes of this assignment, you’ll be looking at a small subset of these images to simulate a small data regime, with only 40 images of each class for training.</p><p><img src="https://drive.google.com/uc?id=1tbrqp8-NJ59VBpS5T_ibrQzEpgtZ3suw" alt="alt text"></p><h3 id="Initializations"><a href="#Initializations" class="headerlink" title="Initializations"></a>Initializations</h3><p>You will begin by importing some useful libraries and packages and defining a visualization function that has been provided. You will also be re-using your conditional generator and functions code from earlier assignments. This will let you control what class of images to augment for your classifier.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for our testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span>, nrow=<span class="number">5</span>, show=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu()</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    <span class="keyword">if</span> show:</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure><h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels of the output image, a scalar</span></span><br><span class="line"><span class="string">              (CIFAR100 is in color (red, green, blue), so 3 is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="number">10</span>, im_chan=<span class="number">3</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(input_dim, hidden_dim * <span class="number">4</span>, kernel_size=<span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim, kernel_size=<span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">2</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, input_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = noise.view(len(noise), self.input_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, input_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, input_dim, device=device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_vectors</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?)</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">    x: (n_samples, ?) the first vector. </span></span><br><span class="line"><span class="string">        In this assignment, this will be the noise vector of shape (n_samples, z_dim), </span></span><br><span class="line"><span class="string">        but you shouldn't need to know the second dimension's size.</span></span><br><span class="line"><span class="string">    y: (n_samples, ?) the second vector.</span></span><br><span class="line"><span class="string">        Once again, in this assignment this will be the one-hot class vector </span></span><br><span class="line"><span class="string">        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.cat([x, y], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_hot_labels</span><span class="params">(labels, n_classes)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?)</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">    labels: (n_samples, 1) </span></span><br><span class="line"><span class="string">    n_classes: a single integer corresponding to the total number of classes in the dataset</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> F.one_hot(labels, n_classes)</span><br></pre></td></tr></table></figure><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can begin training your models.<br>First, you will define some new parameters:</p><ul><li>cifar100_shape: the number of pixels in each CIFAR image, which has dimensions 32 x 32 and three channel (for red, green, and blue) so 3 x 32 x 32</li><li>n_classes: the number of classes in CIFAR100 (e.g. airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cifar100_shape = (<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">n_classes = <span class="number">100</span></span><br></pre></td></tr></table></figure><p>And you also include the same parameters from previous assignments:</p><ul><li>criterion: the loss function</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>device: the device type</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">10000</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br></pre></td></tr></table></figure><p>Then, you want to set your generator’s input dimension. Recall that for conditional GANs, the generator’s input is the noise vector concatenated with the class vector.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">generator_input_dim = z_dim + n_classes</span><br></pre></td></tr></table></figure><h4 id="Classifier"><a href="#Classifier" class="headerlink" title="Classifier"></a>Classifier</h4><p>For the classifier, you will use the same code that you wrote in an earlier assignment (the same as previous code for the discriminator as well since the discriminator is a real/fake classifier).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Classifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Classifier Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_chan: the number of channels of the output image, a scalar</span></span><br><span class="line"><span class="string">        n_classes: the total number of classes in the dataset, an integer scalar</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_chan, n_classes, hidden_dim=<span class="number">32</span>)</span>:</span></span><br><span class="line">        super(Classifier, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            self.make_classifier_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_classifier_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_classifier_block(hidden_dim * <span class="number">2</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_classifier_block(hidden_dim * <span class="number">4</span>, n_classes, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_classifier_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a classifier block; </span></span><br><span class="line"><span class="string">        a convolution, a batchnorm (except in the final layer), and an activation (except in the final</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the classifier: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns an n_classes-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with im_chan channels</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        class_pred = self.disc(image)</span><br><span class="line">        <span class="keyword">return</span> class_pred.view(len(class_pred), <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h4 id="Pre-training-Optional"><a href="#Pre-training-Optional" class="headerlink" title="Pre-training (Optional)"></a>Pre-training (Optional)</h4><p>You are provided the code to pre-train the models (GAN and classifier) given to you in this assignment. However, this is intended only for your personal curiosity — for the assignment to run as intended, you should not use any checkpoints besides the ones given to you.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This code is here for you to train your own generator or classifier </span></span><br><span class="line"><span class="comment"># outside the assignment on the full dataset if you'd like -- for the purposes </span></span><br><span class="line"><span class="comment"># of this assignment, please use the provided checkpoints</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">      im_chan: the number of channels of the output image, a scalar</span></span><br><span class="line"><span class="string">            (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">      hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_chan=<span class="number">3</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            self.make_disc_block(im_chan, hidden_dim, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim * <span class="number">2</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim * <span class="number">4</span>, <span class="number">1</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_disc_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; </span></span><br><span class="line"><span class="string">        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_chan)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        disc_pred = self.disc(image)</span><br><span class="line">        <span class="keyword">return</span> disc_pred.view(len(disc_pred), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_generator</span><span class="params">()</span>:</span></span><br><span class="line">    gen = Generator(generator_input_dim).to(device)</span><br><span class="line">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">    discriminator_input_dim = cifar100_shape[<span class="number">0</span>] + n_classes</span><br><span class="line">    disc = Discriminator(discriminator_input_dim).to(device)</span><br><span class="line">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">            torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">            torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">            torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">    gen = gen.apply(weights_init)</span><br><span class="line">    disc = disc.apply(weights_init)</span><br><span class="line"></span><br><span class="line">    criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">    cur_step = <span class="number">0</span></span><br><span class="line">    mean_generator_loss = <span class="number">0</span></span><br><span class="line">    mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="comment"># Dataloader returns the batches and the labels</span></span><br><span class="line">        <span class="keyword">for</span> real, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">            cur_batch_size = len(real)</span><br><span class="line">            <span class="comment"># Flatten the batch of real images from the dataset</span></span><br><span class="line">            real = real.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Convert the labels from the dataloader into one-hot versions of those labels</span></span><br><span class="line">            one_hot_labels = get_one_hot_labels(labels.to(device), n_classes).float()</span><br><span class="line"></span><br><span class="line">            image_one_hot_labels = one_hot_labels[:, :, <span class="keyword">None</span>, <span class="keyword">None</span>]</span><br><span class="line">            image_one_hot_labels = image_one_hot_labels.repeat(<span class="number">1</span>, <span class="number">1</span>, cifar100_shape[<span class="number">1</span>], cifar100_shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update discriminator ###</span></span><br><span class="line">            <span class="comment"># Zero out the discriminator gradients</span></span><br><span class="line">            disc_opt.zero_grad()</span><br><span class="line">            <span class="comment"># Get noise corresponding to the current batch_size </span></span><br><span class="line">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        </span><br><span class="line">            <span class="comment"># Combine the vectors of the noise and the one-hot labels for the generator</span></span><br><span class="line">            noise_and_labels = combine_vectors(fake_noise, one_hot_labels)</span><br><span class="line">            fake = gen(noise_and_labels)</span><br><span class="line">            <span class="comment"># Combine the vectors of the images and the one-hot labels for the discriminator</span></span><br><span class="line">            fake_image_and_labels = combine_vectors(fake.detach(), image_one_hot_labels)</span><br><span class="line">            real_image_and_labels = combine_vectors(real, image_one_hot_labels)</span><br><span class="line">            disc_fake_pred = disc(fake_image_and_labels)</span><br><span class="line">            disc_real_pred = disc(real_image_and_labels)</span><br><span class="line"></span><br><span class="line">            disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span><br><span class="line">            disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span><br><span class="line">            disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line">            disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">            disc_opt.step() </span><br><span class="line"></span><br><span class="line">            <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">            mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update generator ###</span></span><br><span class="line">            <span class="comment"># Zero out the generator gradients</span></span><br><span class="line">            gen_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Pass the discriminator the combination of the fake images and the one-hot labels</span></span><br><span class="line">            fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)</span><br><span class="line"></span><br><span class="line">            disc_fake_pred = disc(fake_image_and_labels)</span><br><span class="line">            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span><br><span class="line">            gen_loss.backward()</span><br><span class="line">            gen_opt.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">            mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">                show_tensor_images(fake)</span><br><span class="line">                show_tensor_images(real)</span><br><span class="line">                mean_generator_loss = <span class="number">0</span></span><br><span class="line">                mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">            cur_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_classifier</span><span class="params">()</span>:</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    n_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    validation_dataloader = DataLoader(</span><br><span class="line">        CIFAR100(<span class="string">"."</span>, train=<span class="keyword">False</span>, download=<span class="keyword">True</span>, transform=transform),</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">    display_step = <span class="number">10</span></span><br><span class="line">    batch_size = <span class="number">512</span></span><br><span class="line">    lr = <span class="number">0.0002</span></span><br><span class="line">    device = <span class="string">'cuda'</span></span><br><span class="line">    classifier = Classifier(cifar100_shape[<span class="number">0</span>], n_classes).to(device)</span><br><span class="line">    classifier_opt = torch.optim.Adam(classifier.parameters(), lr=lr)</span><br><span class="line">    cur_step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> real, labels <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">            cur_batch_size = len(real)</span><br><span class="line">            real = real.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update classifier ###</span></span><br><span class="line">            <span class="comment"># Get noise corresponding to the current batch_size</span></span><br><span class="line">            classifier_opt.zero_grad()</span><br><span class="line">            labels_hat = classifier(real.detach())</span><br><span class="line">            classifier_loss = criterion(labels_hat, labels)</span><br><span class="line">            classifier_loss.backward()</span><br><span class="line">            classifier_opt.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span>:</span><br><span class="line">                classifier_val_loss = <span class="number">0</span></span><br><span class="line">                classifier_correct = <span class="number">0</span></span><br><span class="line">                num_validation = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> val_example, val_label <span class="keyword">in</span> validation_dataloader:</span><br><span class="line">                    cur_batch_size = len(val_example)</span><br><span class="line">                    num_validation += cur_batch_size</span><br><span class="line">                    val_example = val_example.to(device)</span><br><span class="line">                    val_label = val_label.to(device)</span><br><span class="line">                    labels_hat = classifier(val_example)</span><br><span class="line">                    classifier_val_loss += criterion(labels_hat, val_label) * cur_batch_size</span><br><span class="line">                    classifier_correct += (labels_hat.argmax(<span class="number">1</span>) == val_label).float().sum()</span><br><span class="line"></span><br><span class="line">                print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: "</span></span><br><span class="line">                        <span class="string">f"Classifier loss: <span class="subst">&#123;classifier_val_loss.item() / num_validation&#125;</span>, "</span></span><br><span class="line">                        <span class="string">f"classifier accuracy: <span class="subst">&#123;classifier_correct.item() / num_validation&#125;</span>"</span>)</span><br><span class="line">            cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="Tuning-the-Classifier"><a href="#Tuning-the-Classifier" class="headerlink" title="Tuning the Classifier"></a>Tuning the Classifier</h2><p>After two courses, you’ve probably had some fun debugging your GANs and have started to consider yourself a bug master. For this assignment, your mastery will be put to the test on some interesting bugs… well, bugs as in insects.</p><p>As a bug master, you want a classifier capable of classifying different species of bugs: bees, beetles, butterflies, caterpillar, and more. Luckily, you found a great dataset with a lot of animal species and objects, and you trained your classifier on that.</p><p>But the bug classes don’t do as well as you would like. Now your plan is to train a GAN on the same data so it can generate new bugs to make your classifier better at distinguishing between all of your favorite bugs!</p><p>You will fine-tune your model by augmenting the original real data with fake data and during that process, observe how to increase the accuracy of your classifier with these fake, GAN-generated bugs. After this, you will prove your worth as a bug master.</p><h4 id="Sampling-Ratio"><a href="#Sampling-Ratio" class="headerlink" title="Sampling Ratio"></a>Sampling Ratio</h4><p>Suppose that you’ve decided that although you have this pre-trained general generator and this general classifier, capable of identifying 100 classes with some accuracy (~17%), what you’d really like is a model that can classify the five different kinds of bugs in the dataset. You’ll fine-tune your model by augmenting your data with the generated images. Keep in mind that both the generator and the classifier were trained on the same images: the 40 images per class you painstakingly found so your generator may not be great. This is the caveat with data augmentation, ultimately you are still bound by the real data that you have but you want to try and create more. To make your models even better, you would need to take some more bug photos, label them, and add them to your training set and/or use higher quality photos.</p><p>To start, you’ll first need to write some code to sample a combination of real and generated images. Given a probability, <code>p_real</code>, you’ll need to generate a combined tensor where roughly <code>p_real</code> of the returned images are sampled from the real images. Note that you should not interpolate the images here: you should choose each image from the real or fake set with a given probability. For example, if your real images are a tensor of <code>[[1, 2, 3, 4, 5]]</code> and your fake images are a tensor of <code>[[-1, -2, -3, -4, -5]]</code>, and <code>p_real = 0.2</code>, two potential return values are <code>[[1, -2, 3, -4, -5]]</code> or <code>[[-1, 2, -3, -4, -5]]</code></p><p>In addition, we will expect the images to remain in the same order to maintain their alignment with their labels (this applies to the fake images too!). </p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">combine_sample</font></code></b></font></summary>1.   This code probably shouldn't be much longer than 3 lines2.   You can index using a set of booleans which have the same length as your tensor3.   You want to generate an unbiased sample, which you can do (for example) with `torch.rand(length_reals) > p`.4.   There are many approaches here that will give a correct answer here. You may find [`torch.rand`](https://pytorch.org/docs/stable/generated/torch.rand.html) or [`torch.bernoulli`](https://pytorch.org/docs/master/generated/torch.bernoulli.html) useful. 5.   You don't want to edit an argument in place, so you may find [`cur_tensor.clone()`](https://pytorch.org/docs/stable/tensors.html) useful too, which makes a copy of `cur_tensor`. </details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type(torch.distributions.uniform.Uniform(<span class="number">0</span>,<span class="number">1</span>).sample())</span><br></pre></td></tr></table></figure><pre><code>torch.Tensor</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: combine_sample</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_sample</span><span class="params">(real, fake, p_real)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function to take a set of real and fake images of the same length (x)</span></span><br><span class="line"><span class="string">    and produce a combined tensor with length (x) and sampled at the target probability</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real: a tensor of real images, length (x)</span></span><br><span class="line"><span class="string">        fake: a tensor of fake images, length (x)</span></span><br><span class="line"><span class="string">        p_real: the probability the images are sampled from the real set</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    u = torch.distributions.uniform.Uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> real.shape[<span class="number">0</span>] == fake.shape[<span class="number">0</span>]</span><br><span class="line">    x = real.shape[<span class="number">0</span>]</span><br><span class="line">    target_images = torch.zeros_like(real)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(x):</span><br><span class="line">        <span class="keyword">if</span> u.sample().item() &lt; p_real:</span><br><span class="line">            target_images[_] = real[_]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_images[_] = fake[_]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> target_images</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">n_test_samples = <span class="number">9999</span></span><br><span class="line">test_combination = combine_sample(</span><br><span class="line">    torch.ones(n_test_samples, <span class="number">1</span>), </span><br><span class="line">    torch.zeros(n_test_samples, <span class="number">1</span>), </span><br><span class="line">    <span class="number">0.3</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># Check that the shape is right</span></span><br><span class="line"><span class="keyword">assert</span> tuple(test_combination.shape) == (n_test_samples, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># Check that the ratio is right</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_combination.mean() - <span class="number">0.3</span>) &lt; <span class="number">0.05</span></span><br><span class="line"><span class="comment"># Make sure that no mixing happened</span></span><br><span class="line"><span class="keyword">assert</span> test_combination.median() &lt; <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">test_combination = combine_sample(</span><br><span class="line">    torch.ones(n_test_samples, <span class="number">10</span>, <span class="number">10</span>), </span><br><span class="line">    torch.zeros(n_test_samples, <span class="number">10</span>, <span class="number">10</span>), </span><br><span class="line">    <span class="number">0.8</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># Check that the shape is right</span></span><br><span class="line"><span class="keyword">assert</span> tuple(test_combination.shape) == (n_test_samples, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># Make sure that no mixing happened</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((test_combination.sum([<span class="number">1</span>, <span class="number">2</span>]).median()) - <span class="number">100</span>) &lt; <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">test_reals = torch.arange(n_test_samples)[:, <span class="keyword">None</span>].float()</span><br><span class="line">test_fakes = torch.zeros(n_test_samples, <span class="number">1</span>)</span><br><span class="line">test_saved = (test_reals.clone(), test_fakes.clone())</span><br><span class="line">test_combination = combine_sample(test_reals, test_fakes, <span class="number">0.3</span>)</span><br><span class="line"><span class="comment"># Make sure that the sample isn't biased</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((test_combination.mean() - <span class="number">1500</span>)) &lt; <span class="number">100</span></span><br><span class="line"><span class="comment"># Make sure no inputs were changed</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_saved[<span class="number">0</span>] - test_reals).sum() &lt; <span class="number">1e-3</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_saved[<span class="number">1</span>] - test_fakes).sum() &lt; <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line">test_fakes = torch.arange(n_test_samples)[:, <span class="keyword">None</span>].float()</span><br><span class="line">test_combination = combine_sample(test_reals, test_fakes, <span class="number">0.3</span>)</span><br><span class="line"><span class="comment"># Make sure that the order is maintained</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_combination - test_reals).sum() &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="comment"># Check that the solution matches the input device</span></span><br><span class="line">    <span class="keyword">assert</span> str(combine_sample(</span><br><span class="line">        torch.ones(n_test_samples, <span class="number">10</span>, <span class="number">10</span>).cuda(), </span><br><span class="line">        torch.zeros(n_test_samples, <span class="number">10</span>, <span class="number">10</span>).cuda(),</span><br><span class="line">        <span class="number">0.8</span></span><br><span class="line">    ).device).startswith(<span class="string">"cuda"</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Now you have a challenge: find a <code>p_real</code> and a generator image such that your classifier gets an average of a 51% accuracy or higher on the insects, when evaluated with the <code>eval_augmentation</code> function. <strong>You’ll need to fill in <code>find_optimal</code> to find these parameters to solve this part!</strong> Note that if your answer takes a very long time to run, you may need to hard-code the solution it finds. </p><p>When you’re training a generator, you will often have to look at different checkpoints and choose one that does the best (either empirically or using some evaluation method). Here, you are given four generator checkpoints: <code>gen_1.pt</code>, <code>gen_2.pt</code>, <code>gen_3.pt</code>, <code>gen_4.pt</code>. You’ll also have some scratch area to write whatever code you’d like to solve this problem, but you must return a <code>p_real</code> and an image name of your selected generator checkpoint. You can hard-code/brute-force these numbers if you would like, but you are encouraged to try to solve this problem in a more general way. In practice, you would also want a test set (since it is possible to overfit on a validation set), but for simplicity you can just focus on the validation set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: find_optimal</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_optimal</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># In the following section, you can write the code to choose your optimal answer</span></span><br><span class="line">    <span class="comment"># You can even use the eval_augmentation function in your code if you'd like!</span></span><br><span class="line">    gen_names = [</span><br><span class="line">        <span class="string">"gen_1.pt"</span>,</span><br><span class="line">        <span class="string">"gen_2.pt"</span>,</span><br><span class="line">        <span class="string">"gen_3.pt"</span>,</span><br><span class="line">        <span class="string">"gen_4.pt"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#### START CODE HERE #### </span></span><br><span class="line">    </span><br><span class="line">    best_p_real, best_gen_name = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    max_score = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p_real <span class="keyword">in</span> [<span class="number">0.2</span>,<span class="number">0.4</span>,<span class="number">0.6</span>,<span class="number">0.8</span>]:</span><br><span class="line">        <span class="keyword">for</span> gen_name <span class="keyword">in</span> gen_names:</span><br><span class="line">            score = eval_augmentation(p_real, gen_name, n_test = <span class="number">5</span>)</span><br><span class="line">            <span class="keyword">if</span> score &gt; max_score:</span><br><span class="line">                best_p_real = p_real</span><br><span class="line">                best_gen_name = gen_name</span><br><span class="line">                </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> best_p_real, best_gen_name</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">augmented_train</span><span class="params">(p_real, gen_name)</span>:</span></span><br><span class="line">    gen = Generator(generator_input_dim).to(device)</span><br><span class="line">    gen.load_state_dict(torch.load(gen_name))</span><br><span class="line"></span><br><span class="line">    classifier = Classifier(cifar100_shape[<span class="number">0</span>], n_classes).to(device)</span><br><span class="line">    classifier.load_state_dict(torch.load(<span class="string">"class.pt"</span>))</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line">    train_set = torch.load(<span class="string">"insect_train.pt"</span>)</span><br><span class="line">    val_set = torch.load(<span class="string">"insect_val.pt"</span>)</span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        torch.utils.data.TensorDataset(train_set[<span class="string">"images"</span>], train_set[<span class="string">"labels"</span>]),</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=<span class="keyword">True</span></span><br><span class="line">    )</span><br><span class="line">    validation_dataloader = DataLoader(</span><br><span class="line">        torch.utils.data.TensorDataset(val_set[<span class="string">"images"</span>], val_set[<span class="string">"labels"</span>]),</span><br><span class="line">        batch_size=batch_size</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    display_step = <span class="number">1</span></span><br><span class="line">    lr = <span class="number">0.0002</span></span><br><span class="line">    n_epochs = <span class="number">20</span></span><br><span class="line">    classifier_opt = torch.optim.Adam(classifier.parameters(), lr=lr)</span><br><span class="line">    cur_step = <span class="number">0</span></span><br><span class="line">    best_score = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> real, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">            real = real.to(device)</span><br><span class="line">            <span class="comment"># Flatten the image</span></span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            one_hot_labels = get_one_hot_labels(labels.to(device), n_classes).float()</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update classifier ###</span></span><br><span class="line">            <span class="comment"># Get noise corresponding to the current batch_size</span></span><br><span class="line">            classifier_opt.zero_grad()</span><br><span class="line">            cur_batch_size = len(labels)</span><br><span class="line">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">            noise_and_labels = combine_vectors(fake_noise, one_hot_labels)</span><br><span class="line">            fake = gen(noise_and_labels)</span><br><span class="line"></span><br><span class="line">            target_images = combine_sample(real.clone(), fake.clone(), p_real)</span><br><span class="line">            labels_hat = classifier(target_images.detach())</span><br><span class="line">            classifier_loss = criterion(labels_hat, labels)</span><br><span class="line">            classifier_loss.backward()</span><br><span class="line">            classifier_opt.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Calculate the accuracy on the validation set</span></span><br><span class="line">            <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">                classifier_val_loss = <span class="number">0</span></span><br><span class="line">                classifier_correct = <span class="number">0</span></span><br><span class="line">                num_validation = <span class="number">0</span></span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    <span class="keyword">for</span> val_example, val_label <span class="keyword">in</span> validation_dataloader:</span><br><span class="line">                        cur_batch_size = len(val_example)</span><br><span class="line">                        num_validation += cur_batch_size</span><br><span class="line">                        val_example = val_example.to(device)</span><br><span class="line">                        val_label = val_label.to(device)</span><br><span class="line">                        labels_hat = classifier(val_example)</span><br><span class="line">                        classifier_val_loss += criterion(labels_hat, val_label) * cur_batch_size</span><br><span class="line">                        classifier_correct += (labels_hat.argmax(<span class="number">1</span>) == val_label).float().sum()</span><br><span class="line">                    accuracy = classifier_correct.item() / num_validation</span><br><span class="line">                    <span class="keyword">if</span> accuracy &gt; best_score:</span><br><span class="line">                        best_score = accuracy</span><br><span class="line">            cur_step += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> best_score</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_augmentation</span><span class="params">(p_real, gen_name, n_test=<span class="number">20</span>)</span>:</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_test):</span><br><span class="line">        total += augmented_train(p_real, gen_name)</span><br><span class="line">    <span class="keyword">return</span> total / n_test</span><br><span class="line"></span><br><span class="line">best_p_real, best_gen_name = find_optimal()</span><br><span class="line">performance = eval_augmentation(best_p_real, best_gen_name)</span><br><span class="line">print(<span class="string">f"Your model had an accuracy of <span class="subst">&#123;performance:<span class="number">0.1</span>%&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">assert</span> performance &gt; <span class="number">0.51</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Your model had an accuracy of 51.7%Success!</code></pre><p>You’ll likely find that the worst performance is when the generator is performing alone: this corresponds to the case where you might be trying to hide the underlying examples from the classifier. Perhaps you don’t want other people to know about your specific bugs!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">accuracies = []</span><br><span class="line">p_real_all = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">21</span>)</span><br><span class="line"><span class="keyword">for</span> p_real_vis <span class="keyword">in</span> tqdm(p_real_all):</span><br><span class="line">    accuracies += [eval_augmentation(p_real_vis, best_gen_name, n_test=<span class="number">4</span>)]</span><br><span class="line">plt.plot(p_real_all.tolist(), accuracies)</span><br><span class="line">plt.ylabel(<span class="string">"Accuracy"</span>)</span><br><span class="line">_ = plt.xlabel(<span class="string">"Percent Real Images"</span>)</span><br></pre></td></tr></table></figure><pre><code>HBox(children=(FloatProgress(value=0.0, max=21.0), HTML(value=&#39;&#39;)))</code></pre><p><img src="output_24_2.png" alt="png"></p><p>Here’s a visualization of what the generator is actually generating, with real examples of each class above the corresponding generated image.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">examples = [<span class="number">4</span>, <span class="number">41</span>, <span class="number">80</span>, <span class="number">122</span>, <span class="number">160</span>]</span><br><span class="line">train_images = torch.load(<span class="string">"insect_train.pt"</span>)[<span class="string">"images"</span>][examples]</span><br><span class="line">train_labels = torch.load(<span class="string">"insect_train.pt"</span>)[<span class="string">"labels"</span>][examples]</span><br><span class="line"></span><br><span class="line">one_hot_labels = get_one_hot_labels(train_labels.to(device), n_classes).float()</span><br><span class="line">fake_noise = get_noise(len(train_images), z_dim, device=device)</span><br><span class="line">noise_and_labels = combine_vectors(fake_noise, one_hot_labels)</span><br><span class="line">gen = Generator(generator_input_dim).to(device)</span><br><span class="line">gen.load_state_dict(torch.load(best_gen_name))</span><br><span class="line"></span><br><span class="line">fake = gen(noise_and_labels)</span><br><span class="line">show_tensor_images(torch.cat([train_images.cpu(), fake.cpu()]))</span><br></pre></td></tr></table></figure><p><img src="output_26_0.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GAN-for-Data-Augmentation&quot;&gt;&lt;a href=&quot;#GAN-for-Data-Augmentation&quot; class=&quot;headerlink&quot; title=&quot;GAN for Data Augmentation&quot;&gt;&lt;/a&gt;GAN for Dat
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Implement your agent</title>
    <link href="https://zhangruochi.com/Implement-your-agent/2020/10/19/"/>
    <id>https://zhangruochi.com/Implement-your-agent/2020/10/19/</id>
    <published>2020-10-19T08:45:27.000Z</published>
    <updated>2020-10-19T08:46:39.371Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Implement-your-agent"><a href="#Assignment-2-Implement-your-agent" class="headerlink" title="Assignment 2 - Implement your agent"></a>Assignment 2 - Implement your agent</h1><p>Welcome to Course 4, Programming Assignment 2! We have learned about reinforcement learning algorithms for prediction and control in previous courses and extended those algorithms to large state spaces using function approximation. One example of this was in assignment 2 of course 3 where we implemented semi-gradient TD for prediction and used a neural network as the function approximator. In this notebook, we will build a reinforcement learning agent for control, again using a neural network for function approximation. This combination of neural network function approximators and reinforcement learning algorithms, often referred to as Deep RL, is an active area of research and has led to many impressive results (e. g., AlphaGo: <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" target="_blank" rel="noopener">https://deepmind.com/research/case-studies/alphago-the-story-so-far</a>).</p><p><strong>In this assignment, you will:</strong></p><ol><li>Extend the neural network code from assignment 2 of course 3 to output action-values instead of state-values.</li><li>Write up the Adam algorithm for neural network optimization.</li><li>Understand experience replay buffers.</li><li>Implement Softmax action-selection.</li><li>Build an Expected Sarsa agent by putting all the pieces together.</li><li>Solve Lunar Lander with your agent.</li></ol><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><ul><li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li><li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> : Library for plotting graphs in Python.</li><li><a href="http://www.jmlr.org/papers/v10/tanner09a.html" target="_blank" rel="noopener">RL-Glue</a>, BaseEnvironment, BaseAgent : Library and abstract classes to inherit from  for reinforcement learning experiments.</li><li><a href="https://gym.openai.com/envs/LunarLander-v2/" target="_blank" rel="noopener">LunarLanderEnvironment</a> : An RLGlue environment that wraps a LundarLander environment implementation from OpenAI Gym.</li><li><a href="https://docs.python.org/3/library/collections.html#collections.deque" target="_blank" rel="noopener">collections.deque</a>: a double-ended queue implementation. We use deque to implement the experience replay buffer.</li><li><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" target="_blank" rel="noopener">copy.deepcopy</a>: As objects are not passed by value in python, we often need to make copies of mutable objects. copy.deepcopy allows us to make a new object with the same contents as another object. (Take a look at this link if you are interested to learn more: <a href="https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/" target="_blank" rel="noopener">https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/</a>)</li><li><a href="https://github.com/tqdm/tqdm" target="_blank" rel="noopener">tqdm</a> : A package to display progress bar when running experiments</li><li><a href="https://docs.python.org/3/library/os.html" target="_blank" rel="noopener">os</a>: Package used to interface with the operating system. Here we use it for creating a results folder when it does not exist.</li><li><a href="https://docs.python.org/3/library/shutil.html" target="_blank" rel="noopener">shutil</a>: Package used to operate on files and folders. Here we use it for creating a zip file of the results folder.</li><li>plot_script: Used for plotting learning curves using matplotlib.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="comment"># DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> environment <span class="keyword">import</span> BaseEnvironment</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lunar_lander <span class="keyword">import</span> LunarLanderEnvironment</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> plot_script <span class="keyword">import</span> plot_result</span><br></pre></td></tr></table></figure><h2 id="Section-1-Action-Value-Network"><a href="#Section-1-Action-Value-Network" class="headerlink" title="Section 1: Action-Value Network"></a>Section 1: Action-Value Network</h2><p>This section includes the function approximator that we use in our agent, a neural network. In Course 3 Assignment 2, we used a neural network as the function approximator for a policy evaluation problem. In this assignment, we will use a neural network for approximating the action-value function in a control problem. The main difference between approximating a state-value function and an action-value function using a neural network is that in the former the output layer only includes one unit whereas in the latter the output layer includes as many units as the number of actions. </p><p>In the cell below, you will specify the architecture of the action-value neural network. More specifically, you will specify <code>self.layer_size</code> in the <code>__init__()</code> function. </p><p>We have already provided <code>get_action_values()</code> and <code>get_TD_update()</code> methods. The former computes the action-value function by doing a forward pass and the latter computes the gradient of the action-value function with respect to the weights times the TD error. These <code>get_action_values()</code> and <code>get_TD_update()</code> methods are similar to the <code>get_value()</code> and <code>get_gradient()</code> methods that you implemented in Course 3 Assignment 2. The main difference is that in this notebook, they are designed to be applied to batches of states instead of one state. You will later use these functions for implementing the agent.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Work Required: Yes. Fill in the code for layer_sizes in __init__ (~1 Line). </span></span><br><span class="line"><span class="comment"># Also go through the rest of the code to ensure your understanding is correct.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActionValueNetwork</span>:</span></span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the layer_sizes member variable (~1 Line).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, network_config)</span>:</span></span><br><span class="line">        self.state_dim = network_config.get(<span class="string">"state_dim"</span>)</span><br><span class="line">        self.num_hidden_units = network_config.get(<span class="string">"num_hidden_units"</span>)</span><br><span class="line">        self.num_actions = network_config.get(<span class="string">"num_actions"</span>)</span><br><span class="line">        </span><br><span class="line">        self.rand_generator = np.random.RandomState(network_config.get(<span class="string">"seed"</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Specify self.layer_size which shows the number of nodes in each layer</span></span><br><span class="line">        <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">        self.layer_sizes = [self.state_dim,self.num_hidden_units, self.num_actions]</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize the weights of the neural network</span></span><br><span class="line">        <span class="comment"># self.weights is an array of dictionaries with each dictionary corresponding to </span></span><br><span class="line">        <span class="comment"># the weights from one layer to the next. Each dictionary includes W and b</span></span><br><span class="line">        self.weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(self.layer_sizes) - <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(self.layer_sizes) - <span class="number">1</span>):</span><br><span class="line">            self.weights[i][<span class="string">'W'</span>] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + <span class="number">1</span>])</span><br><span class="line">            self.weights[i][<span class="string">'b'</span>] = np.zeros((<span class="number">1</span>, self.layer_sizes[i + <span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_action_values</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            s (Numpy array): The state.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action-values (Numpy array) calculated using the network's weights.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        W0, b0 = self.weights[<span class="number">0</span>][<span class="string">'W'</span>], self.weights[<span class="number">0</span>][<span class="string">'b'</span>]</span><br><span class="line">        psi = np.dot(s, W0) + b0</span><br><span class="line">        x = np.maximum(psi, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        W1, b1 = self.weights[<span class="number">1</span>][<span class="string">'W'</span>], self.weights[<span class="number">1</span>][<span class="string">'b'</span>]</span><br><span class="line">        q_vals = np.dot(x, W1) + b1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> q_vals</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_TD_update</span><span class="params">(self, s, delta_mat)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            s (Numpy array): The state.</span></span><br><span class="line"><span class="string">            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  </span></span><br><span class="line"><span class="string">            correspond to one state in the batch. Each row has only one non-zero element </span></span><br><span class="line"><span class="string">            which is the TD-error corresponding to the action taken.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The TD update (Array of dictionaries with gradient times TD errors) for the network's weights</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        W0, b0 = self.weights[<span class="number">0</span>][<span class="string">'W'</span>], self.weights[<span class="number">0</span>][<span class="string">'b'</span>]</span><br><span class="line">        W1, b1 = self.weights[<span class="number">1</span>][<span class="string">'W'</span>], self.weights[<span class="number">1</span>][<span class="string">'b'</span>]</span><br><span class="line">        </span><br><span class="line">        psi = np.dot(s, W0) + b0</span><br><span class="line">        x = np.maximum(psi, <span class="number">0</span>)</span><br><span class="line">        dx = (psi &gt; <span class="number">0</span>).astype(float)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># td_update has the same structure as self.weights, that is an array of dictionaries.</span></span><br><span class="line">        <span class="comment"># td_update[0]["W"], td_update[0]["b"], td_update[1]["W"], and td_update[1]["b"] have the same shape as </span></span><br><span class="line">        <span class="comment"># self.weights[0]["W"], self.weights[0]["b"], self.weights[1]["W"], and self.weights[1]["b"] respectively</span></span><br><span class="line">        td_update = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.weights))]</span><br><span class="line">         </span><br><span class="line">        v = delta_mat</span><br><span class="line">        td_update[<span class="number">1</span>][<span class="string">'W'</span>] = np.dot(x.T, v) * <span class="number">1.</span> / s.shape[<span class="number">0</span>]</span><br><span class="line">        td_update[<span class="number">1</span>][<span class="string">'b'</span>] = np.sum(v, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>) * <span class="number">1.</span> / s.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        v = np.dot(v, W1.T) * dx</span><br><span class="line">        td_update[<span class="number">0</span>][<span class="string">'W'</span>] = np.dot(s.T, v) * <span class="number">1.</span> / s.shape[<span class="number">0</span>]</span><br><span class="line">        td_update[<span class="number">0</span>][<span class="string">'b'</span>] = np.sum(v, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>) * <span class="number">1.</span> / s.shape[<span class="number">0</span>]</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> td_update</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No. You may wish to read the relevant paper for more information on this weight initialization</span></span><br><span class="line">    <span class="comment"># (Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_saxe</span><span class="params">(self, rows, cols)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            rows (int): number of input units for layer.</span></span><br><span class="line"><span class="string">            cols (int): number of output units for layer.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tensor = self.rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (rows, cols))</span><br><span class="line">        <span class="keyword">if</span> rows &lt; cols:</span><br><span class="line">            tensor = tensor.T</span><br><span class="line">        tensor, r = np.linalg.qr(tensor)</span><br><span class="line">        d = np.diag(r, <span class="number">0</span>)</span><br><span class="line">        ph = np.sign(d)</span><br><span class="line">        tensor *= ph</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rows &lt; cols:</span><br><span class="line">            tensor = tensor.T</span><br><span class="line">        <span class="keyword">return</span> tensor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">            A copy of the current weights of this network.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> deepcopy(self.weights)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_weights</span><span class="params">(self, weights)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            weights (list of dictionaries): Consists of weights that this network will set as its own weights.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.weights = deepcopy(weights)</span><br></pre></td></tr></table></figure><p>Run the cell below to test your implementation of the <code>__init__()</code> function for ActionValueNetwork:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for ActionValueNetwork __init__() ## </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;</span><br><span class="line">    <span class="string">"state_dim"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">"num_actions"</span>: <span class="number">3</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_network = ActionValueNetwork(network_config)</span><br><span class="line">print(<span class="string">"layer_sizes:"</span>, test_network.layer_sizes)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_network.layer_sizes, np.array([<span class="number">5</span>, <span class="number">20</span>, <span class="number">3</span>])))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)"</span>)</span><br></pre></td></tr></table></figure><pre><code>layer_sizes: [5, 20, 3]Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre><p><strong>Expected output:</strong></p><pre><code>layer_sizes: [ 5 20  3]</code></pre><h2 id="Section-2-Adam-Optimizer"><a href="#Section-2-Adam-Optimizer" class="headerlink" title="Section 2: Adam Optimizer"></a>Section 2: Adam Optimizer</h2><p>In this assignment, you will use the Adam algorithm for updating the weights of your action-value network. As you may remember from Course 3 Assignment 2, the Adam algorithm is a more advanced variant of stochastic gradient descent (SGD). The Adam algorithm improves the SGD update with two concepts: adaptive vector stepsizes and momentum. It keeps running estimates of the mean and second moment of the updates, denoted by $\mathbf{m}$ and $\mathbf{v}$ respectively:</p><script type="math/tex; mode=display">\mathbf{m_t} = \beta_m \mathbf{m_{t-1}} + (1 - \beta_m)g_t \\\mathbf{v_t} = \beta_v \mathbf{v_{t-1}} + (1 - \beta_v)g^2_t</script><p>Here, $\beta_m$ and $\beta_v$ are fixed parameters controlling the linear combinations above and $g_t$ is the update at time $t$ (generally the gradients, but here the TD error times the gradients).</p><p>Given that $\mathbf{m}$ and $\mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $\mathbf{\hat{m}}$ and $\mathbf{\hat{v}}$ as:</p><script type="math/tex; mode=display">\mathbf{\hat{m}_t} = \frac{\mathbf{m_t}}{1 - \beta_m^t} \\\mathbf{\hat{v}_t} = \frac{\mathbf{v_t}}{1 - \beta_v^t}</script><p>The weights are then updated as follows:</p><script type="math/tex; mode=display">\mathbf{w_t} = \mathbf{w_{t-1}} + \frac{\alpha}{\sqrt{\mathbf{\hat{v}_t}}+\epsilon} \mathbf{\hat{m}_t}</script><p>Here, $\alpha$ is the step size parameter and $\epsilon$ is another small parameter to keep the denominator from being zero.</p><p>In the cell below, you will implement the <code>__init__()</code> and <code>update_weights()</code> methods for the Adam algorithm. In <code>__init__()</code>, you will initialize <code>self.m</code> and <code>self.v</code>. In <code>update_weights()</code>, you will compute new weights given the input weights and an update $g$ (here <code>td_errors_times_gradients</code>) according to the equations above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Work Required: Yes. Fill in code in __init__ and update_weights (~9-11 Lines).</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the initialization for self.m and self.v (~4 Lines).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer_sizes, </span></span></span><br><span class="line"><span class="function"><span class="params">                 optimizer_info)</span>:</span></span><br><span class="line">        self.layer_sizes = layer_sizes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Specify Adam algorithm's hyper parameters</span></span><br><span class="line">        self.step_size = optimizer_info.get(<span class="string">"step_size"</span>)</span><br><span class="line">        self.beta_m = optimizer_info.get(<span class="string">"beta_m"</span>)</span><br><span class="line">        self.beta_v = optimizer_info.get(<span class="string">"beta_v"</span>)</span><br><span class="line">        self.epsilon = optimizer_info.get(<span class="string">"epsilon"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize Adam algorithm's m and v</span></span><br><span class="line">        self.m = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(self.layer_sizes))]</span><br><span class="line">        self.v = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(self.layer_sizes))]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(self.layer_sizes) - <span class="number">1</span>):</span><br><span class="line">            <span class="comment">### START CODE HERE (~4 Lines)</span></span><br><span class="line">            <span class="comment"># Hint: The initialization for m and v should look very much like the initializations of the weights</span></span><br><span class="line">            <span class="comment"># except for the fact that initialization here is to zeroes (see description above.)</span></span><br><span class="line">            self.m[i][<span class="string">"W"</span>] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            self.m[i][<span class="string">"b"</span>] = np.zeros((<span class="number">1</span>,self.layer_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">"W"</span>] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">"b"</span>] = np.zeros((<span class="number">1</span>,self.layer_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            <span class="comment">### END CODE HERE</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to </span></span><br><span class="line">        <span class="comment"># the time step t. We can calculate these powers using an incremental product. At initialization then, </span></span><br><span class="line">        <span class="comment"># beta_m_product and beta_v_product should be ...? (Note that timesteps start at 1 and if we were to </span></span><br><span class="line">        <span class="comment"># start from 0, the denominator would be 0.)</span></span><br><span class="line">        self.beta_m_product = self.beta_m</span><br><span class="line">        self.beta_v_product = self.beta_v</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the weight updates (~5-7 lines).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weights</span><span class="params">(self, weights, td_errors_times_gradients)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            weights (Array of dictionaries): The weights of the neural network.</span></span><br><span class="line"><span class="string">            td_errors_times_gradients (Array of dictionaries): The gradient of the </span></span><br><span class="line"><span class="string">            action-values with respect to the network's weights times the TD-error</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The updated weights (Array of dictionaries).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights)):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> weights[i].keys():</span><br><span class="line">                <span class="comment">### START CODE HERE (~5-7 Lines)</span></span><br><span class="line">                <span class="comment"># Hint: Follow the equations above. First, you should update m and v and then compute </span></span><br><span class="line">                <span class="comment"># m_hat and v_hat. Finally, compute how much the weights should be incremented by.</span></span><br><span class="line">                <span class="comment"># self.m[i][param] = None</span></span><br><span class="line">                <span class="comment"># self.v[i][param] = None</span></span><br><span class="line">                <span class="comment"># m_hat = None</span></span><br><span class="line">                <span class="comment"># v_hat = None</span></span><br><span class="line">                self.m[i][param] = self.beta_m * self.m[i][param] + (<span class="number">1</span>-self.beta_m)*td_errors_times_gradients[i][param]</span><br><span class="line">                self.v[i][param] = self.beta_v * self.v[i][param] + (<span class="number">1</span>-self.beta_v)*(td_errors_times_gradients[i][param] * td_errors_times_gradients[i][param])</span><br><span class="line">                </span><br><span class="line">                m_hat = self.m[i][param]/(<span class="number">1</span> - self.beta_m_product)</span><br><span class="line">                v_hat = self.v[i][param]/(<span class="number">1</span> - self.beta_v_product)</span><br><span class="line">                </span><br><span class="line">                weight_update = (self.step_size * m_hat) / (np.sqrt(v_hat) + self.epsilon)</span><br><span class="line">                <span class="comment">### END CODE HERE</span></span><br><span class="line">                </span><br><span class="line">                weights[i][param] = weights[i][param] + weight_update</span><br><span class="line">        <span class="comment"># Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to </span></span><br><span class="line">        <span class="comment">### update self.beta_m_product and self.beta_v_product</span></span><br><span class="line">        self.beta_m_product *= self.beta_m</span><br><span class="line">        self.beta_v_product *= self.beta_v</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure><p>Run the following code to test your implementation of the <code>__init__()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for Adam __init__() ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;<span class="string">"state_dim"</span>: <span class="number">5</span>,</span><br><span class="line">                  <span class="string">"num_hidden_units"</span>: <span class="number">2</span>,</span><br><span class="line">                  <span class="string">"num_actions"</span>: <span class="number">3</span></span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">optimizer_info = &#123;<span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">                  <span class="string">"beta_m"</span>: <span class="number">0.99</span>,</span><br><span class="line">                  <span class="string">"beta_v"</span>: <span class="number">0.999</span>,</span><br><span class="line">                  <span class="string">"epsilon"</span>: <span class="number">0.0001</span></span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">network = ActionValueNetwork(network_config)</span><br><span class="line">test_adam = Adam(network.layer_sizes, optimizer_info)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"m[0][\"W\"] shape: &#123;&#125;"</span>.format(test_adam.m[<span class="number">0</span>][<span class="string">"W"</span>].shape))</span><br><span class="line">print(<span class="string">"m[0][\"b\"] shape: &#123;&#125;"</span>.format(test_adam.m[<span class="number">0</span>][<span class="string">"b"</span>].shape))</span><br><span class="line">print(<span class="string">"m[1][\"W\"] shape: &#123;&#125;"</span>.format(test_adam.m[<span class="number">1</span>][<span class="string">"W"</span>].shape))</span><br><span class="line">print(<span class="string">"m[1][\"b\"] shape: &#123;&#125;"</span>.format(test_adam.m[<span class="number">1</span>][<span class="string">"b"</span>].shape), <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.m[<span class="number">0</span>][<span class="string">"W"</span>].shape, np.array([<span class="number">5</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.m[<span class="number">0</span>][<span class="string">"b"</span>].shape, np.array([<span class="number">1</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.m[<span class="number">1</span>][<span class="string">"W"</span>].shape, np.array([<span class="number">2</span>, <span class="number">3</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.m[<span class="number">1</span>][<span class="string">"b"</span>].shape, np.array([<span class="number">1</span>, <span class="number">3</span>])))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"v[0][\"W\"] shape: &#123;&#125;"</span>.format(test_adam.v[<span class="number">0</span>][<span class="string">"W"</span>].shape))</span><br><span class="line">print(<span class="string">"v[0][\"b\"] shape: &#123;&#125;"</span>.format(test_adam.v[<span class="number">0</span>][<span class="string">"b"</span>].shape))</span><br><span class="line">print(<span class="string">"v[1][\"W\"] shape: &#123;&#125;"</span>.format(test_adam.v[<span class="number">1</span>][<span class="string">"W"</span>].shape))</span><br><span class="line">print(<span class="string">"v[1][\"b\"] shape: &#123;&#125;"</span>.format(test_adam.v[<span class="number">1</span>][<span class="string">"b"</span>].shape), <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.v[<span class="number">0</span>][<span class="string">"W"</span>].shape, np.array([<span class="number">5</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.v[<span class="number">0</span>][<span class="string">"b"</span>].shape, np.array([<span class="number">1</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.v[<span class="number">1</span>][<span class="string">"W"</span>].shape, np.array([<span class="number">2</span>, <span class="number">3</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.v[<span class="number">1</span>][<span class="string">"b"</span>].shape, np.array([<span class="number">1</span>, <span class="number">3</span>])))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.all(test_adam.m[<span class="number">0</span>][<span class="string">"W"</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.all(test_adam.m[<span class="number">0</span>][<span class="string">"b"</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.all(test_adam.m[<span class="number">1</span>][<span class="string">"W"</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.all(test_adam.m[<span class="number">1</span>][<span class="string">"b"</span>]==<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.all(test_adam.v[<span class="number">0</span>][<span class="string">"W"</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.all(test_adam.v[<span class="number">0</span>][<span class="string">"b"</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.all(test_adam.v[<span class="number">1</span>][<span class="string">"W"</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.all(test_adam.v[<span class="number">1</span>][<span class="string">"b"</span>]==<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)"</span>)</span><br></pre></td></tr></table></figure><pre><code>m[0][&quot;W&quot;] shape: (5, 2)m[0][&quot;b&quot;] shape: (1, 2)m[1][&quot;W&quot;] shape: (2, 3)m[1][&quot;b&quot;] shape: (1, 3) v[0][&quot;W&quot;] shape: (5, 2)v[0][&quot;b&quot;] shape: (1, 2)v[1][&quot;W&quot;] shape: (2, 3)v[1][&quot;b&quot;] shape: (1, 3) Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre><p><strong>Expected output:</strong></p><pre><code>m[0][&quot;W&quot;] shape: (5, 2)m[0][&quot;b&quot;] shape: (1, 2)m[1][&quot;W&quot;] shape: (2, 3)m[1][&quot;b&quot;] shape: (1, 3) v[0][&quot;W&quot;] shape: (5, 2)v[0][&quot;b&quot;] shape: (1, 2)v[1][&quot;W&quot;] shape: (2, 3)v[1][&quot;b&quot;] shape: (1, 3) </code></pre><p>Run the following code to test your implementation of the <code>update_weights()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for Adam update_weights() ##</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;<span class="string">"state_dim"</span>: <span class="number">5</span>,</span><br><span class="line">                  <span class="string">"num_hidden_units"</span>: <span class="number">2</span>,</span><br><span class="line">                  <span class="string">"num_actions"</span>: <span class="number">3</span></span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">optimizer_info = &#123;<span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">                  <span class="string">"beta_m"</span>: <span class="number">0.99</span>,</span><br><span class="line">                  <span class="string">"beta_v"</span>: <span class="number">0.999</span>,</span><br><span class="line">                  <span class="string">"epsilon"</span>: <span class="number">0.0001</span></span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">network = ActionValueNetwork(network_config)</span><br><span class="line">test_adam = Adam(network.layer_sizes, optimizer_info)</span><br><span class="line"></span><br><span class="line">rand_generator = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize m and v</span></span><br><span class="line">test_adam.m[<span class="number">0</span>][<span class="string">"W"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">test_adam.m[<span class="number">0</span>][<span class="string">"b"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">test_adam.m[<span class="number">1</span>][<span class="string">"W"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">test_adam.m[<span class="number">1</span>][<span class="string">"b"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">test_adam.v[<span class="number">0</span>][<span class="string">"W"</span>] = np.abs(rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">5</span>, <span class="number">2</span>)))</span><br><span class="line">test_adam.v[<span class="number">0</span>][<span class="string">"b"</span>] = np.abs(rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line">test_adam.v[<span class="number">1</span>][<span class="string">"W"</span>] = np.abs(rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">test_adam.v[<span class="number">1</span>][<span class="string">"b"</span>] = np.abs(rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify weights</span></span><br><span class="line">weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(test_adam.layer_sizes))]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"W"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"b"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"W"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"b"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify g</span></span><br><span class="line">g = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(test_adam.layer_sizes))]</span><br><span class="line">g[<span class="number">0</span>][<span class="string">"W"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">g[<span class="number">0</span>][<span class="string">"b"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">g[<span class="number">1</span>][<span class="string">"W"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">g[<span class="number">1</span>][<span class="string">"b"</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">updated_weights = test_adam.update_weights(weights, g)</span><br><span class="line"></span><br><span class="line"><span class="comment"># updated weights asserts</span></span><br><span class="line">updated_weights_answer = np.load(<span class="string">"asserts/update_weights.npz"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"updated_weights[0][\"W\"]\n"</span>, updated_weights[<span class="number">0</span>][<span class="string">"W"</span>], <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"updated_weights[0][\"b\"]\n"</span>, updated_weights[<span class="number">0</span>][<span class="string">"b"</span>], <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"updated_weights[1][\"W\"]\n"</span>, updated_weights[<span class="number">1</span>][<span class="string">"W"</span>], <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"updated_weights[1][\"b\"]\n"</span>, updated_weights[<span class="number">1</span>][<span class="string">"b"</span>], <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">"W"</span>], updated_weights_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">"b"</span>], updated_weights_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">"W"</span>], updated_weights_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">"b"</span>], updated_weights_answer[<span class="string">"b1"</span>]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)"</span>)</span><br></pre></td></tr></table></figure><pre><code>updated_weights[0][&quot;W&quot;] [[-1.03112528  2.08618453] [-0.15531623  0.02412129] [-0.76656476 -0.65405898] [-0.92569612 -0.24916335] [-0.92180119  0.72137957]] updated_weights[0][&quot;b&quot;] [[-0.44392532 -0.69588495]] updated_weights[1][&quot;W&quot;] [[ 0.13962892  0.48820826  0.41311548] [ 0.3958054  -0.20738072 -0.47172585]] updated_weights[1][&quot;b&quot;] [[-0.48917533 -0.61934122 -1.48771198]] Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre><p><strong>Expected output:</strong></p><pre><code>updated_weights[0][&quot;W&quot;] [[-1.03112528  2.08618453] [-0.15531623  0.02412129] [-0.76656476 -0.65405898] [-0.92569612 -0.24916335] [-0.92180119  0.72137957]] updated_weights[0][&quot;b&quot;] [[-0.44392532 -0.69588495]] updated_weights[1][&quot;W&quot;] [[ 0.13962892  0.48820826  0.41311548] [ 0.3958054  -0.20738072 -0.47172585]] updated_weights[1][&quot;b&quot;] [[-0.48917533 -0.61934122 -1.48771198]] </code></pre><h2 id="Section-3-Experience-Replay-Buffers"><a href="#Section-3-Experience-Replay-Buffers" class="headerlink" title="Section 3: Experience Replay Buffers"></a>Section 3: Experience Replay Buffers</h2><p>In Course 3, you implemented agents that update value functions once for each sample. We can use a more efficient approach for updating value functions. You have seen an example of an efficient approach in Course 2 when implementing Dyna. The idea behind Dyna is to learn a model using sampled experience, obtain simulated experience from the model, and improve the value function using the simulated experience.</p><p>Experience replay is a simple method that can get some of the advantages of Dyna by saving a buffer of experience and using the data stored in the buffer as a model. This view of prior data as a model works because the data represents actual transitions from the underlying MDP. Furthermore, as a side note, this kind of model that is not learned and simply a collection of experience can be called non-parametric as it can be ever-growing as opposed to a parametric model where the transitions are learned to be represented with a fixed set of parameters or weights.</p><p>We have provided the implementation of the experience replay buffer in the cell below. ReplayBuffer includes two main functions: <code>append()</code> and <code>sample()</code>. <code>append()</code> adds an experience transition to the buffer as an array that includes the state, action, reward, terminal flag (indicating termination of the episode), and next_state. <code>sample()</code> gets a batch of experiences from the buffer with size <code>minibatch_size</code>.</p><p>You will use the <code>append()</code> and <code>sample()</code> functions when implementing the agent.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell! </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No. However, do go through the code to ensure your understanding is correct.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, minibatch_size, seed)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            size (integer): The size of the replay buffer.              </span></span><br><span class="line"><span class="string">            minibatch_size (integer): The sample size.</span></span><br><span class="line"><span class="string">            seed (integer): The seed for the random number generator. </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.buffer = []</span><br><span class="line">        self.minibatch_size = minibatch_size</span><br><span class="line">        self.rand_generator = np.random.RandomState(seed)</span><br><span class="line">        self.max_size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self, state, action, reward, terminal, next_state)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): The state.              </span></span><br><span class="line"><span class="string">            action (integer): The action.</span></span><br><span class="line"><span class="string">            reward (float): The reward.</span></span><br><span class="line"><span class="string">            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.</span></span><br><span class="line"><span class="string">            next_state (Numpy array): The next state.           </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> len(self.buffer) == self.max_size:</span><br><span class="line">            <span class="keyword">del</span> self.buffer[<span class="number">0</span>]</span><br><span class="line">        self.buffer.append([state, action, reward, terminal, next_state])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            A list of transition tuples including state, action, reward, terinal, and next_state</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)</span><br><span class="line">        <span class="keyword">return</span> [self.buffer[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> idxs]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.buffer)</span><br></pre></td></tr></table></figure><h2 id="Section-4-Softmax-Policy"><a href="#Section-4-Softmax-Policy" class="headerlink" title="Section 4: Softmax Policy"></a>Section 4: Softmax Policy</h2><p>In this assignment, you will use a softmax policy. One advantage of a softmax policy is that it explores according to the action-values, meaning that an action with a moderate value has a higher chance of getting selected compared to an action with a lower value. Contrast this with an $\epsilon$-greedy policy which does not consider the individual action values when choosing an exploratory action in a state and instead chooses randomly when doing so.</p><p>The probability of selecting each action according to the softmax policy is shown below:</p><script type="math/tex; mode=display">Pr{(A_t=a | S_t=s)} \hspace{0.1cm} \dot{=} \hspace{0.1cm} \frac{e^{Q(s, a)/\tau}}{\sum_{b \in A}e^{Q(s, b)/\tau}}</script><p>where $\tau$ is the temperature parameter which controls how much the agent focuses on the highest valued actions. The smaller the temperature, the more the agent selects the greedy action. Conversely, when the temperature is high, the agent selects among actions more uniformly random.</p><p>Given that a softmax policy exponentiates action values, if those values are large, exponentiating them could get very large. To implement the softmax policy in a numerically stable way, we often subtract the maximum action-value from the action-values. If we do so, the probability of selecting each action looks as follows:</p><script type="math/tex; mode=display">Pr{(A_t=a | S_t=s)} \hspace{0.1cm} \dot{=} \hspace{0.1cm} \frac{e^{Q(s, a)/\tau - max_{c}Q(s, c)/\tau}}{\sum_{b \in A}e^{Q(s, b)/\tau - max_{c}Q(s, c)/\tau}}</script><p>In the cell below, you will implement the <code>softmax()</code> function. In order to do so, you could break the above computation into smaller steps:</p><ul><li>compute the preference, $H(a)$, for taking each action by dividing the action-values by the temperature parameter $\tau$,</li><li>subtract the maximum preference across the actions from the preferences to avoid overflow, and,</li><li>compute the probability of taking each action.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(action_values, tau=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). </span></span><br><span class="line"><span class="string">                       The action-values computed by an action-value network.              </span></span><br><span class="line"><span class="string">        tau (float): The temperature parameter scalar.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over</span></span><br><span class="line"><span class="string">        the actions representing the policy.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (~2 Lines)</span></span><br><span class="line">    <span class="comment"># Compute the preferences by dividing the action-values by the temperature parameter tau</span></span><br><span class="line">    preferences = action_values / tau</span><br><span class="line">    <span class="comment"># Compute the maximum preference across the actions</span></span><br><span class="line">    max_preference = np.max(preferences,axis = <span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting </span></span><br><span class="line">    <span class="comment"># when subtracting the maximum preference from the preference of each action.</span></span><br><span class="line">    reshaped_max_preference = max_preference.reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~2 Lines)</span></span><br><span class="line">    <span class="comment"># Compute the numerator, i.e., the exponential of the preference - the max preference.</span></span><br><span class="line">    exp_preferences = np.exp(preferences - reshaped_max_preference)</span><br><span class="line">    <span class="comment"># Compute the denominator, i.e., the sum over the numerator along the actions axis.</span></span><br><span class="line">    sum_of_exp_preferences = np.sum(exp_preferences,axis = <span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting </span></span><br><span class="line">    <span class="comment"># when dividing the numerator by the denominator.</span></span><br><span class="line">    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Lines)</span></span><br><span class="line">    <span class="comment"># Compute the action probabilities according to the equation in the previous cell.</span></span><br><span class="line">    action_probs = exp_preferences / reshaped_sum_of_exp_preferences</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># squeeze() removes any singleton dimensions. It is used here because this function is used in the </span></span><br><span class="line">    <span class="comment"># agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in </span></span><br><span class="line">    <span class="comment"># the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.</span></span><br><span class="line">    action_probs = action_probs.squeeze()</span><br><span class="line">    <span class="keyword">return</span> action_probs</span><br></pre></td></tr></table></figure><p>Run the cell below to test your implementation of the <code>softmax()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for softmax() ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">rand_generator = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line">action_values = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">tau = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">action_probs = softmax(action_values, tau)</span><br><span class="line">print(<span class="string">"action_probs"</span>, action_probs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(action_probs, np.array([</span><br><span class="line">    [<span class="number">0.25849645</span>, <span class="number">0.01689625</span>, <span class="number">0.05374514</span>, <span class="number">0.67086216</span>],</span><br><span class="line">    [<span class="number">0.84699852</span>, <span class="number">0.00286345</span>, <span class="number">0.13520063</span>, <span class="number">0.01493741</span>]</span><br><span class="line">])))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)"</span>)</span><br></pre></td></tr></table></figure><pre><code>action_probs [[0.25849645 0.01689625 0.05374514 0.67086216] [0.84699852 0.00286345 0.13520063 0.01493741]]Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre><p><strong>Expected output:</strong></p><pre><code>action_probs [[0.25849645 0.01689625 0.05374514 0.67086216] [0.84699852 0.00286345 0.13520063 0.01493741]]</code></pre><h2 id="Section-5-Putting-the-pieces-together"><a href="#Section-5-Putting-the-pieces-together" class="headerlink" title="Section 5: Putting the pieces together"></a>Section 5: Putting the pieces together</h2><p>In this section, you will combine components from the previous sections to write up an RL-Glue Agent. The main component that you will implement is the action-value network updates with experience sampled from the experience replay buffer.</p><p>At time $t$, we have an action-value function represented as a neural network, say $Q_t$. We want to update our action-value function and get a new one we can use at the next timestep. We will get this $Q_{t+1}$ using multiple replay steps that each result in an intermediate action-value function $Q_{t+1}^{i}$ where $i$ indexes which replay step we are at.</p><p>In each replay step, we sample a batch of experiences from the replay buffer and compute a minibatch Expected-SARSA update. Across these N replay steps, we will use the current “un-updated” action-value network at time $t$, $Q_t$, for computing the action-values of the next-states. This contrasts using the most recent action-values from the last replay step $Q_{t+1}^{i}$. We make this choice to have targets that are stable across replay steps. Here is the pseudocode for performing the updates:</p><script type="math/tex; mode=display">\begin{align}& Q_t \leftarrow \text{action-value network at timestep t (current action-value network)}\\& \text{Initialize } Q_{t+1}^1 \leftarrow Q_t\\& \text{For } i \text{ in } [1, ..., N] \text{ (i.e. N} \text{  replay steps)}:\\& \hspace{1cm} s, a, r, t, s'\leftarrow \text{Sample batch of experiences from experience replay buffer} \\& \hspace{1cm} \text{Do Expected Sarsa update with } Q_t: Q_{t+1}^{i+1}(s, a) \leftarrow Q_{t+1}^{i}(s, a) + \alpha \cdot \left[r + \gamma \left(\sum_{b} \pi(b | s') Q_t(s', b)\right) - Q_{t+1}^{i}(s, a)\right]\\& \hspace{1.5cm} \text{ making sure to add the } \gamma \left(\sum_{b} \pi(b | s') Q_t(s', b)\right) \text{ for non-terminal transitions only.} \\& \text{After N replay steps, we set } Q_{t+1}^{N} \text{ as } Q_{t+1} \text{ and have a new } Q_{t+1} \text{for time step } t + 1 \text{ that we will fix in the next set of updates. }\end{align}</script><p>As you can see in the pseudocode, after sampling a batch of experiences, we do many computations. The basic idea however is that we are looking to compute a form of a TD error. In order to so, we can take the following steps:</p><ul><li>compute the action-values for the next states using the action-value network $Q_{t}$,</li><li>compute the policy $\pi(b | s’)$ induced by the action-values $Q_{t}$ (using the softmax function you implemented before),</li><li>compute the Expected sarsa targets $r + \gamma \left(\sum_{b} \pi(b | s’) Q_t(s’, b)\right)$,</li><li>compute the action-values for the current states using the latest $Q_{t + 1}$, and,</li><li>compute the TD-errors with the Expected Sarsa targets.</li></ul><p>For the third step above, you can start by computing $\pi(b | s’) Q_t(s’, b)$ followed by summation to get $\hat{v}_\pi(s’) = \left(\sum_{b} \pi(b | s’) Q_t(s’, b)\right)$. $\hat{v}_\pi(s’)$ is an estimate of the value of the next state. Note for terminal next states, $\hat{v}_\pi(s’) = 0$. Finally, we add the rewards to the discount times $\hat{v}_\pi(s’)$.</p><p>You will implement these steps in the <code>get_td_error()</code> function below which given a batch of experiences (including states, next_states, actions, rewards, terminals), fixed action-value network (current_q), and action-value network (network), computes the TD error in the form of a 1D array of size batch_size.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Work Required: Yes. Fill in code in get_td_error (~9 Lines).</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_td_error</span><span class="params">(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        states (Numpy array): The batch of states with the shape (batch_size, state_dim).</span></span><br><span class="line"><span class="string">        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).</span></span><br><span class="line"><span class="string">        actions (Numpy array): The batch of actions with the shape (batch_size,).</span></span><br><span class="line"><span class="string">        rewards (Numpy array): The batch of rewards with the shape (batch_size,).</span></span><br><span class="line"><span class="string">        discount (float): The discount factor.</span></span><br><span class="line"><span class="string">        terminals (Numpy array): The batch of terminals with the shape (batch_size,).</span></span><br><span class="line"><span class="string">        network (ActionValueNetwork): The latest state of the network that is getting replay updates.</span></span><br><span class="line"><span class="string">        current_q (ActionValueNetwork): The fixed network used for computing the targets, </span></span><br><span class="line"><span class="string">                                        and particularly, the action-values at the next-states.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The TD errors (Numpy array) for actions taken, of shape (batch_size,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Note: Here network is the latest state of the network that is getting replay updates. In other words, </span></span><br><span class="line">    <span class="comment"># the network represents Q_&#123;t+1&#125;^&#123;i&#125; whereas current_q represents Q_t, the fixed network used for computing the </span></span><br><span class="line">    <span class="comment"># targets, and particularly, the action-values at the next-states.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute action values at next states using current_q network</span></span><br><span class="line">    <span class="comment"># Note that q_next_mat is a 2D array of shape (batch_size, num_actions)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    q_next_mat = current_q.get_action_values(next_states)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute policy at next state by passing the action-values in q_next_mat to softmax()</span></span><br><span class="line">    <span class="comment"># Note that probs_mat is a 2D array of shape (batch_size, num_actions)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    probs_mat = softmax(q_next_mat,tau)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the estimate of the next state value, v_next_vec.</span></span><br><span class="line">    <span class="comment"># Hint: sum the action-values for the next_states weighted by the policy, probs_mat. Then, multiply by</span></span><br><span class="line">    <span class="comment"># (1 - terminals) to make sure v_next_vec is zero for terminal next states.</span></span><br><span class="line">    <span class="comment"># Note that v_next_vec is a 1D array of shape (batch_size,)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~3 Lines)</span></span><br><span class="line">    v_next_vec = np.sum(q_next_mat * probs_mat , axis = <span class="number">1</span>) * (<span class="number">1</span>-terminals)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute Expected Sarsa target</span></span><br><span class="line">    <span class="comment"># Note that target_vec is a 1D array of shape (batch_size,)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    target_vec = rewards + discount * v_next_vec</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute action values at the current states for all actions using network</span></span><br><span class="line">    <span class="comment"># Note that q_mat is a 2D array of shape (batch_size, num_actions)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    q_mat = network.get_action_values(states)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Batch Indices is an array from 0 to the batch size - 1. </span></span><br><span class="line">    batch_indices = np.arange(q_mat.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute q_vec by selecting q(s, a) from q_mat for taken actions</span></span><br><span class="line">    <span class="comment"># Use batch_indices as the index for the first dimension of q_mat</span></span><br><span class="line">    <span class="comment"># Note that q_vec is a 1D array of shape (batch_size)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    q_vec = q_mat[batch_indices,actions]</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute TD errors for actions taken</span></span><br><span class="line">    <span class="comment"># Note that delta_vec is a 1D array of shape (batch_size)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    delta_vec = target_vec - q_vec</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> delta_vec</span><br></pre></td></tr></table></figure><p>Run the following code to test your implementation of the <code>get_td_error()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for get_td_error() ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">data = np.load(<span class="string">"asserts/get_td_error_1.npz"</span>, allow_pickle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">states = data[<span class="string">"states"</span>]</span><br><span class="line">next_states = data[<span class="string">"next_states"</span>]</span><br><span class="line">actions = data[<span class="string">"actions"</span>]</span><br><span class="line">rewards = data[<span class="string">"rewards"</span>]</span><br><span class="line">discount = data[<span class="string">"discount"</span>]</span><br><span class="line">terminals = data[<span class="string">"terminals"</span>]</span><br><span class="line">tau = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;<span class="string">"state_dim"</span>: <span class="number">8</span>,</span><br><span class="line">                  <span class="string">"num_hidden_units"</span>: <span class="number">512</span>,</span><br><span class="line">                  <span class="string">"num_actions"</span>: <span class="number">4</span></span><br><span class="line">                  &#125;</span><br><span class="line"></span><br><span class="line">network = ActionValueNetwork(network_config)</span><br><span class="line">network.set_weights(data[<span class="string">"network_weights"</span>])</span><br><span class="line"></span><br><span class="line">current_q = ActionValueNetwork(network_config)</span><br><span class="line">current_q.set_weights(data[<span class="string">"current_q_weights"</span>])</span><br><span class="line"></span><br><span class="line">delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)</span><br><span class="line">answer_delta_vec = data[<span class="string">"delta_vec"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(delta_vec, answer_delta_vec))</span><br><span class="line">print(<span class="string">"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)"</span>)</span><br></pre></td></tr></table></figure><pre><code>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre><p>Now that you implemented the <code>get_td_error()</code> function, you can use it to implement the <code>optimize_network()</code> function. In this function, you will:</p><ul><li>get the TD-errors vector from <code>get_td_error()</code>,</li><li>make the TD-errors into a matrix using zeroes for actions not taken in the transitions,</li><li>pass the TD-errors matrix to the <code>get_TD_update()</code> function of network to calculate the gradients times TD errors, and,</li><li>perform an ADAM optimizer step.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Work Required: Yes. Fill in code in optimize_network (~2 Lines).</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_network</span><span class="params">(experiences, discount, optimizer, network, current_q, tau)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        experiences (Numpy array): The batch of experiences including the states, actions, </span></span><br><span class="line"><span class="string">                                   rewards, terminals, and next_states.</span></span><br><span class="line"><span class="string">        discount (float): The discount factor.</span></span><br><span class="line"><span class="string">        network (ActionValueNetwork): The latest state of the network that is getting replay updates.</span></span><br><span class="line"><span class="string">        current_q (ActionValueNetwork): The fixed network used for computing the targets, </span></span><br><span class="line"><span class="string">                                        and particularly, the action-values at the next-states.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get states, action, rewards, terminals, and next_states from experiences</span></span><br><span class="line">    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))</span><br><span class="line">    states = np.concatenate(states)</span><br><span class="line">    next_states = np.concatenate(next_states)</span><br><span class="line">    rewards = np.array(rewards)</span><br><span class="line">    terminals = np.array(terminals)</span><br><span class="line">    batch_size = states.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute TD error using the get_td_error function</span></span><br><span class="line">    <span class="comment"># Note that q_vec is a 1D array of shape (batch_size)</span></span><br><span class="line">    delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Batch Indices is an array from 0 to the batch_size - 1. </span></span><br><span class="line">    batch_indices = np.arange(batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make a td error matrix of shape (batch_size, num_actions)</span></span><br><span class="line">    <span class="comment"># delta_mat has non-zero value only for actions taken</span></span><br><span class="line">    delta_mat = np.zeros((batch_size, network.num_actions))</span><br><span class="line">    delta_mat[batch_indices, actions] = delta_vec</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pass delta_mat to compute the TD errors times the gradients of the network's weights from back-propagation</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE</span></span><br><span class="line">    td_update = network.get_TD_update(states,delta_mat)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Pass network.get_weights and the td_update to the optimizer to get updated weights</span></span><br><span class="line">    <span class="comment">### START CODE HERE</span></span><br><span class="line">    weights = optimizer.update_weights(network.get_weights(), td_update)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    network.set_weights(weights)</span><br></pre></td></tr></table></figure><p>Run the following code to test your implementation of the <code>optimize_network()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for optimize_network() ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">input_data = np.load(<span class="string">"asserts/optimize_network_input_1.npz"</span>, allow_pickle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">experiences = list(input_data[<span class="string">"experiences"</span>])</span><br><span class="line">discount = input_data[<span class="string">"discount"</span>]</span><br><span class="line">tau = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;<span class="string">"state_dim"</span>: <span class="number">8</span>,</span><br><span class="line">                  <span class="string">"num_hidden_units"</span>: <span class="number">512</span>,</span><br><span class="line">                  <span class="string">"num_actions"</span>: <span class="number">4</span></span><br><span class="line">                  &#125;</span><br><span class="line"></span><br><span class="line">network = ActionValueNetwork(network_config)</span><br><span class="line">network.set_weights(input_data[<span class="string">"network_weights"</span>])</span><br><span class="line"></span><br><span class="line">current_q = ActionValueNetwork(network_config)</span><br><span class="line">current_q.set_weights(input_data[<span class="string">"current_q_weights"</span>])</span><br><span class="line"></span><br><span class="line">optimizer_config = &#123;<span class="string">'step_size'</span>: <span class="number">3e-5</span>, </span><br><span class="line">                    <span class="string">'beta_m'</span>: <span class="number">0.9</span>, </span><br><span class="line">                    <span class="string">'beta_v'</span>: <span class="number">0.999</span>,</span><br><span class="line">                    <span class="string">'epsilon'</span>: <span class="number">1e-8</span></span><br><span class="line">                   &#125;</span><br><span class="line">optimizer = Adam(network.layer_sizes, optimizer_config)</span><br><span class="line">optimizer.m = input_data[<span class="string">"optimizer_m"</span>]</span><br><span class="line">optimizer.v = input_data[<span class="string">"optimizer_v"</span>]</span><br><span class="line">optimizer.beta_m_product = input_data[<span class="string">"optimizer_beta_m_product"</span>]</span><br><span class="line">optimizer.beta_v_product = input_data[<span class="string">"optimizer_beta_v_product"</span>]</span><br><span class="line"></span><br><span class="line">optimize_network(experiences, discount, optimizer, network, current_q, tau)</span><br><span class="line">updated_weights = network.get_weights()</span><br><span class="line"></span><br><span class="line">output_data = np.load(<span class="string">"asserts/optimize_network_output_1.npz"</span>, allow_pickle=<span class="keyword">True</span>)</span><br><span class="line">answer_updated_weights = output_data[<span class="string">"updated_weights"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">"W"</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">"W"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">"b"</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">"b"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">"W"</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">"W"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">"b"</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">"b"</span>]))</span><br><span class="line">print(<span class="string">"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)"</span>)</span><br></pre></td></tr></table></figure><pre><code>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre><p>Now that you implemented the <code>optimize_network()</code> function, you can implement the agent. In the cell below, you will fill the <code>agent_step()</code> and <code>agent_end()</code> functions. You should:</p><ul><li>select an action (only in <code>agent_step()</code>),</li><li>add transitions (consisting of the state, action, reward, terminal, and next state) to the replay buffer, and,</li><li>update the weights of the neural network by doing multiple replay steps and calling the <code>optimize_network()</code> function that you implemented above.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Work Required: Yes. Fill in code in agent_step and agent_end (~7 Lines).</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.name = <span class="string">"expected_sarsa_agent"</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_config)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_config dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            network_config: dictionary,</span></span><br><span class="line"><span class="string">            optimizer_config: dictionary,</span></span><br><span class="line"><span class="string">            replay_buffer_size: integer,</span></span><br><span class="line"><span class="string">            minibatch_sz: integer, </span></span><br><span class="line"><span class="string">            num_replay_updates_per_step: float</span></span><br><span class="line"><span class="string">            discount_factor: float,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.replay_buffer = ReplayBuffer(agent_config[<span class="string">'replay_buffer_size'</span>], </span><br><span class="line">                                          agent_config[<span class="string">'minibatch_sz'</span>], agent_config.get(<span class="string">"seed"</span>))</span><br><span class="line">        self.network = ActionValueNetwork(agent_config[<span class="string">'network_config'</span>])</span><br><span class="line">        self.optimizer = Adam(self.network.layer_sizes, agent_config[<span class="string">"optimizer_config"</span>])</span><br><span class="line">        self.num_actions = agent_config[<span class="string">'network_config'</span>][<span class="string">'num_actions'</span>]</span><br><span class="line">        self.num_replay = agent_config[<span class="string">'num_replay_updates_per_step'</span>]</span><br><span class="line">        self.discount = agent_config[<span class="string">'gamma'</span>]</span><br><span class="line">        self.tau = agent_config[<span class="string">'tau'</span>]</span><br><span class="line">        </span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_config.get(<span class="string">"seed"</span>))</span><br><span class="line">        </span><br><span class="line">        self.last_state = <span class="keyword">None</span></span><br><span class="line">        self.last_action = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        self.sum_rewards = <span class="number">0</span></span><br><span class="line">        self.episode_steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the action. </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        action_values = self.network.get_action_values(state)</span><br><span class="line">        probs_batch = softmax(action_values, self.tau)</span><br><span class="line">        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.sum_rewards = <span class="number">0</span></span><br><span class="line">        self.episode_steps = <span class="number">0</span></span><br><span class="line">        self.last_state = np.array([state])</span><br><span class="line">        self.last_action = self.policy(self.last_state)</span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the action selection, replay-buffer update, </span></span><br><span class="line">    <span class="comment"># weights update using optimize_network, and updating last_state and last_action (~5 lines).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment's step based, where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        self.sum_rewards += reward</span><br><span class="line">        self.episode_steps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Make state an array of shape (1, state_dim) to add a batch dimension and</span></span><br><span class="line">        <span class="comment"># to later match the get_action_values() and get_TD_update() functions</span></span><br><span class="line">        state = np.array([state])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select action</span></span><br><span class="line">        <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">        action = self.policy(state)</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Append new experience to replay buffer</span></span><br><span class="line">        <span class="comment"># Note: look at the replay_buffer append function for the order of arguments</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">        self.replay_buffer.append(self.last_state,self.last_action,reward,<span class="number">0</span>,state)</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform replay steps:</span></span><br><span class="line">        <span class="keyword">if</span> self.replay_buffer.size() &gt; self.replay_buffer.minibatch_size:</span><br><span class="line">            current_q = deepcopy(self.network)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.num_replay):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get sample experiences from the replay buffer</span></span><br><span class="line">                experiences = self.replay_buffer.sample()</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Call optimize_network to update the weights of the network (~1 Line)</span></span><br><span class="line">                <span class="comment">### START CODE HERE</span></span><br><span class="line">                optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau)</span><br><span class="line">                <span class="comment">### END CODE HERE</span></span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Update the last state and last action.</span></span><br><span class="line">        <span class="comment">### START CODE HERE (~2 Lines)</span></span><br><span class="line">        self.last_state = state</span><br><span class="line">        self.last_action = action</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the replay-buffer update and</span></span><br><span class="line">    <span class="comment"># update of the weights using optimize_network (~2 lines).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.sum_rewards += reward</span><br><span class="line">        self.episode_steps += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set terminal state to an array of zeros</span></span><br><span class="line">        state = np.zeros_like(self.last_state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append new experience to replay buffer</span></span><br><span class="line">        <span class="comment"># Note: look at the replay_buffer append function for the order of arguments</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">        self.replay_buffer.append(self.last_state,self.last_action,reward,<span class="number">1</span>,state)</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform replay steps:</span></span><br><span class="line">        <span class="keyword">if</span> self.replay_buffer.size() &gt; self.replay_buffer.minibatch_size:</span><br><span class="line">            current_q = deepcopy(self.network)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.num_replay):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get sample experiences from the replay buffer</span></span><br><span class="line">                experiences = self.replay_buffer.sample()</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Call optimize_network to update the weights of the network</span></span><br><span class="line">                <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">                optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau)</span><br><span class="line">                <span class="comment">### END CODE HERE</span></span><br><span class="line">                </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> message == <span class="string">"get_sum_reward"</span>:</span><br><span class="line">            <span class="keyword">return</span> self.sum_rewards</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">"Unrecognized Message!"</span>)</span><br></pre></td></tr></table></figure><p>Run the following code to test your implementation of the <code>agent_step()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for agent_step() ## </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">             <span class="string">'network_config'</span>: &#123;</span><br><span class="line">                 <span class="string">'state_dim'</span>: <span class="number">8</span>,</span><br><span class="line">                 <span class="string">'num_hidden_units'</span>: <span class="number">256</span>,</span><br><span class="line">                 <span class="string">'num_hidden_layers'</span>: <span class="number">1</span>,</span><br><span class="line">                 <span class="string">'num_actions'</span>: <span class="number">4</span></span><br><span class="line">             &#125;,</span><br><span class="line">             <span class="string">'optimizer_config'</span>: &#123;</span><br><span class="line">                 <span class="string">'step_size'</span>: <span class="number">3e-5</span>, </span><br><span class="line">                 <span class="string">'beta_m'</span>: <span class="number">0.9</span>, </span><br><span class="line">                 <span class="string">'beta_v'</span>: <span class="number">0.999</span>,</span><br><span class="line">                 <span class="string">'epsilon'</span>: <span class="number">1e-8</span></span><br><span class="line">             &#125;,</span><br><span class="line">             <span class="string">'replay_buffer_size'</span>: <span class="number">32</span>,</span><br><span class="line">             <span class="string">'minibatch_sz'</span>: <span class="number">32</span>,</span><br><span class="line">             <span class="string">'num_replay_updates_per_step'</span>: <span class="number">4</span>,</span><br><span class="line">             <span class="string">'gamma'</span>: <span class="number">0.99</span>,</span><br><span class="line">             <span class="string">'tau'</span>: <span class="number">1000.0</span>,</span><br><span class="line">             <span class="string">'seed'</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize agent</span></span><br><span class="line">agent = Agent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load agent network, optimizer, replay_buffer from the agent_input_1.npz file</span></span><br><span class="line">input_data = np.load(<span class="string">"asserts/agent_input_1.npz"</span>, allow_pickle=<span class="keyword">True</span>)</span><br><span class="line">agent.network.set_weights(input_data[<span class="string">"network_weights"</span>])</span><br><span class="line">agent.optimizer.m = input_data[<span class="string">"optimizer_m"</span>]</span><br><span class="line">agent.optimizer.v = input_data[<span class="string">"optimizer_v"</span>]</span><br><span class="line">agent.optimizer.beta_m_product = input_data[<span class="string">"optimizer_beta_m_product"</span>]</span><br><span class="line">agent.optimizer.beta_v_product = input_data[<span class="string">"optimizer_beta_v_product"</span>]</span><br><span class="line">agent.replay_buffer.rand_generator.seed(int(input_data[<span class="string">"replay_buffer_seed"</span>]))</span><br><span class="line"><span class="keyword">for</span> experience <span class="keyword">in</span> input_data[<span class="string">"replay_buffer"</span>]:</span><br><span class="line">    agent.replay_buffer.buffer.append(experience)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform agent_step multiple times</span></span><br><span class="line">last_state_array = input_data[<span class="string">"last_state_array"</span>]</span><br><span class="line">last_action_array = input_data[<span class="string">"last_action_array"</span>]</span><br><span class="line">state_array = input_data[<span class="string">"state_array"</span>]</span><br><span class="line">reward_array = input_data[<span class="string">"reward_array"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    agent.last_state = last_state_array[i]</span><br><span class="line">    agent.last_action = last_action_array[i]</span><br><span class="line">    state = state_array[i]</span><br><span class="line">    reward = reward_array[i]</span><br><span class="line">    </span><br><span class="line">    agent.agent_step(reward, state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Load expected values for last_state, last_action, weights, and replay_buffer </span></span><br><span class="line">    output_data = np.load(<span class="string">"asserts/agent_step_output_&#123;&#125;.npz"</span>.format(i), allow_pickle=<span class="keyword">True</span>)</span><br><span class="line">    answer_last_state = output_data[<span class="string">"last_state"</span>]</span><br><span class="line">    answer_last_action = output_data[<span class="string">"last_action"</span>]</span><br><span class="line">    answer_updated_weights = output_data[<span class="string">"updated_weights"</span>]</span><br><span class="line">    answer_replay_buffer = output_data[<span class="string">"replay_buffer"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for last_state and last_action</span></span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(answer_last_state, agent.last_state))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(answer_last_action, agent.last_action))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for replay_buffer </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(answer_replay_buffer.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(answer_replay_buffer.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">assert</span>(np.allclose(np.asarray(agent.replay_buffer.buffer)[i, j], answer_replay_buffer[i, j]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for network.weights</span></span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">0</span>][<span class="string">"W"</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">"W"</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">0</span>][<span class="string">"b"</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">"b"</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">1</span>][<span class="string">"W"</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">"W"</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">1</span>][<span class="string">"b"</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">"b"</span>]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)"</span>)</span><br></pre></td></tr></table></figure><pre><code>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre><p>Run the following code to test your implementation of the <code>agent_end()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for agent_end() ## </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">             <span class="string">'network_config'</span>: &#123;</span><br><span class="line">                 <span class="string">'state_dim'</span>: <span class="number">8</span>,</span><br><span class="line">                 <span class="string">'num_hidden_units'</span>: <span class="number">256</span>,</span><br><span class="line">                 <span class="string">'num_hidden_layers'</span>: <span class="number">1</span>,</span><br><span class="line">                 <span class="string">'num_actions'</span>: <span class="number">4</span></span><br><span class="line">             &#125;,</span><br><span class="line">             <span class="string">'optimizer_config'</span>: &#123;</span><br><span class="line">                 <span class="string">'step_size'</span>: <span class="number">3e-5</span>, </span><br><span class="line">                 <span class="string">'beta_m'</span>: <span class="number">0.9</span>, </span><br><span class="line">                 <span class="string">'beta_v'</span>: <span class="number">0.999</span>,</span><br><span class="line">                 <span class="string">'epsilon'</span>: <span class="number">1e-8</span></span><br><span class="line">             &#125;,</span><br><span class="line">             <span class="string">'replay_buffer_size'</span>: <span class="number">32</span>,</span><br><span class="line">             <span class="string">'minibatch_sz'</span>: <span class="number">32</span>,</span><br><span class="line">             <span class="string">'num_replay_updates_per_step'</span>: <span class="number">4</span>,</span><br><span class="line">             <span class="string">'gamma'</span>: <span class="number">0.99</span>,</span><br><span class="line">             <span class="string">'tau'</span>: <span class="number">1000</span>,</span><br><span class="line">             <span class="string">'seed'</span>: <span class="number">0</span></span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize agent</span></span><br><span class="line">agent = Agent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load agent network, optimizer, replay_buffer from the agent_input_1.npz file</span></span><br><span class="line">input_data = np.load(<span class="string">"asserts/agent_input_1.npz"</span>, allow_pickle=<span class="keyword">True</span>)</span><br><span class="line">agent.network.set_weights(input_data[<span class="string">"network_weights"</span>])</span><br><span class="line">agent.optimizer.m = input_data[<span class="string">"optimizer_m"</span>]</span><br><span class="line">agent.optimizer.v = input_data[<span class="string">"optimizer_v"</span>]</span><br><span class="line">agent.optimizer.beta_m_product = input_data[<span class="string">"optimizer_beta_m_product"</span>]</span><br><span class="line">agent.optimizer.beta_v_product = input_data[<span class="string">"optimizer_beta_v_product"</span>]</span><br><span class="line">agent.replay_buffer.rand_generator.seed(int(input_data[<span class="string">"replay_buffer_seed"</span>]))</span><br><span class="line"><span class="keyword">for</span> experience <span class="keyword">in</span> input_data[<span class="string">"replay_buffer"</span>]:</span><br><span class="line">    agent.replay_buffer.buffer.append(experience)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform agent_step multiple times</span></span><br><span class="line">last_state_array = input_data[<span class="string">"last_state_array"</span>]</span><br><span class="line">last_action_array = input_data[<span class="string">"last_action_array"</span>]</span><br><span class="line">state_array = input_data[<span class="string">"state_array"</span>]</span><br><span class="line">reward_array = input_data[<span class="string">"reward_array"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    agent.last_state = last_state_array[i]</span><br><span class="line">    agent.last_action = last_action_array[i]</span><br><span class="line">    reward = reward_array[i]</span><br><span class="line">    </span><br><span class="line">    agent.agent_end(reward)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load expected values for last_state, last_action, weights, and replay_buffer </span></span><br><span class="line">    output_data = np.load(<span class="string">"asserts/agent_end_output_&#123;&#125;.npz"</span>.format(i), allow_pickle=<span class="keyword">True</span>)</span><br><span class="line">    answer_updated_weights = output_data[<span class="string">"updated_weights"</span>]</span><br><span class="line">    answer_replay_buffer = output_data[<span class="string">"replay_buffer"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for replay_buffer </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(answer_replay_buffer.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(answer_replay_buffer.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">assert</span>(np.allclose(np.asarray(agent.replay_buffer.buffer)[i, j], answer_replay_buffer[i, j]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for network.weights</span></span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">0</span>][<span class="string">"W"</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">"W"</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">0</span>][<span class="string">"b"</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">"b"</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">1</span>][<span class="string">"W"</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">"W"</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">1</span>][<span class="string">"b"</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">"b"</span>]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)"</span>)</span><br></pre></td></tr></table></figure><pre><code>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre><h2 id="Section-6-Run-Experiment"><a href="#Section-6-Run-Experiment" class="headerlink" title="Section 6: Run Experiment"></a>Section 6: Run Experiment</h2><p>Now that you implemented the agent, we can use it to run an experiment on the Lunar Lander problem. We will plot the learning curve of the agent to visualize learning progress. To plot the learning curve, we use the sum of rewards in an episode as the performance measure. We have provided for you the experiment/plot code in the cell below which you can go ahead and run. Note that running the cell below has taken approximately 10 minutes in prior testing.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(environment, agent, environment_parameters, agent_parameters, experiment_parameters)</span>:</span></span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># save sum of reward at the end of each episode</span></span><br><span class="line">    agent_sum_reward = np.zeros((experiment_parameters[<span class="string">"num_runs"</span>], </span><br><span class="line">                                 experiment_parameters[<span class="string">"num_episodes"</span>]))</span><br><span class="line"></span><br><span class="line">    env_info = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    agent_info = agent_parameters</span><br><span class="line"></span><br><span class="line">    <span class="comment"># one agent setting</span></span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_runs"</span>]+<span class="number">1</span>):</span><br><span class="line">        agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">        agent_info[<span class="string">"network_config"</span>][<span class="string">"seed"</span>] = run</span><br><span class="line">        env_info[<span class="string">"seed"</span>] = run</span><br><span class="line"></span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> tqdm(range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_episodes"</span>]+<span class="number">1</span>)):</span><br><span class="line">            <span class="comment"># run episode</span></span><br><span class="line">            rl_glue.rl_episode(experiment_parameters[<span class="string">"timeout"</span>])</span><br><span class="line">            </span><br><span class="line">            episode_reward = rl_glue.rl_agent_message(<span class="string">"get_sum_reward"</span>)</span><br><span class="line">            agent_sum_reward[run - <span class="number">1</span>, episode - <span class="number">1</span>] = episode_reward</span><br><span class="line">    save_name = <span class="string">"&#123;&#125;"</span>.format(rl_glue.agent.name)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'results'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'results'</span>)</span><br><span class="line">    np.save(<span class="string">"results/sum_reward_&#123;&#125;"</span>.format(save_name), agent_sum_reward)</span><br><span class="line">    shutil.make_archive(<span class="string">'results'</span>, <span class="string">'zip'</span>, <span class="string">'results'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">300</span>,</span><br><span class="line">    <span class="comment"># OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after </span></span><br><span class="line">    <span class="comment"># some number of timesteps. Here we use the default of 1000.</span></span><br><span class="line">    <span class="string">"timeout"</span> : <span class="number">1000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;&#125;</span><br><span class="line"></span><br><span class="line">current_env = LunarLanderEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">'network_config'</span>: &#123;</span><br><span class="line">        <span class="string">'state_dim'</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">'num_hidden_units'</span>: <span class="number">256</span>,</span><br><span class="line">        <span class="string">'num_actions'</span>: <span class="number">4</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">'optimizer_config'</span>: &#123;</span><br><span class="line">        <span class="string">'step_size'</span>: <span class="number">1e-3</span>,</span><br><span class="line">        <span class="string">'beta_m'</span>: <span class="number">0.9</span>, </span><br><span class="line">        <span class="string">'beta_v'</span>: <span class="number">0.999</span>,</span><br><span class="line">        <span class="string">'epsilon'</span>: <span class="number">1e-8</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">'replay_buffer_size'</span>: <span class="number">50000</span>,</span><br><span class="line">    <span class="string">'minibatch_sz'</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">'num_replay_updates_per_step'</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">'gamma'</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">'tau'</span>: <span class="number">0.001</span></span><br><span class="line">&#125;</span><br><span class="line">current_agent = Agent</span><br><span class="line"></span><br><span class="line"><span class="comment"># run experiment</span></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br></pre></td></tr></table></figure><pre><code> 51%|█████     | 152/300 [07:08&lt;09:09,  3.71s/it]</code></pre><p>Run the cell below to see the comparison between the agent that you implemented and a random agent for the one run and 300 episodes. Note that the <code>plot_result()</code> function smoothes the learning curve by applying a sliding window on the performance measure. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_result([<span class="string">"expected_sarsa_agent"</span>, <span class="string">"random_agent"</span>])</span><br></pre></td></tr></table></figure><p>In the following cell you can visualize the performance of the agent with a correct implementation. As you can see, the agent initially crashes quite quickly (Episode 0). Then, the agent learns to avoid crashing by expending fuel and staying far above the ground. Finally however, it learns to land smoothly within the landing zone demarcated by the two flags (Episode 275).</p><p>In the learning curve above, you can see that sum of reward over episode has quite a high-variance at the beginning. However, the performance seems to be improving. The experiment that you ran was for 300 episodes and 1 run. To understand how the agent performs in the long run, we provide below the learning curve for the agent trained for 3000 episodes with performance averaged over 30 runs.<br><img src="3000_episodes.png" alt="Drawing" style="width: 500px;"><br>You can see that the agent learns a reasonably good policy within 3000 episodes, gaining sum of reward bigger than 200. Note that because of the high-variance in the agent performance, we also smoothed the learning curve. </p><h3 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up!"></a>Wrapping up!</h3><p>You have successfully implemented Course 4 Programming Assignment 2.</p><p>You have implemented an <strong>Expected Sarsa agent with a neural network and the Adam optimizer</strong> and used it for solving the Lunar Lander problem! You implemented different components of the agent including:</p><ul><li>a neural network for function approximation,</li><li>the Adam algorithm for optimizing the weights of the neural network,</li><li>a Softmax policy,</li><li>the replay steps for updating the action-value function using the experiences sampled from a replay buffer</li></ul><p>You tested the agent for a single parameter setting. In the next assignment, you will perform a parameter study on the step-size parameter to gain insight about the effect of step-size on the performance of your agent.</p><p>Note: Apart from using the <code>Submit</code> button in the notebook, you have to submit an additional zip file containing the ‘npy’ files that were generated from running the experiment cells. In order to do so:</p><ol><li>Generate the zip file by running the experiment cells in the notebook. On the top of the notebook, navigate to <code>File-&gt;Open</code> to open the directory view of this assignment. Select the checkbox next to <code>results.zip</code> and click on <code>Download.</code> Alternatively, you can download the results folder and run <code>zip -jr results.zip results/</code> (The flag ‘j’ is required by the grader!).</li><li>Go to the “My submission” tab on the programming assignment and click on “+ Create submission”.</li><li>Click on “PA2 Data-file Grader” and upload your results.zip.</li></ol><p><strong><em>These account for 25% of the marks, so don’t forget to do so!</em></strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Implement-your-agent&quot;&gt;&lt;a href=&quot;#Assignment-2-Implement-your-agent&quot; class=&quot;headerlink&quot; title=&quot;Assignment 2 - Implement y
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Average Reward Softmax Actor-Critic</title>
    <link href="https://zhangruochi.com/Average-Reward-Softmax-Actor-Critic/2020/10/16/"/>
    <id>https://zhangruochi.com/Average-Reward-Softmax-Actor-Critic/2020/10/16/</id>
    <published>2020-10-16T07:06:08.000Z</published>
    <updated>2020-10-16T07:07:03.295Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-4-Average-Reward-Softmax-Actor-Critic"><a href="#Assignment-4-Average-Reward-Softmax-Actor-Critic" class="headerlink" title="Assignment 4 - Average Reward Softmax Actor-Critic"></a>Assignment 4 - Average Reward Softmax Actor-Critic</h1><p>Welcome to your Course 3 Programming Assignment 4. In this assignment, you will implement <strong>Average Reward Softmax Actor-Critic</strong> in the Pendulum Swing-Up problem that you have seen earlier in the lecture. Through this assignment you will get hands-on experience in implementing actor-critic methods on a continuing task.</p><p><strong>In this assignment, you will:</strong></p><pre><code>1. Implement softmax actor-critic agent on a continuing task using the average reward formulation.2. Understand how to parameterize the policy as a function to learn, in a discrete action environment.3. Understand how to (approximately) sample the gradient of this objective to update the actor.4. Understand how to update the critic using differential TD error.</code></pre><h2 id="Pendulum-Swing-Up-Environment"><a href="#Pendulum-Swing-Up-Environment" class="headerlink" title="Pendulum Swing-Up Environment"></a>Pendulum Swing-Up Environment</h2><p>In this assignment, we will be using a Pendulum environment, adapted from <a href="http://www.incompleteideas.net/papers/SSR-98.pdf" target="_blank" rel="noopener">Santamaría et al. (1998)</a>. This is also the same environment that we used in the lecture. The diagram below illustrates the environment.</p><p><img src="pendulum_env.png" alt="Drawing" style="width: 400px;"></p><p>The environment consists of single pendulum that can swing 360 degrees. The pendulum is actuated by applying a torque on its pivot point. The goal is to get the pendulum to balance up-right from its resting position (hanging down at the bottom with no velocity) and maintain it as long as possible. The pendulum can move freely, subject only to gravity and the action applied by the agent. </p><p>The state is 2-dimensional, which consists of the current angle $\beta \in [-\pi, \pi]$ (angle from the vertical upright position) and current angular velocity $\dot{\beta} \in (-2\pi, 2\pi)$. The angular velocity is constrained in order to avoid damaging the pendulum system. If the angular velocity reaches this limit during simulation, the pendulum is reset to the resting position.<br>The action is the angular acceleration, with discrete values $a \in \{-1, 0, 1\}$ applied to the pendulum.<br>For more details on environment dynamics you can refer to the original paper.</p><p>The goal is to swing-up the pendulum and maintain its upright angle. Hence, the reward is the negative absolute angle from the vertical position: $R_{t} = -|\beta_{t}|$</p><p>Furthermore, since the goal is to reach and maintain a vertical position, there are no terminations nor episodes. Thus this problem can be formulated as a continuing task.</p><p>Similar to the Mountain Car task, the action in this pendulum environment is not strong enough to move the pendulum directly to the desired position. The agent must learn to first move the pendulum away from its desired position and gain enough momentum to successfully swing-up the pendulum. And even after reaching the upright position the agent must learn to continually balance the pendulum in this unstable position.</p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>You will use the following packages in this assignment.</p><ul><li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li><li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> : Library for plotting graphs in Python.</li><li><a href="http://www.jmlr.org/papers/v10/tanner09a.html" target="_blank" rel="noopener">RL-Glue</a> : Library for reinforcement learning experiments.</li><li><a href="https://alexhagen.github.io/jdc/" target="_blank" rel="noopener">jdc</a> : Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li><li><a href="https://tqdm.github.io/" target="_blank" rel="noopener">tqdm</a> : A package to display progress bar when running experiments</li><li>plot_script : custom script to plot results</li><li><a href="http://incompleteideas.net/tiles/tiles3.html" target="_blank" rel="noopener">tiles3</a> : A package that implements tile-coding.</li><li>pendulum_env : Pendulum Swing-up Environment</li></ul><p><strong>Please do not import other libraries</strong> — this will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="comment"># DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> pendulum_env <span class="keyword">import</span> PendulumEnvironment</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> plot_script</span><br><span class="line"><span class="keyword">import</span> tiles3 <span class="keyword">as</span> tc</span><br></pre></td></tr></table></figure><h2 id="Section-1-Create-Tile-Coding-Helper-Function"><a href="#Section-1-Create-Tile-Coding-Helper-Function" class="headerlink" title="Section 1: Create Tile Coding Helper Function"></a>Section 1: Create Tile Coding Helper Function</h2><p>In this section, we are going to build a tile coding class for our agent that will make it easier to make calls to our tile coder.</p><p>Tile-coding is introduced in Section 9.5.4 of the textbook as a way to create features that can both provide good generalization and discrimination. We have already used it in our last programming assignment as well. </p><p>Similar to the last programming assignment, we are going to make a function specific for tile coding for our Pendulum Swing-up environment. We will also use the <a href="http://incompleteideas.net/tiles/tiles3.html" target="_blank" rel="noopener">Tiles3 library</a>.</p><p>To get the tile coder working we need to:</p><pre><code>1) create an index hash table using tc.IHT(), 2) scale the inputs for the tile coder based on number of tiles and range of values each input could take3) call tc.tileswrap to get active tiles back.</code></pre><p>However, we need to make one small change to this tile coder.<br>Note that in this environment the state space contains angle, which is between $[-\pi, \pi]$. If we tile-code this state space in the usual way, the agent may think the value of states corresponding to an angle of $-\pi$ is very different from angle of $\pi$ when in fact they are the same! To remedy this and allow generalization between angle $= -\pi$ and angle $= \pi$, we need to use <strong>wrap tile coder</strong>.</p><p>The usage of wrap tile coder is almost identical to the original tile coder, except that we also need to provide the <code>wrapwidth</code> argument for the dimension we want to wrap over (hence only for angle, and <code>None</code> for angular velocity). More details of wrap tile coder is also provided in <a href="http://incompleteideas.net/tiles/tiles3.html" target="_blank" rel="noopener">Tiles3 library</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PendulumTileCoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, iht_size=<span class="number">4096</span>, num_tilings=<span class="number">32</span>, num_tiles=<span class="number">8</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initializes the MountainCar Tile Coder</span></span><br><span class="line"><span class="string">        Initializers:</span></span><br><span class="line"><span class="string">        iht_size -- int, the size of the index hash table, typically a power of 2</span></span><br><span class="line"><span class="string">        num_tilings -- int, the number of tilings</span></span><br><span class="line"><span class="string">        num_tiles -- int, the number of tiles. Here both the width and height of the tiles are the same</span></span><br><span class="line"><span class="string">                            </span></span><br><span class="line"><span class="string">        Class Variables:</span></span><br><span class="line"><span class="string">        self.iht -- tc.IHT, the index hash table that the tile coder will use</span></span><br><span class="line"><span class="string">        self.num_tilings -- int, the number of tilings the tile coder will use</span></span><br><span class="line"><span class="string">        self.num_tiles -- int, the number of tiles the tile coder will use</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        self.num_tilings = num_tilings</span><br><span class="line">        self.num_tiles = num_tiles </span><br><span class="line">        self.iht = tc.IHT(iht_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_tiles</span><span class="params">(self, angle, ang_vel)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Takes in an angle and angular velocity from the pendulum environment</span></span><br><span class="line"><span class="string">        and returns a numpy array of active tiles.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        angle -- float, the angle of the pendulum between -np.pi and np.pi</span></span><br><span class="line"><span class="string">        ang_vel -- float, the angular velocity of the agent between -2*np.pi and 2*np.pi</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        returns:</span></span><br><span class="line"><span class="string">        tiles -- np.array, active tiles</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Use the ranges above and scale the angle and angular velocity between [0, 1]</span></span><br><span class="line">        <span class="comment"># then multiply by the number of tiles so they are scaled between [0, self.num_tiles]</span></span><br><span class="line">        </span><br><span class="line">        angle_scaled = <span class="number">0</span></span><br><span class="line">        ang_vel_scaled = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        angle_scaled = (angle + np.pi) / (np.pi + np.pi) * self.num_tiles</span><br><span class="line">        ang_vel_scaled = (ang_vel + <span class="number">2</span>*np.pi) / ( <span class="number">2</span>*np.pi + <span class="number">2</span>*np.pi) * self.num_tiles</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get tiles by calling tc.tileswrap method</span></span><br><span class="line">        <span class="comment"># wrapwidths specify which dimension to wrap over and its wrapwidth</span></span><br><span class="line">        tiles = tc.tileswrap(self.iht, self.num_tilings, [angle_scaled, ang_vel_scaled], wrapwidths=[self.num_tiles, <span class="keyword">False</span>])</span><br><span class="line">                    </span><br><span class="line">        <span class="keyword">return</span> np.array(tiles)</span><br></pre></td></tr></table></figure><p>Run the following code to verify <code>PendulumTilecoder</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for PendulumTileCoder ##</span></span><br><span class="line"><span class="comment"># Your tile coder should also work for other num. tilings and num. tiles</span></span><br><span class="line">angles = np.linspace(-np.pi, np.pi, num=<span class="number">5</span>)</span><br><span class="line">vels = np.linspace(<span class="number">-2</span> * np.pi, <span class="number">2</span> * np.pi, num=<span class="number">5</span>)</span><br><span class="line">test_obs = list(itertools.product(angles, vels))</span><br><span class="line"></span><br><span class="line">pdtc = PendulumTileCoder(iht_size=<span class="number">4096</span>, num_tilings=<span class="number">8</span>, num_tiles=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">result=[]</span><br><span class="line"><span class="keyword">for</span> obs <span class="keyword">in</span> test_obs:</span><br><span class="line">    angle, ang_vel = obs</span><br><span class="line">    tiles = pdtc.get_tiles(angle=angle, ang_vel=ang_vel)</span><br><span class="line">    result.append(tiles)</span><br><span class="line">    </span><br><span class="line">expected = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">15</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">31</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">35</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">40</span>, <span class="number">39</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">40</span>, <span class="number">43</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">43</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">31</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">45</span>, <span class="number">46</span>, <span class="number">44</span>, <span class="number">47</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">35</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">40</span>, <span class="number">39</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">40</span>, <span class="number">43</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">43</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">15</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">45</span>, <span class="number">46</span>, <span class="number">44</span>, <span class="number">47</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">15</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">19</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(expected == np.array(result))</span><br></pre></td></tr></table></figure><h2 id="Section-2-Create-Average-Reward-Softmax-Actor-Critic-Agent"><a href="#Section-2-Create-Average-Reward-Softmax-Actor-Critic-Agent" class="headerlink" title="Section 2: Create Average Reward Softmax Actor-Critic Agent"></a>Section 2: Create Average Reward Softmax Actor-Critic Agent</h2><p>Now that we implemented PendulumTileCoder let’s create the agent that interacts with the environment. We will implement the same average reward Actor-Critic algorithm presented in the videos.</p><p>This agent has two components: an Actor and a Critic. The Actor learns a parameterized policy while the Critic learns a state-value function. The environment has discrete actions; your Actor implementation will use a softmax policy with exponentiated action-preferences. The Actor learns with the sample-based estimate for the gradient of the average reward objective. The Critic learns using the average reward version of the semi-gradient TD(0) algorithm.</p><p>In this section, you will be implementing <code>agent_policy</code>, <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>.</p><h2 id="Section-2-1-Implement-Helper-Functions"><a href="#Section-2-1-Implement-Helper-Functions" class="headerlink" title="Section 2-1: Implement Helper Functions"></a>Section 2-1: Implement Helper Functions</h2><p>Let’s first define a couple of useful helper functions.</p><h2 id="Section-2-1a-Compute-Softmax-Probability"><a href="#Section-2-1a-Compute-Softmax-Probability" class="headerlink" title="Section 2-1a: Compute Softmax Probability"></a>Section 2-1a: Compute Softmax Probability</h2><p>In this part you will implement <code>compute_softmax_prob</code>.</p><p>This function computes softmax probability for all actions, given actor weights <code>actor_w</code> and active tiles <code>tiles</code>. This function will be later used in <code>agent_policy</code> to sample appropriate action.</p><p>First, recall how the softmax policy is represented from state-action preferences: $\large \pi(a|s, \mathbf{\theta}) \doteq \frac{e^{h(s,a,\mathbf{\theta})}}{\sum_{b}e^{h(s,b,\mathbf{\theta})}}$.</p><p><strong>state-action preference</strong> is defined as $h(s,a, \mathbf{\theta}) \doteq \mathbf{\theta}^T \mathbf{x}_h(s,a)$.</p><p>Given active tiles <code>tiles</code> for state <code>s</code>, state-action preference $\mathbf{\theta}^T \mathbf{x}_h(s,a)$ can be computed by <code>actor_w[a][tiles].sum()</code>.</p><p>We will also use <strong>exp-normalize trick</strong>, in order to avoid possible numerical overflow.<br>Consider the following:</p><p>$\large \pi(a|s, \mathbf{\theta}) \doteq \frac{e^{h(s,a,\mathbf{\theta})}}{\sum_{b}e^{h(s,b,\mathbf{\theta})}} = \frac{e^{h(s,a,\mathbf{\theta}) - c} e^c}{\sum_{b}e^{h(s,b,\mathbf{\theta}) - c} e^c} = \frac{e^{h(s,a,\mathbf{\theta}) - c}}{\sum_{b}e^{h(s,b,\mathbf{\theta}) - c}}$ </p><p>$\pi(\cdot|s, \mathbf{\theta})$ is shift-invariant, and the policy remains the same when we subtract a constant $c \in \mathbb{R}$ from state-action preferences.</p><p>Normally we use $c = \max_b h(s,b, \mathbf{\theta})$, to prevent any overflow due to exponentiating large numbers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_softmax_prob</span><span class="params">(actor_w, tiles)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes softmax probability for all actions</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    actor_w - np.array, an array of actor weights</span></span><br><span class="line"><span class="string">    tiles - np.array, an array of active tiles</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    softmax_prob - np.array, an array of size equal to num. actions, and sums to 1.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First compute the list of state-action preferences (1~2 lines)</span></span><br><span class="line">    <span class="comment"># state_action_preferences = ? (list of size 3)</span></span><br><span class="line">    state_action_preferences = []</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    state_action_preferences = actor_w[:,tiles].sum(axis = <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the constant c by finding the maximum of state-action preferences (use np.max) (1 line)</span></span><br><span class="line">    <span class="comment"># c = ? (float)</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    c = np.max(state_action_preferences)</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the numerator by subtracting c from state-action preferences and exponentiating it (use np.exp) (1 line)</span></span><br><span class="line">    <span class="comment"># numerator = ? (list of size 3)</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    numerator = np.exp( state_action_preferences - c)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Next compute the denominator by summing the values in the numerator (use np.sum) (1 line)</span></span><br><span class="line">    <span class="comment"># denominator = ? (float)</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    denominator = np.sum(numerator)</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a probability array by dividing each element in numerator array by denominator (1 line)</span></span><br><span class="line">    <span class="comment"># We will store this probability array in self.softmax_prob as it will be useful later when updating the Actor</span></span><br><span class="line">    <span class="comment"># softmax_prob = ? (list of size 3)</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    softmax_prob = numerator / denominator</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> softmax_prob</span><br></pre></td></tr></table></figure><p>Run the following code to verify <code>compute_softmax_prob</code>.</p><p>We will test the method by building a softmax policy from state-action preferences [-1,1,2].</p><p>The sampling probability should then roughly match $[\frac{e^{-1}}{e^{-1}+e^1+e^2}, \frac{e^{1}}{e^{-1}+e^1+e^2}, \frac{e^2}{e^{-1}+e^1+e^2}] \approx$ [0.0351, 0.2595, 0.7054]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set tile-coder</span></span><br><span class="line">iht_size = <span class="number">4096</span></span><br><span class="line">num_tilings = <span class="number">8</span></span><br><span class="line">num_tiles = <span class="number">8</span></span><br><span class="line">test_tc = PendulumTileCoder(iht_size=iht_size, num_tilings=num_tilings, num_tiles=num_tiles)</span><br><span class="line"></span><br><span class="line">num_actions = <span class="number">3</span></span><br><span class="line">actions = list(range(num_actions))</span><br><span class="line">actor_w = np.zeros((len(actions), iht_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># setting actor weights such that state-action preferences are always [-1, 1, 2]</span></span><br><span class="line">actor_w[<span class="number">0</span>] = <span class="number">-1.</span>/num_tilings</span><br><span class="line">actor_w[<span class="number">1</span>] = <span class="number">1.</span>/num_tilings</span><br><span class="line">actor_w[<span class="number">2</span>] = <span class="number">2.</span>/num_tilings</span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain active_tiles from state</span></span><br><span class="line">state = [-np.pi, <span class="number">0.</span>]</span><br><span class="line">angle, ang_vel = state</span><br><span class="line">active_tiles = test_tc.get_tiles(angle, ang_vel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute softmax probability</span></span><br><span class="line">softmax_prob = compute_softmax_prob(actor_w, active_tiles) </span><br><span class="line">print(<span class="string">'softmax probability: &#123;&#125;'</span>.format(softmax_prob))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(softmax_prob, [<span class="number">0.03511903</span>, <span class="number">0.25949646</span>, <span class="number">0.70538451</span>])</span><br></pre></td></tr></table></figure><pre><code>softmax probability: [0.03511903 0.25949646 0.70538451]</code></pre><h2 id="Section-2-2-Implement-Agent-Methods"><a href="#Section-2-2-Implement-Agent-Methods" class="headerlink" title="Section 2-2: Implement Agent Methods"></a>Section 2-2: Implement Agent Methods</h2><p>Let’s first define methods that initialize the agent. <code>agent_init()</code> initializes all the variables that the agent will need.</p><p>Now that we have implemented helper functions, let’s create an agent. In this part, you will implement <code>agent_start()</code> and <code>agent_step()</code>. We do not need to implement <code>agent_end()</code> because there is no termination in our continuing task. </p><p><code>compute_softmax_prob()</code> is used in <code>agent_policy()</code>, which in turn will be used in <code>agent_start()</code> and <code>agent_step()</code>. We have implemented <code>agent_policy()</code> for you.</p><p>When performing updates to the Actor and Critic, recall their respective updates in the Actor-Critic algorithm video.</p><p>We approximate $q_\pi$ in the Actor update using one-step bootstrapped return($R_{t+1} - \bar{R} + \hat{v}(S_{t+1}, \mathbf{w})$) subtracted by current state-value($\hat{v}(S_{t}, \mathbf{w})$), equivalent to TD error $\delta$.</p><p>$\delta_t = R_{t+1} - \bar{R} + \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_{t}, \mathbf{w}) \hspace{6em} (1)$</p><p><strong>Average Reward update rule</strong>: $\bar{R} \leftarrow \bar{R} + \alpha^{\bar{R}}\delta \hspace{4.3em} (2)$</p><p><strong>Critic weight update rule</strong>: $\mathbf{w} \leftarrow \mathbf{w} + \alpha^{\mathbf{w}}\delta\nabla \hat{v}(s,\mathbf{w}) \hspace{2.5em} (3)$</p><p><strong>Actor weight update rule</strong>: $\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha^{\mathbf{\theta}}\delta\nabla ln \pi(A|S,\mathbf{\theta}) \hspace{1.4em} (4)$</p><p>However, since we are using linear function approximation and parameterizing a softmax policy, the above update rule can be further simplified using:</p><p>$\nabla \hat{v}(s,\mathbf{w}) = \mathbf{x}(s) \hspace{14.2em} (5)$</p><p>$\nabla ln \pi(A|S,\mathbf{\theta}) = \mathbf{x}_h(s,a) - \sum_b \pi(b|s, \mathbf{\theta})\mathbf{x}_h(s,b) \hspace{3.3em} (6)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCriticSoftmaxAgent</span><span class="params">(BaseAgent)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.rand_generator = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        self.actor_step_size = <span class="keyword">None</span></span><br><span class="line">        self.critic_step_size = <span class="keyword">None</span></span><br><span class="line">        self.avg_reward_step_size = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        self.tc = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        self.avg_reward = <span class="keyword">None</span></span><br><span class="line">        self.critic_w = <span class="keyword">None</span></span><br><span class="line">        self.actor_w = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        self.actions = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        self.softmax_prob = <span class="keyword">None</span></span><br><span class="line">        self.prev_tiles = <span class="keyword">None</span></span><br><span class="line">        self.last_action = <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the semi-gradient TD(0) state aggregation agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            "iht_size": int</span></span><br><span class="line"><span class="string">            "num_tilings": int,</span></span><br><span class="line"><span class="string">            "num_tiles": int,</span></span><br><span class="line"><span class="string">            "actor_step_size": float,</span></span><br><span class="line"><span class="string">            "critic_step_size": float,</span></span><br><span class="line"><span class="string">            "avg_reward_step_size": float,</span></span><br><span class="line"><span class="string">            "num_actions": int,</span></span><br><span class="line"><span class="string">            "seed": int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># set random seed for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>)) </span><br><span class="line"></span><br><span class="line">        iht_size = agent_info.get(<span class="string">"iht_size"</span>)</span><br><span class="line">        num_tilings = agent_info.get(<span class="string">"num_tilings"</span>)</span><br><span class="line">        num_tiles = agent_info.get(<span class="string">"num_tiles"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize self.tc to the tile coder we created</span></span><br><span class="line">        self.tc = PendulumTileCoder(iht_size=iht_size, num_tilings=num_tilings, num_tiles=num_tiles)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># set step-size accordingly (we normally divide actor and critic step-size by num. tilings (p.217-218 of textbook))</span></span><br><span class="line">        self.actor_step_size = agent_info.get(<span class="string">"actor_step_size"</span>)/num_tilings</span><br><span class="line">        self.critic_step_size = agent_info.get(<span class="string">"critic_step_size"</span>)/num_tilings</span><br><span class="line">        self.avg_reward_step_size = agent_info.get(<span class="string">"avg_reward_step_size"</span>)</span><br><span class="line"></span><br><span class="line">        self.actions = list(range(agent_info.get(<span class="string">"num_actions"</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set initial values of average reward, actor weights, and critic weights</span></span><br><span class="line">        <span class="comment"># We initialize actor weights to three times the iht_size. </span></span><br><span class="line">        <span class="comment"># Recall this is because we need to have one set of weights for each of the three actions.</span></span><br><span class="line">        self.avg_reward = <span class="number">0.0</span></span><br><span class="line">        self.actor_w = np.zeros((len(self.actions), iht_size))</span><br><span class="line">        self.critic_w = np.zeros(iht_size)</span><br><span class="line"></span><br><span class="line">        self.softmax_prob = <span class="keyword">None</span></span><br><span class="line">        self.prev_tiles = <span class="keyword">None</span></span><br><span class="line">        self.last_action = <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_policy</span><span class="params">(self, active_tiles)</span>:</span></span><br><span class="line">        <span class="string">""" policy of the agent</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            active_tiles (Numpy array): active tiles returned by tile coder</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action selected according to the policy</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute softmax probability</span></span><br><span class="line">        softmax_prob = compute_softmax_prob(self.actor_w, active_tiles)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Sample action from the softmax probability array</span></span><br><span class="line">        <span class="comment"># self.rand_generator.choice() selects an element from the array with the specified probability</span></span><br><span class="line">        chosen_action = self.rand_generator.choice(self.actions, p=softmax_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># save softmax_prob as it will be useful later when updating the Actor</span></span><br><span class="line">        self.softmax_prob = softmax_prob</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> chosen_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the environment's env_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        angle, ang_vel = state</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Use self.tc to get active_tiles using angle and ang_vel (2 lines)</span></span><br><span class="line">        <span class="comment"># set current_action by calling self.agent_policy with active_tiles</span></span><br><span class="line">        <span class="comment"># active_tiles = ?</span></span><br><span class="line">        <span class="comment"># current_action = ?</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        active_tiles = self.tc.get_tiles(angle, ang_vel)</span><br><span class="line">        current_action = self.agent_policy(active_tiles)</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        self.prev_tiles = np.copy(active_tiles)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the environment's step based on </span></span><br><span class="line"><span class="string">                                where the agent ended up after the</span></span><br><span class="line"><span class="string">                                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        angle, ang_vel = state</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Use self.tc to get active_tiles using angle and ang_vel (1 line)</span></span><br><span class="line">        <span class="comment"># active_tiles = ?    </span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        active_tiles = self.tc.get_tiles(angle, ang_vel)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Compute delta using Equation (1) (1 line)</span></span><br><span class="line">        <span class="comment"># delta = ?</span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">    </span><br><span class="line">        delta = reward - self.avg_reward + self.critic_w[active_tiles].sum() - self.critic_w[self.last_action].sum()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update average reward using Equation (2) (1 line)</span></span><br><span class="line">        <span class="comment"># self.avg_reward += ?</span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.avg_reward += self.avg_reward_step_size * delta</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># update critic weights using Equation (3) and (5) (1 line)</span></span><br><span class="line">        <span class="comment"># self.critic_w[self.prev_tiles] += ?</span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.critic_w[self.prev_tiles] += self.critic_step_size * delta</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># update actor weights using Equation (4) and (6)</span></span><br><span class="line">        <span class="comment"># We use self.softmax_prob saved from the previous timestep</span></span><br><span class="line">        <span class="comment"># We leave it as an exercise to verify that the code below corresponds to the equation.</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> self.actions:</span><br><span class="line">            <span class="keyword">if</span> a == self.last_action:</span><br><span class="line">                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (<span class="number">1</span> - self.softmax_prob[a])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (<span class="number">0</span> - self.softmax_prob[a])</span><br><span class="line"></span><br><span class="line">        <span class="comment">### set current_action by calling self.agent_policy with active_tiles (1 line)</span></span><br><span class="line">        <span class="comment"># current_action = ? </span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        current_action = self.agent_policy(active_tiles)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        self.prev_tiles = active_tiles</span><br><span class="line">        self.last_action = current_action</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> message == <span class="string">'get avg reward'</span>:</span><br><span class="line">            <span class="keyword">return</span> self.avg_reward</span><br></pre></td></tr></table></figure><p>Run the following code to verify <code>agent_start()</code>.<br>Although there is randomness due to <code>self.rand_generator.choice()</code> in <code>agent_policy()</code>, we control the seed so your output should match the expected output. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"iht_size"</span>: <span class="number">4096</span>,</span><br><span class="line">    <span class="string">"num_tilings"</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">"num_tiles"</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">"actor_step_size"</span>: <span class="number">1e-1</span>,</span><br><span class="line">    <span class="string">"critic_step_size"</span>: <span class="number">1e-0</span>,</span><br><span class="line">    <span class="string">"avg_reward_step_size"</span>: <span class="number">1e-2</span>,</span><br><span class="line">    <span class="string">"num_actions"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">99</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = ActorCriticSoftmaxAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">state = [-np.pi, <span class="number">0.</span>]</span><br><span class="line"></span><br><span class="line">test_agent.agent_start(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(test_agent.prev_tiles == [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"><span class="keyword">assert</span> test_agent.last_action == <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"agent active_tiles: &#123;&#125;"</span>.format(test_agent.prev_tiles))</span><br><span class="line">print(<span class="string">"agent selected action: &#123;&#125;"</span>.format(test_agent.last_action))</span><br></pre></td></tr></table></figure><pre><code>agent active_tiles: [0 1 2 3 4 5 6 7]agent selected action: 2</code></pre><p>Run the following code to verify <code>agent_step()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make sure agent_start() and agent_policy() are working correctly first.</span></span><br><span class="line"><span class="comment"># agent_step() should work correctly for other arbitrary state transitions in addition to this test case.</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">"seed"</span>: <span class="number">99</span>&#125;</span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"iht_size"</span>: <span class="number">4096</span>,</span><br><span class="line">    <span class="string">"num_tilings"</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">"num_tiles"</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">"actor_step_size"</span>: <span class="number">1e-1</span>,</span><br><span class="line">    <span class="string">"critic_step_size"</span>: <span class="number">1e-0</span>,</span><br><span class="line">    <span class="string">"avg_reward_step_size"</span>: <span class="number">1e-2</span>,</span><br><span class="line">    <span class="string">"num_actions"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">99</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rl_glue = RLGlue(PendulumEnvironment, ActorCriticSoftmaxAgent)</span><br><span class="line">rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># start env/agent</span></span><br><span class="line">rl_glue.rl_start()</span><br><span class="line">rl_glue.rl_step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># simple alias</span></span><br><span class="line">agent = rl_glue.agent</span><br><span class="line"></span><br><span class="line">print(<span class="string">"agent next_action: &#123;&#125;"</span>.format(agent.last_action))</span><br><span class="line">print(<span class="string">"agent avg reward: &#123;&#125;\n"</span>.format(agent.avg_reward))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.avg_reward == <span class="number">-0.03139092653589793</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"agent first 10 values of actor weights[0]: \n&#123;&#125;\n"</span>.format(agent.actor_w[<span class="number">0</span>][:<span class="number">10</span>]))</span><br><span class="line">print(<span class="string">"agent first 10 values of actor weights[1]: \n&#123;&#125;\n"</span>.format(agent.actor_w[<span class="number">1</span>][:<span class="number">10</span>]))</span><br><span class="line">print(<span class="string">"agent first 10 values of actor weights[2]: \n&#123;&#125;\n"</span>.format(agent.actor_w[<span class="number">2</span>][:<span class="number">10</span>]))</span><br><span class="line">print(<span class="string">"agent first 10 values of critic weights: \n&#123;&#125;"</span>.format(agent.critic_w[:<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.actor_w[<span class="number">0</span>][:<span class="number">10</span>], [<span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.actor_w[<span class="number">1</span>][:<span class="number">10</span>], [<span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.actor_w[<span class="number">2</span>][:<span class="number">10</span>], [<span class="number">-0.02615911</span>, <span class="number">-0.02615911</span>, <span class="number">-0.02615911</span>, <span class="number">-0.02615911</span>, <span class="number">-0.02615911</span>, <span class="number">-0.02615911</span>, <span class="number">-0.02615911</span>, <span class="number">-0.02615911</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.critic_w[:<span class="number">10</span>], [<span class="number">-0.39238658</span>, <span class="number">-0.39238658</span>, <span class="number">-0.39238658</span>, <span class="number">-0.39238658</span>, <span class="number">-0.39238658</span>, <span class="number">-0.39238658</span>, <span class="number">-0.39238658</span>, <span class="number">-0.39238658</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure><pre><code>agent next_action: 1agent avg reward: -0.03139092653589793agent first 10 values of actor weights[0]: [0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.         0.        ]agent first 10 values of actor weights[1]: [0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.         0.        ]agent first 10 values of actor weights[2]: [-0.02615911 -0.02615911 -0.02615911 -0.02615911 -0.02615911 -0.02615911 -0.02615911 -0.02615911  0.          0.        ]agent first 10 values of critic weights: [-0.39238658 -0.39238658 -0.39238658 -0.39238658 -0.39238658 -0.39238658 -0.39238658 -0.39238658  0.          0.        ]</code></pre><h2 id="Section-3-Run-Experiment"><a href="#Section-3-Run-Experiment" class="headerlink" title="Section 3: Run Experiment"></a>Section 3: Run Experiment</h2><p>Now that we’ve implemented all the components of environment and agent, let’s run an experiment!<br>We want to see whether our agent is successful at learning the optimal policy of balancing the pendulum upright. We will plot total return over time, as well as the exponential average of the reward over time. We also do multiple runs in order to be confident about our results.  </p><p>The experiment/plot code is provided in the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define function to run experiment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(environment, agent, environment_parameters, agent_parameters, experiment_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># sweep agent parameters</span></span><br><span class="line">    <span class="keyword">for</span> num_tilings <span class="keyword">in</span> agent_parameters[<span class="string">'num_tilings'</span>]:</span><br><span class="line">        <span class="keyword">for</span> num_tiles <span class="keyword">in</span> agent_parameters[<span class="string">"num_tiles"</span>]:</span><br><span class="line">            <span class="keyword">for</span> actor_ss <span class="keyword">in</span> agent_parameters[<span class="string">"actor_step_size"</span>]:</span><br><span class="line">                <span class="keyword">for</span> critic_ss <span class="keyword">in</span> agent_parameters[<span class="string">"critic_step_size"</span>]:</span><br><span class="line">                    <span class="keyword">for</span> avg_reward_ss <span class="keyword">in</span> agent_parameters[<span class="string">"avg_reward_step_size"</span>]:</span><br><span class="line">                        </span><br><span class="line">                        env_info = &#123;&#125;</span><br><span class="line">                        agent_info = &#123;<span class="string">"num_tilings"</span>: num_tilings,</span><br><span class="line">                                      <span class="string">"num_tiles"</span>: num_tiles,</span><br><span class="line">                                      <span class="string">"actor_step_size"</span>: actor_ss,</span><br><span class="line">                                      <span class="string">"critic_step_size"</span>: critic_ss,</span><br><span class="line">                                      <span class="string">"avg_reward_step_size"</span>: avg_reward_ss,</span><br><span class="line">                                      <span class="string">"num_actions"</span>: agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                                      <span class="string">"iht_size"</span>: agent_parameters[<span class="string">"iht_size"</span>]&#125;            </span><br><span class="line">            </span><br><span class="line">                        <span class="comment"># results to save</span></span><br><span class="line">                        return_per_step = np.zeros((experiment_parameters[<span class="string">"num_runs"</span>], experiment_parameters[<span class="string">"max_steps"</span>]))</span><br><span class="line">                        exp_avg_reward_per_step = np.zeros((experiment_parameters[<span class="string">"num_runs"</span>], experiment_parameters[<span class="string">"max_steps"</span>]))</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># using tqdm we visualize progress bars </span></span><br><span class="line">                        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_runs"</span>]+<span class="number">1</span>)):</span><br><span class="line">                            env_info[<span class="string">"seed"</span>] = run</span><br><span class="line">                            agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">                </span><br><span class="line">                            rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">                            rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">                            num_steps = <span class="number">0</span></span><br><span class="line">                            total_return = <span class="number">0.</span></span><br><span class="line">                            return_arr = []</span><br><span class="line"></span><br><span class="line">                            <span class="comment"># exponential average reward without initial bias</span></span><br><span class="line">                            exp_avg_reward = <span class="number">0.0</span></span><br><span class="line">                            exp_avg_reward_ss = <span class="number">0.01</span></span><br><span class="line">                            exp_avg_reward_normalizer = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                            <span class="keyword">while</span> num_steps &lt; experiment_parameters[<span class="string">'max_steps'</span>]:</span><br><span class="line">                                num_steps += <span class="number">1</span></span><br><span class="line">                                </span><br><span class="line">                                rl_step_result = rl_glue.rl_step()</span><br><span class="line">                                </span><br><span class="line">                                reward = rl_step_result[<span class="number">0</span>]</span><br><span class="line">                                total_return += reward</span><br><span class="line">                                return_arr.append(reward)</span><br><span class="line">                                avg_reward = rl_glue.rl_agent_message(<span class="string">"get avg reward"</span>)</span><br><span class="line"></span><br><span class="line">                                exp_avg_reward_normalizer = exp_avg_reward_normalizer + exp_avg_reward_ss * (<span class="number">1</span> - exp_avg_reward_normalizer)</span><br><span class="line">                                ss = exp_avg_reward_ss / exp_avg_reward_normalizer</span><br><span class="line">                                exp_avg_reward += ss * (reward - exp_avg_reward)</span><br><span class="line">                                </span><br><span class="line">                                return_per_step[run<span class="number">-1</span>][num_steps<span class="number">-1</span>] = total_return</span><br><span class="line">                                exp_avg_reward_per_step[run<span class="number">-1</span>][num_steps<span class="number">-1</span>] = exp_avg_reward</span><br><span class="line">                                                        </span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'results'</span>):</span><br><span class="line">                            os.makedirs(<span class="string">'results'</span>)</span><br><span class="line">                </span><br><span class="line">                        save_name = <span class="string">"ActorCriticSoftmax_tilings_&#123;&#125;_tiledim_&#123;&#125;_actor_ss_&#123;&#125;_critic_ss_&#123;&#125;_avg_reward_ss_&#123;&#125;"</span>.format(num_tilings, num_tiles, actor_ss, critic_ss, avg_reward_ss)</span><br><span class="line">                        total_return_filename = <span class="string">"results/&#123;&#125;_total_return.npy"</span>.format(save_name)</span><br><span class="line">                        exp_avg_reward_filename = <span class="string">"results/&#123;&#125;_exp_avg_reward.npy"</span>.format(save_name)</span><br><span class="line"></span><br><span class="line">                        np.save(total_return_filename, return_per_step)</span><br><span class="line">                        np.save(exp_avg_reward_filename, exp_avg_reward_per_step)</span><br></pre></td></tr></table></figure><h2 id="Section-3-1-Run-Experiment-with-32-tilings-size-8x8"><a href="#Section-3-1-Run-Experiment-with-32-tilings-size-8x8" class="headerlink" title="Section 3-1: Run Experiment with 32 tilings, size 8x8"></a>Section 3-1: Run Experiment with 32 tilings, size 8x8</h2><p>We will first test our implementation using 32 tilings, of size 8x8. We saw from the earlier assignment using tile-coding that many tilings promote fine discrimination, and broad tiles allows more generalization.<br>We conducted a wide sweep of meta-parameters in order to find the best meta-parameters for our Pendulum Swing-up task. </p><p>We swept over the following range of meta-parameters and the best meta-parameter is boldfaced below:</p><p>actor step-size: $\{\frac{2^{-6}}{32}, \frac{2^{-5}}{32}, \frac{2^{-4}}{32}, \frac{2^{-3}}{32}, \mathbf{\frac{2^{-2}}{32}}, \frac{2^{-1}}{32}, \frac{2^{0}}{32}, \frac{2^{1}}{32}\}$</p><p>critic step-size: $\{\frac{2^{-4}}{32}, \frac{2^{-3}}{32}, \frac{2^{-2}}{32}, \frac{2^{-1}}{32}, \frac{2^{0}}{32}, \mathbf{\frac{2^{1}}{32}}, \frac{3}{32}, \frac{2^{2}}{32}\}$</p><p>avg reward step-size: $\{2^{-11}, 2^{-10} , 2^{-9} , 2^{-8}, 2^{-7}, \mathbf{2^{-6}}, 2^{-5}, 2^{-4}, 2^{-3}, 2^{-2}\}$  </p><p>We will do 50 runs using the above best meta-parameter setting to verify your agent.<br>Note that running the experiment cell below will take <strong>_approximately 5 min_</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"max_steps"</span> : <span class="number">20000</span>,</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">50</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line"><span class="comment"># Each element is an array because we will be later sweeping over multiple values</span></span><br><span class="line"><span class="comment"># actor and critic step-sizes are divided by num. tilings inside the agent</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_tilings"</span>: [<span class="number">32</span>],</span><br><span class="line">    <span class="string">"num_tiles"</span>: [<span class="number">8</span>],</span><br><span class="line">    <span class="string">"actor_step_size"</span>: [<span class="number">2</span>**(<span class="number">-2</span>)],</span><br><span class="line">    <span class="string">"critic_step_size"</span>: [<span class="number">2</span>**<span class="number">1</span>],</span><br><span class="line">    <span class="string">"avg_reward_step_size"</span>: [<span class="number">2</span>**(<span class="number">-6</span>)],</span><br><span class="line">    <span class="string">"num_actions"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"iht_size"</span>: <span class="number">4096</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = PendulumEnvironment</span><br><span class="line">current_agent = ActorCriticSoftmaxAgent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_script.plot_result(agent_parameters, <span class="string">'results'</span>)</span><br></pre></td></tr></table></figure><pre><code>  0%|          | 0/50 [00:00&lt;?, ?it/s]</code></pre><p>Run the following code to verify your experimental result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for experimental result ##</span></span><br><span class="line">filename = <span class="string">'ActorCriticSoftmax_tilings_32_tiledim_8_actor_ss_0.25_critic_ss_2_avg_reward_ss_0.015625_exp_avg_reward'</span></span><br><span class="line">agent_exp_avg_reward = np.load(<span class="string">'results/&#123;&#125;.npy'</span>.format(filename), allow_pickle=<span class="keyword">True</span>)</span><br><span class="line">result_med = np.median(agent_exp_avg_reward, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">answer_range = np.load(<span class="string">'correct_npy/exp_avg_reward_answer_range.npy'</span>, allow_pickle=<span class="keyword">True</span>)</span><br><span class="line">upper_bound = answer_range.item()[<span class="string">'upper-bound'</span>]</span><br><span class="line">lower_bound = answer_range.item()[<span class="string">'lower-bound'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># check if result is within answer range</span></span><br><span class="line">all_correct = np.all(result_med &lt;= upper_bound) <span class="keyword">and</span> np.all(result_med &gt;= lower_bound)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_correct:</span><br><span class="line">    print(<span class="string">"Your experiment results are correct!"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"Your experiment results does not match with ours. Please check if you have implemented all methods correctly."</span>)</span><br></pre></td></tr></table></figure><h2 id="Section-3-2-Performance-Metric-and-Meta-Parameter-Sweeps"><a href="#Section-3-2-Performance-Metric-and-Meta-Parameter-Sweeps" class="headerlink" title="Section 3-2: Performance Metric and Meta-Parameter Sweeps"></a>Section 3-2: Performance Metric and Meta-Parameter Sweeps</h2><h3 id="Performance-Metric"><a href="#Performance-Metric" class="headerlink" title="Performance Metric"></a>Performance Metric</h3><p>To evaluate performance, we plotted both the return and exponentially weighted average reward over time. </p><p>In the first plot, the return is negative because the reward is negative at every state except when the pendulum is in the upright position. As the policy improves over time, the agent accumulates less negative reward, and thus the return decreases slowly. Towards the end the slope is almost flat indicating the policy has stabilized to a good policy. When using this plot however, it can be difficult to distinguish whether it has learned an optimal policy. The near-optimal policy in this Pendulum Swing-up Environment is to maintain the pendulum in the upright position indefinitely, getting near 0 reward at each time step. We would have to examine the slope of the curve but it can be hard to compare the slope of different curves.</p><p>The second plot using exponential average reward gives a better visualization. We can see that towards the end the value is near 0, indicating it is getting near 0 reward at each time step. Here, the exponentially weighted average reward shouldn’t be confused with the agent’s internal estimate of the average reward. To be more specific, we used an exponentially weighted average of the actual reward without initial bias (Refer to Exercise 2.7 from the textbook (p.35) to read more about removing the initial bias). If we used sample averages instead, later rewards would have decreasing impact on the average and would not be able to represent the agent’s performance with respect to its current policy effectively.</p><p>It is easier to see whether the agent has learned a good policy in the second plot than the first plot. If the learned policy is optimal, the exponential average reward would be close to 0.</p><p>Furthermore, how did we pick the best meta-parameter from the sweeps? A common method would be to pick the meta-parameter that results in the largest Area Under the Curve (AUC). However, this is not always what we want. We want to find a set of meta-parameters that learns a good final policy. When using AUC as the criteria, we may pick meta-parameters that allows the agent to learn fast but converge to a worse policy. In our case, we selected the meta-parameter setting that obtained the most exponential average reward over the last 5000 time steps. </p><h3 id="Parameter-Sensitivity"><a href="#Parameter-Sensitivity" class="headerlink" title="Parameter Sensitivity"></a>Parameter Sensitivity</h3><p>In addition to finding the best meta-parameters it is also equally important to plot <strong>parameter sensitivity curves</strong> to understand how our algorithm behaves.</p><p>In our simulated Pendulum problem, we can extensively test our agent with different meta-parameter configurations but it would be quite expensive to do so in real life. Parameter sensitivity curves can provide us insight into how our algorithms might behave in general. It can help us identify a good range of each meta-parameters as well as how sensitive the performance is with respect to each meta-parameter.</p><p>Here are the sensitivity curves for the three step-sizes we swept over:</p><p><img src="sensitivity_combined.png" alt="Drawing" style="width: 1000px;"></p><p>On the y-axis we use the performance measure, which is the average of the exponential average reward over the 5000 time steps, averaged over 50 different runs. On the x-axis is the meta-parameter we are testing. For the given meta-parameter, the remaining meta-parameters are chosen such that it obtains the best performance.</p><p>The curves are quite rounded, indicating the agent performs well for these wide range of values. It indicates that the agent is not too sensitive to these meta-parameters. Furthermore, looking at the y-axis values we can observe that average reward step-size is particularly less sensitive than actor step-size and critic step-size.</p><p>But how do we know that we have sufficiently covered a wide range of meta-parameters? It is important that the best value is not on the edge but in the middle of the meta-parameter sweep range in these sensitivity curves. Otherwise this may indicate that there could be better meta-parameter values that we did not sweep over.</p><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><h3 id="Congratulations-You-have-successfully-implemented-Course-3-Programming-Assignment-4"><a href="#Congratulations-You-have-successfully-implemented-Course-3-Programming-Assignment-4" class="headerlink" title="Congratulations! You have successfully implemented Course 3 Programming Assignment 4."></a><strong>Congratulations!</strong> You have successfully implemented Course 3 Programming Assignment 4.</h3><p>You have implemented your own <strong>Average Reward Actor-Critic with Softmax Policy</strong> agent in the Pendulum Swing-up Environment. You implemented the environment based on information about the state/action space and transition dynamics. Furthermore, you have learned how to implement an agent in a continuing task using the average reward formulation. We parameterized the policy using softmax of action-preferences over discrete action spaces, and used Actor-Critic to learn the policy.</p><p>To summarize, you have learned how to:</p><pre><code>1. Implement softmax actor-critic agent on a continuing task using the average reward formulation.2. Understand how to parameterize the policy as a function to learn, in a discrete action environment.3. Understand how to (approximately) sample the gradient of this objective to update the actor.4. Understand how to update the critic using differential TD error.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-4-Average-Reward-Softmax-Actor-Critic&quot;&gt;&lt;a href=&quot;#Assignment-4-Average-Reward-Softmax-Actor-Critic&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Function Approximation and Control</title>
    <link href="https://zhangruochi.com/Function-Approximation-and-Control/2020/10/15/"/>
    <id>https://zhangruochi.com/Function-Approximation-and-Control/2020/10/15/</id>
    <published>2020-10-15T04:00:24.000Z</published>
    <updated>2020-10-15T04:01:26.316Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-3-Function-Approximation-and-Control"><a href="#Assignment-3-Function-Approximation-and-Control" class="headerlink" title="Assignment 3: Function Approximation and Control"></a>Assignment 3: Function Approximation and Control</h1><p>Welcome to Assignment 3. In this notebook you will learn how to:</p><ul><li>Use function approximation in the control setting</li><li>Implement the Sarsa algorithm using tile coding</li><li>Compare three settings for tile coding to see their effect on our agent</li></ul><p>As with the rest of the notebooks do not import additional libraries or adjust grading cells as this will break the grader.</p><p>MAKE SURE TO RUN ALL OF THE CELLS SO THE GRADER GETS THE OUTPUT IT NEEDS</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import Necessary Libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tiles3 <span class="keyword">as</span> tc</span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> argmax</span><br><span class="line"><span class="keyword">import</span> mountaincar_env</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><p>In the above cell, we import the libraries we need for this assignment. You may have noticed that we import mountaincar_env. This is the <strong>Mountain Car Task</strong> introduced in <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=267" target="_blank" rel="noopener">Section 10.1 of the textbook</a>. The task is for an under powered car to make it to the top of a hill:<br><img src="mountaincar.png" alt="Mountain Car" title="Mountain Car"><br>The car is under-powered so the agent needs to learn to rock back and forth to get enough momentum to reach the goal. At each time step the agent receives from the environment its current velocity (a float between -0.07 and 0.07), and it’s current position (a float between -1.2 and 0.5). Because our state is continuous there are a potentially infinite number of states that our agent could be in. We need a function approximation method to help the agent deal with this. In this notebook we will use tile coding. We provide a tile coding implementation for you to use, imported above with tiles3.</p><h2 id="Section-0-Tile-Coding-Helper-Function"><a href="#Section-0-Tile-Coding-Helper-Function" class="headerlink" title="Section 0: Tile Coding Helper Function"></a>Section 0: Tile Coding Helper Function</h2><p>To begin we are going to build a tile coding class for our Sarsa agent that will make it easier to make calls to our tile coder.</p><h3 id="Tile-Coding-Function"><a href="#Tile-Coding-Function" class="headerlink" title="Tile Coding Function"></a>Tile Coding Function</h3><p>Tile coding is introduced in <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=239" target="_blank" rel="noopener">Section 9.5.4 of the textbook</a> of the textbook as a way to create features that can both provide good generalization and discrimination. It consists of multiple overlapping tilings, where each tiling is a partitioning of the space into tiles.<br><img src="tilecoding.png" alt="Tile Coding" title="Tile Coding"></p><p>To help keep our agent code clean we are going to make a function specific for tile coding for our Mountain Car environment. To help we are going to use the Tiles3 library. This is a Python 3 implementation of the tile coder. To start take a look at the documentation: <a href="http://incompleteideas.net/tiles/tiles3.html" target="_blank" rel="noopener">Tiles3 documentation</a><br>To get the tile coder working we need to implement a few pieces:</p><ul><li>First: create an index hash table - this is done for you in the init function using tc.IHT.</li><li>Second is to scale the inputs for the tile coder based on the number of tiles and the range of values each input could take. The tile coder needs to take in a number in range [0, 1], or scaled to be [0, 1] * num_tiles. For more on this refer to the <a href="http://incompleteideas.net/tiles/tiles3.html" target="_blank" rel="noopener">Tiles3 documentation</a>.</li><li>Finally we call tc.tiles to get the active tiles back.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MountainCarTileCoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, iht_size=<span class="number">4096</span>, num_tilings=<span class="number">8</span>, num_tiles=<span class="number">8</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initializes the MountainCar Tile Coder</span></span><br><span class="line"><span class="string">        Initializers:</span></span><br><span class="line"><span class="string">        iht_size -- int, the size of the index hash table, typically a power of 2</span></span><br><span class="line"><span class="string">        num_tilings -- int, the number of tilings</span></span><br><span class="line"><span class="string">        num_tiles -- int, the number of tiles. Here both the width and height of the</span></span><br><span class="line"><span class="string">                     tile coder are the same</span></span><br><span class="line"><span class="string">        Class Variables:</span></span><br><span class="line"><span class="string">        self.iht -- tc.IHT, the index hash table that the tile coder will use</span></span><br><span class="line"><span class="string">        self.num_tilings -- int, the number of tilings the tile coder will use</span></span><br><span class="line"><span class="string">        self.num_tiles -- int, the number of tiles the tile coder will use</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.iht = tc.IHT(iht_size)</span><br><span class="line">        self.num_tilings = num_tilings</span><br><span class="line">        self.num_tiles = num_tiles</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_tiles</span><span class="params">(self, position, velocity)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Takes in a position and velocity from the mountaincar environment</span></span><br><span class="line"><span class="string">        and returns a numpy array of active tiles.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        position -- float, the position of the agent between -1.2 and 0.5</span></span><br><span class="line"><span class="string">        velocity -- float, the velocity of the agent between -0.07 and 0.07</span></span><br><span class="line"><span class="string">        returns:</span></span><br><span class="line"><span class="string">        tiles - np.array, active tiles</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Use the ranges above and self.num_tiles to scale position and velocity to the range [0, 1]</span></span><br><span class="line">        <span class="comment"># then multiply that range with self.num_tiles so it scales from [0, num_tiles]</span></span><br><span class="line">        </span><br><span class="line">        position_scaled = <span class="number">0</span></span><br><span class="line">        velocity_scaled = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        position_scaled = ( position + <span class="number">1.2</span> ) / (<span class="number">0.5</span> + <span class="number">1.2</span>) * self.num_tiles</span><br><span class="line">        velocity_scaled = ( velocity + <span class="number">0.07</span> ) / ( <span class="number">0.07</span> + <span class="number">0.07</span> ) * self.num_tiles</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the tiles using tc.tiles, with self.iht, self.num_tilings and [scaled position, scaled velocity]</span></span><br><span class="line">        <span class="comment"># nothing to implment here</span></span><br><span class="line">        tiles = tc.tiles(self.iht, self.num_tilings, [position_scaled, velocity_scaled])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> np.array(tiles)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create a range of positions and velocities to test</span></span><br><span class="line"><span class="comment"># then test every element in the cross-product between these lists</span></span><br><span class="line">pos_tests = np.linspace(<span class="number">-1.2</span>, <span class="number">0.5</span>, num=<span class="number">5</span>)</span><br><span class="line">vel_tests = np.linspace(<span class="number">-0.07</span>, <span class="number">0.07</span>, num=<span class="number">5</span>)</span><br><span class="line">tests = list(itertools.product(pos_tests, vel_tests))</span><br><span class="line"></span><br><span class="line">mctc = MountainCarTileCoder(iht_size=<span class="number">1024</span>, num_tilings=<span class="number">8</span>, num_tiles=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">t = []</span><br><span class="line"><span class="keyword">for</span> test <span class="keyword">in</span> tests:</span><br><span class="line">    position, velocity = test</span><br><span class="line">    tiles = mctc.get_tiles(position=position, velocity=velocity)</span><br><span class="line">    t.append(tiles)</span><br><span class="line"></span><br><span class="line">expected = [</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">15</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">31</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">35</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">40</span>, <span class="number">39</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">40</span>, <span class="number">43</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">43</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">31</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">45</span>, <span class="number">46</span>, <span class="number">44</span>, <span class="number">47</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">35</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">48</span>, <span class="number">49</span>, <span class="number">50</span>, <span class="number">51</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">40</span>, <span class="number">39</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">50</span>, <span class="number">54</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">40</span>, <span class="number">43</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">55</span>, <span class="number">54</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">43</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">55</span>, <span class="number">58</span>],</span><br><span class="line">    [<span class="number">45</span>, <span class="number">46</span>, <span class="number">44</span>, <span class="number">47</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">59</span>, <span class="number">58</span>],</span><br><span class="line">    [<span class="number">60</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">63</span>, <span class="number">48</span>, <span class="number">49</span>, <span class="number">50</span>, <span class="number">51</span>],</span><br><span class="line">    [<span class="number">60</span>, <span class="number">61</span>, <span class="number">64</span>, <span class="number">63</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">50</span>, <span class="number">54</span>],</span><br><span class="line">    [<span class="number">65</span>, <span class="number">66</span>, <span class="number">64</span>, <span class="number">67</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">55</span>, <span class="number">54</span>],</span><br><span class="line">    [<span class="number">65</span>, <span class="number">66</span>, <span class="number">68</span>, <span class="number">67</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">55</span>, <span class="number">58</span>],</span><br><span class="line">    [<span class="number">69</span>, <span class="number">70</span>, <span class="number">68</span>, <span class="number">71</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">59</span>, <span class="number">58</span>],</span><br><span class="line">]</span><br><span class="line"><span class="keyword">assert</span> np.all(expected == np.array(t))</span><br></pre></td></tr></table></figure><h2 id="Section-1-Sarsa-Agent"><a href="#Section-1-Sarsa-Agent" class="headerlink" title="Section 1: Sarsa Agent"></a>Section 1: Sarsa Agent</h2><p>We are now going to use the functions that we just created to implement the Sarsa algorithm. Recall from class that Sarsa stands for State, Action, Reward, State, Action.</p><p>For this case we have given you an argmax function similar to what you wrote back in Course 1 Assignment 1. Recall, this is different than the argmax function that is used by numpy, which returns the first index of a maximum value. We want our argmax function to arbitrarily break ties, which is what the imported argmax function does. The given argmax function takes in an array of values and returns an int of the chosen action:<br>argmax(action values)</p><p>There are multiple ways that we can deal with actions for the tile coder. Here we are going to use one simple method - make the size of the weight vector equal to (iht_size, num_actions). This will give us one weight vector for each action and one weight for each tile.</p><p>Use the above function to help fill in select_action, agent_start, agent_step, and agent_end.</p><p>Hints:</p><p>1) The tile coder returns a list of active indexes (e.g. [1, 12, 22]). You can index a numpy array using an array of values - this will return an array of the values at each of those indices. So in order to get the value of a state we can index our weight vector using the action and the array of tiles that the tile coder returns:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">This will give us an array of values, one for each active tile, and we sum the result to get the value of that state-action pair.</span><br><span class="line"></span><br><span class="line">2) In the case of a binary feature vector (such as the tile coder), the derivative is 1 at each of the active tiles, and zero otherwise.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># -----------</span><br><span class="line"># Graded Cell</span><br><span class="line"># -----------</span><br><span class="line">class SarsaAgent(BaseAgent):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Initialization of Sarsa Agent. All values are set to None so they can</span><br><span class="line">    be initialized in the agent_init method.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.last_action = None</span><br><span class="line">        self.last_state = None</span><br><span class="line">        self.epsilon = None</span><br><span class="line">        self.gamma = None</span><br><span class="line">        self.iht_size = None</span><br><span class="line">        self.w = None</span><br><span class="line">        self.alpha = None</span><br><span class="line">        self.num_tilings = None</span><br><span class="line">        self.num_tiles = None</span><br><span class="line">        self.mctc = None</span><br><span class="line">        self.initial_weights = None</span><br><span class="line">        self.num_actions = None</span><br><span class="line">        self.previous_tiles = None</span><br><span class="line"></span><br><span class="line">    def agent_init(self, agent_info=&#123;&#125;):</span><br><span class="line">        &quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot;</span><br><span class="line">        self.num_tilings = agent_info.get(&quot;num_tilings&quot;, 8)</span><br><span class="line">        self.num_tiles = agent_info.get(&quot;num_tiles&quot;, 8)</span><br><span class="line">        self.iht_size = agent_info.get(&quot;iht_size&quot;, 4096)</span><br><span class="line">        self.epsilon = agent_info.get(&quot;epsilon&quot;, 0.0)</span><br><span class="line">        self.gamma = agent_info.get(&quot;gamma&quot;, 1.0)</span><br><span class="line">        self.alpha = agent_info.get(&quot;alpha&quot;, 0.5) / self.num_tilings</span><br><span class="line">        self.initial_weights = agent_info.get(&quot;initial_weights&quot;, 0.0)</span><br><span class="line">        self.num_actions = agent_info.get(&quot;num_actions&quot;, 3)</span><br><span class="line">        </span><br><span class="line">        # We initialize self.w to three times the iht_size. Recall this is because</span><br><span class="line">        # we need to have one set of weights for each action.</span><br><span class="line">        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights</span><br><span class="line">        </span><br><span class="line">        # We initialize self.mctc to the mountaincar verions of the </span><br><span class="line">        # tile coder that we created</span><br><span class="line">        self.tc = MountainCarTileCoder(iht_size=self.iht_size, </span><br><span class="line">                                         num_tilings=self.num_tilings, </span><br><span class="line">                                         num_tiles=self.num_tiles)</span><br><span class="line"></span><br><span class="line">    def select_action(self, tiles):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Selects an action using epsilon greedy</span><br><span class="line">        Args:</span><br><span class="line">        tiles - np.array, an array of active tiles</span><br><span class="line">        Returns:</span><br><span class="line">        (chosen_action, action_value) - (int, float), tuple of the chosen action</span><br><span class="line">                                        and it&apos;s value</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        action_values = []</span><br><span class="line">        chosen_action = None</span><br><span class="line">        </span><br><span class="line">        # First loop through the weights of each action and populate action_values</span><br><span class="line">        # with the action value for each action and tiles instance</span><br><span class="line">        </span><br><span class="line">        # Use np.random.random to decide if an exploritory action should be taken</span><br><span class="line">        # and set chosen_action to a random action if it is</span><br><span class="line">        # Otherwise choose the greedy action using the given argmax </span><br><span class="line">        # function and the action values (don&apos;t use numpy&apos;s armax)</span><br><span class="line">        </span><br><span class="line">        # ----------------</span><br><span class="line">        # your code here</span><br><span class="line">        </span><br><span class="line">        for i in range(self.num_actions):</span><br><span class="line">            action_values.append(self.w[i][tiles].sum())</span><br><span class="line"></span><br><span class="line">        if np.random.random() &lt; self.epsilon:</span><br><span class="line">            chosen_action = np.random.choice(self.num_actions)</span><br><span class="line">        else:</span><br><span class="line">            chosen_action = argmax(action_values)</span><br><span class="line">        </span><br><span class="line">        # ----------------</span><br><span class="line"></span><br><span class="line">        return chosen_action, action_values[chosen_action]</span><br><span class="line">    </span><br><span class="line">    def agent_start(self, state):</span><br><span class="line">        &quot;&quot;&quot;The first method called when the experiment starts, called after</span><br><span class="line">        the environment starts.</span><br><span class="line">        Args:</span><br><span class="line">            state (Numpy array): the state observation from the</span><br><span class="line">                environment&apos;s evn_start function.</span><br><span class="line">        Returns:</span><br><span class="line">            The first action the agent takes.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        position, velocity = state</span><br><span class="line">        </span><br><span class="line">        # Use self.tc to set active_tiles using position and velocity</span><br><span class="line">        # set current_action to the epsilon greedy chosen action using</span><br><span class="line">        # the select_action function above with the active tiles</span><br><span class="line">        </span><br><span class="line">        # ----------------</span><br><span class="line">        # your code here</span><br><span class="line">        active_tiles = self.tc.get_tiles(position, velocity)</span><br><span class="line">        current_action, _ = self.select_action(active_tiles)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        self.previous_tiles = np.copy(active_tiles)</span><br><span class="line">        </span><br><span class="line">        return self.last_action</span><br><span class="line"></span><br><span class="line">    def agent_step(self, reward, state):</span><br><span class="line">        &quot;&quot;&quot;A step taken by the agent.</span><br><span class="line">        Args:</span><br><span class="line">            reward (float): the reward received for taking the last action taken</span><br><span class="line">            state (Numpy array): the state observation from the</span><br><span class="line">                environment&apos;s step based, where the agent ended up after the</span><br><span class="line">                last step</span><br><span class="line">        Returns:</span><br><span class="line">            The action the agent is taking.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # choose the action here</span><br><span class="line">        position, velocity = state</span><br><span class="line">        </span><br><span class="line">        # Use self.tc to set active_tiles using position and velocity</span><br><span class="line">        # set current_action and action_value to the epsilon greedy chosen action using</span><br><span class="line">        # the select_action function above with the active tiles</span><br><span class="line">        </span><br><span class="line">        # Update self.w at self.previous_tiles and self.previous action</span><br><span class="line">        # using the reward, action_value, self.gamma, self.w,</span><br><span class="line">        # self.alpha, and the Sarsa update from the textbook</span><br><span class="line">        </span><br><span class="line">        # ----------------</span><br><span class="line">        # your code here</span><br><span class="line">        active_tiles = self.tc.get_tiles(position, velocity)</span><br><span class="line">        current_action, action_value = self.select_action(active_tiles)</span><br><span class="line">        last_action_value = self.w[self.last_action][self.previous_tiles].sum()</span><br><span class="line">        delta = reward + self.gamma * action_value - last_action_value</span><br><span class="line">        grad = np.zeros_like(self.w)</span><br><span class="line">        grad[self.last_action][self.previous_tiles] = 1</span><br><span class="line">        self.w += self.alpha * delta * grad</span><br><span class="line">        </span><br><span class="line">        # ----------------</span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        self.previous_tiles = np.copy(active_tiles)</span><br><span class="line">        return self.last_action</span><br><span class="line"></span><br><span class="line">    def agent_end(self, reward):</span><br><span class="line">        &quot;&quot;&quot;Run when the agent terminates.</span><br><span class="line">        Args:</span><br><span class="line">            reward (float): the reward the agent received for entering the</span><br><span class="line">                terminal state.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # Update self.w at self.previous_tiles and self.previous action</span><br><span class="line">        # using the reward, self.gamma, self.w,</span><br><span class="line">        # self.alpha, and the Sarsa update from the textbook</span><br><span class="line">        # Hint - there is no action_value used here because this is the end</span><br><span class="line">        # of the episode.</span><br><span class="line">        </span><br><span class="line">        # ----------------</span><br><span class="line">        # your code here</span><br><span class="line">        last_action_value = self.w[self.last_action][self.previous_tiles].sum()</span><br><span class="line">        </span><br><span class="line">        grad = np.zeros_like(self.w)</span><br><span class="line">        grad[self.last_action][self.previous_tiles] = 1</span><br><span class="line">        </span><br><span class="line">        self.w += self.alpha * (reward - last_action_value) * grad</span><br><span class="line">        </span><br><span class="line">        # ----------------</span><br><span class="line">        </span><br><span class="line">    def agent_cleanup(self):</span><br><span class="line">        &quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot;</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    def agent_message(self, message):</span><br><span class="line">        &quot;&quot;&quot;A function used to pass information from the agent to the experiment.</span><br><span class="line">        Args:</span><br><span class="line">            message: The message passed to the agent.</span><br><span class="line">        Returns:</span><br><span class="line">            The response (or answer) to the message.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent = SarsaAgent()</span><br><span class="line">agent.agent_init(&#123;<span class="string">"epsilon"</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">agent.w = np.array([np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]), np.array([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])])</span><br><span class="line"></span><br><span class="line">action_distribution = np.zeros(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    chosen_action, action_value = agent.select_action(np.array([<span class="number">0</span>,<span class="number">1</span>]))</span><br><span class="line">    action_distribution[chosen_action] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">print(<span class="string">"action distribution:"</span>, action_distribution)</span><br><span class="line"><span class="comment"># notice that the two non-greedy actions are roughly uniformly distributed</span></span><br><span class="line"><span class="keyword">assert</span> np.all(action_distribution == [<span class="number">29</span>, <span class="number">35</span>, <span class="number">936</span>])</span><br><span class="line"></span><br><span class="line">agent = SarsaAgent()</span><br><span class="line">agent.agent_init(&#123;<span class="string">"epsilon"</span>: <span class="number">0.0</span>&#125;)</span><br><span class="line">agent.w = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">chosen_action, action_value = agent.select_action([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> chosen_action == <span class="number">2</span></span><br><span class="line"><span class="keyword">assert</span> action_value == <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># test update</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line">agent = SarsaAgent()</span><br><span class="line">agent.agent_init(&#123;<span class="string">"epsilon"</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line"></span><br><span class="line">agent.agent_start((<span class="number">0.1</span>, <span class="number">0.3</span>))</span><br><span class="line">agent.agent_step(<span class="number">1</span>, (<span class="number">0.02</span>, <span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(agent.w[<span class="number">0</span>,<span class="number">0</span>:<span class="number">8</span>] == <span class="number">0.0625</span>)</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.w[<span class="number">1</span>:] == <span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>action distribution: [ 29.  35. 936.]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">num_runs = <span class="number">10</span></span><br><span class="line">num_episodes = <span class="number">50</span></span><br><span class="line">env_info = &#123;<span class="string">"num_tiles"</span>: <span class="number">8</span>, <span class="string">"num_tilings"</span>: <span class="number">8</span>&#125;</span><br><span class="line">agent_info = &#123;&#125;</span><br><span class="line">all_steps = []</span><br><span class="line"></span><br><span class="line">agent = SarsaAgent</span><br><span class="line">env = mountaincar_env.Environment</span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(num_runs):</span><br><span class="line">    <span class="keyword">if</span> run % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"RUN: &#123;&#125;"</span>.format(run))</span><br><span class="line"></span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">    steps_per_episode = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">15000</span>)</span><br><span class="line">        steps_per_episode.append(rl_glue.num_steps)</span><br><span class="line"></span><br><span class="line">    all_steps.append(np.array(steps_per_episode))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Run time: &#123;&#125;"</span>.format(time.time() - start))</span><br><span class="line"></span><br><span class="line">mean = np.mean(all_steps, axis=<span class="number">0</span>)</span><br><span class="line">plt.plot(mean)</span><br><span class="line"></span><br><span class="line"><span class="comment"># because we set the random seed, these values should be *exactly* the same</span></span><br><span class="line"><span class="keyword">assert</span> np.allclose(mean, [<span class="number">1432.5</span>, <span class="number">837.9</span>, <span class="number">694.4</span>, <span class="number">571.4</span>, <span class="number">515.2</span>, <span class="number">380.6</span>, <span class="number">379.4</span>, <span class="number">369.6</span>, <span class="number">357.2</span>, <span class="number">316.5</span>, <span class="number">291.1</span>, <span class="number">305.3</span>, <span class="number">250.1</span>, <span class="number">264.9</span>, <span class="number">235.4</span>, <span class="number">242.1</span>, <span class="number">244.4</span>, <span class="number">245.</span>, <span class="number">221.2</span>, <span class="number">229.</span>, <span class="number">238.3</span>, <span class="number">211.2</span>, <span class="number">201.1</span>, <span class="number">208.3</span>, <span class="number">185.3</span>, <span class="number">207.1</span>, <span class="number">191.6</span>, <span class="number">204.</span>, <span class="number">214.5</span>, <span class="number">207.9</span>, <span class="number">195.9</span>, <span class="number">206.4</span>, <span class="number">194.9</span>, <span class="number">191.1</span>, <span class="number">195.</span>, <span class="number">186.6</span>, <span class="number">171.</span>, <span class="number">177.8</span>, <span class="number">171.1</span>, <span class="number">174.</span>, <span class="number">177.1</span>, <span class="number">174.5</span>, <span class="number">156.9</span>, <span class="number">174.3</span>, <span class="number">164.1</span>, <span class="number">179.3</span>, <span class="number">167.4</span>, <span class="number">156.1</span>, <span class="number">158.4</span>, <span class="number">154.4</span>])</span><br></pre></td></tr></table></figure><pre><code>RUN: 0RUN: 5Run time: 13.416615009307861</code></pre><p><img src="output_15_1.png" alt="png"></p><p>The learning rate of your agent should look similar to ours, though it will not look exactly the same.If there are some spikey points that is okay. Due to stochasticity,  a few episodes may have taken much longer, causing some spikes in the plot. The trend of the line should be similar, though, generally decreasing to about 200 steps per run.<br><img src="sarsa_agent_initial.png" alt="alt text" title="Logo Title Text 1"></p><p>This result was using 8 tilings with 8x8 tiles on each. Let’s see if we can do better, and what different tilings look like. We will also text 2 tilings of 16x16 and 4 tilings of 32x32. These three choices produce the same number of features (512), but distributed quite differently. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare the three</span></span><br><span class="line">num_runs = <span class="number">20</span></span><br><span class="line">num_episodes = <span class="number">100</span></span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line"></span><br><span class="line">agent_runs = []</span><br><span class="line"><span class="comment"># alphas = [0.2, 0.4, 0.5, 1.0]</span></span><br><span class="line">alphas = [<span class="number">0.5</span>]</span><br><span class="line">agent_info_options = [&#123;<span class="string">"num_tiles"</span>: <span class="number">16</span>, <span class="string">"num_tilings"</span>: <span class="number">2</span>, <span class="string">"alpha"</span>: <span class="number">0.5</span>&#125;,</span><br><span class="line">                      &#123;<span class="string">"num_tiles"</span>: <span class="number">4</span>, <span class="string">"num_tilings"</span>: <span class="number">32</span>, <span class="string">"alpha"</span>: <span class="number">0.5</span>&#125;,</span><br><span class="line">                      &#123;<span class="string">"num_tiles"</span>: <span class="number">8</span>, <span class="string">"num_tilings"</span>: <span class="number">8</span>, <span class="string">"alpha"</span>: <span class="number">0.5</span>&#125;]</span><br><span class="line">agent_info_options = [&#123;<span class="string">"num_tiles"</span> : agent[<span class="string">"num_tiles"</span>], </span><br><span class="line">                       <span class="string">"num_tilings"</span>: agent[<span class="string">"num_tilings"</span>],</span><br><span class="line">                       <span class="string">"alpha"</span> : alpha&#125; <span class="keyword">for</span> agent <span class="keyword">in</span> agent_info_options <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</span><br><span class="line"></span><br><span class="line">agent = SarsaAgent</span><br><span class="line">env = mountaincar_env.Environment</span><br><span class="line"><span class="keyword">for</span> agent_info <span class="keyword">in</span> agent_info_options:</span><br><span class="line">    all_steps = []</span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(num_runs):</span><br><span class="line">        <span class="keyword">if</span> run % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"RUN: &#123;&#125;"</span>.format(run))</span><br><span class="line">        env = mountaincar_env.Environment</span><br><span class="line">        </span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        steps_per_episode = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">            rl_glue.rl_episode(<span class="number">15000</span>)</span><br><span class="line">            steps_per_episode.append(rl_glue.num_steps)</span><br><span class="line">        all_steps.append(np.array(steps_per_episode))</span><br><span class="line">    </span><br><span class="line">    agent_runs.append(np.mean(np.array(all_steps), axis=<span class="number">0</span>))</span><br><span class="line">    print(<span class="string">"stepsize:"</span>, rl_glue.agent.alpha)</span><br><span class="line">    print(<span class="string">"Run Time: &#123;&#125;"</span>.format(time.time() - start))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">10</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot(np.array(agent_runs).T)</span><br><span class="line">plt.xlabel(<span class="string">"Episode"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Steps Per Episode"</span>)</span><br><span class="line">plt.yscale(<span class="string">"linear"</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1000</span>)</span><br><span class="line">plt.legend([<span class="string">"num_tiles: &#123;&#125;, num_tilings: &#123;&#125;, alpha: &#123;&#125;"</span>.format(agent_info[<span class="string">"num_tiles"</span>], </span><br><span class="line">                                                               agent_info[<span class="string">"num_tilings"</span>],</span><br><span class="line">                                                               agent_info[<span class="string">"alpha"</span>])</span><br><span class="line">            <span class="keyword">for</span> agent_info <span class="keyword">in</span> agent_info_options])</span><br></pre></td></tr></table></figure><pre><code>RUN: 0RUN: 5RUN: 10RUN: 15stepsize: 0.25Run Time: 71.2762451171875RUN: 0RUN: 5RUN: 10RUN: 15stepsize: 0.015625Run Time: 38.23225665092468RUN: 0RUN: 5RUN: 10RUN: 15stepsize: 0.0625Run Time: 42.84800481796265&lt;matplotlib.legend.Legend at 0x7fbda7e76910&gt;</code></pre><p><img src="output_18_2.png" alt="png"></p><p>Here we can see that using 32 tilings and 4 x 4 tiles does a little better than 8 tilings with 8x8 tiles. Both seem to do much better than using 2 tilings, with 16 x 16 tiles.</p><h2 id="Section-3-Conclusion"><a href="#Section-3-Conclusion" class="headerlink" title="Section 3: Conclusion"></a>Section 3: Conclusion</h2><p>Congratulations! You have learned how to implement a control agent using function approximation. In this notebook you learned how to:</p><ul><li>Use function approximation in the control setting</li><li>Implement the Sarsa algorithm using tile coding</li><li>Compare three settings for tile coding to see their effect on our agent</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-3-Function-Approximation-and-Control&quot;&gt;&lt;a href=&quot;#Assignment-3-Function-Approximation-and-Control&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Semi Gradient TD with a Neural Network</title>
    <link href="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/"/>
    <id>https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/</id>
    <published>2020-10-14T09:18:55.000Z</published>
    <updated>2020-10-16T07:04:08.748Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Semi-gradient-TD-with-a-Neural-Network"><a href="#Assignment-2-Semi-gradient-TD-with-a-Neural-Network" class="headerlink" title="Assignment 2 - Semi-gradient TD with a Neural Network"></a>Assignment 2 - Semi-gradient TD with a Neural Network</h1><p>Welcome to Course 3 Programming Assignment 2. In the previous assignment, you implemented semi-gradient TD with State Aggregation for solving a <strong>policy evaluation task</strong>. In this assignment, you will implement <strong>semi-gradient TD with a simple Neural Network</strong> and use it for the same policy evaluation problem. </p><p>You will implement an agent to evaluate a fixed policy on the 500-State Randomwalk. As you may remember from the previous assignment, the 500-state Randomwalk includes 500 states. Each episode begins with the agent at the center and terminates when the agent goes far left beyond state 1 or far right beyond state 500. At each time step, the agent selects to move either left or right with equal probability. The environment determines how much the agent moves in the selected direction.</p><p><strong>In this assignment, you will:</strong></p><ul><li>Implement stochastic gradient descent method for state-value prediction.</li><li>Implement semi-gradient TD with a neural network as the function approximator and Adam algorithm.</li><li>Compare performance of semi-gradient TD with a neural network and semi-gradient TD with tile-coding.</li></ul><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>We import the following libraries that are required for this assignment:</p><ul><li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li><li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> : Library for plotting graphs in Python.</li><li><a href="http://www.jmlr.org/papers/v10/tanner09a.html" target="_blank" rel="noopener">RL-Glue</a> : Library for reinforcement learning experiments.</li><li><a href="https://tqdm.github.io/" target="_blank" rel="noopener">tqdm</a> : A package to display progress bar when running experiments.</li><li>BaseOptimizer : An abstract class that specifies the optimizer API for Agent.</li><li>plot_script : Custom script to plot results.</li><li>RandomWalkEnvironment : The Randomwalk environment script from Course 3 Assignment 1.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="comment"># DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os, shutil</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> environment <span class="keyword">import</span> BaseEnvironment</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> optimizer <span class="keyword">import</span> BaseOptimizer</span><br><span class="line"><span class="keyword">import</span> plot_script</span><br><span class="line"><span class="keyword">from</span> randomwalk_environment <span class="keyword">import</span> RandomWalkEnvironment</span><br></pre></td></tr></table></figure><h2 id="Section-1-Create-semi-gradient-TD-with-a-Neural-Network"><a href="#Section-1-Create-semi-gradient-TD-with-a-Neural-Network" class="headerlink" title="Section 1: Create semi-gradient TD with a Neural Network"></a>Section 1: Create semi-gradient TD with a Neural Network</h2><p>In this section, you will implement an Agent that learns with semi-gradient TD with a neural network. You will use a neural network with one hidden layer. The input of the neural network is the one-hot encoding of the state number. We use the one-hot encoding of the state number instead of the state number itself because we do not want to build the prior knowledge that integer number inputs close to each other have similar values. The hidden layer contains 100 rectifier linear units (ReLUs) which pass their input if it is bigger than one and return 0 otherwise. ReLU gates are commonly used in neural networks due to their nice properties such as the sparsity of the activation and having non-vanishing gradients. The output of the neural network is the estimated state value. It is a linear function of the hidden units as is commonly the case when estimating the value of a continuous target using neural networks.</p><p>The neural network looks like this:<br><img src="nn_structure.png" alt></p><p>For a given input, $s$, value of $s$ is computed by:</p><script type="math/tex; mode=display">\begin{align} \psi &= sW^{[0]} + b^{[0]} \\x &= \textit{max}(0, \psi) \\v &= xW^{[1]} + b^{[1]}\end{align}</script><p>where $W^{[0]}$, $b^{[0]}$, $W^{[1]}$, $b^{[1]}$  are the parameters of the network and will be learned when training the agent.</p><h2 id="1-1-Implement-helper-methods"><a href="#1-1-Implement-helper-methods" class="headerlink" title="1-1: Implement helper methods"></a>1-1: Implement helper methods</h2><p>Before implementing the agent, you first implement some helper functions which you will later use in agent’s main methods. </p><h3 id="Implement-get-value"><a href="#Implement-get-value" class="headerlink" title="Implement get_value()"></a>Implement <code>get_value()</code></h3><p>First, you will implement get_value() method which feeds an input $s$ into the neural network and returns the output of the network $v$ according to the equations above. To implement get_value(), take into account the following notes:</p><ul><li><code>get_value()</code> gets the one-hot encoded state number denoted by s as an input. </li><li><p><code>get_value()</code> receives the weights of the neural network as input, denoted by weights and structured as an array of dictionaries. Each dictionary corresponds to weights from one layer of the neural network to the next. Each dictionary includes $W$ and $b$. The shape of the elements in weights are as follows:</p><ul><li>weights[0][“W”]: num_states $\times$ num_hidden_units</li><li>weights[0][“b”]: 1 $\times$ num_hidden_units</li><li>weights[1][“W”]: num_hidden_units $\times$ 1</li><li>weights[1][“b”]: 1 $\times$ 1</li></ul></li><li><p>The input of the neural network is a sparse vector. To make computation faster, we take advantage of input sparsity. To do so, we provided a helper method <code>my_matmul()</code>. <strong>Make sure that you use <code>my_matmul()</code> for all matrix multiplications except for element-wise multiplications in this notebook.</strong></p></li><li>The max operator used for computing $x$ is element-wise. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_matmul</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given matrices x1 and x2, return the multiplication of them</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    result = np.zeros((x1.shape[<span class="number">0</span>], x2.shape[<span class="number">1</span>]))</span><br><span class="line">    x1_non_zero_indices = x1.nonzero()</span><br><span class="line">    <span class="keyword">if</span> x1.shape[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">and</span> len(x1_non_zero_indices[<span class="number">1</span>]) == <span class="number">1</span>:</span><br><span class="line">        result = x2[x1_non_zero_indices[<span class="number">1</span>], :]</span><br><span class="line">    <span class="keyword">elif</span> x1.shape[<span class="number">1</span>] == <span class="number">1</span> <span class="keyword">and</span> len(x1_non_zero_indices[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        result[x1_non_zero_indices[<span class="number">0</span>], :] = x2 * x1[x1_non_zero_indices[<span class="number">0</span>], <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = np.matmul(x1, x2)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_value</span><span class="params">(s, weights)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute value of input s given the weights of a neural network</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### Compute the ouput of the neural network, v, for input s</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    hidden = my_matmul(s, weights[<span class="number">0</span>][<span class="string">"W"</span>]) + weights[<span class="number">0</span>][<span class="string">"b"</span>]</span><br><span class="line">    x = np.maximum(hidden, np.zeros_like(hidden))</span><br><span class="line">    v = my_matmul(x, weights[<span class="number">1</span>][<span class="string">"W"</span>]) + weights[<span class="number">1</span>][<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><p>Run the following code to test your implementation of the <code>get_value()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 10 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line">s = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">"asserts/get_value_weights.npz"</span>)</span><br><span class="line">weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W0"</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b0"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W1"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">estimated_value = get_value(s, weights)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Estimated value: &#123;&#125;"</span>.format(estimated_value))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(estimated_value, [[<span class="number">-0.21915705</span>]]))</span><br></pre></td></tr></table></figure><pre><code>Estimated value: [[-0.21915705]]</code></pre><p><strong>Expected output</strong>:</p><pre><code>Estimated value: [[-0.21915705]]</code></pre><h3 id="Implement-get-gradient"><a href="#Implement-get-gradient" class="headerlink" title="Implement get_gradient()"></a>Implement <code>get_gradient()</code></h3><p>You will also implement <code>get_gradient()</code> method which computes the gradient of the value function for a given input, using backpropagation. You will later use this function to update the value function. </p><p>As you know, we compute the value of a state $s$ according to: </p><script type="math/tex; mode=display">\begin{align} \psi &= sW^{[0]} + b^{[0]} \\x &= \textit{max}(0, \psi) \\v &= xW^{[1]} + b^{[1]}\end{align}</script><p>To update the weights of the neural network ($W^{[0]}$, $b^{[0]}$, $W^{[1]}$, $b^{[1]}$), we compute the gradient of $v$ with respect to the weights according to:</p><script type="math/tex; mode=display">\begin{align} \frac{\partial v}{\partial W^{[0]}} &= s^T(W^{[1]T} \odot I_{x>0}) \\\frac{\partial v}{\partial b^{[0]}} &= W^{[1]T} \odot I_{x>0} \\\frac{\partial v}{\partial W^{[1]}} &= x^T \\\frac{\partial v}{\partial b^{[1]}} &= 1\end{align}</script><p>where $\odot$ denotes element-wise matrix multiplication and $I_{x&gt;0}$ is the gradient of the ReLU activation function which is an indicator whose $i$th element is 1 if $x[i]&gt;0$ and 0 otherwise.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gradient</span><span class="params">(s, weights)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given inputs s and weights, return the gradient of v with respect to the weights</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Compute the gradient of the value function with respect to W0, b0, W1, b1 for input s</span></span><br><span class="line">    <span class="comment"># grads[0]["W"] = ?</span></span><br><span class="line">    <span class="comment"># grads[0]["b"] = ?</span></span><br><span class="line">    <span class="comment"># grads[1]["W"] = ?</span></span><br><span class="line">    <span class="comment"># grads[1]["b"] = ?</span></span><br><span class="line">    <span class="comment"># Note that grads[0]["W"], grads[0]["b"], grads[1]["W"], and grads[1]["b"] should have the same shape as </span></span><br><span class="line">    <span class="comment"># weights[0]["W"], weights[0]["b"], weights[1]["W"], and weights[1]["b"] respectively</span></span><br><span class="line">    <span class="comment"># Note that to compute the gradients, you need to compute the activation of the hidden layer (x)</span></span><br><span class="line"></span><br><span class="line">    grads = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights))]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    hidden = my_matmul(s, weights[<span class="number">0</span>][<span class="string">"W"</span>]) + weights[<span class="number">0</span>][<span class="string">"b"</span>]</span><br><span class="line">    x = np.maximum(hidden, np.zeros_like(hidden))</span><br><span class="line">    </span><br><span class="line">    grads[<span class="number">0</span>][<span class="string">"W"</span>] = my_matmul(np.transpose(s),np.maximum(np.transpose(weights[<span class="number">1</span>][<span class="string">"W"</span>]),<span class="number">0</span>))</span><br><span class="line">    grads[<span class="number">0</span>][<span class="string">"b"</span>] = np.maximum(np.transpose(weights[<span class="number">1</span>][<span class="string">"W"</span>]),<span class="number">0</span>)</span><br><span class="line">    grads[<span class="number">1</span>][<span class="string">"W"</span>] = np.transpose(x)</span><br><span class="line">    grads[<span class="number">1</span>][<span class="string">"b"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>Run the following code to test your implementation of the <code>get_gradient()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line">s = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">"asserts/get_gradient_weights.npz"</span>)</span><br><span class="line">weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W0"</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b0"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W1"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">grads = get_gradient(s, weights)</span><br><span class="line"></span><br><span class="line">grads_answer = np.load(<span class="string">"asserts/get_gradient_grads.npz"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">0</span>][<span class="string">"W"</span>], grads_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">0</span>][<span class="string">"b"</span>], grads_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">1</span>][<span class="string">"W"</span>], grads_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">1</span>][<span class="string">"b"</span>], grads_answer[<span class="string">"b1"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Expected output</strong>:</p><pre><code>grads[0][&quot;W&quot;] [[0.         0.        ] [0.         0.        ] [0.         0.        ] [0.76103773 0.12167502] [0.         0.        ]] grads[0][&quot;b&quot;] [[0.76103773 0.12167502]] grads[1][&quot;W&quot;] [[0.69198983] [0.82403662]] grads[1][&quot;b&quot;] [[1.]] </code></pre><h3 id="Implement-stochastic-gradient-descent-method-for-state-value-prediction"><a href="#Implement-stochastic-gradient-descent-method-for-state-value-prediction" class="headerlink" title="Implement stochastic gradient descent method for state-value prediction"></a>Implement stochastic gradient descent method for state-value prediction</h3><p>In this section, you will implement stochastic gradient descent (SGD) method for state_value prediction. Here is the basic SGD update for state-value prediction with TD:</p><script type="math/tex; mode=display">\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha \delta_t \nabla \hat{v}(S_t,\mathbf{w_{t}})</script><p>At each time step, we update the weights in the direction  $g_t = \delta_t \nabla \hat{v}(S_t,\mathbf{w_t})$ using a fixed step-size $\alpha$. $\delta_t = R_{t+1} + \gamma \hat{v}(S_{t+1},\mathbf{w_{t}}) - \hat{v}(S_t,\mathbf{w_t})$ is the TD-error. $\nabla \hat{v}(S_t,\mathbf{w_{t}})$ is the gradient of the value function with respect to the weights.</p><p>The following cell includes the SGD class. You will complete the <code>update_weight()</code> method of SGD assuming that the weights and update g are provided.</p><p><strong>As you know, in this assignment, we structured the weights as an array of dictionaries. Note that the updates $g_t$, in the case of TD, is $\delta_t \nabla \hat{v}(S_t,\mathbf{w_t})$. As a result, $g_t$ has the same structure as $\nabla \hat{v}(S_t,\mathbf{w_t})$ which is also an array of dictionaries.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SGD</span><span class="params">(BaseOptimizer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimizer_init</span><span class="params">(self, optimizer_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the optimizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the stochastic gradient descent method.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume optimizer_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            step_size: float</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.step_size = optimizer_info.get(<span class="string">"step_size"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weights</span><span class="params">(self, weights, g)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Given weights and update g, return updated weights</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights)):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> weights[i].keys():</span><br><span class="line">                </span><br><span class="line">                <span class="comment">### update weights</span></span><br><span class="line">                <span class="comment"># weights[i][param] = None</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                weights[i][param] += self.step_size * g[i][param]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure><p>Run the following code to test your implementation of the <code>update_weights()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">"asserts/update_weights_weights.npz"</span>)</span><br><span class="line">weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W0"</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b0"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W1"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">g_data = np.load(<span class="string">"asserts/update_weights_g.npz"</span>)</span><br><span class="line">g = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">g[<span class="number">0</span>][<span class="string">"W"</span>] = g_data[<span class="string">"W0"</span>]</span><br><span class="line">g[<span class="number">0</span>][<span class="string">"b"</span>] = g_data[<span class="string">"b0"</span>]</span><br><span class="line">g[<span class="number">1</span>][<span class="string">"W"</span>] = g_data[<span class="string">"W1"</span>]</span><br><span class="line">g[<span class="number">1</span>][<span class="string">"b"</span>] = g_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">test_sgd = SGD()</span><br><span class="line">optimizer_info = &#123;<span class="string">"step_size"</span>: <span class="number">0.3</span>&#125;</span><br><span class="line">test_sgd.optimizer_init(optimizer_info)</span><br><span class="line">updated_weights = test_sgd.update_weights(weights, g)</span><br><span class="line"></span><br><span class="line"><span class="comment"># updated weights asserts</span></span><br><span class="line">updated_weights_answer = np.load(<span class="string">"asserts/update_weights_updated_weights.npz"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">"W"</span>], updated_weights_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">"b"</span>], updated_weights_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">"W"</span>], updated_weights_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">"b"</span>], updated_weights_answer[<span class="string">"b1"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Expected output</strong>:</p><pre><code>updated_weights[0][&quot;W&quot;] [[ 1.17899492  0.53656321] [ 0.58008221  1.47666572] [ 1.01909411 -1.10248056] [ 0.72490408  0.06828853] [-0.20609725  0.69034095]] updated_weights[0][&quot;b&quot;] [[-0.18484533  0.92844539]] updated_weights[1][&quot;W&quot;] [[0.70488257] [0.58150878]] updated_weights[1][&quot;b&quot;] [[0.88467086]] </code></pre><h3 id="Adam-Algorithm"><a href="#Adam-Algorithm" class="headerlink" title="Adam Algorithm"></a>Adam Algorithm</h3><p>In this assignment, instead of using SGD for updating the weights, we use a more advanced algorithm called Adam. The Adam algorithm improves the SGD update with two concepts: adaptive vector step-sizes and momentum. It keeps estimates of the mean and second moment of the updates, denoted by $\mathbf{m}$ and $\mathbf{v}$ respectively:</p><script type="math/tex; mode=display">\mathbf{m_t} = \beta_m \mathbf{m_{t-1}} + (1 - \beta_m)g_t \\\mathbf{v_t} = \beta_v \mathbf{v_{t-1}} + (1 - \beta_v)g^2_t</script><p>Given that $\mathbf{m}$ and $\mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $\mathbf{\hat{m}}$ and $\mathbf{\hat{v}}$ as:</p><script type="math/tex; mode=display">\mathbf{\hat{m_t}} = \frac{\mathbf{m_t}}{1 - \beta_m^t} \\\mathbf{\hat{v_t}} = \frac{\mathbf{v_t}}{1 - \beta_v^t}</script><p>The weights are then updated as follows:</p><script type="math/tex; mode=display">\mathbf{w_t} = \mathbf{w_{t-1}} + \frac{\alpha}{\sqrt{\mathbf{\hat{v_t}}}+\epsilon} \mathbf{\hat{m_t}}</script><p>When implementing the agent you will use the Adam algorithm instead of SGD because it is more efficient. We have already provided you the implementation of the Adam algorithm in the cell below. You will use it when implementing your agent. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(BaseOptimizer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimizer_init</span><span class="params">(self, optimizer_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the optimizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume optimizer_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: integer,</span></span><br><span class="line"><span class="string">            num_hidden_layer: integer,</span></span><br><span class="line"><span class="string">            num_hidden_units: integer,</span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            self.beta_m: float</span></span><br><span class="line"><span class="string">            self.beta_v: float</span></span><br><span class="line"><span class="string">            self.epsilon: float</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        self.num_states = optimizer_info.get(<span class="string">"num_states"</span>)</span><br><span class="line">        self.num_hidden_layer = optimizer_info.get(<span class="string">"num_hidden_layer"</span>)</span><br><span class="line">        self.num_hidden_units = optimizer_info.get(<span class="string">"num_hidden_units"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Specify Adam algorithm's hyper parameters</span></span><br><span class="line">        self.step_size = optimizer_info.get(<span class="string">"step_size"</span>)</span><br><span class="line">        self.beta_m = optimizer_info.get(<span class="string">"beta_m"</span>)</span><br><span class="line">        self.beta_v = optimizer_info.get(<span class="string">"beta_v"</span>)</span><br><span class="line">        self.epsilon = optimizer_info.get(<span class="string">"epsilon"</span>)</span><br><span class="line"></span><br><span class="line">        self.layer_size = np.array([self.num_states, self.num_hidden_units, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize Adam algorithm's m and v</span></span><br><span class="line">        self.m = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        self.v = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize self.m[i]["W"], self.m[i]["b"], self.v[i]["W"], self.v[i]["b"] to zero</span></span><br><span class="line">            self.m[i][<span class="string">"W"</span>] = np.zeros((self.layer_size[i], self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.m[i][<span class="string">"b"</span>] = np.zeros((<span class="number">1</span>, self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">"W"</span>] = np.zeros((self.layer_size[i], self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">"b"</span>] = np.zeros((<span class="number">1</span>, self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat</span></span><br><span class="line">        self.beta_m_product = self.beta_m</span><br><span class="line">        self.beta_v_product = self.beta_v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weights</span><span class="params">(self, weights, g)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Given weights and update g, return updated weights</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights)):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment">### update self.m and self.v</span></span><br><span class="line">                self.m[i][param] = self.beta_m * self.m[i][param] + (<span class="number">1</span> - self.beta_m) * g[i][param]</span><br><span class="line">                self.v[i][param] = self.beta_v * self.v[i][param] + (<span class="number">1</span> - self.beta_v) * (g[i][param] * g[i][param])</span><br><span class="line"></span><br><span class="line">                <span class="comment">### compute m_hat and v_hat</span></span><br><span class="line">                m_hat = self.m[i][param] / (<span class="number">1</span> - self.beta_m_product)</span><br><span class="line">                v_hat = self.v[i][param] / (<span class="number">1</span> - self.beta_v_product)</span><br><span class="line"></span><br><span class="line">                <span class="comment">### update weights</span></span><br><span class="line">                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)</span><br><span class="line">                </span><br><span class="line">        <span class="comment">### update self.beta_m_product and self.beta_v_product</span></span><br><span class="line">        self.beta_m_product *= self.beta_m</span><br><span class="line">        self.beta_v_product *= self.beta_v</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure><h2 id="1-2-Implement-Agent-Methods"><a href="#1-2-Implement-Agent-Methods" class="headerlink" title="1-2: Implement Agent Methods"></a>1-2: Implement Agent Methods</h2><p>In this section, you will implement <code>agent_init()</code>, <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code>.</p><p>In <code>agent_init()</code>, you will:</p><ul><li>specify the neural network structure by filling self.layer_size with the size of the input layer, hidden layer, and output layer.</li><li>initialize the network’s parameters. We show the parameters as an array of dictionaries, self.weights, where each dictionary corresponds to weights from one layer to the next. Each dictionary includes $W$ and $b$. </li></ul><script type="math/tex; mode=display">\sqrt{ \frac{2}{ input \, of \, each \, node } }</script><p>This initialization heuristic is commonly used when using ReLU gates and helps keep the output of a neuron from getting too big or too small. To initialize the network’s parameters, use <strong>self.rand_generator.normal()</strong> which draws random samples from a normal distribution. The parameters of self.rand_generator.normal are mean of the distribution, standard deviation of the distribution, and output shape in the form of tuple of integers.</p><p>In <code>agent_start()</code>, you will:</p><ul><li>specify self.last_state and self.last_action.</li></ul><p>In <code>agent_step()</code> and <code>agent_end()</code>, you will:</p><ul><li>compute the TD error using $v(S_t)$ and $v(S_{t+1})$. To compute the value function for $S_t$ and $S_{t+1}$, you will get their one-hot encoding using <code>one_hot()</code> method that we provided below. You feed the one-hot encoded state number to the neural networks using <code>get_value()</code> method that you implemented above. Note that <code>one_hot()</code> method returns the one-hot encoding of a state as a numpy array of shape (1, num_states).</li><li>retrieve the gradients using <code>get_gradient()</code> function that you implemented.</li><li>use Adam_algorithm that we provided to update the neural network’s parameters, self.weights.</li><li>use <code>agent_policy()</code> method to select actions with. (only in <code>agent_step()</code>)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(state, num_states)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given num_state and a state, return the one-hot encoding of the state</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Create the one-hot encoding of state</span></span><br><span class="line">    <span class="comment"># one_hot_vector is a numpy array of shape (1, num_states)</span></span><br><span class="line">    </span><br><span class="line">    one_hot_vector = np.zeros((<span class="number">1</span>, num_states))</span><br><span class="line">    one_hot_vector[<span class="number">0</span>, int((state - <span class="number">1</span>))] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot_vector</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.name = <span class="string">"td_agent"</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the semi-gradient TD with a Neural Network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: integer,</span></span><br><span class="line"><span class="string">            num_hidden_layer: integer,</span></span><br><span class="line"><span class="string">            num_hidden_units: integer,</span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            discount_factor: float,</span></span><br><span class="line"><span class="string">            self.beta_m: float</span></span><br><span class="line"><span class="string">            self.beta_v: float</span></span><br><span class="line"><span class="string">            self.epsilon: float</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Set random seed for weights initialization for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set random seed for policy for each run</span></span><br><span class="line">        self.policy_rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set attributes according to agent_info</span></span><br><span class="line">        self.num_states = agent_info.get(<span class="string">"num_states"</span>)</span><br><span class="line">        self.num_hidden_layer = agent_info.get(<span class="string">"num_hidden_layer"</span>)</span><br><span class="line">        self.num_hidden_units = agent_info.get(<span class="string">"num_hidden_units"</span>)</span><br><span class="line">        self.discount_factor = agent_info.get(<span class="string">"discount_factor"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Define the neural network's structure</span></span><br><span class="line">        <span class="comment"># Specify self.layer_size which shows the number of nodes in each layer</span></span><br><span class="line">        <span class="comment"># self.layer_size = np.array([None, None, None])</span></span><br><span class="line">        <span class="comment"># Hint: Checkout the NN diagram at the beginning of the notebook</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.layer_size = np.array([self.num_states, self.num_hidden_units, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the neural network's parameter</span></span><br><span class="line">        self.weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Initialize self.weights[i]["W"] and self.weights[i]["b"] using self.rand_generator.normal()</span></span><br><span class="line">            <span class="comment"># Note that The parameters of self.rand_generator.normal are mean of the distribution, </span></span><br><span class="line">            <span class="comment"># standard deviation of the distribution, and output shape in the form of tuple of integers.</span></span><br><span class="line">            <span class="comment"># To specify output shape, use self.layer_size.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line">            <span class="comment"># your code here</span></span><br><span class="line">            self.weights[i][<span class="string">"W"</span>] = self.rand_generator.normal(loc=<span class="number">0</span>, scale= np.sqrt(<span class="number">2.0</span> / self.layer_size[i]) ,size = (self.layer_size[i],self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.weights[i][<span class="string">"b"</span>] = self.rand_generator.normal(loc=<span class="number">0</span>, scale= np.sqrt(<span class="number">2.0</span> / self.layer_size[i]) ,size = (<span class="number">1</span>,self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Specify the optimizer</span></span><br><span class="line">        self.optimizer = Adam()</span><br><span class="line">        self.optimizer.optimizer_init(&#123;</span><br><span class="line">            <span class="string">"num_states"</span>: agent_info[<span class="string">"num_states"</span>],</span><br><span class="line">            <span class="string">"num_hidden_layer"</span>: agent_info[<span class="string">"num_hidden_layer"</span>],</span><br><span class="line">            <span class="string">"num_hidden_units"</span>: agent_info[<span class="string">"num_hidden_units"</span>],</span><br><span class="line">            <span class="string">"step_size"</span>: agent_info[<span class="string">"step_size"</span>],</span><br><span class="line">            <span class="string">"beta_m"</span>: agent_info[<span class="string">"beta_m"</span>],</span><br><span class="line">            <span class="string">"beta_v"</span>: agent_info[<span class="string">"beta_v"</span>],</span><br><span class="line">            <span class="string">"epsilon"</span>: agent_info[<span class="string">"epsilon"</span>],</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">        self.last_state = <span class="keyword">None</span></span><br><span class="line">        self.last_action = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_policy</span><span class="params">(self, state)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Set chosen_action as 0 or 1 with equal probability. </span></span><br><span class="line">        chosen_action = self.policy_rand_generator.choice([<span class="number">0</span>,<span class="number">1</span>])    </span><br><span class="line">        <span class="keyword">return</span> chosen_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### select action given state (using self.agent_policy()), and save current state and action</span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_action = self.agent_policy(state)</span><br><span class="line">        self.last_state = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment's step based, where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        last_state_vec = one_hot(self.last_state, self.num_states)</span><br><span class="line">        last_value = get_value(last_state_vec, self.weights)</span><br><span class="line"></span><br><span class="line">        state_vec = one_hot(state, self.num_states)</span><br><span class="line">        value = get_value(state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Compute TD error</span></span><br><span class="line">        <span class="comment"># delta = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        delta = reward + self.discount_factor * value - last_value</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Retrieve gradients</span></span><br><span class="line">        <span class="comment"># grads = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        grads = get_gradient(last_state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Compute g (1 line)</span></span><br><span class="line">        g = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment"># g[i][param] = None</span></span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                g[i][param] = grads[i][param] * delta</span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update the weights using self.optimizer</span></span><br><span class="line">        <span class="comment"># self.weights = None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.optimizer.update_weights(self.weights, g)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update self.last_state and self.last_action</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_action = self.agent_policy(state)</span><br><span class="line">        self.last_state = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        last_state_vec = one_hot(self.last_state, self.num_states)</span><br><span class="line">        last_value = get_value(last_state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### compute TD error</span></span><br><span class="line">        <span class="comment"># delta = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        delta = reward - last_value</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Retrieve gradients</span></span><br><span class="line">        <span class="comment"># grads = None</span></span><br><span class="line">        grads = get_gradient(last_state_vec, self.weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Compute g</span></span><br><span class="line">        g = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment"># g[i][param] = None</span></span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                g[i][param] = grads[i][param] * delta</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update the weights using self.optimizer</span></span><br><span class="line">        <span class="comment"># self.weights = None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.optimizer.update_weights(self.weights, g)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> message == <span class="string">'get state value'</span>:</span><br><span class="line">            state_value = np.zeros(self.num_states)</span><br><span class="line">            <span class="keyword">for</span> state <span class="keyword">in</span> range(<span class="number">1</span>, self.num_states + <span class="number">1</span>):</span><br><span class="line">                s = one_hot(state, self.num_states)</span><br><span class="line">                state_value[state - <span class="number">1</span>] = get_value(s, self.weights)</span><br><span class="line">            <span class="keyword">return</span> state_value</span><br></pre></td></tr></table></figure><p>Run the following code to test your implementation of the <code>agent_init()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.25</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"layer_size: &#123;&#125;"</span>.format(test_agent.layer_size))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.layer_size, np.array([agent_info[<span class="string">"num_states"</span>], </span><br><span class="line">                                                    agent_info[<span class="string">"num_hidden_units"</span>], </span><br><span class="line">                                                    <span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>].shape == (agent_info[<span class="string">"num_states"</span>], agent_info[<span class="string">"num_hidden_units"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>].shape == (<span class="number">1</span>, agent_info[<span class="string">"num_hidden_units"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>].shape == (agent_info[<span class="string">"num_hidden_units"</span>], <span class="number">1</span>))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>].shape == (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">agent_weight_answer = np.load(<span class="string">"asserts/agent_init_weights_1.npz"</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>], agent_weight_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>], agent_weight_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>], agent_weight_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>], agent_weight_answer[<span class="string">"b1"</span>]))</span><br></pre></td></tr></table></figure><pre><code>layer_size: [5 2 1]</code></pre><p><strong>Expected output</strong>:</p><pre><code>layer_size: [5 2 1]weights[0][&quot;W&quot;] shape: (5, 2)weights[0][&quot;b&quot;] shape: (1, 2)weights[1][&quot;W&quot;] shape: (2, 1)weights[1][&quot;b&quot;] shape: (1, 1) weights[0][&quot;W&quot;] [[ 1.11568467  0.25308164] [ 0.61900825  1.4172653 ] [ 1.18114738 -0.6180848 ] [ 0.60088868 -0.0957267 ] [-0.06528133  0.25968529]] weights[0][&quot;b&quot;] [[0.09110115 0.91976332]] weights[1][&quot;W&quot;] [[0.76103773] [0.12167502]] weights[1][&quot;b&quot;] [[0.44386323]]</code></pre><p>Run the following code to test your implementation of the <code>agent_start()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">10</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose state = 250</span></span><br><span class="line">state = <span class="number">250</span></span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line">test_agent.agent_start(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_state == <span class="number">250</span>)</span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_action == <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>Expected output</strong>:</p><pre><code>Agent state: 250Agent selected action: 1</code></pre><p>Run the following code to test your implementation of the <code>agent_step()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load initial weights</span></span><br><span class="line">agent_initial_weight = np.load(<span class="string">"asserts/agent_step_initial_weights.npz"</span>)</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>] = agent_initial_weight[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>] = agent_initial_weight[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>] = agent_initial_weight[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>] = agent_initial_weight[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load m and v for the optimizer</span></span><br><span class="line">m_data = np.load(<span class="string">"asserts/agent_step_initial_m.npz"</span>)</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">"W"</span>] = m_data[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">"b"</span>] = m_data[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">"W"</span>] = m_data[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">"b"</span>] = m_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">v_data = np.load(<span class="string">"asserts/agent_step_initial_v.npz"</span>)</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">"W"</span>] = v_data[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">"b"</span>] = v_data[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">"W"</span>] = v_data[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">"b"</span>] = v_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 3</span></span><br><span class="line">start_state = <span class="number">3</span></span><br><span class="line">test_agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and the next state observed was State 1</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">next_state = <span class="number">1</span></span><br><span class="line">test_agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line">agent_updated_weight_answer = np.load(<span class="string">"asserts/agent_step_updated_weights.npz"</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>], agent_updated_weight_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>], agent_updated_weight_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>], agent_updated_weight_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>], agent_updated_weight_answer[<span class="string">"b1"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_state == <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_action == <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>Expected output</strong>:</p><pre><code>updated_weights[0][&quot;W&quot;] [[ 1.10893459  0.30763738] [ 0.63690565  1.14778865] [ 1.23397791 -0.48152743] [ 0.72792093 -0.15829832] [ 0.15021996  0.39822163]] updated_weights[0][&quot;b&quot;] [[0.29798822 0.96254535]] updated_weights[1][&quot;W&quot;] [[0.76628754] [0.11486511]] updated_weights[1][&quot;b&quot;] [[0.58530057]] Agent last state: 1Agent last action: 1 </code></pre><p>Run the following code to test your implementation of the <code>agent_end()</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load initial weights</span></span><br><span class="line">agent_initial_weight = np.load(<span class="string">"asserts/agent_end_initial_weights.npz"</span>)</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>] = agent_initial_weight[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>] = agent_initial_weight[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>] = agent_initial_weight[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>] = agent_initial_weight[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load m and v for the optimizer</span></span><br><span class="line">m_data = np.load(<span class="string">"asserts/agent_step_initial_m.npz"</span>)</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">"W"</span>] = m_data[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">"b"</span>] = m_data[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">"W"</span>] = m_data[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">"b"</span>] = m_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">v_data = np.load(<span class="string">"asserts/agent_step_initial_v.npz"</span>)</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">"W"</span>] = v_data[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">"b"</span>] = v_data[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">"W"</span>] = v_data[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">"b"</span>] = v_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 4</span></span><br><span class="line">start_state = <span class="number">4</span></span><br><span class="line">test_agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and reached the terminal state</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">test_agent.agent_end(reward)</span><br><span class="line"></span><br><span class="line"><span class="comment"># updated weights asserts</span></span><br><span class="line">agent_updated_weight_answer = np.load(<span class="string">"asserts/agent_end_updated_weights.npz"</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>], agent_updated_weight_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>], agent_updated_weight_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>], agent_updated_weight_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>], agent_updated_weight_answer[<span class="string">"b1"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Expected output:</strong></p><pre><code>updated_weights[0][&quot;W&quot;] [[ 1.10893459  0.30763738] [ 0.63690565  1.14778865] [ 1.17531054 -0.51043162] [ 0.75062903 -0.13736817] [ 0.15021996  0.39822163]] updated_weights[0][&quot;b&quot;] [[0.30846523 0.95937346]] updated_weights[1][&quot;W&quot;] [[0.68861703] [0.15986364]] updated_weights[1][&quot;b&quot;] [[0.586074]] </code></pre><h2 id="Section-2-Run-Experiment"><a href="#Section-2-Run-Experiment" class="headerlink" title="Section 2 - Run Experiment"></a>Section 2 - Run Experiment</h2><p>Now that you implemented the agent, we can run the experiment. Similar to Course 3 Programming Assignment 1, we will plot the learned state value function and the learning curve of the TD agent. To plot the learning curve, we use Root Mean Squared Value Error (RMSVE). </p><h2 id="2-1-Run-Experiment-for-Semi-gradient-TD-with-a-Neural-Network"><a href="#2-1-Run-Experiment-for-Semi-gradient-TD-with-a-Neural-Network" class="headerlink" title="2-1: Run Experiment for Semi-gradient TD with a Neural Network"></a>2-1: Run Experiment for Semi-gradient TD with a Neural Network</h2><p>We have already provided you the experiment/plot code, so you can go ahead and run the two cells below.</p><p>Note that running the cell below will take <strong>approximately 12 minutes</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">true_state_val = np.load(<span class="string">'data/true_V.npy'</span>)    </span><br><span class="line">state_distribution = np.load(<span class="string">'data/state_distribution.npy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_RMSVE</span><span class="params">(learned_state_val)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span>(len(true_state_val) == len(learned_state_val) == len(state_distribution))</span><br><span class="line">    MSVE = np.sum(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))</span><br><span class="line">    RMSVE = np.sqrt(MSVE)</span><br><span class="line">    <span class="keyword">return</span> RMSVE</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define function to run experiment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(environment, agent, environment_parameters, agent_parameters, experiment_parameters)</span>:</span></span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># save rmsve at the end of each episode</span></span><br><span class="line">    agent_rmsve = np.zeros((experiment_parameters[<span class="string">"num_runs"</span>], </span><br><span class="line">                            int(experiment_parameters[<span class="string">"num_episodes"</span>]/experiment_parameters[<span class="string">"episode_eval_frequency"</span>]) + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save learned state value at the end of each run</span></span><br><span class="line">    agent_state_val = np.zeros((experiment_parameters[<span class="string">"num_runs"</span>], </span><br><span class="line">                                environment_parameters[<span class="string">"num_states"</span>]))</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">"num_states"</span>: environment_parameters[<span class="string">"num_states"</span>],</span><br><span class="line">                <span class="string">"start_state"</span>: environment_parameters[<span class="string">"start_state"</span>],</span><br><span class="line">                <span class="string">"left_terminal_state"</span>: environment_parameters[<span class="string">"left_terminal_state"</span>],</span><br><span class="line">                <span class="string">"right_terminal_state"</span>: environment_parameters[<span class="string">"right_terminal_state"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span>: environment_parameters[<span class="string">"num_states"</span>],</span><br><span class="line">                  <span class="string">"num_hidden_layer"</span>: agent_parameters[<span class="string">"num_hidden_layer"</span>],</span><br><span class="line">                  <span class="string">"num_hidden_units"</span>: agent_parameters[<span class="string">"num_hidden_units"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span>: agent_parameters[<span class="string">"step_size"</span>],</span><br><span class="line">                  <span class="string">"discount_factor"</span>: environment_parameters[<span class="string">"discount_factor"</span>],</span><br><span class="line">                  <span class="string">"beta_m"</span>: agent_parameters[<span class="string">"beta_m"</span>],</span><br><span class="line">                  <span class="string">"beta_v"</span>: agent_parameters[<span class="string">"beta_v"</span>],</span><br><span class="line">                  <span class="string">"epsilon"</span>: agent_parameters[<span class="string">"epsilon"</span>]</span><br><span class="line">                 &#125;</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'Setting - Neural Network with 100 hidden units'</span>)</span><br><span class="line">    os.system(<span class="string">'sleep 1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># one agent setting</span></span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_runs"</span>]+<span class="number">1</span>)):</span><br><span class="line">        env_info[<span class="string">"seed"</span>] = run</span><br><span class="line">        agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute initial RMSVE before training</span></span><br><span class="line">        current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">        agent_rmsve[run<span class="number">-1</span>, <span class="number">0</span>] = calc_RMSVE(current_V)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_episodes"</span>]+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># run episode</span></span><br><span class="line">            rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> episode % experiment_parameters[<span class="string">"episode_eval_frequency"</span>] == <span class="number">0</span>:</span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">                agent_rmsve[run<span class="number">-1</span>, int(episode/experiment_parameters[<span class="string">"episode_eval_frequency"</span>])] = calc_RMSVE(current_V)</span><br><span class="line">            <span class="keyword">elif</span> episode == experiment_parameters[<span class="string">"num_episodes"</span>]: <span class="comment"># if last episode</span></span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line"></span><br><span class="line">        agent_state_val[run<span class="number">-1</span>, :] = current_V</span><br><span class="line"></span><br><span class="line">    save_name = <span class="string">"&#123;&#125;"</span>.format(rl_glue.agent.name).replace(<span class="string">'.'</span>,<span class="string">''</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'results'</span>):</span><br><span class="line">                os.makedirs(<span class="string">'results'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save avg. state value</span></span><br><span class="line">    np.save(<span class="string">"results/V_&#123;&#125;"</span>.format(save_name), agent_state_val)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># save avg. rmsve</span></span><br><span class="line">    np.savez(<span class="string">"results/RMSVE_&#123;&#125;"</span>.format(save_name), rmsve = agent_rmsve,</span><br><span class="line">                                                   eval_freq = experiment_parameters[<span class="string">"episode_eval_frequency"</span>],</span><br><span class="line">                                                   num_episodes = experiment_parameters[<span class="string">"num_episodes"</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">20</span>,</span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">1000</span>,</span><br><span class="line">    <span class="string">"episode_eval_frequency"</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episode</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">500</span>,</span><br><span class="line">    <span class="string">"start_state"</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">"left_terminal_state"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"right_terminal_state"</span> : <span class="number">501</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.001</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.999</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = RandomWalkEnvironment</span><br><span class="line">current_agent = TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># run experiment</span></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot result</span></span><br><span class="line">plot_script.plot_result([<span class="string">"td_agent"</span>])</span><br><span class="line"></span><br><span class="line">shutil.make_archive(<span class="string">'results'</span>, <span class="string">'zip'</span>, <span class="string">'results'</span>)</span><br></pre></td></tr></table></figure><pre><code>Setting - Neural Network with 100 hidden units</code></pre><p><img src="output_39_2.png" alt="png"></p><pre><code>&#39;/home/jovyan/work/release/TD-NN/results.zip&#39;</code></pre><p>You plotted the learning curve for 1000 episodes. As you can see the RMSVE is still decreasing. Here we provide the pre-computed result for 5000 episodes and 20 runs so that you can see the performance of semi-gradient TD with a neural network after being trained for a long time.</p><p><img src="nn_5000_episodes.png" alt></p><p>Does semi-gradient TD with a neural network find a good approximation within 5000 episodes? </p><p>As you may remember from the previous assignment, semi-gradient TD with 10-state aggregation converged within 100 episodes. Why is TD with a neural network slower?</p><p>Would it be faster if we decrease the number of hidden units? Or what about if we increase the number of hidden units?</p><h2 id="2-2-Compare-Performance-of-Semi-gradient-TD-with-a-Neural-Network-and-Semi-gradient-TD-with-Tile-coding"><a href="#2-2-Compare-Performance-of-Semi-gradient-TD-with-a-Neural-Network-and-Semi-gradient-TD-with-Tile-coding" class="headerlink" title="2-2: Compare Performance of Semi-gradient TD with a Neural Network and Semi-gradient TD with Tile-coding"></a>2-2: Compare Performance of Semi-gradient TD with a Neural Network and Semi-gradient TD with Tile-coding</h2><p>In this section, we compare the performance of semi-gradient TD with a Neural Network and semi-gradient TD with tile-coding. Tile-coding is a kind of coarse coding that uses multiple overlapping partitions of the state space to produce features. For tile-coding, we used 50 tilings each with 6 tiles. We set the step-size for semi-gradient TD with tile-coding to $\frac{0.1}{tilings}$. See the figure below for the comparison between semi-gradient TD with tile-coding and semi-gradient TD with a neural network and Adam algorithm. This result is for 5000 episodes and 20 runs:</p><p><img src="nn_vs_tc.png" alt></p><p>How are the results?</p><p>Semi-gradient TD with tile-coding is much faster than semi-gradient TD with a neural network. Why?</p><p>Which method has a lower RMSVE at the end of 5000 episodes?</p><h3 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up!"></a>Wrapping up!</h3><p>You have successfully implemented Course 3 Programming Assignment 2.</p><p>You have implemented <strong>semi-gradient TD with a Neural Network and Adam algorithm</strong> in 500-state Random Walk. </p><p>You also compared semi-gradient TD with a neural network and semi-gradient TD with tile-coding. </p><p>From the experiments and lectures, you should be more familiar with some of the strengths and weaknesses of using neural networks as the function approximator for an RL agent. On one hand, neural networks are powerful function approximators capable of representing a wide class of functions. They are also capable of producing features without exclusively relying on hand-crafted mechanisms. On the other hand, compared to a linear function approximator with tile-coding, neural networks can be less sample efficient. When implementing your own Reinforcement Learning agents, you may consider these strengths and weaknesses to choose the proper function approximator for your problems.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Semi-gradient-TD-with-a-Neural-Network&quot;&gt;&lt;a href=&quot;#Assignment-2-Semi-gradient-TD-with-a-Neural-Network&quot; class=&quot;headerlin
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>TD with State Aggregation </title>
    <link href="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/"/>
    <id>https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/</id>
    <published>2020-10-13T08:06:20.000Z</published>
    <updated>2020-10-13T08:07:45.508Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-1-TD-with-State-Aggregation"><a href="#Assignment-1-TD-with-State-Aggregation" class="headerlink" title="Assignment 1 - TD with State Aggregation"></a>Assignment 1 - TD with State Aggregation</h1><p>Welcome to your Course 3 Programming Assignment 1. In this assignment, you will implement <strong>semi-gradient TD(0) with State Aggregation</strong> in an environment with a large state space. This assignment will focus on the <strong>policy evaluation task</strong> (prediction problem) where the goal is to accurately estimate state values under a given (fixed) policy.</p><p><strong>In this assignment, you will:</strong></p><ol><li>Implement semi-gradient TD(0) with function approximation (state aggregation).</li><li>Understand how to use supervised learning approaches to approximate value functions.</li><li>Compare the impact of different resolutions of state aggregation, and see first hand how function approximation can speed up learning through generalization.</li></ol><p><strong>Note: You can create new cells for debugging purposes but please do not duplicate any Read-only cells. This may break the grader.</strong></p><h2 id="500-State-RandomWalk-Environment"><a href="#500-State-RandomWalk-Environment" class="headerlink" title="500-State RandomWalk Environment"></a>500-State RandomWalk Environment</h2><p>In this assignment, we will implement and use a smaller 500 state  version of the problem we covered in lecture  (see “State Aggregation with Monte Carlo”, and Example 9.1 in the <a href="http://www.incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">textbook</a>). The diagram below illustrates the problem.</p><p><img src="randomwalk_diagram.png" alt></p><p>There are 500 states numbered from 1 to 500, left to right, and all episodes begin with the agent located at the center, in state 250. For simplicity, we will consider state 0 and state 501 as the left and right terminal states respectively. </p><p>The episode terminates when the agent reaches the terminal state (state 0) on the left, or the terminal state (state 501) on the right. Termination on the left (state 0) gives the agent a reward of -1, and termination on the right (state 501) gives the agent a reward of +1.</p><p>The agent can take one of two actions: go left or go right. If the agent chooses the left action, then it transitions uniform randomly into one of the 100 neighboring states to its left. If the agent chooses the right action, then it transitions randomly into one of the 100 neighboring states to its right. </p><p>States near the edge may have fewer than 100 neighboring states on that side. In this case, all transitions that would have taken the agent past the edge result in termination. If the agent takes the left action from state 50, then it has a 0.5 chance of terminating on the left. If it takes the right action from state 499, then it has a 0.99 chance of terminating on the right.</p><h3 id="Your-Goal"><a href="#Your-Goal" class="headerlink" title="Your Goal"></a>Your Goal</h3><p>For this assignment, we will consider the problem of <strong>policy evaluation</strong>: estimating state-value function for a fixed policy.You will evaluate a uniform random policy in the 500-State Random Walk environment. This policy takes the right action with 0.5 probability and the left with 0.5 probability, regardless of which state it is in. </p><p>This environment has a relatively large number of states. Generalization can significantly speed learning as we will show in this assignment. Often in realistic environments, states are high-dimensional and continuous. For these problems, function approximation is not just useful, it is also necessary.</p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>You will use the following packages in this assignment.</p><ul><li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li><li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> : Library for plotting graphs in Python.</li><li><a href="http://www.jmlr.org/papers/v10/tanner09a.html" target="_blank" rel="noopener">RL-Glue</a> : Library for reinforcement learning experiments.</li><li><a href="https://alexhagen.github.io/jdc/" target="_blank" rel="noopener">jdc</a> : Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li><li><a href="https://tqdm.github.io/" target="_blank" rel="noopener">tqdm</a> : A package to display progress bar when running experiments</li><li>plot_script : custom script to plot results</li></ul><p><strong>Please do not import other libraries</strong> - this will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> environment <span class="keyword">import</span> BaseEnvironment</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> plot_script</span><br></pre></td></tr></table></figure><h2 id="Section-1-Create-the-500-State-RandomWalk-Environment"><a href="#Section-1-Create-the-500-State-RandomWalk-Environment" class="headerlink" title="Section 1: Create the 500-State RandomWalk Environment"></a>Section 1: Create the 500-State RandomWalk Environment</h2><p>In this section we have provided you with the implementation of the 500-State RandomWalk Environment. It is useful to know how the environment is implemented. We will also use this environment in the next programming assignment. </p><p>Once the agent chooses which direction to move, the environment determines how far the agent is moved in that direction. Assume the agent passes either 0 (indicating left) or 1 (indicating right) to the environment.</p><p>Methods needed to implement the environment are: <code>env_init</code>, <code>env_start</code>, and <code>env_step</code>.</p><ul><li><code>env_init</code>: This method sets up the environment at the very beginning of the experiment. Relevant parameters are passed through <code>env_info</code> dictionary.</li><li><code>env_start</code>: This is the first method called when the experiment starts, returning the start state.</li><li><code>env_step</code>: This method takes in action and returns reward, next_state, and is_terminal.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomWalkEnvironment</span><span class="params">(BaseEnvironment)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_init</span><span class="params">(self, env_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Setup for the environment called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Set parameters needed to setup the 500-state random walk environment.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Assume env_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: 500 [int],</span></span><br><span class="line"><span class="string">            start_state: 250 [int],</span></span><br><span class="line"><span class="string">            left_terminal_state: 0 [int],</span></span><br><span class="line"><span class="string">            right_terminal_state: 501 [int],</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set random seed for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(env_info.get(<span class="string">"seed"</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set each class attribute</span></span><br><span class="line">        self.num_states = env_info[<span class="string">"num_states"</span>] </span><br><span class="line">        self.start_state = env_info[<span class="string">"start_state"</span>] </span><br><span class="line">        self.left_terminal_state = env_info[<span class="string">"left_terminal_state"</span>] </span><br><span class="line">        self.right_terminal_state = env_info[<span class="string">"right_terminal_state"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        The first method called when the experiment starts, called before the</span></span><br><span class="line"><span class="string">        agent starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first state from the environment.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># set self.reward_state_term tuple</span></span><br><span class="line">        reward = <span class="number">0.0</span></span><br><span class="line">        state = self.start_state</span><br><span class="line">        is_terminal = <span class="keyword">False</span></span><br><span class="line">                </span><br><span class="line">        self.reward_state_term = (reward, state, is_terminal)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># return first state from the environment</span></span><br><span class="line">        <span class="keyword">return</span> self.reward_state_term[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            action: The action taken by the agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            (float, state, Boolean): a tuple of the reward, state,</span></span><br><span class="line"><span class="string">                and boolean indicating if it's terminal.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        last_state = self.reward_state_term[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set reward, current_state, and is_terminal</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># action: specifies direction of movement - 0 (indicating left) or 1 (indicating right)  [int]</span></span><br><span class="line">        <span class="comment"># current state: next state after taking action from the last state [int]</span></span><br><span class="line">        <span class="comment"># reward: -1 if terminated left, 1 if terminated right, 0 otherwise [float]</span></span><br><span class="line">        <span class="comment"># is_terminal: indicates whether the episode terminated [boolean]</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Given action (direction of movement), determine how much to move in that direction from last_state</span></span><br><span class="line">        <span class="comment"># All transitions beyond the terminal state are absorbed into the terminal state.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> action == <span class="number">0</span>: <span class="comment"># left</span></span><br><span class="line">            current_state = max(self.left_terminal_state, last_state + self.rand_generator.choice(range(<span class="number">-100</span>,<span class="number">0</span>)))</span><br><span class="line">        <span class="keyword">elif</span> action == <span class="number">1</span>: <span class="comment"># right</span></span><br><span class="line">            current_state = min(self.right_terminal_state, last_state + self.rand_generator.choice(range(<span class="number">1</span>,<span class="number">101</span>)))</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Wrong action value"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># terminate left</span></span><br><span class="line">        <span class="keyword">if</span> current_state == self.left_terminal_state: </span><br><span class="line">            reward = <span class="number">-1.0</span></span><br><span class="line">            is_terminal = <span class="keyword">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># terminate right</span></span><br><span class="line">        <span class="keyword">elif</span> current_state == self.right_terminal_state:</span><br><span class="line">            reward = <span class="number">1.0</span></span><br><span class="line">            is_terminal = <span class="keyword">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reward = <span class="number">0.0</span></span><br><span class="line">            is_terminal = <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">        self.reward_state_term = (reward, current_state, is_terminal)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.reward_state_term</span><br></pre></td></tr></table></figure><h2 id="Section-2-Create-Semi-gradient-TD-0-Agent-with-State-Aggregation"><a href="#Section-2-Create-Semi-gradient-TD-0-Agent-with-State-Aggregation" class="headerlink" title="Section 2: Create Semi-gradient TD(0) Agent with State Aggregation"></a>Section 2: Create Semi-gradient TD(0) Agent with State Aggregation</h2><p>Now let’s create the Agent that interacts with the Environment.</p><p>You will create an Agent that learns with semi-gradient TD(0) with state aggregation.<br>For state aggregation, if the resolution (num_groups) is 10, then 500 states are partitioned into 10 groups of 50 states each (i.e., states 1-50 are one group, states 51-100 are another, and so on.)</p><p>Hence, 50 states would share the same feature and value estimate, and there would be 10 distinct features. The feature vector for each state is a one-hot feature vector of length 10, with a single one indicating the group for that state. (one-hot vector of length 10)</p><h2 id="Section-2-1-Implement-Useful-Functions"><a href="#Section-2-1-Implement-Useful-Functions" class="headerlink" title="Section 2-1: Implement Useful Functions"></a>Section 2-1: Implement Useful Functions</h2><p>Before we implement the agent, we need to define a couple of useful helper functions.</p><p><strong>Please note all random method calls should be called through random number generator. Also do not use random method calls unless specified. In the agent, only <code>agent_policy</code> requires random method calls.</strong></p><h2 id="Section-2-1a-Selecting-actions"><a href="#Section-2-1a-Selecting-actions" class="headerlink" title="Section 2-1a: Selecting actions"></a>Section 2-1a: Selecting actions</h2><p>In this part we have implemented <code>agent_policy()</code> for you.</p><p>This method is used in <code>agent_start()</code> and <code>agent_step()</code> to select appropriate action.<br>Normally, the agent acts differently given state, but in this environment the agent chooses randomly to move either left or right with equal probability.</p><p>Agent returns 0 for left, and 1 for right.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_policy</span><span class="params">(rand_generator, state)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given random number generator and state, returns an action according to the agent's policy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        rand_generator: Random number generator</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        chosen action [int]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set chosen_action as 0 or 1 with equal probability</span></span><br><span class="line">    <span class="comment"># state is unnecessary for this agent policy</span></span><br><span class="line">    chosen_action = rand_generator.choice([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> chosen_action</span><br></pre></td></tr></table></figure><h2 id="Section-2-1b-Processing-State-Features-with-State-Aggregation"><a href="#Section-2-1b-Processing-State-Features-with-State-Aggregation" class="headerlink" title="Section 2-1b: Processing State Features with State Aggregation"></a>Section 2-1b: Processing State Features with State Aggregation</h2><p>In this part you will implement <code>get_state_feature()</code></p><p>This method takes in a state and returns the aggregated feature (one-hot-vector) of that state.<br>The feature vector size is determined by <code>num_groups</code>. Use <code>state</code> and <code>num_states_in_group</code> to determine which element in the feature vector is active.</p><p><code>get_state_feature()</code> is necessary whenever the agent receives a state and needs to convert it to a feature for learning. The features will thus be used in <code>agent_step()</code> and <code>agent_end()</code> when the agent updates its state values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_state_feature</span><span class="params">(num_states_in_group, num_groups, state)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given state, return the feature of that state</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_states_in_group [int]</span></span><br><span class="line"><span class="string">        num_groups [int] </span></span><br><span class="line"><span class="string">        state [int] : 1~500</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        one_hot_vector [numpy array]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### Generate state feature (2~4 lines)</span></span><br><span class="line">    <span class="comment"># Create one_hot_vector with size of the num_groups, according to state</span></span><br><span class="line">    <span class="comment"># For simplicity, assume num_states is always perfectly divisible by num_groups</span></span><br><span class="line">    <span class="comment"># Note that states start from index 1, not 0!</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Example:</span></span><br><span class="line">    <span class="comment"># If num_states = 100, num_states_in_group = 20, num_groups = 5,</span></span><br><span class="line">    <span class="comment"># one_hot_vector would be of size 5.</span></span><br><span class="line">    <span class="comment"># For states 1~20, one_hot_vector would be: [1, 0, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    <span class="comment"># one_hot_vector = ?</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    one_hot_vector = np.zeros((num_groups,))</span><br><span class="line">    index,_ = divmod(state,num_states_in_group)</span><br><span class="line">    <span class="keyword">if</span> _ == <span class="number">0</span>:</span><br><span class="line">        index -= <span class="number">1</span></span><br><span class="line">    one_hot_vector[index] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot_vector</span><br></pre></td></tr></table></figure><p>Run the following code to verify your <code>get_state_feature()</code> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Given that num_states = 10 and num_groups = 5, test get_state_feature()</span></span><br><span class="line"><span class="comment"># There are states 1~10, and the state feature vector would be of size 5.</span></span><br><span class="line"><span class="comment"># Only one element would be active for any state feature vector.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get_state_feature() should support various values of num_states, num_groups, not just this example</span></span><br><span class="line"><span class="comment"># For simplicity, assume num_states will always be perfectly divisible by num_groups</span></span><br><span class="line">num_states = <span class="number">10</span></span><br><span class="line">num_groups = <span class="number">5</span></span><br><span class="line">num_states_in_group = int(num_states / num_groups)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 1st group, state = 1</span></span><br><span class="line">state = <span class="number">1</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"1st group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 2nd group, state = 3</span></span><br><span class="line">state = <span class="number">3</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"2nd group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 3rd group, state = 6</span></span><br><span class="line">state = <span class="number">6</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"3rd group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 4th group, state = 7</span></span><br><span class="line">state = <span class="number">7</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"4th group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 5th group, state = 10</span></span><br><span class="line">state = <span class="number">10</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line">print(<span class="string">"5th group: &#123;&#125;"</span>.format(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>1st group: [1. 0. 0. 0. 0.]2nd group: [0. 1. 0. 0. 0.]3rd group: [0. 0. 1. 0. 0.]4th group: [0. 0. 0. 1. 0.]5th group: [0. 0. 0. 0. 1.]</code></pre><h2 id="Section-2-2-Implement-Agent-Methods"><a href="#Section-2-2-Implement-Agent-Methods" class="headerlink" title="Section 2-2: Implement Agent Methods"></a>Section 2-2: Implement Agent Methods</h2><p>Now that we have implemented all the helper functions, let’s create an agent. In this part, you will implement <code>agent_init()</code>, <code>agent_start()</code>, <code>agent_step()</code> and <code>agent_end()</code>. You will have to use <code>agent_policy()</code> that we implemented above. We will implement <code>agent_message()</code> later, when returning the learned state-values.</p><p>To save computation time, we precompute features for all states beforehand in <code>agent_init()</code>. The pre-computed features are saved in <code>self.all_state_features</code> numpy array. Hence, you do not  need to call <code>get_state_feature()</code> every time in <code>agent_step()</code> and <code>agent_end()</code>.</p><p>The shape of <code>self.all_state_features</code> numpy array is <code>(num_states, feature_size)</code>, with features of states from State 1-500. Note that index 0 stores features for State 1 (Features for State 0 does not exist). Use <code>self.all_state_features</code> to access each feature vector for a state.</p><p>When saving state values in the agent, recall how the state values are represented with linear function approximation.</p><p><strong>State Value Representation</strong>: $\hat{v}(s,\mathbf{w}) = \mathbf{w}\cdot\mathbf{x^T}$ where $\mathbf{w}$ is a weight vector and $\mathbf{x}$ is the feature vector of the state.</p><p>When performing TD(0) updates with Linear Function Approximation, recall how we perform semi-gradient TD(0) updates using supervised learning.</p><p><strong>semi-gradient TD(0) Weight Update Rule</strong>: $\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha [R_{t+1} + \gamma \hat{v}(S_{t+1},\mathbf{w}) - \hat{v}(S_t,\mathbf{w})] \nabla \hat{v}(S_t,\mathbf{w})$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create TDAgent</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.num_states = <span class="keyword">None</span></span><br><span class="line">        self.num_groups = <span class="keyword">None</span></span><br><span class="line">        self.step_size = <span class="keyword">None</span></span><br><span class="line">        self.discount_factor = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the semi-gradient TD(0) state aggregation agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: 500 [int],</span></span><br><span class="line"><span class="string">            num_groups: int, </span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            discount_factor: float,</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># set random seed for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>)) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># set class attributes</span></span><br><span class="line">        self.num_states = agent_info.get(<span class="string">"num_states"</span>)</span><br><span class="line">        self.num_groups = agent_info.get(<span class="string">"num_groups"</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">"step_size"</span>)</span><br><span class="line">        self.discount_factor = agent_info.get(<span class="string">"discount_factor"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pre-compute all observable features</span></span><br><span class="line">        num_states_in_group = int(self.num_states / self.num_groups)</span><br><span class="line">        self.all_state_features = np.array([get_state_feature(num_states_in_group, self.num_groups, state) <span class="keyword">for</span> state <span class="keyword">in</span> range(<span class="number">1</span>, self.num_states + <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># initialize all weights to zero using numpy array with correct size</span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights = np.zeros((self.num_groups,))</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        self.last_state = <span class="keyword">None</span></span><br><span class="line">        self.last_action = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            self.last_action [int] : The first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment">### select action given state (using agent_policy), and save current state and action</span></span><br><span class="line">        <span class="comment"># Use self.rand_generator for agent_policy</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_state = state</span><br><span class="line">        self.last_action = agent_policy(self.rand_generator, self.last_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward [float]: the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state [int]: the state from the environment's step, where the agent ended up after the last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            self.last_action [int] : The action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get relevant feature</span></span><br><span class="line">        current_state_feature = self.all_state_features[state<span class="number">-1</span>] </span><br><span class="line">        last_state_feature = self.all_state_features[self.last_state<span class="number">-1</span>] </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### update weights and select action</span></span><br><span class="line">        <span class="comment"># (Hint: np.dot method is useful!)</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Update weights:</span></span><br><span class="line">        <span class="comment">#     use self.weights, current_state_feature, and last_state_feature</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Select action:</span></span><br><span class="line">        <span class="comment">#     use self.rand_generator for agent_policy</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Current state and selected action should be saved to self.last_state and self.last_action at the end</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights += self.step_size * (reward + self.discount_factor * (np.dot(self.weights,current_state_feature)) - np.dot(self.weights,last_state_feature)) * last_state_feature </span><br><span class="line">        self.last_state = state</span><br><span class="line">        self.last_action = agent_policy(self.rand_generator, self.last_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get relevant feature</span></span><br><span class="line">        last_state_feature = self.all_state_features[self.last_state<span class="number">-1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### update weights</span></span><br><span class="line">        <span class="comment"># Update weights using self.weights and last_state_feature</span></span><br><span class="line">        <span class="comment"># (Hint: np.dot method is useful!)</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># Note that here you don't need to choose action since the agent has reached a terminal state</span></span><br><span class="line">        <span class="comment"># Therefore you should not update self.last_state and self.last_action</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights += self.step_size * (reward - np.dot(self.weights,last_state_feature)) * last_state_feature </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="comment"># We will implement this method later</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>Run the following code to verify <code>agent_init()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(agent.weights == <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> agent.weights.shape == (<span class="number">10</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check attributes</span></span><br><span class="line">print(<span class="string">"num_states: &#123;&#125;"</span>.format(agent.num_states))</span><br><span class="line">print(<span class="string">"num_groups: &#123;&#125;"</span>.format(agent.num_groups))</span><br><span class="line">print(<span class="string">"step_size: &#123;&#125;"</span>.format(agent.step_size))</span><br><span class="line">print(<span class="string">"discount_factor: &#123;&#125;"</span>.format(agent.discount_factor))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"weights shape: &#123;&#125;"</span>.format(agent.weights.shape))</span><br><span class="line">print(<span class="string">"weights init. value: &#123;&#125;"</span>.format(agent.weights))</span><br></pre></td></tr></table></figure><pre><code>num_states: 500num_groups: 10step_size: 0.1discount_factor: 1.0weights shape: (10,)weights init. value: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</code></pre><p>Run the following code to verify <code>agent_start()</code>.<br>Although there is randomness due to <code>rand_generator.choice()</code> in <code>agent_policy()</code>, we control the seed so your output should match the expected output. </p><p>Make sure <code>rand_generator.choice()</code> is called only once per <code>agent_policy()</code> call.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose state = 250</span></span><br><span class="line">state = <span class="number">250</span></span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">action = agent.agent_start(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">250</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Agent state: &#123;&#125;"</span>.format(agent.last_state))</span><br><span class="line">print(<span class="string">"Agent selected action: &#123;&#125;"</span>.format(agent.last_action))</span><br></pre></td></tr></table></figure><pre><code>Agent state: 250Agent selected action: 1</code></pre><p>Run the following code to verify <code>agent_step()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the weights to arbitrary values to verify the correctness of weight update</span></span><br><span class="line">agent.weights = np.array([<span class="number">-1.5</span>, <span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>, <span class="number">-1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 50</span></span><br><span class="line">start_state = <span class="number">50</span></span><br><span class="line">action = agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and the next state observed was State 120</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">next_state = <span class="number">120</span></span><br><span class="line">action = agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Updated weights: &#123;&#125;"</span>.format(agent.weights))</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [<span class="number">-0.26</span>, <span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0.</span>, <span class="number">-0.5</span>, <span class="number">-1.</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">120</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"last state: &#123;&#125;"</span>.format(agent.last_state))</span><br><span class="line">print(<span class="string">"last action: &#123;&#125;"</span>.format(agent.last_action))</span><br><span class="line"></span><br><span class="line"><span class="comment"># let's do another</span></span><br><span class="line">reward = <span class="number">-22</span></span><br><span class="line">next_state = <span class="number">222</span></span><br><span class="line">action = agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [<span class="number">-0.26</span>, <span class="number">0.5</span>, <span class="number">-1.165</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0</span>, <span class="number">-0.5</span>, <span class="number">-1</span>])</span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">222</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">0</span></span><br></pre></td></tr></table></figure><pre><code>Updated weights: [-0.26  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]last state: 120last action: 1</code></pre><p>Run the following code to verify <code>agent_end()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the weights to arbitrary values to verify the correctness of weight update</span></span><br><span class="line">agent.weights = np.array([<span class="number">-1.5</span>, <span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>, <span class="number">-1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 50</span></span><br><span class="line">start_state = <span class="number">50</span></span><br><span class="line">action = agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and reached the terminal state</span></span><br><span class="line">agent.agent_end(<span class="number">10.0</span>)</span><br><span class="line">print(<span class="string">"Updated weights: &#123;&#125;"</span>.format(agent.weights))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [<span class="number">-0.35</span>, <span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">-0.5</span>, <span class="number">1.5</span>, <span class="number">0.</span>, <span class="number">-0.5</span>, <span class="number">-1.</span>])</span><br></pre></td></tr></table></figure><pre><code>Updated weights: [-0.35  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]</code></pre><p><strong>Expected output</strong>: (Note only the 1st element was changed, and the result is different from <code>agent_step()</code> )</p><pre><code>Initial weights: [-1.5  0.5  1.  -0.5  1.5 -0.5  1.5  0.  -0.5 -1. ]Updated weights: [-0.35  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]</code></pre><h2 id="Section-2-3-Returning-Learned-State-Values"><a href="#Section-2-3-Returning-Learned-State-Values" class="headerlink" title="Section 2-3: Returning Learned State Values"></a>Section 2-3: Returning Learned State Values</h2><p>You are almost done! Now let’s implement a code block in <code>agent_message()</code> that returns the learned state values.</p><p>The method <code>agent_message()</code> will return the learned state_value array when <code>message == &#39;get state value&#39;</code>.</p><p><strong>Hint</strong>: Think about how state values are represented with linear function approximation. <code>state_value</code> array will be a 1D array with length equal to the number of states.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">'get state value'</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### return state_value</span></span><br><span class="line">        <span class="comment"># Use self.all_state_features and self.weights to return the vector of all state values</span></span><br><span class="line">        <span class="comment"># Hint: Use np.dot()</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># state_value = ?</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        state_value = np.dot(self.all_state_features, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> state_value</span><br></pre></td></tr></table></figure><p>Run the following code to verify <code>get_state_val()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">"num_groups"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">test_state_val = agent.agent_message(<span class="string">'get state value'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> test_state_val.shape == (<span class="number">20</span>,)</span><br><span class="line"><span class="keyword">assert</span> np.all(test_state_val == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"State value shape: &#123;&#125;"</span>.format(test_state_val.shape))</span><br><span class="line">print(<span class="string">"Initial State value for all states: &#123;&#125;"</span>.format(test_state_val))</span><br></pre></td></tr></table></figure><pre><code>State value shape: (20,)Initial State value for all states: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</code></pre><p><strong>Expected Output</strong>:</p><pre><code>State value shape: (20,)Initial State value for all states: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</code></pre><h2 id="Section-3-Run-Experiment"><a href="#Section-3-Run-Experiment" class="headerlink" title="Section 3: Run Experiment"></a>Section 3: Run Experiment</h2><p>Now that we’ve implemented all the components of environment and agent, let’s run an experiment! We will plot two things: (1) the learned state value function and compare it against the true state values, and (2) a learning curve depicting the error in the learned value estimates over episodes. For the learning curve, what should we plot to see if the agent is learning well?</p><h2 id="Section-3-1-Prediction-Objective-Root-Mean-Squared-Value-Error"><a href="#Section-3-1-Prediction-Objective-Root-Mean-Squared-Value-Error" class="headerlink" title="Section 3-1: Prediction Objective (Root Mean Squared Value Error)"></a>Section 3-1: Prediction Objective (Root Mean Squared Value Error)</h2><p>Recall that the Prediction Objective in function approximation is Mean Squared Value Error $\overline{VE}(\mathbf{w}) \doteq \sum\limits_{s \in \mathcal{S}}\mu(s)[v_\pi(s)-\hat{v}(s,\mathbf{w})]^2$</p><p>We will use the square root of this measure, the root $\overline{VE}$ to give a rough measure of how much the learned values differ from the true values.</p><p><code>calc RMSVE()</code> computes the Root Mean Squared Value Error given learned state value $\hat{v}(s, \mathbf{w})$.<br>We provide you with true state value $v_\pi(s)$ and state distribution $\mu(s)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Here we provide you with the true state value and state distribution</span></span><br><span class="line">true_state_val = np.load(<span class="string">'data/true_V.npy'</span>)    </span><br><span class="line">state_distribution = np.load(<span class="string">'data/state_distribution.npy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_RMSVE</span><span class="params">(learned_state_val)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span>(len(true_state_val) == len(learned_state_val) == len(state_distribution))</span><br><span class="line">    MSVE = np.sum(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))</span><br><span class="line">    RMSVE = np.sqrt(MSVE)</span><br><span class="line">    <span class="keyword">return</span> RMSVE</span><br></pre></td></tr></table></figure><h2 id="Section-3-2a-Run-Experiment-with-10-State-Aggregation"><a href="#Section-3-2a-Run-Experiment-with-10-State-Aggregation" class="headerlink" title="Section 3-2a: Run Experiment with 10-State Aggregation"></a>Section 3-2a: Run Experiment with 10-State Aggregation</h2><p>We have provided you the experiment/plot code in the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define function to run experiment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(environment, agent, environment_parameters, agent_parameters, experiment_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Sweep Agent parameters</span></span><br><span class="line">    <span class="keyword">for</span> num_agg_states <span class="keyword">in</span> agent_parameters[<span class="string">"num_groups"</span>]:</span><br><span class="line">        <span class="keyword">for</span> step_size <span class="keyword">in</span> agent_parameters[<span class="string">"step_size"</span>]:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save rmsve at the end of each evaluation episode</span></span><br><span class="line">            <span class="comment"># size: num_episode / episode_eval_frequency + 1 (includes evaluation at the beginning of training)</span></span><br><span class="line">            agent_rmsve = np.zeros(int(experiment_parameters[<span class="string">"num_episodes"</span>]/experiment_parameters[<span class="string">"episode_eval_frequency"</span>]) + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save learned state value at the end of each run</span></span><br><span class="line">            agent_state_val = np.zeros(environment_parameters[<span class="string">"num_states"</span>])</span><br><span class="line"></span><br><span class="line">            env_info = &#123;<span class="string">"num_states"</span>: environment_parameters[<span class="string">"num_states"</span>],</span><br><span class="line">                        <span class="string">"start_state"</span>: environment_parameters[<span class="string">"start_state"</span>],</span><br><span class="line">                        <span class="string">"left_terminal_state"</span>: environment_parameters[<span class="string">"left_terminal_state"</span>],</span><br><span class="line">                        <span class="string">"right_terminal_state"</span>: environment_parameters[<span class="string">"right_terminal_state"</span>]&#125;</span><br><span class="line"></span><br><span class="line">            agent_info = &#123;<span class="string">"num_states"</span>: environment_parameters[<span class="string">"num_states"</span>],</span><br><span class="line">                          <span class="string">"num_groups"</span>: num_agg_states,</span><br><span class="line">                          <span class="string">"step_size"</span>: step_size,</span><br><span class="line">                          <span class="string">"discount_factor"</span>: environment_parameters[<span class="string">"discount_factor"</span>]&#125;</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'Setting - num. agg. states: &#123;&#125;, step_size: &#123;&#125;'</span>.format(num_agg_states, step_size))</span><br><span class="line">            os.system(<span class="string">'sleep 0.2'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># one agent setting</span></span><br><span class="line">            <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_runs"</span>]+<span class="number">1</span>)):</span><br><span class="line">                env_info[<span class="string">"seed"</span>] = run</span><br><span class="line">                agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">                rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Compute initial RMSVE before training</span></span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">                agent_rmsve[<span class="number">0</span>] += calc_RMSVE(current_V)</span><br><span class="line">                    </span><br><span class="line">                <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_episodes"</span>]+<span class="number">1</span>):</span><br><span class="line">                    <span class="comment"># run episode</span></span><br><span class="line">                    rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> episode % experiment_parameters[<span class="string">"episode_eval_frequency"</span>] == <span class="number">0</span>:</span><br><span class="line">                        current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">                        agent_rmsve[int(episode/experiment_parameters[<span class="string">"episode_eval_frequency"</span>])] += calc_RMSVE(current_V)</span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># store only one run of state value</span></span><br><span class="line">                <span class="keyword">if</span> run == <span class="number">50</span>:</span><br><span class="line">                    agent_state_val = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># rmsve averaged over runs</span></span><br><span class="line">            agent_rmsve /= experiment_parameters[<span class="string">"num_runs"</span>]</span><br><span class="line">            </span><br><span class="line">            save_name = <span class="string">"&#123;&#125;_agg_states_&#123;&#125;_step_size_&#123;&#125;"</span>.format(<span class="string">'TD_agent'</span>, num_agg_states, step_size).replace(<span class="string">'.'</span>,<span class="string">''</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'results'</span>):</span><br><span class="line">                os.makedirs(<span class="string">'results'</span>)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># save avg. state value</span></span><br><span class="line">            np.save(<span class="string">"results/V_&#123;&#125;"</span>.format(save_name), agent_state_val)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># save avg. rmsve</span></span><br><span class="line">            np.save(<span class="string">"results/RMSVE_&#123;&#125;"</span>.format(save_name), agent_rmsve)</span><br></pre></td></tr></table></figure><p>We will first test our implementation using state aggregation with resolution of 10, with three different step sizes: {0.01, 0.05, 0.1}.</p><p>Note that running the experiment cell below will take <strong>_approximately 5 min_</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">50</span>,</span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">2000</span>,</span><br><span class="line">    <span class="string">"episode_eval_frequency"</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episodes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">500</span>, </span><br><span class="line">    <span class="string">"start_state"</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">"left_terminal_state"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"right_terminal_state"</span> : <span class="number">501</span>, </span><br><span class="line">    <span class="string">"discount_factor"</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line"><span class="comment"># Each element is an array because we will be later sweeping over multiple values</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_groups"</span>: [<span class="number">10</span>],</span><br><span class="line">    <span class="string">"step_size"</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = RandomWalkEnvironment</span><br><span class="line">current_agent = TDAgent</span><br><span class="line"></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_script.plot_result(agent_parameters, <span class="string">'results'</span>)</span><br></pre></td></tr></table></figure><pre><code>Setting - num. agg. states: 10, step_size: 0.01100%|██████████| 50/50 [01:32&lt;00:00,  1.85s/it]Setting - num. agg. states: 10, step_size: 0.05100%|██████████| 50/50 [01:33&lt;00:00,  1.87s/it]Setting - num. agg. states: 10, step_size: 0.1100%|██████████| 50/50 [01:31&lt;00:00,  1.83s/it]</code></pre><p><img src="output_36_6.png" alt="png"></p><p>Is the learned state value plot with step-size=0.01 similar to Figure 9.2 (p.208) in Sutton and Barto?</p><p>(Note that our environment has less states: 500 states and we have done 2000 episodes, and averaged the performance over 50 runs)</p><p>Look at  the plot of the learning curve. Does RMSVE decrease over time?</p><p>Would it be possible to reduce RMSVE to 0?</p><p>You should see the RMSVE decrease over time, but the error seems to plateau. It is impossible to reduce RMSVE to 0, because of function approximation (and we do not decay the step-size parameter to zero). With function approximation, the agent has limited resources and has to trade-off the accuracy of one state for another state.</p><p>Run the following code to verify your experimental result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_groups"</span>: [<span class="number">10</span>],</span><br><span class="line">    <span class="string">"step_size"</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">all_correct = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">for</span> num_agg_states <span class="keyword">in</span> agent_parameters[<span class="string">"num_groups"</span>]:</span><br><span class="line">    <span class="keyword">for</span> step_size <span class="keyword">in</span> agent_parameters[<span class="string">"step_size"</span>]:</span><br><span class="line">        filename = <span class="string">'RMSVE_TD_agent_agg_states_&#123;&#125;_step_size_&#123;&#125;'</span>.format(num_agg_states, step_size).replace(<span class="string">'.'</span>,<span class="string">''</span>)</span><br><span class="line">        agent_RMSVE = np.load(<span class="string">'results/&#123;&#125;.npy'</span>.format(filename))</span><br><span class="line">        correct_RMSVE = np.load(<span class="string">'correct_npy/&#123;&#125;.npy'</span>.format(filename))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.allclose(agent_RMSVE, correct_RMSVE):</span><br><span class="line">            all_correct=<span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_correct:</span><br><span class="line">    print(<span class="string">"Your experiment results are correct!"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"Your experiment results does not match with ours. Please check if you have implemented all methods correctly."</span>)</span><br></pre></td></tr></table></figure><pre><code>Your experiment results are correct!</code></pre><h2 id="Section-3-2b-Run-Experiment-with-Different-State-Aggregation-Resolution-and-Step-Size"><a href="#Section-3-2b-Run-Experiment-with-Different-State-Aggregation-Resolution-and-Step-Size" class="headerlink" title="Section 3-2b: Run Experiment with Different State Aggregation Resolution and Step-Size"></a>Section 3-2b: Run Experiment with Different State Aggregation Resolution and Step-Size</h2><p>In this section, we will run some more experiments to see how different parameter settings affect the results!</p><p>In particular, we will test several values of <code>num_groups</code> and <code>step_size</code>. Parameter sweeps although necessary, can take lots of time. So now that you have verified your experiment result, here we show you the results of the parameter sweeps that you would see when running the sweeps yourself.</p><p>We tested several different values of <code>num_groups</code>: {10, 100, 500}, and <code>step-size</code>: {0.01, 0.05, 0.1}. As before, we performed 2000 episodes per run, and averaged the results over 50 runs for each setting.</p><p>Run the cell below to display the sweep results.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make sure to verify your experiment result with the test cell above.</span></span><br><span class="line"><span class="comment"># Otherwise the sweep results will not be displayed.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">50</span>,</span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">2000</span>,</span><br><span class="line">    <span class="string">"episode_eval_frequency"</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episodes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">500</span>,</span><br><span class="line">    <span class="string">"start_state"</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">"left_terminal_state"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"right_terminal_state"</span> : <span class="number">501</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line"><span class="comment"># Each element is an array because we will be sweeping over multiple values</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_groups"</span>: [<span class="number">10</span>, <span class="number">100</span>, <span class="number">500</span>],</span><br><span class="line">    <span class="string">"step_size"</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_correct:</span><br><span class="line">    plot_script.plot_result(agent_parameters, <span class="string">'correct_npy'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"Make sure your experiment result is correct! Otherwise the sweep results will not be displayed."</span>)</span><br></pre></td></tr></table></figure><p><img src="output_41_0.png" alt="png"></p><p><img src="output_41_1.png" alt="png"></p><p><img src="output_41_2.png" alt="png"></p><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Let’s think about the results of our parameter study.</p><h3 id="State-Aggregation"><a href="#State-Aggregation" class="headerlink" title="State Aggregation"></a>State Aggregation</h3><ul><li><p>Which state aggregation resolution do you think is the best after running 2000 episodes? Which state aggregation resolution do you think would be the best if we could train for only 200 episodes? What if we could train for a million episodes?</p></li><li><p>Should we use tabular representation (state aggregation of resolution 500) whenever possible? Why might we want to use function approximation?</p></li></ul><p>From the plots, using 100 state aggregation with step-size 0.05 reaches the best performance: the lowest RMSVE after 2000 episodes. If the agent can only be trained for 200 episodes, then 10 state aggregation with step-size 0.05 reaches the lowest error. Increasing the resolution of state aggregation makes the function approximation closer to a  tabular representation, which would be able to learn exactly correct state values for all states. But learning will be slower. </p><h3 id="Step-Size"><a href="#Step-Size" class="headerlink" title="Step-Size"></a>Step-Size</h3><ul><li>How did different step-sizes affect learning?</li></ul><p>The best step-size is different for different state aggregation resolutions. A larger step-size allows the agent to learn faster, but might not perform as well asymptotically. A smaller step-size causes it to learn more slowly, but may perform well asymptotically.</p><h3 id="Congratulations-You-have-successfully-implemented-Course-3-Programming-Assignment-1"><a href="#Congratulations-You-have-successfully-implemented-Course-3-Programming-Assignment-1" class="headerlink" title="Congratulations! You have successfully implemented Course 3 Programming Assignment 1."></a><strong>Congratulations!</strong> You have successfully implemented Course 3 Programming Assignment 1.</h3><p>You have implemented <strong>semi-gradient TD(0) with State Aggregation</strong> in a 500-state Random Walk. We used an environment with a large but discrete state space, where it was possible to compute the true state values. This allowed us to compare the values learned by your agent to the true state values. The same state aggregation function approximation can also be applied to continuous state space environments, where comparison to the true values is not usually possible.</p><p>You also successfully applied supervised learning approaches to approximate value functions with semi-gradient TD(0). </p><p>Finally, we plotted the learned state values and compared with true state values. We also compared learning curves of different state aggregation resolutions and learning rates. </p><p>From the results, you can  see why it is often desirable to use function approximation, even when tabular learning is possible. Asymptotically, an agent with tabular representation would be able to learn the true state value function, but it would learn much more slowly compared to an agent with function approximation. On the other hand, we also want to ensure we do not reduce discrimination too far (a coarse state aggregation resolution), because it will hurt the asymptotic performance.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-1-TD-with-State-Aggregation&quot;&gt;&lt;a href=&quot;#Assignment-1-TD-with-State-Aggregation&quot; class=&quot;headerlink&quot; title=&quot;Assignment 1 - T
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Components of StyleGAN </title>
    <link href="https://zhangruochi.com/Components-of-StyleGAN/2020/10/13/"/>
    <id>https://zhangruochi.com/Components-of-StyleGAN/2020/10/13/</id>
    <published>2020-10-13T01:54:33.000Z</published>
    <updated>2020-10-13T03:21:06.697Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Components-of-StyleGAN"><a href="#Components-of-StyleGAN" class="headerlink" title="Components of StyleGAN"></a>Components of StyleGAN</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to implement various components of StyleGAN, including the truncation trick, the mapping layer, noise injection, adaptive instance normalization (AdaIN), and progressive growing. </p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Understand the components of StyleGAN that differ from the traditional GAN.</li><li>Implement the components of StyleGAN.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will begin by importing some packages from PyTorch and defining a visualization function which will be useful later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">16</span>, size=<span class="params">(<span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>)</span>, nrow=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images,</span></span><br><span class="line"><span class="string">    size per image, and images per row, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu().clamp_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=nrow, padding=<span class="number">0</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="Truncation-Trick"><a href="#Truncation-Trick" class="headerlink" title="Truncation Trick"></a>Truncation Trick</h2><p>The first component you will implement is the truncation trick. Remember that this is done after the model is trained and when you are sampling beautiful outputs. The truncation trick resamples the noise vector $z$ from a truncated normal distribution which allows you to tune the generator’s fidelity/diversity. The truncation value is at least 0, where 1 means there is little truncation (high diversity) and 0 means the distribution is all truncated except for the mean (high quality/fidelity). This trick is not exclusive to StyleGAN. In fact, you may recall playing with it in an earlier GAN notebook.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: get_truncated_noise</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> truncnorm</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_truncated_noise</span><span class="params">(n_samples, z_dim, truncation)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating truncated noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    and truncation value, creates a tensor of that shape filled with random</span></span><br><span class="line"><span class="string">    numbers from the truncated normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        truncation: the truncation value, a non-negative scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    truncated_noise = truncnorm.rvs(-truncation, truncation, size=(n_samples, z_dim))</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(truncated_noise)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the truncation sample</span></span><br><span class="line"><span class="keyword">assert</span> tuple(get_truncated_noise(n_samples=<span class="number">10</span>, z_dim=<span class="number">5</span>, truncation=<span class="number">0.7</span>).shape) == (<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">simple_noise = get_truncated_noise(n_samples=<span class="number">1000</span>, z_dim=<span class="number">10</span>, truncation=<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">assert</span> simple_noise.max() &gt; <span class="number">0.199</span> <span class="keyword">and</span> simple_noise.max() &lt; <span class="number">2</span></span><br><span class="line"><span class="keyword">assert</span> simple_noise.min() &lt; <span class="number">-0.199</span> <span class="keyword">and</span> simple_noise.min() &gt; <span class="number">-0.2</span></span><br><span class="line"><span class="keyword">assert</span> simple_noise.std() &gt; <span class="number">0.113</span> <span class="keyword">and</span> simple_noise.std() &lt; <span class="number">0.117</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Mapping-z-→-w"><a href="#Mapping-z-→-w" class="headerlink" title="Mapping $z$ → $w$"></a>Mapping $z$ → $w$</h2><p>The next component you need to implement is the mapping network. It takes the noise vector, $z$, and maps it to an intermediate noise vector, $w$. This makes it so $z$ can be represented in a more disentangled space which makes the features easier to control later.</p><p>The mapping network in StyleGAN is composed of 8 layers, but for your implementation, you will use a neural network with 3 layers. This is to save time training later.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">MappingLayers</font></code></b></font></summary>1.   This code should be five lines.2.   You need 3 linear layers and should use ReLU activations.3.   Your linear layers should be input -> hidden_dim -> hidden_dim -> output.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: MappingLayers</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MappingLayers</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Mapping Layers Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">        w_dim: the dimension of the intermediate noise vector, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim, hidden_dim, w_dim)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.mapping = nn.Sequential(</span><br><span class="line">            <span class="comment"># Please write a neural network which takes in tensors of </span></span><br><span class="line">            <span class="comment"># shape (n_samples, z_dim) and outputs (n_samples, w_dim)</span></span><br><span class="line">            <span class="comment"># with a hidden layer with hidden_dim neurons</span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            nn.Linear(z_dim,hidden_dim,bias=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Linear(hidden_dim,hidden_dim,bias=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Linear(hidden_dim,w_dim,bias=<span class="keyword">True</span>)</span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of MappingLayers: </span></span><br><span class="line"><span class="string">        Given an initial noise tensor, returns the intermediate noise tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.mapping(noise)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_mapping</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.mapping</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the mapping function</span></span><br><span class="line">map_fn = MappingLayers(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>)</span><br><span class="line"><span class="keyword">assert</span> tuple(map_fn(torch.randn(<span class="number">2</span>, <span class="number">10</span>)).shape) == (<span class="number">2</span>, <span class="number">30</span>)</span><br><span class="line"><span class="keyword">assert</span> len(map_fn.mapping) &gt; <span class="number">4</span></span><br><span class="line">outputs = map_fn(torch.randn(<span class="number">1000</span>, <span class="number">10</span>))</span><br><span class="line"><span class="keyword">assert</span> outputs.std() &gt; <span class="number">0.05</span> <span class="keyword">and</span> outputs.std() &lt; <span class="number">0.3</span></span><br><span class="line"><span class="keyword">assert</span> outputs.min() &gt; <span class="number">-2</span> <span class="keyword">and</span> outputs.min() &lt; <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> outputs.max() &lt; <span class="number">2</span> <span class="keyword">and</span> outputs.max() &gt; <span class="number">0</span></span><br><span class="line">layers = [str(x).replace(<span class="string">' '</span>, <span class="string">''</span>).replace(<span class="string">'inplace=True'</span>, <span class="string">''</span>) <span class="keyword">for</span> x <span class="keyword">in</span> map_fn.get_mapping()]</span><br><span class="line"><span class="keyword">assert</span> layers == [<span class="string">'Linear(in_features=10,out_features=20,bias=True)'</span>, </span><br><span class="line">                  <span class="string">'ReLU()'</span>, </span><br><span class="line">                  <span class="string">'Linear(in_features=20,out_features=20,bias=True)'</span>, </span><br><span class="line">                  <span class="string">'ReLU()'</span>, </span><br><span class="line">                  <span class="string">'Linear(in_features=20,out_features=30,bias=True)'</span>]</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Random-Noise-Injection"><a href="#Random-Noise-Injection" class="headerlink" title="Random Noise Injection"></a>Random Noise Injection</h2><p>Next, you will implement the random noise injection that occurs before every AdaIN block. To do this, you need to create a noise tensor that is the same size as the current feature map (image).</p><p>The noise tensor is not entirely random; it is initialized as one random channel that is then multiplied by learned weights for each channel in the image. For example, imagine an image has 512 channels and its height and width are (4 x 4). You would first create a random (4 x 4) noise matrix with one channel. Then, your model would create 512 values—one for each channel. Next, you multiply the (4 x 4) matrix by each one of these values. This creates a “random” tensor of 512 channels and (4 x 4) pixels, the same dimensions as the image. Finally, you add this noise tensor to the image. This introduces uncorrelated noise and is meant to increase the diversity in the image.</p><p>New starting weights are generated for every new layer, or generator, where this class is used. Within a layer, every following time the noise injection is called, you take another step with the optimizer and the weights that you use for each channel are optimized (i.e. learned).</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">InjectNoise</font></code></b></font></summary>1.   The weight should have the shape (1, channels, 1, 1).</details><!-- <details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">InjectNoise</font></code></b></font></summary>1.   Remember that you only make the noise for one channel (it is then multiplied by random values to create ones for the other channels).</details> --><!-- (not sure how??) You'll find the get_noise function from before helpful here --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: InjectNoise</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InjectNoise</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Inject Noise Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        channels: the number of channels the image has, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channels)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = nn.Parameter( <span class="comment"># You use nn.Parameter so that these weights can be optimized</span></span><br><span class="line">            <span class="comment"># Initiate the weights for the channels from a random normal distribution</span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            torch.randn(size = (<span class="number">1</span>, channels, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of InjectNoise: Given an image, </span></span><br><span class="line"><span class="string">        returns the image with random noise added.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: the feature map of shape (n_samples, channels, width, height)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># Set the appropriate shape for the noise!</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        n_samples, channels, width, height = image.shape</span><br><span class="line">        noise_shape = (n_samples, <span class="number">1</span>, width, height)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        noise = torch.randn(noise_shape, device=image.device) <span class="comment"># Creates the random noise</span></span><br><span class="line">        <span class="keyword">return</span> image + self.weight * noise <span class="comment"># Applies to image after multiplying by the weight for each channel</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.weight</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_noise_channels = <span class="number">3000</span></span><br><span class="line">test_noise_samples = <span class="number">20</span></span><br><span class="line">fake_images = torch.randn(test_noise_samples, test_noise_channels, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">inject_noise = InjectNoise(test_noise_channels)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(inject_noise.weight.std() - <span class="number">1</span>) &lt; <span class="number">0.1</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(inject_noise.weight.mean()) &lt; <span class="number">0.1</span></span><br><span class="line"><span class="keyword">assert</span> type(inject_noise.get_weight()) == torch.nn.parameter.Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(inject_noise.weight.shape) == (<span class="number">1</span>, test_noise_channels, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">inject_noise.weight = nn.Parameter(torch.ones_like(inject_noise.weight))</span><br><span class="line"><span class="comment"># Check that something changed</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images)).mean() &gt; <span class="number">0.1</span></span><br><span class="line"><span class="comment"># Check that the change is per-channel</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images).std(<span class="number">0</span>)).mean() &gt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images).std(<span class="number">1</span>)).mean() &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images).std(<span class="number">2</span>)).mean() &gt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images).std(<span class="number">3</span>)).mean() &gt; <span class="number">1e-4</span></span><br><span class="line"><span class="comment"># Check that the per-channel change is roughly normal</span></span><br><span class="line">per_channel_change = (inject_noise(fake_images) - fake_images).mean(<span class="number">1</span>).std()</span><br><span class="line"><span class="keyword">assert</span> per_channel_change &gt; <span class="number">0.9</span> <span class="keyword">and</span> per_channel_change &lt; <span class="number">1.1</span></span><br><span class="line"><span class="comment"># Make sure that the weights are being used at all</span></span><br><span class="line">inject_noise.weight = nn.Parameter(torch.zeros_like(inject_noise.weight))</span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images)).mean() &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> len(inject_noise.weight.shape) == <span class="number">4</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Adaptive-Instance-Normalization-AdaIN"><a href="#Adaptive-Instance-Normalization-AdaIN" class="headerlink" title="Adaptive Instance Normalization (AdaIN)"></a>Adaptive Instance Normalization (AdaIN)</h2><p>The next component you will implement is AdaIN. To increase control over the image, you inject $w$ — the intermediate noise vector — multiple times throughout StyleGAN. This is done by transforming it into a set of style parameters and introducing the style to the image through AdaIN. Given an image ($x_i$) and the intermediate vector ($w$), AdaIN takes the instance normalization of the image and multiplies it by the style scale ($y_s$) and adds the style bias ($y_b$). You need to calculate the learnable style scale and bias by using linear mappings from $w$.</p><h1 id="text-AdaIN-boldsymbol-mathrm-x-i-boldsymbol-mathrm-y-boldsymbol-mathrm-y-s-i-frac-boldsymbol-mathrm-x-i-mu-boldsymbol-mathrm-x-i-sigma-boldsymbol-mathrm-x-i-boldsymbol-mathrm-y-b-i"><a href="#text-AdaIN-boldsymbol-mathrm-x-i-boldsymbol-mathrm-y-boldsymbol-mathrm-y-s-i-frac-boldsymbol-mathrm-x-i-mu-boldsymbol-mathrm-x-i-sigma-boldsymbol-mathrm-x-i-boldsymbol-mathrm-y-b-i" class="headerlink" title="$ \text{AdaIN}(\boldsymbol{\mathrm{x}}_i, \boldsymbol{\mathrm{y}}) = \boldsymbol{\mathrm{y}}_{s,i} \frac{\boldsymbol{\mathrm{x}}_i - \mu(\boldsymbol{\mathrm{x}}_i)}{\sigma(\boldsymbol{\mathrm{x}}_i)} + \boldsymbol{\mathrm{y}}_{b,i} $"></a>$ \text{AdaIN}(\boldsymbol{\mathrm{x}}_i, \boldsymbol{\mathrm{y}}) = \boldsymbol{\mathrm{y}}_{s,i} \frac{\boldsymbol{\mathrm{x}}_i - \mu(\boldsymbol{\mathrm{x}}_i)}{\sigma(\boldsymbol{\mathrm{x}}_i)} + \boldsymbol{\mathrm{y}}_{b,i} $</h1><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">forward</font></code></b></font></summary>1.   Remember the equation for AdaIN.2.   The instance normalized image, style scale, and style shift have already been calculated for you.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: AdaIN</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdaIN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    AdaIN Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        channels: the number of channels the image has, a scalar</span></span><br><span class="line"><span class="string">        w_dim: the dimension of the intermediate noise vector, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channels, w_dim)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the input per-dimension</span></span><br><span class="line">        self.instance_norm = nn.InstanceNorm2d(channels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># You want to map w to a set of style weights per channel.</span></span><br><span class="line">        <span class="comment"># Replace the Nones with the correct dimensions - keep in mind that </span></span><br><span class="line">        <span class="comment"># both linear maps transform a w vector into style weights </span></span><br><span class="line">        <span class="comment"># corresponding to the number of image channels.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        self.style_scale_transform = nn.Linear(w_dim, channels)</span><br><span class="line">        self.style_shift_transform = nn.Linear(w_dim, channels)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image, w)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of AdaIN: Given an image and intermediate noise vector w, </span></span><br><span class="line"><span class="string">        returns the normalized image that has been scaled and shifted by the style.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: the feature map of shape (n_samples, channels, width, height)</span></span><br><span class="line"><span class="string">            w: the intermediate noise vector</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        normalized_image = self.instance_norm(image)</span><br><span class="line">        style_scale = self.style_scale_transform(w)[:, :, <span class="keyword">None</span>, <span class="keyword">None</span>]</span><br><span class="line">        style_shift = self.style_shift_transform(w)[:, :, <span class="keyword">None</span>, <span class="keyword">None</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate the transformed image</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        transformed_image = style_scale * normalized_image + style_shift</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        <span class="keyword">return</span> transformed_image</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_style_scale_transform</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.style_scale_transform</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_style_shift_transform</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.style_shift_transform</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">w_channels = <span class="number">50</span></span><br><span class="line">image_channels = <span class="number">20</span></span><br><span class="line">image_size = <span class="number">30</span></span><br><span class="line">n_test = <span class="number">10</span></span><br><span class="line">adain = AdaIN(image_channels, w_channels)</span><br><span class="line">test_w = torch.randn(n_test, w_channels)</span><br><span class="line"><span class="keyword">assert</span> adain.style_scale_transform(test_w).shape == adain.style_shift_transform(test_w).shape</span><br><span class="line"><span class="keyword">assert</span> adain.style_scale_transform(test_w).shape[<span class="number">-1</span>] == image_channels</span><br><span class="line"><span class="keyword">assert</span> tuple(adain(torch.randn(n_test, image_channels, image_size, image_size), test_w).shape) == (n_test, image_channels, image_size, image_size)</span><br><span class="line"></span><br><span class="line">w_channels = <span class="number">3</span></span><br><span class="line">image_channels = <span class="number">2</span></span><br><span class="line">image_size = <span class="number">3</span></span><br><span class="line">n_test = <span class="number">1</span></span><br><span class="line">adain = AdaIN(image_channels, w_channels)</span><br><span class="line"></span><br><span class="line">adain.style_scale_transform.weight.data = torch.ones_like(adain.style_scale_transform.weight.data) / <span class="number">4</span></span><br><span class="line">adain.style_scale_transform.bias.data = torch.zeros_like(adain.style_scale_transform.bias.data)</span><br><span class="line">adain.style_shift_transform.weight.data = torch.ones_like(adain.style_shift_transform.weight.data) / <span class="number">5</span></span><br><span class="line">adain.style_shift_transform.bias.data = torch.zeros_like(adain.style_shift_transform.bias.data)</span><br><span class="line">test_input = torch.ones(n_test, image_channels, image_size, image_size)</span><br><span class="line">test_input[:, :, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">test_w = torch.ones(n_test, w_channels)</span><br><span class="line">test_output = adain(test_input, test_w)</span><br><span class="line"><span class="keyword">assert</span>(torch.abs(test_output[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>] - <span class="number">3</span> / <span class="number">5</span> + torch.sqrt(torch.tensor(<span class="number">9</span> / <span class="number">8</span>))) &lt; <span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">assert</span>(torch.abs(test_output[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>] - <span class="number">3</span> / <span class="number">5</span> - torch.sqrt(torch.tensor(<span class="number">9</span> / <span class="number">32</span>))) &lt; <span class="number">1e-4</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Progressive-Growing-in-StyleGAN"><a href="#Progressive-Growing-in-StyleGAN" class="headerlink" title="Progressive Growing in StyleGAN"></a>Progressive Growing in StyleGAN</h2><p>The final StyleGAN component that you will create is progressive growing. This helps StyleGAN to create high resolution images by gradually doubling the image’s size until the desired size.</p><p>You will start by creating a block for the StyleGAN generator. This is comprised of an upsampling layer, a convolutional layer, random noise injection, an AdaIN layer, and an activation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: MicroStyleGANGeneratorBlock</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MicroStyleGANGeneratorBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Micro StyleGAN Generator Block Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        in_chan: the number of channels in the input, a scalar</span></span><br><span class="line"><span class="string">        out_chan: the number of channels wanted in the output, a scalar</span></span><br><span class="line"><span class="string">        w_dim: the dimension of the intermediate noise vector, a scalar</span></span><br><span class="line"><span class="string">        kernel_size: the size of the convolving kernel</span></span><br><span class="line"><span class="string">        starting_size: the size of the starting image</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_chan, out_chan, w_dim, kernel_size, starting_size, use_upsample=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.use_upsample = use_upsample</span><br><span class="line">        <span class="comment"># Replace the Nones in order to:</span></span><br><span class="line">        <span class="comment"># 1. Upsample to the starting_size, bilinearly (https://pytorch.org/docs/master/generated/torch.nn.Upsample.html)</span></span><br><span class="line">        <span class="comment"># 2. Create a kernel_size convolution which takes in </span></span><br><span class="line">        <span class="comment">#    an image with in_chan and outputs one with out_chan (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)</span></span><br><span class="line">        <span class="comment"># 3. Create an object to inject noise</span></span><br><span class="line">        <span class="comment"># 4. Create an AdaIN object</span></span><br><span class="line">        <span class="comment"># 5. Create a LeakyReLU activation with slope 0.2</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        <span class="keyword">if</span> self.use_upsample:</span><br><span class="line">            self.upsample = nn.Upsample((starting_size, starting_size), mode=<span class="string">"bilinear"</span>)</span><br><span class="line">        self.conv = nn.Conv2d(in_chan, out_chan, <span class="number">3</span>, padding=<span class="number">1</span>) <span class="comment"># Padding is used to maintain the image size</span></span><br><span class="line">        self.inject_noise = InjectNoise(out_chan)</span><br><span class="line">        self.adain = AdaIN(out_chan, w_dim)</span><br><span class="line">        self.activation = nn.LeakyReLU(negative_slope = <span class="number">0.2</span>, inplace = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, w)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of MicroStyleGANGeneratorBlock: Given an x and w, </span></span><br><span class="line"><span class="string">        computes a StyleGAN generator block.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: the input into the generator, feature map of shape (n_samples, channels, width, height)</span></span><br><span class="line"><span class="string">            w: the intermediate noise vector</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> self.use_upsample:</span><br><span class="line">            x = self.upsample(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.inject_noise(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.adain(x, w)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">test_stylegan_block = MicroStyleGANGeneratorBlock(in_chan=<span class="number">128</span>, out_chan=<span class="number">64</span>, w_dim=<span class="number">256</span>, kernel_size=<span class="number">3</span>, starting_size=<span class="number">8</span>)</span><br><span class="line">test_x = torch.ones(<span class="number">1</span>, <span class="number">128</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">test_x[:, :, <span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">test_w = torch.ones(<span class="number">1</span>, <span class="number">256</span>)</span><br><span class="line">test_x = test_stylegan_block.upsample(test_x)</span><br><span class="line"><span class="keyword">assert</span> tuple(test_x.shape) == (<span class="number">1</span>, <span class="number">128</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_x.mean() - <span class="number">0.75</span>) &lt; <span class="number">1e-4</span></span><br><span class="line">test_x = test_stylegan_block.conv(test_x)</span><br><span class="line"><span class="keyword">assert</span> tuple(test_x.shape) == (<span class="number">1</span>, <span class="number">64</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">test_x = test_stylegan_block.inject_noise(test_x)</span><br><span class="line">test_x = test_stylegan_block.activation(test_x)</span><br><span class="line"><span class="keyword">assert</span> test_x.min() &lt; <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> -test_x.min() / test_x.max() &lt; <span class="number">0.4</span></span><br><span class="line">test_x = test_stylegan_block.adain(test_x, test_w) </span><br><span class="line">foo = test_stylegan_block(torch.ones(<span class="number">10</span>, <span class="number">128</span>, <span class="number">4</span>, <span class="number">4</span>), torch.ones(<span class="number">10</span>, <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Now, you can implement progressive growing. </p><p>StyleGAN starts with a constant 4 x 4 (x 512 channel) tensor which is put through an iteration of the generator without upsampling. The output is some noise that can then be transformed into a blurry 4 x 4 image. This is where the progressive growing process begins. The 4 x 4 noise can be further passed through a generator block with upsampling to produce an 8 x 8 output. However, this will be done gradually.</p><p>You will simulate progressive growing from an 8 x 8 image to a 16 x 16 image. Instead of simply passing it to the generator block with upsampling, StyleGAN gradually trains the generator to the new size by mixing in an image that was only upsampled. By mixing an upsampled 8 x 8 image (which is 16 x 16) with increasingly more of the 16 x 16 generator output, the generator is more stable as it progressively trains. As such, you will do two separate operations with the 8 x 8 noise:</p><ol><li>Pass it into the next generator block to create an output noise, that you will then transform to an image.</li><li>Transform it into an image and then upsample it to be 16 x 16.</li></ol><p>You will now have two images that are both double the resolution of the 8 x 8 noise. Then, using an alpha ($\alpha$) term, you combine the higher resolution images obtained from (1) and (2). You would then pass this into the discriminator and use the feedback to update the weights of your generator. The key here is that the $\alpha$ term is gradually increased until eventually, only the image from (1), the generator, is used. That is your final image or you could continue this process to make a 32 x 32 image or 64 x 64, 128 x 128, etc. </p><p>This micro model you will implement will visualize what the model outputs at a particular stage of training, for a specific value of $\alpha$. However to reiterate, in practice, StyleGAN will slowly phase out the upsampled image by increasing the $\alpha$ parameter over many training steps, doing this process repeatedly with larger and larger alpha values until it is 1—at this point, the combined image is solely comprised of the image from the generator block. This method of gradually training the generator increases the stability and fidelity of the model.</p><!-- by passing a random noise vector in $z$ through the mapping function you wrote to get $w$. $w$ is then passed through the first block of the generator to create your first output noise. --><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">forward</font></code></b></font></summary>1.    You may find [torch.lerp](https://pytorch.org/docs/stable/generated/torch.lerp.html) helpful.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: MicroStyleGANGenerator</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MicroStyleGANGenerator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Micro StyleGAN Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        map_hidden_dim: the mapping inner dimension, a scalar</span></span><br><span class="line"><span class="string">        w_dim: the dimension of the intermediate noise vector, a scalar</span></span><br><span class="line"><span class="string">        in_chan: the dimension of the constant input, usually w_dim, a scalar</span></span><br><span class="line"><span class="string">        out_chan: the number of channels wanted in the output, a scalar</span></span><br><span class="line"><span class="string">        kernel_size: the size of the convolving kernel</span></span><br><span class="line"><span class="string">        hidden_chan: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, </span></span></span><br><span class="line"><span class="function"><span class="params">                 z_dim, </span></span></span><br><span class="line"><span class="function"><span class="params">                 map_hidden_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 w_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 in_chan,</span></span></span><br><span class="line"><span class="function"><span class="params">                 out_chan, </span></span></span><br><span class="line"><span class="function"><span class="params">                 kernel_size, </span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_chan)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.map = MappingLayers(z_dim, map_hidden_dim, w_dim)</span><br><span class="line">        <span class="comment"># Typically this constant is initiated to all ones, but you will initiate to a</span></span><br><span class="line">        <span class="comment"># Gaussian to better visualize the network's effect</span></span><br><span class="line">        self.starting_constant = nn.Parameter(torch.randn(<span class="number">1</span>, in_chan, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">        self.block0 = MicroStyleGANGeneratorBlock(in_chan, hidden_chan, w_dim, kernel_size, <span class="number">4</span>, use_upsample=<span class="keyword">False</span>)</span><br><span class="line">        self.block1 = MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, <span class="number">8</span>)</span><br><span class="line">        self.block2 = MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, <span class="number">16</span>)</span><br><span class="line">        <span class="comment"># You need to have a way of mapping from the output noise to an image, </span></span><br><span class="line">        <span class="comment"># so you learn a 1x1 convolution to transform the e.g. 512 channels into 3 channels</span></span><br><span class="line">        <span class="comment"># (Note that this is simplified, with clipping used in the real StyleGAN)</span></span><br><span class="line">        self.block1_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.block2_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.alpha = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">upsample_to_match_size</span><span class="params">(self, smaller_image, bigger_image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for upsampling an image to the size of another: Given a two images (smaller and bigger), </span></span><br><span class="line"><span class="string">        upsamples the first to have the same dimensions as the second.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            smaller_image: the smaller image to upsample</span></span><br><span class="line"><span class="string">            bigger_image: the bigger image whose dimensions will be upsampled to</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> F.interpolate(smaller_image, size=bigger_image.shape[<span class="number">-2</span>:], mode=<span class="string">'bilinear'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise, return_intermediate=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of MicroStyleGANGenerator: Given noise, </span></span><br><span class="line"><span class="string">        computes a StyleGAN iteration.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">            return_intermediate: a boolean, true to return the images as well (for testing) and false otherwise</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.starting_constant</span><br><span class="line">        w = self.map(noise)</span><br><span class="line">        x = self.block0(x, w)</span><br><span class="line">        x_small = self.block1(x, w) <span class="comment"># First generator run output</span></span><br><span class="line">        x_small_image = self.block1_to_image(x_small)</span><br><span class="line">        x_big = self.block2(x_small, w) <span class="comment"># Second generator run output </span></span><br><span class="line">        x_big_image = self.block2_to_image(x_big)</span><br><span class="line">        x_small_upsample = self.upsample_to_match_size(x_small_image, x_big_image) <span class="comment"># Upsample first generator run output to be same size as second generator run output </span></span><br><span class="line">        <span class="comment"># Interpolate between the upsampled image and the image from the generator using alpha</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        interpolation = torch.lerp(x_small_upsample,x_big_image,self.alpha)</span><br><span class="line">        <span class="comment">#### END CODE HERE #### </span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> return_intermediate:</span><br><span class="line">            <span class="keyword">return</span> interpolation, x_small_upsample, x_big_image</span><br><span class="line">        <span class="keyword">return</span> interpolation</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">z_dim = <span class="number">128</span></span><br><span class="line">out_chan = <span class="number">3</span></span><br><span class="line">truncation = <span class="number">0.7</span></span><br><span class="line"></span><br><span class="line">mu_stylegan = MicroStyleGANGenerator(</span><br><span class="line">    z_dim=z_dim, </span><br><span class="line">    map_hidden_dim=<span class="number">1024</span>,</span><br><span class="line">    w_dim=<span class="number">496</span>,</span><br><span class="line">    in_chan=<span class="number">512</span>,</span><br><span class="line">    out_chan=out_chan, </span><br><span class="line">    kernel_size=<span class="number">3</span>, </span><br><span class="line">    hidden_chan=<span class="number">256</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_samples = <span class="number">10</span></span><br><span class="line">test_result = mu_stylegan(get_truncated_noise(test_samples, z_dim, truncation))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check if the block works</span></span><br><span class="line"><span class="keyword">assert</span> tuple(test_result.shape) == (test_samples, out_chan, <span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that the interpolation is correct</span></span><br><span class="line">mu_stylegan.alpha = <span class="number">1.</span></span><br><span class="line">test_result, _, test_big =  mu_stylegan(</span><br><span class="line">    get_truncated_noise(test_samples, z_dim, truncation), </span><br><span class="line">    return_intermediate=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_result - test_big).mean() &lt; <span class="number">0.001</span></span><br><span class="line">mu_stylegan.alpha = <span class="number">0.</span></span><br><span class="line">test_result, test_small, _ =  mu_stylegan(</span><br><span class="line">    get_truncated_noise(test_samples, z_dim, truncation), </span><br><span class="line">    return_intermediate=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_result - test_small).mean() &lt; <span class="number">0.001</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Running-StyleGAN"><a href="#Running-StyleGAN" class="headerlink" title="Running StyleGAN"></a>Running StyleGAN</h2><p>Finally, you can put all the components together to run an iteration of your micro StyleGAN!</p><p>You can also visualize what this randomly initiated generator can produce. The code will automatically interpolate between different values of alpha so that you can intuitively see what it means to mix the low-resolution and high-resolution images using different values of alpha. In the generated image, the samples start from low alpha values and go to high alpha values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = [<span class="number">15</span>, <span class="number">15</span>]</span><br><span class="line"></span><br><span class="line">viz_samples = <span class="number">10</span></span><br><span class="line"><span class="comment"># The noise is exaggerated for visual effect</span></span><br><span class="line">viz_noise = get_truncated_noise(viz_samples, z_dim, truncation) * <span class="number">10</span></span><br><span class="line"></span><br><span class="line">mu_stylegan.eval()</span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, num=<span class="number">5</span>):</span><br><span class="line">    mu_stylegan.alpha = alpha</span><br><span class="line">    viz_result, _, _ =  mu_stylegan(</span><br><span class="line">        viz_noise, </span><br><span class="line">        return_intermediate=<span class="keyword">True</span>)</span><br><span class="line">    images += [tensor <span class="keyword">for</span> tensor <span class="keyword">in</span> viz_result]</span><br><span class="line">show_tensor_images(torch.stack(images), nrow=viz_samples, num_images=len(images))</span><br><span class="line">mu_stylegan = mu_stylegan.train()</span><br></pre></td></tr></table></figure><p><img src="output_22_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Components-of-StyleGAN&quot;&gt;&lt;a href=&quot;#Components-of-StyleGAN&quot; class=&quot;headerlink&quot; title=&quot;Components of StyleGAN&quot;&gt;&lt;/a&gt;Components of StyleG
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Evaluating GANs</title>
    <link href="https://zhangruochi.com/Evaluating-GANs/2020/10/10/"/>
    <id>https://zhangruochi.com/Evaluating-GANs/2020/10/10/</id>
    <published>2020-10-10T07:34:02.000Z</published>
    <updated>2020-10-13T03:21:12.720Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluating-GANs"><a href="#Evaluating-GANs" class="headerlink" title="Evaluating GANs"></a>Evaluating GANs</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called Fréchet Inception Distance (FID).</p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Understand the challenges associated with evaluating GANs.</li><li>Write code to evaluate the Fréchet Inception Distance.</li></ol><h2 id="Challenges-With-Evaluating-GANs"><a href="#Challenges-With-Evaluating-GANs" class="headerlink" title="Challenges With Evaluating GANs"></a>Challenges With Evaluating GANs</h2><h4 id="Loss-is-Uninformative-of-Performance"><a href="#Loss-is-Uninformative-of-Performance" class="headerlink" title="Loss is Uninformative of Performance"></a>Loss is Uninformative of Performance</h4><p>One aspect that makes evaluating GANs challenging is that the loss tells us little about their performance. Unlike with classifiers, where a low loss on a test set indicates superior performance, a low loss for the generator or discriminator suggests that learning has stopped. </p><h4 id="No-Clear-Non-human-Metric"><a href="#No-Clear-Non-human-Metric" class="headerlink" title="No Clear Non-human Metric"></a>No Clear Non-human Metric</h4><p>If you define the goal of a GAN as “generating images which look real to people” then it’s technically possible to measure this directly: <a href="https://arxiv.org/abs/1904.01121" target="_blank" rel="noopener">you can ask people to act as a discriminator</a>. However, this takes significant time and money so ideally you can use a proxy for this. There is also no “perfect” discriminator that can differentiate reals from fakes - if there were, a lot of machine learning tasks would be solved ;)</p><p>In this notebook, you will implement Fréchet Inception Distance, one method which aims to solve these issues. </p><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>For this notebook, you will again be using <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" rel="noopener">CelebA</a>. You will start by loading a pre-trained generator which has been trained on CelebA.</p><p>Here, you will import some useful libraries and packages. You will also be provided with the generator and noise code from earlier assignments.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> CelebA</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for our testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (CelebA is rgb, so 3 is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_chan=<span class="number">3</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">8</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">8</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = noise.view(len(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure><h2 id="Loading-the-Pre-trained-Model"><a href="#Loading-the-Pre-trained-Model" class="headerlink" title="Loading the Pre-trained Model"></a>Loading the Pre-trained Model</h2><p>Now, you can set the arguments for the model and load the dataset:</p><ul><li>z_dim: the dimension of the noise vector</li><li>image_size: the image size of the input to Inception (more details in the following section)</li><li>device: the device type</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">image_size = <span class="number">299</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(image_size),</span><br><span class="line">    transforms.CenterCrop(image_size),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">in_coursera = <span class="keyword">True</span> <span class="comment"># Set this to false if you're running this outside Coursera</span></span><br><span class="line"><span class="keyword">if</span> in_coursera:</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    data = torch.Tensor(np.load(<span class="string">'fid_images_tensor.npz'</span>, allow_pickle=<span class="keyword">True</span>)[<span class="string">'arr_0'</span>])</span><br><span class="line">    dataset = torch.utils.data.TensorDataset(data, data)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dataset = CelebA(<span class="string">"."</span>, download=<span class="keyword">True</span>, transform=transform)</span><br></pre></td></tr></table></figure><p>Then, you can load and initialize the model with weights from a pre-trained model. This allows you to use the pre-trained model as if you trained it yourself.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen.load_state_dict(torch.load(<span class="string">f"pretrained_celeba.pth"</span>, map_location=torch.device(device))[<span class="string">"gen"</span>])</span><br><span class="line">gen = gen.eval()</span><br></pre></td></tr></table></figure><h2 id="Inception-v3-Network"><a href="#Inception-v3-Network" class="headerlink" title="Inception-v3 Network"></a>Inception-v3 Network</h2><p>Inception-V3 is a neural network trained on <a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a> to classify objects. You may recall from the lectures that ImageNet has over 1 million images to train on. As a result, Inception-V3 does a good job detecting features and classifying images. Here, you will load Inception-V3 as <code>inception_model</code>.</p><!--  In the past, people would use a pretrained Inception network to identify the classes of the objects generated by a GAN and measure how similar the distribution of classes generated was to the true image (using KL divergence). This is known as inception score. However, there are many problems with this metric. Barratt and Sharma's 2018 "[A Note on the Inception Score](https://arxiv.org/pdf/1801.01973.pdf)" highlights many issues with this approach. Among them, they highlight its instability, its exploitability, and the widespread use of Inception Score on models not trained on ImageNet.  --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> inception_v3</span><br><span class="line">inception_model = inception_v3(pretrained=<span class="keyword">False</span>)</span><br><span class="line">inception_model.load_state_dict(torch.load(<span class="string">"inception_v3_google-1a9a5a14.pth"</span>))</span><br><span class="line">inception_model.to(device)</span><br><span class="line">inception_model = inception_model.eval() <span class="comment"># Evaluation mode</span></span><br></pre></td></tr></table></figure><h2 id="Frechet-Inception-Distance"><a href="#Frechet-Inception-Distance" class="headerlink" title="Fréchet Inception Distance"></a>Fréchet Inception Distance</h2><p>Fréchet Inception Distance (FID) was proposed as an improvement over Inception Score and still uses the Inception-v3 network as part of its calculation. However, instead of using the classification labels of the Inception-v3 network, it uses the output from an earlier layer—the layer right before the labels. This is often called the feature layer. Research has shown that deep convolutional neural networks trained on difficult tasks, like classifying many classes, build increasingly sophisticated representations of features going deeper into the network. For example, the first few layers may learn to detect different kinds of edges and curves, while the later layers may have neurons that fire in response to human faces.</p><p>To get the feature layer of a convolutional neural network, you can replace the final fully connected layer with an identity layer that simply returns whatever input it received, unchanged. This essentially removes the final classification layer and leaves you with the intermediate outputs from the layer before.</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">inception_model.fc</font></code></b></font></summary>1.    You may find [torch.nn.Identity()](https://pytorch.org/docs/master/generated/torch.nn.Identity.html) helpful.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: inception_model.fc</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You want to replace the final fully-connected (fc) layer </span></span><br><span class="line"><span class="comment"># with an identity function layer to cut off the classification</span></span><br><span class="line"><span class="comment"># layer and get a feature extractor</span></span><br><span class="line"><span class="comment">#### START CODE HERE ####</span></span><br><span class="line">inception_model.fc = nn.Identity()</span><br><span class="line"><span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_identity_noise = torch.randn(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.equal(test_identity_noise, inception_model.fc(test_identity_noise))</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h3 id="Frechet-Distance"><a href="#Frechet-Distance" class="headerlink" title="Fréchet Distance"></a>Fréchet Distance</h3><p>Fréchet distance uses the values from the feature layer for two sets of images, say reals and fakes, and compares different statistical properties between them to see how different they are. Specifically, Fréchet distance finds the shortest distance needed to walk along two lines, or two curves, simultaneously. The most intuitive explanation of Fréchet distance is as the “minimum leash distance” between two points. Imagine yourself and your dog, both moving along two curves. If you walked on one curve and your dog, attached to a leash, walked on the other at the same pace, what is the least amount of leash that you can give your dog so that you never need to give them more slack during your walk? Using this, the Fréchet distance measures the similarity between these two curves.</p><p>The basic idea is similar for calculating the Fréchet distance between two probability distributions. You’ll start by seeing what this looks like in one-dimensional, also called univariate, space.</p><h4 id="Univariate-Frechet-Distance"><a href="#Univariate-Frechet-Distance" class="headerlink" title="Univariate Fréchet Distance"></a>Univariate Fréchet Distance</h4><p>You can calculate the distance between two normal distributions $X$ and $Y$ with means $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$, as:</p><script type="math/tex; mode=display">d(X,Y) = (\mu_X-\mu_Y)^2 + (\sigma_X-\sigma_Y)^2</script><p>Pretty simple, right? Now you can see how it can be converted to be used in multi-dimensional, which is also called multivariate, space.</p><h4 id="Multivariate-Frechet-Distance"><a href="#Multivariate-Frechet-Distance" class="headerlink" title="Multivariate Fréchet Distance"></a>Multivariate Fréchet Distance</h4><p><strong>Covariance</strong></p><p>To find the Fréchet distance between two multivariate normal distributions, you first need to find the covariance instead of the standard deviation. The covariance, which is the multivariate version of variance (the square of standard deviation), is represented using a square matrix where the side length is equal to the number of dimensions. Since the feature vectors you will be using have 2048 values/weights, the covariance matrix will be 2048 x 2048. But for the sake of an example, this is a covariance matrix in a two-dimensional space:</p><p>$\Sigma = \left(\begin{array}{cc}<br>1 &amp; 0\\<br>0 &amp; 1<br>\end{array}\right)<br>$</p><p>The value at location $(i, j)$ corresponds to the covariance of vector $i$ with vector $j$. Since the covariance of $i$ with $j$ and $j$ with $i$ are equivalent, the matrix will always be symmetric with respect to the diagonal. The diagonal is the covariance of that element with itself. In this example, there are zeros everywhere except the diagonal. That means that the two dimensions are independent of one another, they are completely unrelated.</p><p>The following code cell will visualize this matrix.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import os</span></span><br><span class="line"><span class="comment">#os.environ['KMP_DUPLICATE_LIB_OK']='True'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> MultivariateNormal</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment"># This is for visualization</span></span><br><span class="line">mean = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance = torch.Tensor( <span class="comment"># This matrix shows independence - there are only non-zero values on the diagonal</span></span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">independent_dist = MultivariateNormal(mean, covariance)</span><br><span class="line">samples = independent_dist.sample((<span class="number">10000</span>,))</span><br><span class="line">res = sns.jointplot(samples[:, <span class="number">0</span>], samples[:, <span class="number">1</span>], kind=<span class="string">"kde"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><p>Now, here’s an example of a multivariate normal distribution that has covariance:</p><p>$\Sigma = \left(\begin{array}{cc}<br>2 &amp; -1\\<br>-1 &amp; 2<br>\end{array}\right)<br>$</p><p>And see how it looks:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mean = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">covariance = torch.Tensor(</span><br><span class="line">    [[<span class="number">2</span>, <span class="number">-1</span>],</span><br><span class="line">     [<span class="number">-1</span>, <span class="number">2</span>]]</span><br><span class="line">)</span><br><span class="line">covariant_dist = MultivariateNormal(mean, covariance)</span><br><span class="line">samples = covariant_dist.sample((<span class="number">10000</span>,))</span><br><span class="line">res = sns.jointplot(samples[:, <span class="number">0</span>], samples[:, <span class="number">1</span>], kind=<span class="string">"kde"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_20_0.png" alt="png"></p><p><strong>Formula</strong></p><p>Based on the paper, “<a href="https://core.ac.uk/reader/82269844" target="_blank" rel="noopener">The Fréchet distance between multivariate normal distributions</a>“ by Dowson and Landau (1982), the Fréchet distance between two multivariate normal distributions $X$ and $Y$ is:</p><p>$d(X, Y) = \Vert\mu_X-\mu_Y\Vert^2 + \mathrm{Tr}\left(\Sigma_X+\Sigma_Y - 2 \sqrt{\Sigma_X \Sigma_Y}\right)$</p><p>Similar to the formula for univariate Fréchet distance, you can calculate the distance between the means and the distance between the standard deviations. However, calculating the distance between the standard deviations changes slightly here, as it includes the matrix product and matrix square root. $\mathrm{Tr}$ refers to the trace, the sum of the diagonal elements of a matrix.</p><p>Now you can implement this!</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">frechet_distance</font></code></b></font></summary>1.   You want to implement the above equation in code.2.   You might find the functions `torch.norm` and `torch.trace` helpful here.3.   A matrix_sqrt function is defined for you above -- you need to use it instead of `torch.sqrt()` which only gets the elementwise square root instead of the matrix square root.4.   You can also use the `@` symbol for matrix multiplication.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="comment"># This is the matrix square root function you will be using</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrix_sqrt</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function that takes in a matrix and returns the square root of that matrix.</span></span><br><span class="line"><span class="string">    For an input matrix A, the output matrix B would be such that B @ B is the matrix A.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        x: a matrix</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = x.cpu().detach().numpy()</span><br><span class="line">    y = scipy.linalg.sqrtm(y)</span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(y.real, device=x.device)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: frechet_distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frechet_distance</span><span class="params">(mu_x, mu_y, sigma_x, sigma_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for returning the Fréchet distance between multivariate Gaussians,</span></span><br><span class="line"><span class="string">    parameterized by their means and covariance matrices.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        mu_x: the mean of the first Gaussian, (n_features)</span></span><br><span class="line"><span class="string">        mu_y: the mean of the second Gaussian, (n_features) </span></span><br><span class="line"><span class="string">        sigma_x: the covariance matrix of the first Gaussian, (n_features, n_features)</span></span><br><span class="line"><span class="string">        sigma_y: the covariance matrix of the second Gaussian, (n_features, n_features)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(mu_x - mu_y) + torch.trace(sigma_x + sigma_y - <span class="number">2</span> * matrix_sqrt(sigma_x @ sigma_y))</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"></span><br><span class="line">mean1 = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance1 = torch.Tensor( <span class="comment"># This matrix shows independence - there are only non-zero values on the diagonal</span></span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">dist1 = MultivariateNormal(mean1, covariance1)</span><br><span class="line"></span><br><span class="line">mean2 = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance2 = torch.Tensor( <span class="comment"># This matrix shows dependence </span></span><br><span class="line">    [[<span class="number">2</span>, <span class="number">-1</span>],</span><br><span class="line">     [<span class="number">-1</span>, <span class="number">2</span>]]</span><br><span class="line">)</span><br><span class="line">dist2 = MultivariateNormal(mean2, covariance2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    frechet_distance(</span><br><span class="line">        dist1.mean, dist2.mean,</span><br><span class="line">        dist1.covariance_matrix, dist2.covariance_matrix</span><br><span class="line">    ),</span><br><span class="line">    <span class="number">4</span> - <span class="number">2</span> * torch.sqrt(torch.tensor(<span class="number">3.</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> (frechet_distance(</span><br><span class="line">        dist1.mean, dist1.mean,</span><br><span class="line">        dist1.covariance_matrix, dist1.covariance_matrix</span><br><span class="line">    ).item() == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together!"></a>Putting it all together!</h2><p>Now, you can apply FID to your generator from earlier.</p><p>You will start by defining a bit of helper code to preprocess the image for the Inception-v3 network:<br><!-- This isn't exactly what FID is meant for, since inception scores expect a natural image, but it should give a rough idea of the diversity and quality of your images.  [TODO: move to bottom since image net is trained on nature (cat, dog) images, fidelity (quality)] --></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = torch.nn.functional.interpolate(img, size=(<span class="number">299</span>, <span class="number">299</span>), mode=<span class="string">'bilinear'</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><p>Then, you’ll define a function to calculate the covariance of the features that returns a covariance matrix given a list of values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_covariance</span><span class="params">(features)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(np.cov(features.detach().numpy(), rowvar=<span class="keyword">False</span>))</span><br></pre></td></tr></table></figure><p>Finally, you can use the pre-trained Inception-v3 model to compute features of the real and fake images. With these features, you can then get the covariance and means of these features across many samples. </p><p>First, you get the features of the real and fake images using the Inception-v3 model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">fake_features_list = []</span><br><span class="line">real_features_list = []</span><br><span class="line"></span><br><span class="line">gen.eval()</span><br><span class="line">n_samples = <span class="number">512</span> <span class="comment"># The total number of samples</span></span><br><span class="line">batch_size = <span class="number">4</span> <span class="comment"># Samples per iteration</span></span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">cur_samples = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># You don't need to calculate gradients here, so you do this to save memory</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> real_example, _ <span class="keyword">in</span> tqdm(dataloader, total=n_samples // batch_size): <span class="comment"># Go by batch</span></span><br><span class="line">            real_samples = real_example</span><br><span class="line">            real_features = inception_model(real_samples.to(device)).detach().to(<span class="string">'cpu'</span>) <span class="comment"># Move features to CPU</span></span><br><span class="line">            real_features_list.append(real_features)</span><br><span class="line"></span><br><span class="line">            fake_samples = get_noise(len(real_example), z_dim).to(device)</span><br><span class="line">            fake_samples = preprocess(gen(fake_samples))</span><br><span class="line">            fake_features = inception_model(fake_samples.to(device)).detach().to(<span class="string">'cpu'</span>)</span><br><span class="line">            fake_features_list.append(fake_features)</span><br><span class="line">            cur_samples += len(real_samples)</span><br><span class="line">            <span class="keyword">if</span> cur_samples &gt;= n_samples:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Error in loop"</span>)</span><br></pre></td></tr></table></figure><pre><code>HBox(children=(FloatProgress(value=0.0, max=128.0), HTML(value=&#39;&#39;)))</code></pre><p>Then, you can combine all of the values that you collected for the reals and fakes into large tensors:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Needed as is for autograding</span></span><br><span class="line">fake_features_all = torch.cat(fake_features_list)</span><br><span class="line">real_features_all = torch.cat(real_features_list)</span><br></pre></td></tr></table></figure><p>And calculate the covariance and means of these real and fake features:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the covariance matrix for the fake and real features</span></span><br><span class="line"><span class="comment"># and also calculate the means of the feature over the batch (for each feature dimension mean)</span></span><br><span class="line"><span class="comment">#### START CODE HERE ####</span></span><br><span class="line">mu_fake = torch.mean(fake_features_all, axis = <span class="number">0</span>)</span><br><span class="line">mu_real = torch.mean(real_features_all, axis = <span class="number">0</span>)</span><br><span class="line">sigma_fake = get_covariance(fake_features_all)</span><br><span class="line">sigma_real = get_covariance(real_features_all)</span><br><span class="line"><span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> tuple(sigma_fake.shape) == (fake_features_all.shape[<span class="number">1</span>], fake_features_all.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> torch.abs(sigma_fake[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">2.5e-2</span>) &lt; <span class="number">1e-2</span> <span class="keyword">and</span> torch.abs(sigma_fake[<span class="number">-1</span>, <span class="number">-1</span>] - <span class="number">5e-2</span>) &lt; <span class="number">1e-2</span></span><br><span class="line"><span class="keyword">assert</span> tuple(sigma_real.shape) == (real_features_all.shape[<span class="number">1</span>], real_features_all.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> torch.abs(sigma_real[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">3.5768e-2</span>) &lt; <span class="number">1e-4</span> <span class="keyword">and</span> torch.abs(sigma_real[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">5.3236e-4</span>) &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> tuple(mu_fake.shape) == (fake_features_all.shape[<span class="number">1</span>],)</span><br><span class="line"><span class="keyword">assert</span> tuple(mu_real.shape) == (real_features_all.shape[<span class="number">1</span>],)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(mu_real[<span class="number">0</span>] - <span class="number">0.3099</span>) &lt; <span class="number">0.01</span> <span class="keyword">and</span> torch.abs(mu_real[<span class="number">1</span>] - <span class="number">0.2721</span>) &lt; <span class="number">0.01</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(mu_fake[<span class="number">0</span>] - <span class="number">0.37</span>) &lt; <span class="number">0.05</span> <span class="keyword">and</span> torch.abs(mu_real[<span class="number">1</span>] - <span class="number">0.27</span>) &lt; <span class="number">0.05</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>At this point, you can also visualize what the pairwise multivariate distributions of the inception features look like!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">indices = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">fake_dist = MultivariateNormal(mu_fake[indices], sigma_fake[indices][:, indices])</span><br><span class="line">fake_samples = fake_dist.sample((<span class="number">5000</span>,))</span><br><span class="line">real_dist = MultivariateNormal(mu_real[indices], sigma_real[indices][:, indices])</span><br><span class="line">real_samples = real_dist.sample((<span class="number">5000</span>,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df_fake = pd.DataFrame(fake_samples.numpy(), columns=indices)</span><br><span class="line">df_real = pd.DataFrame(real_samples.numpy(), columns=indices)</span><br><span class="line">df_fake[<span class="string">"is_real"</span>] = <span class="string">"no"</span></span><br><span class="line">df_real[<span class="string">"is_real"</span>] = <span class="string">"yes"</span></span><br><span class="line">df = pd.concat([df_fake, df_real])</span><br><span class="line">sns.pairplot(df, plot_kws=&#123;<span class="string">'alpha'</span>: <span class="number">0.1</span>&#125;, hue=<span class="string">'is_real'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7fa847b2ab38&gt;</code></pre><p><img src="output_37_1.png" alt="png"></p><p>Lastly, you can use your earlier <code>frechet_distance</code> function to calculate the FID and evaluate your GAN. You can see how similar/different the features of the generated images are to the features of the real images. The next cell might take five minutes or so to run in Coursera.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item())</span><br></pre></td></tr></table></figure><pre><code>86.48429107666016</code></pre><p>You’ll notice this model gets a pretty high FID, likely over 30. Since lower is better, and the best models on CelebA get scores in the single-digits, there’s clearly a ways to go with this model. You can use FID to compare different models, as well as different stages of training of the same model. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Evaluating-GANs&quot;&gt;&lt;a href=&quot;#Evaluating-GANs&quot; class=&quot;headerlink&quot; title=&quot;Evaluating GANs&quot;&gt;&lt;/a&gt;Evaluating GANs&lt;/h1&gt;&lt;h3 id=&quot;Goals&quot;&gt;&lt;a hre
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Build a Conditional GAN</title>
    <link href="https://zhangruochi.com/Build-a-Conditional-GAN/2020/10/09/"/>
    <id>https://zhangruochi.com/Build-a-Conditional-GAN/2020/10/09/</id>
    <published>2020-10-09T12:27:13.000Z</published>
    <updated>2020-10-13T03:21:17.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Build-a-Conditional-GAN"><a href="#Build-a-Conditional-GAN" class="headerlink" title="Build a Conditional GAN"></a>Build a Conditional GAN</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate.</p><p>You’ll then do some exploration of the generated images to visualize what the noise and class vectors mean.  </p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Learn the technical difference between a conditional and unconditional GAN.</li><li>Understand the distinction between the class and noise vector in a conditional GAN.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>For this assignment, you will be using the MNIST dataset again, but there’s nothing stopping you from applying this generator code to produce images of animals conditioned on the species or pictures of faces conditioned on facial characteristics.</p><p>Note that this assignment requires no changes to the architectures of the generator or discriminator, only changes to the data passed to both. The generator will no longer take <code>z_dim</code> as an argument, but  <code>input_dim</code> instead, since you need to pass in both the noise and class vectors. In addition to good variable naming, this also means that you can use the generator and discriminator code you have previously written with different parameters.</p><p>You will begin by importing the necessary libraries and building the generator and discriminator.</p><h4 id="Packages-and-Visualization"><a href="#Packages-and-Visualization" class="headerlink" title="Packages and Visualization"></a>Packages and Visualization</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for our testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>, nrow=<span class="number">5</span>, show=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu()</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    <span class="keyword">if</span> show:</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure><h4 id="Generator-and-Noise"><a href="#Generator-and-Noise" class="headerlink" title="Generator and Noise"></a>Generator and Noise</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="number">10</span>, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(input_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, input_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = noise.view(len(noise), self.input_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, input_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, input_dim, device=device)</span><br></pre></td></tr></table></figure><h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">      im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">            (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">      hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            self.make_disc_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_disc_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim * <span class="number">2</span>, <span class="number">1</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_disc_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; </span></span><br><span class="line"><span class="string">        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_chan)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        disc_pred = self.disc(image)</span><br><span class="line">        <span class="keyword">return</span> disc_pred.view(len(disc_pred), <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Class-Input"><a href="#Class-Input" class="headerlink" title="Class Input"></a>Class Input</h2><p>In conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0’s and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes, please create one-hot vectors for each label. There is a class within the PyTorch functional library that can help you.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">get_one_hot_labels</font></code></b></font></summary>1.   This code can be done in one line.2.   The documentation for [F.one_hot](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.one_hot) may be helpful.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_one_hot_labels</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_hot_labels</span><span class="params">(labels, n_classes)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        labels: tensor of labels from the dataloader, size (?)</span></span><br><span class="line"><span class="string">        n_classes: the total number of classes in the dataset, an integer scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> F.one_hot(labels, n_classes)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> (</span><br><span class="line">    get_one_hot_labels(</span><br><span class="line">        labels=torch.Tensor([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]]).long(),</span><br><span class="line">        n_classes=<span class="number">3</span></span><br><span class="line">    ).tolist() == </span><br><span class="line">    [[</span><br><span class="line">      [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line">      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">    ]]</span><br><span class="line">)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator.</p><p>To do this, you will need to write a function that combines two vectors. Remember that you need to ensure that the vectors are the same type: floats. Again, you can look to the PyTorch library for help.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">combine_vectors</font></code></b></font></summary>1.   This code can also be written in one line.2.   The documentation for [torch.cat](https://pytorch.org/docs/master/generated/torch.cat.html) may be helpful.3.   Specifically, you might want to look at what the `dim` argument of `torch.cat` does.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: combine_vectors</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_vectors</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">      x: (n_samples, ?) the first vector. </span></span><br><span class="line"><span class="string">        In this assignment, this will be the noise vector of shape (n_samples, z_dim), </span></span><br><span class="line"><span class="string">        but you shouldn't need to know the second dimension's size.</span></span><br><span class="line"><span class="string">      y: (n_samples, ?) the second vector.</span></span><br><span class="line"><span class="string">        Once again, in this assignment this will be the one-hot class vector </span></span><br><span class="line"><span class="string">        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Note: Make sure this function outputs a float no matter what inputs it receives</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    combined = torch.cat((x.float(),y.float()), dim = <span class="number">1</span>)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">combined = combine_vectors(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]));</span><br><span class="line"><span class="comment"># Check exact order of elements</span></span><br><span class="line"><span class="keyword">assert</span> torch.all(combined == torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>]]))</span><br><span class="line"><span class="comment"># Tests that items are of float type</span></span><br><span class="line"><span class="keyword">assert</span> (type(combined[<span class="number">0</span>][<span class="number">0</span>].item()) == float)</span><br><span class="line"><span class="comment"># Check shapes</span></span><br><span class="line">combined = combine_vectors(torch.randn(<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>), torch.randn(<span class="number">1</span>, <span class="number">8</span>, <span class="number">5</span>));</span><br><span class="line"><span class="keyword">assert</span> tuple(combined.shape) == (<span class="number">1</span>, <span class="number">12</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">assert</span> tuple(combine_vectors(torch.randn(<span class="number">1</span>, <span class="number">10</span>, <span class="number">12</span>).long(), torch.randn(<span class="number">1</span>, <span class="number">20</span>, <span class="number">12</span>).long()).shape) == (<span class="number">1</span>, <span class="number">30</span>, <span class="number">12</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can start to put it all together!<br>First, you will define some new parameters:</p><ul><li>mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it’s black-and-white) so 1 x 28 x 28</li><li>n_classes: the number of classes in MNIST (10, since there are the digits from 0 to 9)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnist_shape = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">n_classes = <span class="number">10</span></span><br></pre></td></tr></table></figure><p>And you also include the same parameters from previous assignments:</p><ul><li>criterion: the loss function</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>device: the device type</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">n_epochs = <span class="number">200</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">'.'</span>, download=<span class="keyword">False</span>, transform=transform),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models. For the generator, you will need to calculate the size of the input vector; recall that for conditional GANs, the generator’s input is the noise vector concatenated with the class vector. For the discriminator, you need to add a channel for every class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_input_dimensions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_input_dimensions</span><span class="params">(z_dim, mnist_shape, n_classes)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for getting the size of the conditional input dimensions </span></span><br><span class="line"><span class="string">    from z_dim, the image shape, and number of classes.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)</span></span><br><span class="line"><span class="string">        n_classes: the total number of classes in the dataset, an integer scalar</span></span><br><span class="line"><span class="string">                (10 for MNIST)</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        generator_input_dim: the input dimensionality of the conditional generator, </span></span><br><span class="line"><span class="string">                          which takes the noise and class vectors</span></span><br><span class="line"><span class="string">        discriminator_im_chan: the number of input channels to the discriminator</span></span><br><span class="line"><span class="string">                            (e.g. C x 28 x 28 for MNIST)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    generator_input_dim = z_dim + n_classes</span><br><span class="line">    discriminator_im_chan = mnist_shape[<span class="number">0</span>] + n_classes</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> generator_input_dim, discriminator_im_chan</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_input_dims</span><span class="params">()</span>:</span></span><br><span class="line">    gen_dim, disc_dim = get_input_dimensions(<span class="number">23</span>, (<span class="number">12</span>, <span class="number">23</span>, <span class="number">52</span>), <span class="number">9</span>)</span><br><span class="line">    <span class="keyword">assert</span> gen_dim == <span class="number">32</span></span><br><span class="line">    <span class="keyword">assert</span> disc_dim == <span class="number">21</span></span><br><span class="line">test_input_dims()</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)</span><br><span class="line"></span><br><span class="line">gen = Generator(input_dim=generator_input_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">disc = Discriminator(im_chan=discriminator_im_chan).to(device)</span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">gen = gen.apply(weights_init)</span><br><span class="line">disc = disc.apply(weights_init)</span><br></pre></td></tr></table></figure><p>Now to train, you would like both your generator and your discriminator to know what class of image should be generated. There are a few locations where you will need to implement code.</p><p>For example, if you’re generating a picture of the number “1”, you would need to:</p><ol><li>Tell that to the generator, so that it knows it should be generating a “1”</li><li>Tell that to the discriminator, so that it knows it should be looking at a “1”. If the discriminator is told it should be looking at a 1 but sees something that’s clearly an 8, it can guess that it’s probably fake</li></ol><p>There are no explicit unit tests here — if this block of code runs and you don’t change any of the other variables, then you’ve done it correctly!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL</span></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">generator_losses = []</span><br><span class="line">discriminator_losses = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#UNIT TEST <span class="doctag">NOTE:</span> Initializations needed for grading</span></span><br><span class="line">noise_and_labels = <span class="keyword">False</span></span><br><span class="line">fake = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">fake_image_and_labels = <span class="keyword">False</span></span><br><span class="line">real_image_and_labels = <span class="keyword">False</span></span><br><span class="line">disc_fake_pred = <span class="keyword">False</span></span><br><span class="line">disc_real_pred = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># Dataloader returns the batches and the labels</span></span><br><span class="line">    <span class="keyword">for</span> real, labels <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line">        <span class="comment"># Flatten the batch of real images from the dataset</span></span><br><span class="line">        real = real.to(device)</span><br><span class="line"></span><br><span class="line">        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)</span><br><span class="line">        image_one_hot_labels = one_hot_labels[:, :, <span class="keyword">None</span>, <span class="keyword">None</span>]</span><br><span class="line">        image_one_hot_labels = image_one_hot_labels.repeat(<span class="number">1</span>, <span class="number">1</span>, mnist_shape[<span class="number">1</span>], mnist_shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the discriminator gradients</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line">        <span class="comment"># Get noise corresponding to the current batch_size </span></span><br><span class="line">        fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Now you can get the images from the generator</span></span><br><span class="line">        <span class="comment"># Steps: 1) Combine the noise vectors and the one-hot labels for the generator</span></span><br><span class="line">        <span class="comment">#        2) Generate the conditioned fake images</span></span><br><span class="line">       </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)</span><br><span class="line">        fake = gen(noise_and_labels)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Make sure that enough images were generated</span></span><br><span class="line">        <span class="keyword">assert</span> len(fake) == len(real)</span><br><span class="line">        <span class="comment"># Check that correct tensors were combined</span></span><br><span class="line">        <span class="keyword">assert</span> tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[<span class="number">1</span>] + one_hot_labels.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># It comes from the correct generator</span></span><br><span class="line">        <span class="keyword">assert</span> tuple(fake.shape) == (len(real), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Now you can get the predictions from the discriminator</span></span><br><span class="line">        <span class="comment"># Steps: 1) Create the input for the discriminator</span></span><br><span class="line">        <span class="comment">#           a) Combine the fake images with image_one_hot_labels, </span></span><br><span class="line">        <span class="comment">#              remember to detach the generator (.detach()) so you do not backpropagate through it</span></span><br><span class="line">        <span class="comment">#           b) Combine the real images with image_one_hot_labels</span></span><br><span class="line">        <span class="comment">#        2) Get the discriminator's prediction on the fakes as disc_fake_pred</span></span><br><span class="line">        <span class="comment">#        3) Get the discriminator's prediction on the reals as disc_real_pred</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        fake_image_and_labels = combine_vectors(fake,image_one_hot_labels)</span><br><span class="line">        real_image_and_labels = combine_vectors(real,image_one_hot_labels)</span><br><span class="line">        disc_fake_pred = disc(fake_image_and_labels)</span><br><span class="line">        disc_real_pred = disc(real_image_and_labels)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Make sure shapes are correct </span></span><br><span class="line">        <span class="keyword">assert</span> tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[<span class="number">1</span>] + image_one_hot_labels.shape[<span class="number">1</span>], <span class="number">28</span> ,<span class="number">28</span>)</span><br><span class="line">        <span class="keyword">assert</span> tuple(real_image_and_labels.shape) == (len(real), real.shape[<span class="number">1</span>] + image_one_hot_labels.shape[<span class="number">1</span>], <span class="number">28</span> ,<span class="number">28</span>)</span><br><span class="line">        <span class="comment"># Make sure that enough predictions were made</span></span><br><span class="line">        <span class="keyword">assert</span> len(disc_real_pred) == len(real)</span><br><span class="line">        <span class="comment"># Make sure that the inputs are different</span></span><br><span class="line">        <span class="keyword">assert</span> torch.any(fake_image_and_labels != real_image_and_labels)</span><br><span class="line">        <span class="comment"># Shapes must match</span></span><br><span class="line">        <span class="keyword">assert</span> tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape)</span><br><span class="line">        <span class="keyword">assert</span> tuple(disc_fake_pred.shape) == tuple(disc_real_pred.shape)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span><br><span class="line">        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span><br><span class="line">        disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">        disc_opt.step() </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        discriminator_losses += [disc_loss.item()]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update generator ###</span></span><br><span class="line">        <span class="comment"># Zero out the generator gradients</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)</span><br><span class="line">        <span class="comment"># This will error if you didn't concatenate your labels to your image correctly</span></span><br><span class="line">        disc_fake_pred = disc(fake_image_and_labels)</span><br><span class="line">        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span><br><span class="line">        gen_loss.backward()</span><br><span class="line">        gen_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the generator losses</span></span><br><span class="line">        generator_losses += [gen_loss.item()]</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            gen_mean = sum(generator_losses[-display_step:]) / display_step</span><br><span class="line">            disc_mean = sum(discriminator_losses[-display_step:]) / display_step</span><br><span class="line">            print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;gen_mean&#125;</span>, discriminator loss: <span class="subst">&#123;disc_mean&#125;</span>"</span>)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            step_bins = <span class="number">20</span></span><br><span class="line">            x_axis = sorted([i * step_bins <span class="keyword">for</span> i <span class="keyword">in</span> range(len(generator_losses) // step_bins)] * step_bins)</span><br><span class="line">            num_examples = (len(generator_losses) // step_bins) * step_bins</span><br><span class="line">            plt.plot(</span><br><span class="line">                range(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(generator_losses[:num_examples]).view(<span class="number">-1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">"Generator Loss"</span></span><br><span class="line">            )</span><br><span class="line">            plt.plot(</span><br><span class="line">                range(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(discriminator_losses[:num_examples]).view(<span class="number">-1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">"Discriminator Loss"</span></span><br><span class="line">            )</span><br><span class="line">            plt.legend()</span><br><span class="line">            plt.show()</span><br><span class="line">        <span class="keyword">elif</span> cur_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!"</span>)</span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h2><p>You can do a bit of exploration now!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Before you explore, you should put the generator</span></span><br><span class="line"><span class="comment"># in eval mode, both in general and so that batch norm</span></span><br><span class="line"><span class="comment"># doesn't cause you issues and is using its eval statistics</span></span><br><span class="line">gen = gen.eval()</span><br></pre></td></tr></table></figure><h4 id="Changing-the-Class-Vector"><a href="#Changing-the-Class-Vector" class="headerlink" title="Changing the Class Vector"></a>Changing the Class Vector</h4><p>You can generate some numbers with your new model! You can add interpolation as well to make it more interesting.</p><p>So starting from a image, you will produce intermediate images that look more and more like the ending image until you get to the final image. Your’re basically morphing one image into another. You can choose what these two images will be using your conditional GAN.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">n_interpolation = <span class="number">9</span> <span class="comment"># Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)</span></span><br><span class="line">interpolation_noise = get_noise(<span class="number">1</span>, z_dim, device=device).repeat(n_interpolation, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interpolate_class</span><span class="params">(first_number, second_number)</span>:</span></span><br><span class="line">    first_label = get_one_hot_labels(torch.Tensor([first_number]).long(), n_classes)</span><br><span class="line">    second_label = get_one_hot_labels(torch.Tensor([second_number]).long(), n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the interpolation vector between the two labels</span></span><br><span class="line">    percent_second_label = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, n_interpolation)[:, <span class="keyword">None</span>]</span><br><span class="line">    interpolation_labels = first_label * (<span class="number">1</span> - percent_second_label) + second_label * percent_second_label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Combine the noise and the labels</span></span><br><span class="line">    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))</span><br><span class="line">    fake = gen(noise_and_labels)</span><br><span class="line">    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">start_plot_number = <span class="number">1</span> <span class="comment"># Choose the start digit</span></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">end_plot_number = <span class="number">5</span> <span class="comment"># Choose the end digit</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">interpolate_class(start_plot_number, end_plot_number)</span><br><span class="line">_ = plt.axis(<span class="string">'off'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Uncomment the following lines of code if you would like to visualize a set of pairwise class </span></span><br><span class="line"><span class="comment">### interpolations for a collection of different numbers, all in a single grid of interpolations.</span></span><br><span class="line"><span class="comment">### You'll also see another visualization like this in the next code block!</span></span><br><span class="line"><span class="comment"># plot_numbers = [2, 3, 4, 5, 7]</span></span><br><span class="line"><span class="comment"># n_numbers = len(plot_numbers)</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(8, 8))</span></span><br><span class="line"><span class="comment"># for i, first_plot_number in enumerate(plot_numbers):</span></span><br><span class="line"><span class="comment">#     for j, second_plot_number in enumerate(plot_numbers):</span></span><br><span class="line"><span class="comment">#         plt.subplot(n_numbers, n_numbers, i * n_numbers + j + 1)</span></span><br><span class="line"><span class="comment">#         interpolate_class(first_plot_number, second_plot_number)</span></span><br><span class="line"><span class="comment">#         plt.axis('off')</span></span><br><span class="line"><span class="comment"># plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br></pre></td></tr></table></figure><p><img src="output_28_0.png" alt="png"></p><h4 id="Changing-the-Noise-Vector"><a href="#Changing-the-Noise-Vector" class="headerlink" title="Changing the Noise Vector"></a>Changing the Noise Vector</h4><p>Now, what happens if you hold the class constant, but instead you change the noise vector? You can also interpolate the noise vector and generate an image at each step.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">n_interpolation = <span class="number">9</span> <span class="comment"># How many intermediate images you want + 2 (for the start and end image)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This time you're interpolating between the noise instead of the labels</span></span><br><span class="line">interpolation_label = get_one_hot_labels(torch.Tensor([<span class="number">5</span>]).long(), n_classes).repeat(n_interpolation, <span class="number">1</span>).float()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interpolate_noise</span><span class="params">(first_noise, second_noise)</span>:</span></span><br><span class="line">    <span class="comment"># This time you're interpolating between the noise instead of the labels</span></span><br><span class="line">    percent_first_noise = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, n_interpolation)[:, <span class="keyword">None</span>].to(device)</span><br><span class="line">    interpolation_noise = first_noise * percent_first_noise + second_noise * (<span class="number">1</span> - percent_first_noise)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Combine the noise and the labels again</span></span><br><span class="line">    noise_and_labels = combine_vectors(interpolation_noise, interpolation_label.to(device))</span><br><span class="line">    fake = gen(noise_and_labels)</span><br><span class="line">    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate noise vectors to interpolate between</span></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">n_noise = <span class="number">5</span> <span class="comment"># Choose the number of noise examples in the grid</span></span><br><span class="line">plot_noises = [get_noise(<span class="number">1</span>, z_dim, device=device) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_noise)]</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i, first_plot_noise <span class="keyword">in</span> enumerate(plot_noises):</span><br><span class="line">    <span class="keyword">for</span> j, second_plot_noise <span class="keyword">in</span> enumerate(plot_noises):</span><br><span class="line">        plt.subplot(n_noise, n_noise, i * n_noise + j + <span class="number">1</span>)</span><br><span class="line">        interpolate_noise(first_plot_noise, second_plot_noise)</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplots_adjust(top=<span class="number">1</span>, bottom=<span class="number">0</span>, left=<span class="number">0</span>, right=<span class="number">1</span>, hspace=<span class="number">0.1</span>, wspace=<span class="number">0</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure><p><img src="output_30_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Build-a-Conditional-GAN&quot;&gt;&lt;a href=&quot;#Build-a-Conditional-GAN&quot; class=&quot;headerlink&quot; title=&quot;Build a Conditional GAN&quot;&gt;&lt;/a&gt;Build a Condition
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Wasserstein GAN with Gradient Penalty (WGAN-GP)</title>
    <link href="https://zhangruochi.com/Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP/2020/10/09/"/>
    <id>https://zhangruochi.com/Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP/2020/10/09/</id>
    <published>2020-10-09T11:11:56.000Z</published>
    <updated>2020-10-13T03:21:24.983Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP"><a href="#Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP" class="headerlink" title="Wasserstein GAN with Gradient Penalty (WGAN-GP)"></a>Wasserstein GAN with Gradient Penalty (WGAN-GP)</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to build a Wasserstein GAN with Gradient Penalty (WGAN-GP) that solves some of the stability issues with the GANs that you have been using up until this point. Specifically, you’ll use a special kind of loss function known as the W-loss, where W stands for Wasserstein, and gradient penalties to prevent mode collapse.</p><p><em>Fun Fact: Wasserstein is named after a mathematician at Penn State, Leonid Vaseršteĭn. You’ll see it abbreviated to W (e.g. WGAN, W-loss, W-distance).</em></p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Get hands-on experience building a more stable GAN: Wasserstein GAN with Gradient Penalty (WGAN-GP).</li><li>Train the more advanced WGAN-GP model.</li></ol><h2 id="Generator-and-Critic"><a href="#Generator-and-Critic" class="headerlink" title="Generator and Critic"></a>Generator and Critic</h2><p>You will begin by importing some useful packages, defining visualization functions, building the generator, and building the critic. Since the changes for WGAN-GP are done to the loss function during training, you can simply reuse your previous GAN code for the generator and critic class. Remember that in WGAN-GP, you no longer use a discriminator that classifies fake and real as 0 and 1 but rather a critic that scores images with real numbers.</p><h4 id="Packages-and-Visualizations"><a href="#Packages-and-Visualizations" class="headerlink" title="Packages and Visualizations"></a>Packages and Visualizations</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu()</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_grad_hook</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function to keep track of gradients for visualization purposes, </span></span><br><span class="line"><span class="string">    which fills the grads list when using model.apply(grad_hook).</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    grads = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">grad_hook</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">            grads.append(m.weight.grad)</span><br><span class="line">    <span class="keyword">return</span> grads, grad_hook</span><br></pre></td></tr></table></figure><h4 id="Generator-and-Noise"><a href="#Generator-and-Noise" class="headerlink" title="Generator and Noise"></a>Generator and Noise</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor,</span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = noise.view(len(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">      n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">      z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">      device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure><h4 id="Critic"><a href="#Critic" class="headerlink" title="Critic"></a>Critic</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Critic Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Critic, self).__init__()</span><br><span class="line">        self.crit = nn.Sequential(</span><br><span class="line">            self.make_crit_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_crit_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_crit_block(hidden_dim * <span class="number">2</span>, <span class="number">1</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_crit_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a critic block of DCGAN;</span></span><br><span class="line"><span class="string">        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the critic: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_chan)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        crit_pred = self.crit(image)</span><br><span class="line">        <span class="keyword">return</span> crit_pred.view(len(crit_pred), <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Training-Initializations"><a href="#Training-Initializations" class="headerlink" title="Training Initializations"></a>Training Initializations</h2><p>Now you can start putting it all together.<br>As usual, you will start by setting the parameters:</p><ul><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>beta_1, beta_2: the momentum terms</li><li>c_lambda: weight of the gradient penalty</li><li>crit_repeats: number of times to update the critic per generator update - there are more details about this in the <em>Putting It All Together</em> section</li><li>device: the device type</li></ul><p>You will also load and transform the MNIST dataset to tensors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">100</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">50</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">beta_1 = <span class="number">0.5</span></span><br><span class="line">beta_2 = <span class="number">0.999</span></span><br><span class="line">c_lambda = <span class="number">10</span></span><br><span class="line">crit_repeats = <span class="number">5</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">'.'</span>, download=<span class="keyword">False</span>, transform=transform),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Then, you can initialize your generator, critic, and optimizers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line">crit = Critic().to(device) </span><br><span class="line">crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">gen = gen.apply(weights_init)</span><br><span class="line">crit = crit.apply(weights_init)</span><br></pre></td></tr></table></figure><h2 id="Gradient-Penalty"><a href="#Gradient-Penalty" class="headerlink" title="Gradient Penalty"></a>Gradient Penalty</h2><p>Calculating the gradient penalty can be broken into two functions: (1) compute the gradient with respect to the images and (2) compute the gradient penalty given the gradient.</p><p>You can start by getting the gradient. The gradient is computed by first creating a mixed image. This is done by weighing the fake and real image using epsilon and then adding them together. Once you have the intermediate image, you can get the critic’s output on the image. Finally, you compute the gradient of the critic score’s on the mixed images (output) with respect to the pixels of the mixed images (input). You will need to fill in the code to get the gradient wherever you see <em>None</em>. There is a test function in the next block for you to test your solution.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gradient</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gradient</span><span class="params">(crit, real, fake, epsilon)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the gradient of the critic's scores with respect to mixes of real and fake images.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        crit: the critic model</span></span><br><span class="line"><span class="string">        real: a batch of real images</span></span><br><span class="line"><span class="string">        fake: a batch of fake images</span></span><br><span class="line"><span class="string">        epsilon: a vector of the uniformly random proportions of real/fake per mixed image</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        gradient: the gradient of the critic's scores, with respect to the mixed image</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Mix the images together</span></span><br><span class="line">    mixed_images = real * epsilon + fake * (<span class="number">1</span> - epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the critic's scores on the mixed images</span></span><br><span class="line">    mixed_scores = crit(mixed_images)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Take the gradient of the scores with respect to the images</span></span><br><span class="line">    gradient = torch.autograd.grad(</span><br><span class="line">        <span class="comment"># Note: You need to take the gradient of outputs with respect to inputs.</span></span><br><span class="line">        <span class="comment"># This documentation may be useful, but it should not be necessary:</span></span><br><span class="line">        <span class="comment"># https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        inputs=mixed_images,</span><br><span class="line">        outputs=mixed_scores,</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        <span class="comment"># These other parameters have to do with the pytorch autograd engine works</span></span><br><span class="line">        grad_outputs=torch.ones_like(mixed_scores), </span><br><span class="line">        create_graph=<span class="keyword">True</span>,</span><br><span class="line">        retain_graph=<span class="keyword">True</span>,</span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> gradient</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="comment"># DO NOT MODIFY THIS</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_get_gradient</span><span class="params">(image_shape)</span>:</span></span><br><span class="line">    real = torch.randn(*image_shape, device=device) + <span class="number">1</span></span><br><span class="line">    fake = torch.randn(*image_shape, device=device) - <span class="number">1</span></span><br><span class="line">    epsilon_shape = [<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> image_shape]</span><br><span class="line">    epsilon_shape[<span class="number">0</span>] = image_shape[<span class="number">0</span>]</span><br><span class="line">    epsilon = torch.rand(epsilon_shape, device=device).requires_grad_()</span><br><span class="line">    gradient = get_gradient(crit, real, fake, epsilon)</span><br><span class="line">    <span class="keyword">assert</span> tuple(gradient.shape) == image_shape</span><br><span class="line">    <span class="keyword">assert</span> gradient.max() &gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> gradient.min() &lt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> gradient</span><br><span class="line"></span><br><span class="line">gradient = test_get_gradient((<span class="number">256</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>The second function you need to complete is to compute the gradient penalty given the gradient. First, you calculate the magnitude of each image’s gradient. The magnitude of a gradient is also called the norm. Then, you calculate the penalty by squaring the distance between each magnitude and the ideal norm of 1 and taking the mean of all the squared distances.</p><p>Again, you will need to fill in the code wherever you see <em>None</em>. There are hints below that you can view if you need help and there is a test function in the next block for you to test your solution.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">gradient_penalty</font></code></b></font></summary>1.   Make sure you take the mean at the end.2.   Note that the magnitude of each gradient has already been calculated for you.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: gradient_penalty</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_penalty</span><span class="params">(gradient)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the gradient penalty, given a gradient.</span></span><br><span class="line"><span class="string">    Given a batch of image gradients, you calculate the magnitude of each image's gradient</span></span><br><span class="line"><span class="string">    and penalize the mean quadratic distance of each magnitude to 1.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gradient: the gradient of the critic's scores, with respect to the mixed image</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        penalty: the gradient penalty</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Flatten the gradients so that each row captures one image</span></span><br><span class="line">    gradient = gradient.view(len(gradient), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the magnitude of every row</span></span><br><span class="line">    gradient_norm = gradient.norm(<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Penalize the mean squared distance of the gradient norms from 1</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    penalty = torch.mean((gradient_norm - <span class="number">1</span>)**<span class="number">2</span>)</span><br><span class="line"><span class="comment">#     print(penalty)</span></span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> penalty</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gradient_penalty</span><span class="params">(image_shape)</span>:</span></span><br><span class="line">    bad_gradient = torch.zeros(*image_shape)</span><br><span class="line">    bad_gradient_penalty = gradient_penalty(bad_gradient)</span><br><span class="line">    <span class="keyword">assert</span> torch.isclose(bad_gradient_penalty, torch.tensor(<span class="number">1.</span>))</span><br><span class="line"></span><br><span class="line">    image_size = torch.prod(torch.Tensor(image_shape[<span class="number">1</span>:]))</span><br><span class="line">    good_gradient = torch.ones(*image_shape) / torch.sqrt(image_size)</span><br><span class="line">    good_gradient_penalty = gradient_penalty(good_gradient)</span><br><span class="line">    <span class="keyword">assert</span> torch.isclose(good_gradient_penalty, torch.tensor(<span class="number">0.</span>))</span><br><span class="line"></span><br><span class="line">    random_gradient = test_get_gradient(image_shape)</span><br><span class="line">    random_gradient_penalty = gradient_penalty(random_gradient)</span><br><span class="line">    <span class="keyword">assert</span> torch.abs(random_gradient_penalty - <span class="number">1</span>) &lt; <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">test_gradient_penalty((<span class="number">256</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Losses"><a href="#Losses" class="headerlink" title="Losses"></a>Losses</h2><p>Next, you need to calculate the loss for the generator and the critic.</p><p>For the generator, the loss is calculated by maximizing the critic’s prediction on the generator’s fake images. The argument has the scores for all fake images in the batch, but you will use the mean of them.</p><p>There are optional hints below and a test function in the next block for you to test your solution.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">get_gen_loss</font></code></b></font></summary>1. This can be written in one line.2. This is the negative of the mean of the critic's scores.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span><span class="params">(crit_fake_pred)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of a generator given the critic's scores of the generator's fake images.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        crit_fake_pred: the critic's scores of the fake images</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        gen_loss: a scalar loss value for the current batch of the generator</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    gen_loss = -torch.mean(crit_fake_pred)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    get_gen_loss(torch.tensor(<span class="number">1.</span>)), torch.tensor(<span class="number">-1.0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    get_gen_loss(torch.rand(<span class="number">10000</span>)), torch.tensor(<span class="number">-0.5</span>), <span class="number">0.05</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>For the critic, the loss is calculated by maximizing the distance between the critic’s predictions on the real images and the predictions on the fake images while also adding a gradient penalty. The gradient penalty is weighed according to lambda. The arguments are the scores for all the images in the batch, and you will use the mean of them.</p><p>There are hints below if you get stuck and a test function in the next block for you to test your solution.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">get_crit_loss</font></code></b></font></summary>1. The higher the mean fake score, the higher the critic's loss is.2. What does this suggest about the mean real score?3. The higher the gradient penalty, the higher the critic's loss is, proportional to lambda.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_crit_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_crit_loss</span><span class="params">(crit_fake_pred, crit_real_pred, gp, c_lambda)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of a critic given the critic's scores for fake and real images,</span></span><br><span class="line"><span class="string">    the gradient penalty, and gradient penalty weight.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        crit_fake_pred: the critic's scores of the fake images</span></span><br><span class="line"><span class="string">        crit_real_pred: the critic's scores of the real images</span></span><br><span class="line"><span class="string">        gp: the unweighted gradient penalty</span></span><br><span class="line"><span class="string">        c_lambda: the current weight of the gradient penalty </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        crit_loss: a scalar for the critic's loss, accounting for the relevant factors</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    crit_loss =  torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> crit_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    get_crit_loss(torch.tensor(<span class="number">1.</span>), torch.tensor(<span class="number">2.</span>), torch.tensor(<span class="number">3.</span>), <span class="number">0.1</span>),</span><br><span class="line">    torch.tensor(<span class="number">-0.7</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    get_crit_loss(torch.tensor(<span class="number">20.</span>), torch.tensor(<span class="number">-20.</span>), torch.tensor(<span class="number">2.</span>), <span class="number">10</span>),</span><br><span class="line">    torch.tensor(<span class="number">60.</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Putting-It-All-Together"><a href="#Putting-It-All-Together" class="headerlink" title="Putting It All Together"></a>Putting It All Together</h2><p>Before you put everything together, there are a few things to note.</p><ol><li>Even on GPU, the <strong>training will run more slowly</strong> than previous labs because the gradient penalty requires you to compute the gradient of a gradient — this means potentially a few minutes per epoch! For best results, run this for as long as you can while on GPU.</li><li>One important difference from earlier versions is that you will <strong>update the critic multiple times</strong> every time you update the generator This helps prevent the generator from overpowering the critic. Sometimes, you might see the reverse, with the generator updated more times than the critic. This depends on architectural (e.g. the depth and width of the network) and algorithmic choices (e.g. which loss you’re using). </li><li>WGAN-GP isn’t necessarily meant to improve overall performance of a GAN, but just <strong>increases stability</strong> and avoids mode collapse. In general, a WGAN will be able to train in a much more stable way than the vanilla DCGAN from last assignment, though it will generally run a bit slower. You should also be able to train your model for more epochs without it collapsing.</li></ol><!-- Once again, be warned that this runs very slowly on a CPU. One way to run this more quickly is to download the .ipynb and upload it to Google Drive, then open it with Google Colab and make the runtime type GPU and replace`device = "cpu"`with`device = "cuda"`and make sure that your `get_noise` function uses the right device.  --><p>Here is a snapshot of what your WGAN-GP outputs should resemble:<br><img src="MNIST_WGAN_Progression.png" alt="MNIST Digits Progression"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">generator_losses = []</span><br><span class="line">critic_losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line">        real = real.to(device)</span><br><span class="line"></span><br><span class="line">        mean_iteration_critic_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(crit_repeats):</span><br><span class="line">            <span class="comment">### Update critic ###</span></span><br><span class="line">            crit_opt.zero_grad()</span><br><span class="line">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">            fake = gen(fake_noise)</span><br><span class="line">            crit_fake_pred = crit(fake.detach())</span><br><span class="line">            crit_real_pred = crit(real)</span><br><span class="line"></span><br><span class="line">            epsilon = torch.rand(len(real), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, device=device, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">            gradient = get_gradient(crit, real, fake.detach(), epsilon)</span><br><span class="line">            gp = gradient_penalty(gradient)</span><br><span class="line">            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Keep track of the average critic loss in this batch</span></span><br><span class="line">            mean_iteration_critic_loss += crit_loss.item() / crit_repeats</span><br><span class="line">            <span class="comment"># Update gradients</span></span><br><span class="line">            crit_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">            <span class="comment"># Update optimizer</span></span><br><span class="line">            crit_opt.step()</span><br><span class="line">        critic_losses += [mean_iteration_critic_loss]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update generator ###</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line">        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        fake_2 = gen(fake_noise_2)</span><br><span class="line">        crit_fake_pred = crit(fake_2)</span><br><span class="line">        </span><br><span class="line">        gen_loss = get_gen_loss(crit_fake_pred)</span><br><span class="line">        gen_loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update the weights</span></span><br><span class="line">        gen_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">        generator_losses += [gen_loss.item()]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Visualization code ###</span></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            gen_mean = sum(generator_losses[-display_step:]) / display_step</span><br><span class="line">            crit_mean = sum(critic_losses[-display_step:]) / display_step</span><br><span class="line">            print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;gen_mean&#125;</span>, critic loss: <span class="subst">&#123;crit_mean&#125;</span>"</span>)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            step_bins = <span class="number">20</span></span><br><span class="line">            num_examples = (len(generator_losses) // step_bins) * step_bins</span><br><span class="line">            plt.plot(</span><br><span class="line">                range(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(generator_losses[:num_examples]).view(<span class="number">-1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">"Generator Loss"</span></span><br><span class="line">            )</span><br><span class="line">            plt.plot(</span><br><span class="line">                range(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(critic_losses[:num_examples]).view(<span class="number">-1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">"Critic Loss"</span></span><br><span class="line">            )</span><br><span class="line">            plt.legend()</span><br><span class="line">            plt.show()</span><br><span class="line"></span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP&quot;&gt;&lt;a href=&quot;#Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP&quot; class=&quot;headerlink&quot; title=&quot;Wa
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Deep Convolutional GAN (DCGAN)</title>
    <link href="https://zhangruochi.com/Deep-Convolutional-GAN-DCGAN/2020/10/09/"/>
    <id>https://zhangruochi.com/Deep-Convolutional-GAN-DCGAN/2020/10/09/</id>
    <published>2020-10-09T08:14:47.000Z</published>
    <updated>2020-10-13T03:21:21.414Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Deep-Convolutional-GAN-DCGAN"><a href="#Deep-Convolutional-GAN-DCGAN" class="headerlink" title="Deep Convolutional GAN (DCGAN)"></a>Deep Convolutional GAN (DCGAN)</h1><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>In this notebook, you’re going to create another GAN using the MNIST dataset. You will implement a Deep Convolutional GAN (DCGAN), a very successful and influential GAN model developed in 2015.</p><p><em>Note: <a href="https://arxiv.org/pdf/1511.06434v1.pdf" target="_blank" rel="noopener">here</a> is the paper if you are interested! It might look dense now, but soon you’ll be able to understand many parts of it :)</em></p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Get hands-on experience making a widely used GAN: Deep Convolutional GAN (DCGAN).</li><li>Train a powerful generative model.</li></ol><p><img src="dcgan-gen.png" alt="Generator architecture"></p><p>Figure: Architectural drawing of a generator from DCGAN from <a href="https://arxiv.org/pdf/1511.06434v1.pdf" target="_blank" rel="noopener">Radford et al (2016)</a>.</p><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h4 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h4><p>Here are the main features of DCGAN (don’t worry about memorizing these, you will be guided through the implementation!): </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Architecture guidelines for stable Deep Convolutional GANs</span><br><span class="line">• Replace any pooling layers with strided convolutions (discriminator) and fractional-strided</span><br><span class="line">convolutions (generator).</span><br><span class="line">• Use BatchNorm in both the generator and the discriminator.</span><br><span class="line">• Remove fully connected hidden layers for deeper architectures.</span><br><span class="line">• Use ReLU activation in generator for all layers except for the output, which uses Tanh.</span><br><span class="line">• Use LeakyReLU activation in the discriminator for all layers.</span><br></pre></td></tr></table></figure><ul><li>Use convolutions without any pooling layers</li><li>Use batchnorm in both the generator and the discriminator</li><li>Don’t use fully connected hidden layers</li><li>Use ReLU activation in the generator for all layers except for the output, which uses a Tanh activation.</li><li>Use LeakyReLU activation in the discriminator for all layers except for the output, which does not use an activation</li></ul><p>You will begin by importing some useful packages and data that will help you create your GAN. You are also provided a visualizer function to help see the images your GAN will create.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu()</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>The first component you will make is the generator. You may notice that instead of passing in the image dimension, you will pass the number of image channels to the generator. This is because with DCGAN, you use convolutions which don’t depend on the number of pixels on an image. However, the number of channels is important to determine the size of the filters.</p><p>You will build a generator using 4 layers (3 hidden layers + 1 output layer). As before, you will need to write a function to create a single block for the generator’s neural network.<br><!-- From the paper, we know to "[u]se batchnorm in both the generator and the discriminator" and "[u]se ReLU activation in generator for all layers except for the output, which uses Tanh." --><br>Since in DCGAN the activation function will be different for the output layer, you will need to check what layer is being created. You are supplied with some tests following the code cell so you can see if you’re on the right track!</p><p>At the end of the generator class, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. You are also given a function to create a noise vector. These functions are the same as the ones from the last assignment.</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">make_gen_block</font></code></b></font></summary>1. You'll find [nn.ConvTranspose2d](https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html) and [nn.BatchNorm2d](https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html) useful!</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Generator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN, </span></span><br><span class="line"><span class="string">        corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#     Steps:</span></span><br><span class="line">        <span class="comment">#       1) Do a transposed convolution using the given parameters.</span></span><br><span class="line">        <span class="comment">#       2) Do a batchnorm, except for the last layer.</span></span><br><span class="line">        <span class="comment">#       3) Follow each batchnorm with a ReLU activation.</span></span><br><span class="line">        <span class="comment">#       4) If its the final layer, use a Tanh activation after the deconvolution.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build the neural block</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace = <span class="keyword">True</span>)</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># Final Layer</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh()</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unsqueeze_noise</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns a copy of that noise with width and height = 1 and channels = z_dim.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> noise.view(len(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.unsqueeze_noise(noise)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Test your make_gen_block() function</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">gen = Generator()</span><br><span class="line">num_test = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line">test_hidden_noise = get_noise(num_test, gen.z_dim)</span><br><span class="line">test_hidden_block = gen.make_gen_block(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line">test_uns_noise = gen.unsqueeze_noise(test_hidden_noise)</span><br><span class="line">hidden_output = test_hidden_block(test_uns_noise)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that it works with other strides</span></span><br><span class="line">test_hidden_block_stride = gen.make_gen_block(<span class="number">20</span>, <span class="number">20</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">test_final_noise = get_noise(num_test, gen.z_dim) * <span class="number">20</span></span><br><span class="line">test_final_block = gen.make_gen_block(<span class="number">10</span>, <span class="number">20</span>, final_layer=<span class="keyword">True</span>)</span><br><span class="line">test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)</span><br><span class="line">final_output = test_final_block(test_final_uns_noise)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line">test_gen_noise = get_noise(num_test, gen.z_dim)</span><br><span class="line">test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)</span><br><span class="line">gen_output = gen(test_uns_gen_noise)</span><br></pre></td></tr></table></figure><p>Here’s the test for your generator block:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TESTS</span></span><br><span class="line"><span class="keyword">assert</span> tuple(hidden_output.shape) == (num_test, <span class="number">20</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">assert</span> hidden_output.max() &gt; <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.min() == <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.2</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &lt; <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(test_hidden_block_stride(hidden_output).shape) == (num_test, <span class="number">20</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> final_output.max().item() == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> final_output.min().item() == <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(gen_output.shape) == (num_test, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="keyword">assert</span> gen_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"><span class="keyword">assert</span> gen_output.std() &lt; <span class="number">0.8</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>The second component you need to create is the discriminator.</p><p>You will use 3 layers in your discriminator’s neural network. Like with the generator, you will need create the function to create a single neural network block for the discriminator.<br>From the paper, we know that we need to “[u]se LeakyReLU activation in the discriminator for all layers.” And for the LeakyReLUs, “the slope of the leak was set to 0.2” in DCGAN.<br>There are also tests at the end for you to use.</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">make_disc_block</font></code></b></font></summary>1. You'll find [nn.Conv2d](https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html), [nn.BatchNorm2d](https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html), and [nn.LeakyReLU](https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html) useful!</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">    hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">16</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            self.make_disc_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_disc_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim * <span class="number">2</span>, <span class="number">1</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_disc_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a discriminator block of DCGAN, </span></span><br><span class="line"><span class="string">        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#     Steps:</span></span><br><span class="line">        <span class="comment">#       1) Add a convolutional layer using the given parameters.</span></span><br><span class="line">        <span class="comment">#       2) Do a batchnorm, except for the last layer.</span></span><br><span class="line">        <span class="comment">#       3) Follow each batchnorm with a LeakyReLU activation with slope 0.2.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Build the neural block</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE #### #</span></span><br><span class="line">                nn.Conv2d(input_channels,output_channels,kernel_size,stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU( negative_slope=<span class="number">0.2</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">          </span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># Final Layer</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE #### #</span></span><br><span class="line">                nn.Conv2d(input_channels,output_channels,kernel_size,stride)</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        disc_pred = self.disc(image)</span><br><span class="line">        <span class="keyword">return</span> disc_pred.view(len(disc_pred), <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Test your make_disc_block() function</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">num_test = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">gen = Generator()</span><br><span class="line">disc = Discriminator()</span><br><span class="line">test_images = gen(get_noise(num_test, gen.z_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line">test_hidden_block = disc.make_disc_block(<span class="number">1</span>, <span class="number">5</span>, kernel_size=<span class="number">6</span>, stride=<span class="number">3</span>)</span><br><span class="line">hidden_output = test_hidden_block(test_images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the final block</span></span><br><span class="line">test_final_block = disc.make_disc_block(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">5</span>, final_layer=<span class="keyword">True</span>)</span><br><span class="line">final_output = test_final_block(test_images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line">disc_output = disc(test_images)</span><br></pre></td></tr></table></figure><p>Here’s a test for your discriminator block:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line"><span class="keyword">assert</span> tuple(hidden_output.shape) == (num_test, <span class="number">5</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"><span class="comment"># Because of the LeakyReLU slope</span></span><br><span class="line"><span class="keyword">assert</span> -hidden_output.min() / hidden_output.max() &gt; <span class="number">0.15</span></span><br><span class="line"><span class="keyword">assert</span> -hidden_output.min() / hidden_output.max() &lt; <span class="number">0.25</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &lt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the final block</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(final_output.shape) == (num_test, <span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">assert</span> final_output.max() &gt; <span class="number">1.0</span></span><br><span class="line"><span class="keyword">assert</span> final_output.min() &lt; <span class="number">-1.0</span></span><br><span class="line"><span class="keyword">assert</span> final_output.std() &gt; <span class="number">0.3</span></span><br><span class="line"><span class="keyword">assert</span> final_output.std() &lt; <span class="number">0.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(disc_output.shape) == (num_test, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> disc_output.std() &gt; <span class="number">0.25</span></span><br><span class="line"><span class="keyword">assert</span> disc_output.std() &lt; <span class="number">0.5</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can put it all together!<br>Remember that these are your parameters:</p><ul><li>criterion: the loss function</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>beta_1, beta_2: the momentum term</li><li>device: the device type</li></ul><p>In addition, be warned that <strong>this runs very slowly on the default CPU</strong>. One way to run this more quickly is to download the .ipynb and upload it to Google Drive, then open it with Google Colab, click on <code>Runtime -&gt; Change runtime type</code> and set hardware accelerator to GPU and replace<br><code>device = &quot;cpu&quot;</code><br>with<br><code>device = &quot;cuda&quot;</code>. The code should then run without any more changes, over 1,000 times faster. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># A learning rate of 0.0002 works well on DCGAN</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># These parameters control the optimizer's momentum, which you can read more about here:</span></span><br><span class="line"><span class="comment"># https://distill.pub/2017/momentum/ but you don’t need to worry about it for this course!</span></span><br><span class="line">beta_1 = <span class="number">0.5</span> </span><br><span class="line">beta_2 = <span class="number">0.999</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can tranform the image values to be between -1 and 1 (the range of the tanh activation)</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">'.'</span>, download=<span class="keyword">False</span>, transform=transform),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Then, you can initialize your generator, discriminator, and optimizers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line">disc = Discriminator().to(device) </span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># You initialize the weights to the normal distribution</span></span><br><span class="line"><span class="comment"># with mean 0 and standard deviation 0.02</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">gen = gen.apply(weights_init)</span><br><span class="line">disc = disc.apply(weights_init)</span><br></pre></td></tr></table></figure><p>Finally, you can train your GAN!<br>For each epoch, you will process the entire dataset in batches. For every batch, you will update the discriminator and generator. Then, you can see DCGAN’s results!</p><p>Here’s roughly the progression you should be expecting. On GPU this takes about 30 seconds per thousand steps. On CPU, this can take about 8 hours per thousand steps. You might notice that in the image of Step 5000, the generator is disproprotionately producing things that look like ones. If the discriminator didn’t learn to detect this imbalance quickly enough, then the generator could just produce more ones. As a result, it may have ended up tricking the discriminator so well that there would be no more improvement, known as mode collapse:<br><img src="MNIST_DCGAN_Progression.png" alt="MNIST Digits Progression"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">50</span></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">mean_generator_loss = <span class="number">0</span></span><br><span class="line">mean_discriminator_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line">        real = real.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Update discriminator ##</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line">        fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        fake = gen(fake_noise)</span><br><span class="line">        disc_fake_pred = disc(fake.detach())</span><br><span class="line">        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span><br><span class="line">        disc_real_pred = disc(real)</span><br><span class="line">        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span><br><span class="line">        disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        disc_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Update generator ##</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line">        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        fake_2 = gen(fake_noise_2)</span><br><span class="line">        disc_fake_pred = disc(fake_2)</span><br><span class="line">        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span><br><span class="line">        gen_loss.backward()</span><br><span class="line">        gen_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">        mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Visualization code ##</span></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            mean_generator_loss = <span class="number">0</span></span><br><span class="line">            mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Deep-Convolutional-GAN-DCGAN&quot;&gt;&lt;a href=&quot;#Deep-Convolutional-GAN-DCGAN&quot; class=&quot;headerlink&quot; title=&quot;Deep Convolutional GAN (DCGAN)&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Your First GAN</title>
    <link href="https://zhangruochi.com/Your-First-GAN/2020/10/09/"/>
    <id>https://zhangruochi.com/Your-First-GAN/2020/10/09/</id>
    <published>2020-10-09T08:09:48.000Z</published>
    <updated>2020-10-09T08:10:33.928Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Your-First-GAN"><a href="#Your-First-GAN" class="headerlink" title="Your First GAN"></a>Your First GAN</h1><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>In this notebook, you’re going to create your first generative adversarial network (GAN) for this course! Specifically, you will build and train a GAN that can generate hand-written images of digits (0-9). You will be using PyTorch in this specialization, so if you’re not familiar with this framework, you may find the <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch documentation</a> useful. The hints will also often include links to relevant documentation.</p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Build the generator and discriminator components of a GAN from scratch.</li><li>Create generator and discriminator loss functions.</li><li>Train your GAN and visualize the generated images.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will begin by importing some useful packages and the dataset you will use to build and train your GAN. You are also provided with a visualizer function to help you investigate the images your GAN will create.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST <span class="comment"># Training dataset</span></span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in a uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu().view(<span class="number">-1</span>, *size)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="MNIST-Dataset"><a href="#MNIST-Dataset" class="headerlink" title="MNIST Dataset"></a>MNIST Dataset</h4><p>The training images your discriminator will be using is from a dataset called <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a>. It contains 60,000 images of handwritten digits, from 0 to 9, like these:</p><p><img src="MnistExamples.png" alt="MNIST Digits"></p><p>You may notice that the images are quite pixelated — this is because they are all only 28 x 28! The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or “color channel”, is needed to represent them (more on this later in the course).</p><h4 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h4><p>You will represent the data using <a href="https://pytorch.org/docs/stable/tensors.html" target="_blank" rel="noopener">tensors</a>. Tensors are a generalization of matrices: for example, a stack of three matrices with the amounts of red, green, and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64.</p><p>Tensors are easy to manipulate and supported by <a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch</a>, the machine learning library you will be using. Feel free to explore them more, but you can imagine these as multi-dimensional matrices or vectors!</p><h4 id="Batches"><a href="#Batches" class="headerlink" title="Batches"></a>Batches</h4><p>While you could train your model after generating one image, it is extremely inefficient and leads to less stable training. In GANs, and in machine learning in general, you will process multiple images per training step. These are called batches.</p><p>This means that your generator will generate an entire batch of images and receive the discriminator’s feedback on each before updating the model. The same goes for the discriminator, it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated.</p><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>The first step is to build the generator component.</p><p>You will start by creating a function to make a single layer/block for the generator’s neural network. Each block should include a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" target="_blank" rel="noopener">linear transformation</a> to map to another shape, a <a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html" target="_blank" rel="noopener">batch normalization</a> for stabilization, and finally a non-linear activation function (you use a <a href="https://pytorch.org/docs/master/generated/torch.nn.ReLU.html" target="_blank" rel="noopener">ReLU here</a>) so the output can be transformed in complex ways. You will learn more about activations and batch normalization later in the course.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_generator_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_generator_block</span><span class="params">(input_dim, output_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for returning a block of the generator's neural network</span></span><br><span class="line"><span class="string">    given input and output dimensions.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        a generator neural network layer, with a linear transformation </span></span><br><span class="line"><span class="string">          followed by a batch normalization and then a relu activation</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment"># Hint: Replace all of the "None" with the appropriate dimensions.</span></span><br><span class="line">        <span class="comment"># The documentation may be useful if you're less familiar with PyTorch:</span></span><br><span class="line">        <span class="comment"># https://pytorch.org/docs/stable/nn.html.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        nn.Linear(input_dim, output_dim),</span><br><span class="line">        nn.BatchNorm1d(output_dim),</span><br><span class="line">        nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the generator block function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_block</span><span class="params">(in_features, out_features, num_test=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    block = get_generator_block(in_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check the three parts</span></span><br><span class="line">    <span class="keyword">assert</span> len(block) == <span class="number">3</span></span><br><span class="line">    <span class="keyword">assert</span> type(block[<span class="number">0</span>]) == nn.Linear</span><br><span class="line">    <span class="keyword">assert</span> type(block[<span class="number">1</span>]) == nn.BatchNorm1d</span><br><span class="line">    <span class="keyword">assert</span> type(block[<span class="number">2</span>]) == nn.ReLU</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check the output shape</span></span><br><span class="line">    test_input = torch.randn(num_test, in_features)</span><br><span class="line">    test_output = block(test_input)</span><br><span class="line">    <span class="keyword">assert</span> tuple(test_output.shape) == (num_test, out_features)</span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.55</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.65</span></span><br><span class="line"></span><br><span class="line">test_gen_block(<span class="number">25</span>, <span class="number">12</span>)</span><br><span class="line">test_gen_block(<span class="number">15</span>, <span class="number">28</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Now you can build the generator class. It will take 3 values:</p><ul><li>The noise vector dimension</li><li>The image dimension</li><li>The initial hidden dimension</li></ul><p>Using these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a <a href="https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html" target="_blank" rel="noopener">sigmoid function</a>. </p><p>Finally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">Generator</font></code></b></font></summary>1. The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function.2. [nn.Linear](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) and [nn.Sigmoid](https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html) will be useful here. </details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Generator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">          (MNIST images are 28 x 28 = 784 so that is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_dim=<span class="number">784</span>, hidden_dim=<span class="number">128</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            get_generator_block(z_dim, hidden_dim),</span><br><span class="line">            get_generator_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            get_generator_block(hidden_dim * <span class="number">2</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            get_generator_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">8</span>),</span><br><span class="line">            <span class="comment"># There is a dropdown with hints if you need them! </span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            nn.Linear(hidden_dim * <span class="number">8</span>, im_dim),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.gen(noise)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Needed for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_gen</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the sequential model</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.gen</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the generator class</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_generator</span><span class="params">(z_dim, im_dim, hidden_dim, num_test=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check there are six modules in the sequential part</span></span><br><span class="line">    <span class="keyword">assert</span> len(gen) == <span class="number">6</span></span><br><span class="line">    test_input = torch.randn(num_test, z_dim)</span><br><span class="line">    test_output = gen(test_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check that the output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(test_output.shape) == (num_test, im_dim)</span><br><span class="line">    <span class="keyword">assert</span> test_output.max() &lt; <span class="number">1</span>, <span class="string">"Make sure to use a sigmoid"</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.min() &gt; <span class="number">0</span>, <span class="string">"Make sure to use a sigmoid"</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.05</span>, <span class="string">"Don't use batchnorm here"</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.15</span>, <span class="string">"Don't use batchnorm here"</span></span><br><span class="line"></span><br><span class="line">test_generator(<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">test_generator(<span class="number">20</span>, <span class="number">8</span>, <span class="number">24</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Noise"><a href="#Noise" class="headerlink" title="Noise"></a>Noise</h2><p>To be able to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don’t all look the same — think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.</p><p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. <code>torch.ones(3, 3, device=device)</code>, or move it onto the target device using <code>torch.ones(3, 3).to(device)</code>. You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as <code>torch.ones_like</code>. In general, use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code> where possible.</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">get_noise</font></code></b></font></summary>1. You will probably find [torch.randn](https://pytorch.org/docs/master/generated/torch.randn.html) useful here.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_noise</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> To use this on GPU with device='cuda', make sure to pass the device </span></span><br><span class="line">    <span class="comment"># argument to the function you use to generate the noise.</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device = device)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the noise vector function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    noise = get_noise(n_samples, z_dim, device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make sure a normal distribution was used</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(noise.shape) == (n_samples, z_dim)</span><br><span class="line">    <span class="keyword">assert</span> torch.abs(noise.std() - torch.tensor(<span class="number">1.0</span>)) &lt; <span class="number">0.01</span></span><br><span class="line">    <span class="keyword">assert</span> str(noise.device).startswith(device)</span><br><span class="line"></span><br><span class="line">test_get_noise(<span class="number">1000</span>, <span class="number">100</span>, <span class="string">'cpu'</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    test_get_noise(<span class="number">1000</span>, <span class="number">32</span>, <span class="string">'cuda'</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>The second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.</p><p><em>Note: You use leaky ReLUs to prevent the “dying ReLU” problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures!</em> </p><div class="table-container"><table><thead><tr><th style="text-align:center">REctified Linear Unit (ReLU)</th><th style="text-align:center">Leaky ReLU</th></tr></thead><tbody><tr><td style="text-align:center"><img src="relu-graph.png" alt></td><td style="text-align:center"><img src="lrelu-graph.png" alt></td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_discriminator_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_discriminator_block</span><span class="params">(input_dim, output_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Block</span></span><br><span class="line"><span class="string">    Function for returning a neural network of the discriminator given input and output dimensions.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        a discriminator neural network layer, with a linear transformation </span></span><br><span class="line"><span class="string">          followed by an nn.LeakyReLU activation with negative slope of 0.2 </span></span><br><span class="line"><span class="string">          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        nn.Linear(input_dim,output_dim),</span><br><span class="line">        nn.LeakyReLU(negative_slope = <span class="number">0.2</span>, inplace = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the discriminator block function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_block</span><span class="params">(in_features, out_features, num_test=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    block = get_discriminator_block(in_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check there are two parts</span></span><br><span class="line">    <span class="keyword">assert</span> len(block) == <span class="number">2</span></span><br><span class="line">    test_input = torch.randn(num_test, in_features)</span><br><span class="line">    test_output = block(test_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check that the shape is right</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(test_output.shape) == (num_test, out_features)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check that the LeakyReLU slope is about 0.2</span></span><br><span class="line">    <span class="keyword">assert</span> -test_output.min() / test_output.max() &gt; <span class="number">0.1</span></span><br><span class="line">    <span class="keyword">assert</span> -test_output.min() / test_output.max() &lt; <span class="number">0.3</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.3</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">test_disc_block(<span class="number">25</span>, <span class="number">12</span>)</span><br><span class="line">test_disc_block(<span class="number">15</span>, <span class="number">28</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Now you can use these blocks to make a discriminator! The discriminator class holds 2 values:</p><ul><li>The image dimension</li><li>The hidden dimension</li></ul><p>The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator’s neural network you are given a forward pass function that takes in an image tensor to be classified.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">            (MNIST images are 28x28 = 784 so that is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_dim=<span class="number">784</span>, hidden_dim=<span class="number">128</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            get_discriminator_block(im_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            get_discriminator_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            get_discriminator_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            <span class="comment"># Hint: You want to transform the final output into a single value,</span></span><br><span class="line">            <span class="comment">#       so add one more linear map.</span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            nn.Linear(hidden_dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.disc(image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Needed for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_disc</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the sequential model</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.disc</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the discriminator class</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_discriminator</span><span class="params">(z_dim, hidden_dim, num_test=<span class="number">100</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    disc = Discriminator(z_dim, hidden_dim).get_disc()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check there are three parts</span></span><br><span class="line">    <span class="keyword">assert</span> len(disc) == <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check the linear layer is correct</span></span><br><span class="line">    test_input = torch.randn(num_test, z_dim)</span><br><span class="line">    test_output = disc(test_input)</span><br><span class="line">    <span class="keyword">assert</span> tuple(test_output.shape) == (num_test, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make sure there's no sigmoid</span></span><br><span class="line">    <span class="keyword">assert</span> test_input.max() &gt; <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> test_input.min() &lt; <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">test_discriminator(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">test_discriminator(<span class="number">20</span>, <span class="number">8</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can put it all together!<br>First, you will set your parameters:</p><ul><li>criterion: the loss function</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>device: the device type, here using a GPU (which runs CUDA), not CPU</li></ul><p>Next, you will load the MNIST dataset as tensors using a dataloader.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set your parameters</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">n_epochs = <span class="number">200</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.00001</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"><span class="comment"># Load MNIST dataset as tensors</span></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">'.'</span>, download=<span class="keyword">False</span>, transform=transforms.ToTensor()),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Now, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">disc = Discriminator().to(device) </span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br></pre></td></tr></table></figure><p>Before you train your GAN, you will need to create functions to calculate the discriminator’s loss and the generator’s loss. This is how the discriminator and generator will know how they are doing and improve themselves. Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</p><p>Remember that you have already defined a loss function earlier (<code>criterion</code>) and you are encouraged to use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code>. If you use <code>torch.ones</code> or <code>torch.zeros</code>, you’ll need to pass <code>device=device</code> to them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_disc_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_disc_loss</span><span class="params">(gen, disc, criterion, real, num_images, z_dim, device)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the discriminator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class="line"><span class="string">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class="line"><span class="string">        criterion: the loss function, which should be used to compare </span></span><br><span class="line"><span class="string">               the discriminator's predictions to the ground truth reality of the images </span></span><br><span class="line"><span class="string">               (e.g. fake = 0, real = 1)</span></span><br><span class="line"><span class="string">        real: a batch of real images</span></span><br><span class="line"><span class="string">        num_images: the number of images the generator should produce, </span></span><br><span class="line"><span class="string">                which is also the length of the real images</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        disc_loss: a torch scalar loss value for the current batch</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">    <span class="comment">#       1) Create noise vectors and generate a batch (num_images) of fake images. </span></span><br><span class="line">    <span class="comment">#            Make sure to pass the device argument to the noise.</span></span><br><span class="line">    <span class="comment">#       2) Get the discriminator's prediction of the fake image </span></span><br><span class="line">    <span class="comment">#            and calculate the loss. Don't forget to detach the generator!</span></span><br><span class="line">    <span class="comment">#            (Remember the loss function you set earlier -- criterion. You need a </span></span><br><span class="line">    <span class="comment">#            'ground truth' tensor in order to calculate the loss. </span></span><br><span class="line">    <span class="comment">#            For example, a ground truth tensor for a fake image is all zeros.)</span></span><br><span class="line">    <span class="comment">#       3) Get the discriminator's prediction of the real image and calculate the loss.</span></span><br><span class="line">    <span class="comment">#       4) Calculate the discriminator's loss by averaging the real and fake loss</span></span><br><span class="line">    <span class="comment">#            and set it to disc_loss.</span></span><br><span class="line">    <span class="comment">#     Note: Please do not use concatenation in your solution. The tests are being updated to </span></span><br><span class="line">    <span class="comment">#           support this, but for now, average the two losses as described in step (4).</span></span><br><span class="line">    <span class="comment">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    fake_images = gen(get_noise(num_images, z_dim, device=device))</span><br><span class="line">    fake_images.detach_()  </span><br><span class="line">    fake_loss = criterion(disc(fake_images),torch.zeros((num_images,<span class="number">1</span>),device = device))</span><br><span class="line">    real_loss = criterion(disc(real),torch.ones((num_images,<span class="number">1</span>),device = device))</span><br><span class="line">    disc_loss = ( fake_loss + real_loss ) / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> disc_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_reasonable</span><span class="params">(num_images=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Don't use explicit casts to cuda - use the device argument</span></span><br><span class="line">    <span class="keyword">import</span> inspect, re</span><br><span class="line">    lines = inspect.getsource(get_disc_loss)</span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r"to\(.cuda.\)"</span>, lines)) <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r"\.cuda\(\)"</span>, lines)) <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = torch.zeros_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.ones(num_images, <span class="number">1</span>)</span><br><span class="line">    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">assert</span> tuple(disc_loss.shape) == (num_images, z_dim)</span><br><span class="line">    <span class="keyword">assert</span> torch.all(torch.abs(disc_loss - <span class="number">0.5</span>) &lt; <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">    gen = torch.ones_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.zeros(num_images, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class="string">'cpu'</span>)) &lt; <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_loss</span><span class="params">(max_tests = <span class="number">10</span>)</span>:</span></span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = Generator(z_dim).to(device)</span><br><span class="line">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">    disc = Discriminator().to(device) </span><br><span class="line">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line">    num_steps = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> dataloader:</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line">        real = real.view(cur_batch_size, <span class="number">-1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate discriminator loss</span></span><br><span class="line">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class="line">        <span class="keyword">assert</span> (disc_loss - <span class="number">0.68</span>).abs() &lt; <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check that they detached correctly</span></span><br><span class="line">        <span class="keyword">assert</span> gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.grad <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        old_weight = disc.disc[<span class="number">0</span>][<span class="number">0</span>].weight.data.clone()</span><br><span class="line">        disc_opt.step()</span><br><span class="line">        new_weight = disc.disc[<span class="number">0</span>][<span class="number">0</span>].weight.data</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check that some discriminator weights changed</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.all(torch.eq(old_weight, new_weight))</span><br><span class="line">        num_steps += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> num_steps &gt;= max_tests:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">test_disc_reasonable()</span><br><span class="line">test_disc_loss()</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span><span class="params">(gen, disc, criterion, num_images, z_dim, device)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the generator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class="line"><span class="string">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class="line"><span class="string">        criterion: the loss function, which should be used to compare </span></span><br><span class="line"><span class="string">               the discriminator's predictions to the ground truth reality of the images </span></span><br><span class="line"><span class="string">               (e.g. fake = 0, real = 1)</span></span><br><span class="line"><span class="string">        num_images: the number of images the generator should produce, </span></span><br><span class="line"><span class="string">                which is also the length of the real images</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        gen_loss: a torch scalar loss value for the current batch</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">    <span class="comment">#       1) Create noise vectors and generate a batch of fake images. </span></span><br><span class="line">    <span class="comment">#           Remember to pass the device argument to the get_noise function.</span></span><br><span class="line">    <span class="comment">#       2) Get the discriminator's prediction of the fake image.</span></span><br><span class="line">    <span class="comment">#       3) Calculate the generator's loss. Remember the generator wants</span></span><br><span class="line">    <span class="comment">#          the discriminator to think that its fake images are real</span></span><br><span class="line">    <span class="comment">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    noises = get_noise(num_images,z_dim, device = device)</span><br><span class="line">    fake_images = gen(noises)</span><br><span class="line">    out = disc(fake_images)</span><br><span class="line">    gen_loss = criterion(out, torch.ones(num_images, <span class="number">1</span>).to(device))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_reasonable</span><span class="params">(num_images=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Don't use explicit casts to cuda - use the device argument</span></span><br><span class="line">    <span class="keyword">import</span> inspect, re</span><br><span class="line">    lines = inspect.getsource(get_gen_loss)</span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r"to\(.cuda.\)"</span>, lines)) <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r"\.cuda\(\)"</span>, lines)) <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = torch.zeros_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.all(torch.abs(gen_loss_tensor) &lt; <span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">#Verify shape. Related to gen_noise parametrization</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class="line"></span><br><span class="line">    gen = torch.ones_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.zeros(num_images, <span class="number">1</span>)</span><br><span class="line">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.all(torch.abs(gen_loss_tensor - <span class="number">1</span>) &lt; <span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">#Verify shape. Related to gen_noise parametrization</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_loss</span><span class="params">(num_images)</span>:</span></span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = Generator(z_dim).to(device)</span><br><span class="line">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">    disc = Discriminator().to(device) </span><br><span class="line">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line">    </span><br><span class="line">    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check that the loss is reasonable</span></span><br><span class="line">    <span class="keyword">assert</span> (gen_loss - <span class="number">0.7</span>).abs() &lt; <span class="number">0.1</span></span><br><span class="line">    gen_loss.backward()</span><br><span class="line">    old_weight = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.clone()</span><br><span class="line">    gen_opt.step()</span><br><span class="line">    new_weight = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight</span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> torch.all(torch.eq(old_weight, new_weight))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test_gen_reasonable(<span class="number">10</span>)</span><br><span class="line">test_gen_loss(<span class="number">18</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Finally, you can put everything together! For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. </p><p>It’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It’s important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.</p><p>After you’ve submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.</p><!-- In addition, be warned that this runs very slowly on a CPU. One way to run this more quickly is to use Google Colab: 1.   Download the .ipynb2.   Upload it to Google Drive and open it with Google Colab3.   Make the runtime type GPU (under “Runtime” -> “Change runtime type” -> Select “GPU” from the dropdown)4.   Replace `device = "cpu"` with `device = "cuda"`5.   Make sure your `get_noise` function uses the right device --><p>But remember, don’t expect anything spectacular: this is only the first lesson. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels.</p><p>You should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes:<br><img src="MNIST_Progression.png" alt="MNIST Digits"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: </span></span><br><span class="line"></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">mean_generator_loss = <span class="number">0</span></span><br><span class="line">mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">test_generator = <span class="keyword">True</span> <span class="comment"># Whether the generator should be tested</span></span><br><span class="line">gen_loss = <span class="keyword">False</span></span><br><span class="line">error = <span class="keyword">False</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Flatten the batch of real images from the dataset</span></span><br><span class="line">        real = real.view(cur_batch_size, <span class="number">-1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the gradients before backpropagation</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate discriminator loss</span></span><br><span class="line">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        disc_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For testing purposes, to keep track of the generator weights</span></span><br><span class="line">        <span class="keyword">if</span> test_generator:</span><br><span class="line">            old_generator_weights = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.detach().clone()</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update generator ###</span></span><br><span class="line">        <span class="comment">#     Hint: This code will look a lot like the discriminator updates!</span></span><br><span class="line">        <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">        <span class="comment">#       1) Zero out the gradients.</span></span><br><span class="line">        <span class="comment">#       2) Calculate the generator loss, assigning it to gen_loss.</span></span><br><span class="line">        <span class="comment">#       3) Backprop through the generator: update the gradients and optimizer.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line">        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)</span><br><span class="line">        gen_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">        gen_opt.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># For testing purposes, to check that your code changes the generator weights</span></span><br><span class="line">        <span class="keyword">if</span> test_generator:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">assert</span> lr &gt; <span class="number">0.0000002</span> <span class="keyword">or</span> (gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.grad.abs().max() &lt; <span class="number">0.0005</span> <span class="keyword">and</span> epoch == <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">assert</span> torch.any(gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.detach().clone() != old_generator_weights)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                error = <span class="keyword">True</span></span><br><span class="line">                print(<span class="string">"Runtime tests have failed"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">        mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Visualization code ###</span></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">            fake = gen(fake_noise)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            mean_generator_loss = <span class="number">0</span></span><br><span class="line">            mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Your-First-GAN&quot;&gt;&lt;a href=&quot;#Your-First-GAN&quot; class=&quot;headerlink&quot; title=&quot;Your First GAN&quot;&gt;&lt;/a&gt;Your First GAN&lt;/h1&gt;&lt;h3 id=&quot;Goal&quot;&gt;&lt;a href=&quot;#G
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Dyna-Q and Dyna-Q+</title>
    <link href="https://zhangruochi.com/Dyna-Q-and-Dyna-Q/2020/09/30/"/>
    <id>https://zhangruochi.com/Dyna-Q-and-Dyna-Q/2020/09/30/</id>
    <published>2020-09-30T08:50:06.000Z</published>
    <updated>2020-09-30T08:50:40.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-Dyna-Q-and-Dyna-Q"><a href="#Assignment-Dyna-Q-and-Dyna-Q" class="headerlink" title="Assignment: Dyna-Q and Dyna-Q+"></a>Assignment: Dyna-Q and Dyna-Q+</h1><p>Welcome to this programming assignment! In this notebook, you will:</p><ol><li>implement the Dyna-Q and Dyna-Q+ algorithms. </li><li>compare their performance on an environment which changes to become ‘better’ than it was before, that is, the task becomes easier. </li></ol><p>We will give you the environment and infrastructure to run the experiment and visualize the performance. The assignment will be graded automatically by comparing the behavior of your agent to our implementations of the algorithms. The random seed will be set explicitly to avoid different behaviors due to randomness. </p><p>Please go through the cells in order. </p><h2 id="The-Shortcut-Maze-Environment"><a href="#The-Shortcut-Maze-Environment" class="headerlink" title="The Shortcut Maze Environment"></a>The Shortcut Maze Environment</h2><p>In this maze environment, the goal is to reach the goal state (G) as fast as possible from the starting state (S). There are four actions â€“ up, down, right, left â€“ which take the agent deterministically from a state to the corresponding neighboring states, except when movement is blocked by a wall (denoted by grey) or the edge of the maze, in which case the agent remains where it is. The reward is +1 on reaching the goal state, 0 otherwise. On reaching the goal state G, the agent returns to the start state S to being a new episode. This is a discounted, episodic task with $\gamma = 0.95$.</p><p><img src="shortcut_env.png" alt="environment" width="400"></p><p>Later in the assignment, we will use a variant of this maze in which a ‘shortcut’ opens up after a certain number of timesteps. We will test if the the Dyna-Q and Dyna-Q+ agents are able to find the newly-opened shorter route to the goal state.</p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>We import the following libraries that are required for this assignment. Primarily, we shall be using the following libraries:</p><ol><li>numpy: the fundamental package for scientific computing with Python.</li><li>matplotlib: the library for plotting graphs in Python.</li><li>RL-Glue: the library for reinforcement learning experiments.</li></ol><p><strong>Please do not import other libraries</strong> as this will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> ShortcutMazeEnvironment</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">'font.size'</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">'figure.figsize'</span>: [<span class="number">8</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure><h2 id="Section-1-Dyna-Q"><a href="#Section-1-Dyna-Q" class="headerlink" title="Section 1: Dyna-Q"></a>Section 1: Dyna-Q</h2><p>Let’s start with a quick recap of the tabular Dyna-Q algorithm.</p><div style="width:80%"><img src="DynaQ.png" alt="DynaQ_pseudocode"></div><p>Dyna-Q involves four basic steps:</p><ol><li>Action selection: given an observation, select an action to be performed (here, using the $\epsilon$-greedy method).</li><li>Direct RL: using the observed next state and reward, update the action values (here, using one-step tabular Q-learning).</li><li>Model learning: using the observed next state and reward, update the model (here, updating a table as the environment is assumed to be deterministic).</li><li>Planning: update the action values by generating $n$ simulated experiences using certain starting states and actions (here, using the random-sample one-step tabular Q-planning method). This is also known as the ‘Indirect RL’ step. The process of choosing the state and action to simulate an experience with is known as ‘search control’.</li></ol><p>Steps 1 and 2 are parts of the <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=153" target="_blank" rel="noopener">tabular Q-learning algorithm</a> and are denoted by line numbers (a)â€“(d) in the pseudocode above. Step 3 is performed in line (e), and Step 4 in the block of lines (f).</p><p>We highly recommend revising the Dyna videos in the course and the material in the RL textbook (in particular, <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=183" target="_blank" rel="noopener">Section 8.2</a>).</p><p>Alright, let’s begin coding.</p><p>As you already know by now, you will develop an agent which interacts with the given environment via RL-Glue. More specifically, you will implement the usual methods <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code> in your <code>DynaQAgent</code> class, along with a couple of helper methods specific to Dyna-Q, namely <code>update_model</code> and <code>planning_step</code>. We will provide detailed comments in each method describing what your code should do. </p><p>Let’s break this down in pieces and do it one-by-one.</p><p>First of all, check out the <code>agent_init</code> method below. As in earlier assignments, some of the attributes are initialized with the data passed inside <code>agent_info</code>. In particular, pay attention to the attributes which are new to <code>DynaQAgent</code>, since you shall be using them later. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynaQAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                num_states (int): The number of states,</span></span><br><span class="line"><span class="string">                num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">                epsilon (float): The parameter for epsilon-greedy exploration,</span></span><br><span class="line"><span class="string">                step_size (float): The step-size,</span></span><br><span class="line"><span class="string">                discount (float): The discount factor,</span></span><br><span class="line"><span class="string">                planning_steps (int): The number of planning steps per environmental interaction</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                random_seed (int): the seed for the RNG used in epsilon-greedy</span></span><br><span class="line"><span class="string">                planning_random_seed (int): the seed for the RNG used in the planner</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, we get the relevant information from agent_info </span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> we use np.random.RandomState(seed) to set the two different RNGs</span></span><br><span class="line">        <span class="comment"># for the planner and the rest of the code</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.num_states = agent_info[<span class="string">"num_states"</span>]</span><br><span class="line">            self.num_actions = agent_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">"You need to pass both 'num_states' and 'num_actions' \</span></span><br><span class="line"><span class="string">                   in agent_info to initialize the action-value table"</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">"discount"</span>, <span class="number">0.95</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">"step_size"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">"epsilon"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.planning_steps = agent_info.get(<span class="string">"planning_steps"</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">'random_seed'</span>, <span class="number">42</span>))</span><br><span class="line">        self.planning_rand_generator = np.random.RandomState(agent_info.get(<span class="string">'planning_random_seed'</span>, <span class="number">42</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, we initialize the attributes required by the agent, e.g., q_values, model, etc.</span></span><br><span class="line">        <span class="comment"># A simple way to implement the model is to have a dictionary of dictionaries, </span></span><br><span class="line">        <span class="comment">#        mapping each state to a dictionary which maps actions to (reward, next state) tuples.</span></span><br><span class="line">        self.q_values = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.actions = list(range(self.num_actions))</span><br><span class="line">        self.past_action = <span class="number">-1</span></span><br><span class="line">        self.past_state = <span class="number">-1</span></span><br><span class="line">        self.model = &#123;&#125; <span class="comment"># model is a dictionary of dictionaries, which maps states to actions to </span></span><br><span class="line">                        <span class="comment"># (reward, next_state) tuples</span></span><br></pre></td></tr></table></figure><p>Now let’s create the <code>update_model</code> method, which performs the ‘Model Update’ step in the pseudocode. It takes a <code>(s, a, s&#39;, r)</code> tuple and stores the next state and reward corresponding to a state-action pair.</p><p>Remember, because the environment is deterministic, an easy way to implement the model is to have a dictionary of encountered states, each mapping to a dictionary of actions taken in those states, which in turn maps to a tuple of next state and reward. In this way, the model can be easily accessed by <code>model[s][a]</code>, which would return the <code>(s&#39;, r)</code> tuple.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_model</span><span class="params">(self, past_state, past_action, state, reward)</span>:</span></span><br><span class="line">    <span class="string">"""updates the model </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        past_state       (int): s</span></span><br><span class="line"><span class="string">        past_action      (int): a</span></span><br><span class="line"><span class="string">        state            (int): s'</span></span><br><span class="line"><span class="string">        reward           (int): r</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Update the model with the (s,a,s',r) tuple (1~4 lines)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> past_state <span class="keyword">in</span> self.model:</span><br><span class="line">        self.model[past_state] = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> past_action <span class="keyword">in</span> self.model[past_state]:</span><br><span class="line">        self.model[past_state][past_action] = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    self.model[past_state][past_action] = (state,reward)</span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-update-model"><a href="#Test-update-model" class="headerlink" title="Test update_model()"></a>Test <code>update_model()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (past_state, past_action, state, reward)</span></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="comment"># action 2 in state 0 leads back to state 0 with a reward of 1</span></span><br><span class="line">    <span class="comment"># or taking action 3 leads to state 1 with reward of 2</span></span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="comment"># taking action 0 in state 2 leads to state 1 with a reward of 1</span></span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><p>Next, you will implement the planning step, the crux of the Dyna-Q algorithm. You shall be calling this <code>planning_step</code> method at every timestep of every trajectory.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">planning_step</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""performs planning, i.e. indirect RL.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The indirect RL step:</span></span><br><span class="line">    <span class="comment"># - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Query the model with this state-action pair for the predicted next state and reward.(~1 line)</span></span><br><span class="line">    <span class="comment"># - Update the action values with this simulated experience.                            (2~4 lines)</span></span><br><span class="line">    <span class="comment"># - Repeat for the required number of planning steps.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the update equation is different for terminal and non-terminal transitions. </span></span><br><span class="line">    <span class="comment"># To differentiate between a terminal and a non-terminal next state, assume that the model stores</span></span><br><span class="line">    <span class="comment"># the terminal state as a dummy state like -1</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Important: remember you have a random number generator 'planning_rand_generator' as </span></span><br><span class="line">    <span class="comment">#     a part of the class which you need to use as self.planning_rand_generator.choice()</span></span><br><span class="line">    <span class="comment">#     For the sake of reproducibility and grading, *do not* use anything else like </span></span><br><span class="line">    <span class="comment">#     np.random.choice() for performing search control.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.planning_steps):</span><br><span class="line">    </span><br><span class="line">        s = self.planning_rand_generator.choice(list(self.model.keys()))</span><br><span class="line">        a = self.planning_rand_generator.choice(list(self.model[s].keys()))</span><br><span class="line"></span><br><span class="line">        (next_s,r) = self.model[s][a]</span><br><span class="line">        <span class="keyword">if</span> next_s == <span class="number">-1</span>:</span><br><span class="line">            expect = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            expect = self.gamma * np.max(self.q_values[next_s])</span><br><span class="line">        self.q_values[s,a] += self.step_size * (r + expect - self.q_values[s,a])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-planning-step"><a href="#Test-planning-step" class="headerlink" title="Test planning_step()"></a>Test <code>planning_step()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">2</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">agent.planning_step()</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q_values, expected_values))</span><br></pre></td></tr></table></figure><p>Now before you move on to implement the rest of the agent methods, here are the helper functions that you’ve used in the previous assessments for choosing an action using an $\epsilon$-greedy policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">    <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q_values (Numpy array): the array of action values</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        action (int): an action with the highest value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    top = float(<span class="string">"-inf"</span>)</span><br><span class="line">    ties = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">        <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">            top = q_values[i]</span><br><span class="line">            ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">            ties.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action_egreedy</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""returns an action using an epsilon-greedy policy w.r.t. the current action-value function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Important: assume you have a random number generator 'rand_generator' as a part of the class</span></span><br><span class="line"><span class="string">                which you can use as self.rand_generator.choice() or self.rand_generator.rand()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (List): coordinates of the agent (two elements)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action taken w.r.t. the aforementioned epsilon-greedy policy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">        action = self.rand_generator.choice(self.actions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        values = self.q_values[state]</span><br><span class="line">        action = self.argmax(values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><p>Next, you will implement the rest of the agent-related methods, namely <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the experiment starts, </span></span><br><span class="line"><span class="string">    called after the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) the first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># given the state, select the action using self.choose_action_egreedy()), </span></span><br><span class="line">    <span class="comment"># and save current state and action (~2 lines)</span></span><br><span class="line">    <span class="comment">### self.past_state = ?</span></span><br><span class="line">    <span class="comment">### self.past_action = ?</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = self.choose_action_egreedy(state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">            last step</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The action the agent takes given this state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># - Direct-RL step (~1-3 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step (~1 line)</span></span><br><span class="line">    <span class="comment"># - `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment"># - Action Selection step (~1 line)</span></span><br><span class="line">    <span class="comment"># Save the current state and action before returning the action to be performed. (~2 lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward + self.gamma * np.max(self.q_values[state]) - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, state, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    action = self.choose_action_egreedy(state)</span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = action</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Called when the agent terminates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">            terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># - Direct RL update with this final transition (1~2 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step with this final transition (~1 line)</span></span><br><span class="line">    <span class="comment"># - One final `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: the final transition needs to be handled carefully. Since there is no next state, </span></span><br><span class="line">    <span class="comment">#       you will have to pass a dummy state (like -1), which you will be using in the planning_step() to </span></span><br><span class="line">    <span class="comment">#       differentiate between updates with usual terminal and non-terminal transitions.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward  - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, <span class="number">-1</span>, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-agent-start-agent-step-and-agent-end"><a href="#Test-agent-start-agent-step-and-agent-end" class="headerlink" title="Test agent_start(), agent_step(), and agent_end()"></a>Test <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">2</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ----------------</span></span><br><span class="line"><span class="comment"># test agent start</span></span><br><span class="line"><span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.model == &#123;&#125;</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q_values == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.3439</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">1</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.41051</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.01</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br></pre></td></tr></table></figure><h3 id="Experiment-Dyna-Q-agent-in-the-maze-environment"><a href="#Experiment-Dyna-Q-agent-in-the-maze-environment" class="headerlink" title="Experiment: Dyna-Q agent in the maze environment"></a>Experiment: Dyna-Q agent in the maze environment</h3><p>Alright. Now we have all the components of the <code>DynaQAgent</code> ready. Let’s try it out on the maze environment! </p><p>The next cell runs an experiment on this maze environment to test your implementation. The initial action values are $0$, the step-size parameter is $0.125$. and the exploration parameter is $\epsilon=0.1$. After the experiment, the sum of rewards in each episode should match the correct result.</p><p>We will try planning steps of $0,5,50$ and compare their performance in terms of the average number of steps taken to reach the goal state in the aforementioned maze environment. For scientific rigor, we will run each experiment $30$ times. In each experiment, we set the initial random-number-generator (RNG) seeds for a fair comparison across algorithms.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_episodes = exp_parameters[<span class="string">'num_episodes'</span>]</span><br><span class="line">    planning_steps_all = agent_parameters[<span class="string">'planning_steps'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = env_parameters                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  <span class="comment"># We pass the agent the information it needs. </span></span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"epsilon"</span>: agent_parameters[<span class="string">"epsilon"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes)) <span class="comment"># for collecting metrics </span></span><br><span class="line">    log_data = &#123;<span class="string">'planning_steps_all'</span> : planning_steps_all&#125;                     <span class="comment"># that shall be plotted later</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Planning steps : '</span>, planning_steps)</span><br><span class="line">        os.system(<span class="string">'sleep 0.5'</span>)                    <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"planning_steps"</span>] = planning_steps  </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = i</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = i</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)          <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_episodes):</span><br><span class="line"></span><br><span class="line">                rl_glue.rl_start()                <span class="comment"># We start an episode. Here we aren't using rl_glue.rl_episode()</span></span><br><span class="line">                                                  <span class="comment"># like the other assessments because we'll be requiring some </span></span><br><span class="line">                is_terminal = <span class="keyword">False</span>               <span class="comment"># data from within the episodes in some of the experiments here </span></span><br><span class="line">                num_steps = <span class="number">0</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, _, action, is_terminal = rl_glue.rl_step()  <span class="comment"># The environment and agent take a step </span></span><br><span class="line">                    num_steps += <span class="number">1</span>                                      <span class="comment"># and return the reward and action taken.</span></span><br><span class="line"></span><br><span class="line">                all_averages[idx][i][j] = num_steps</span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'all_averages'</span>] = all_averages</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_data</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_steps_per_episode</span><span class="params">(data)</span>:</span></span><br><span class="line">    all_averages = data[<span class="string">'all_averages'</span>]</span><br><span class="line">    planning_steps_all = data[<span class="string">'planning_steps_all'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line">        plt.plot(np.mean(all_averages[i], axis=<span class="number">0</span>), label=<span class="string">'Planning steps = '</span>+str(planning_steps))</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Episodes'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Steps\nper\nepisode'</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">    plt.axhline(y=<span class="number">16</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">40</span>,                 <span class="comment"># The number of episodes per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">0</span>, <span class="number">5</span>, <span class="number">50</span>]       <span class="comment"># The list of planning_steps we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">dataq = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_steps_per_episode(dataq)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  0100%|██████████| 30/30 [00:07&lt;00:00,  3.97it/s]Planning steps :  5100%|██████████| 30/30 [00:09&lt;00:00,  3.24it/s]Planning steps :  50100%|██████████| 30/30 [00:53&lt;00:00,  1.79s/it]</code></pre><p><img src="output_27_6.png" alt="png"></p><p>What do you notice?</p><p>As the number of planning steps increases, the number of episodes taken to reach the goal decreases rapidly. Remember that the RNG seed was set the same for all the three values of planning steps, resulting in the same number of steps taken to reach the goal in the first episode. Thereafter, the performance improves. The slowest improvement is when there are $n=0$ planning steps, i.e., for the non-planning Q-learning agent, even though the step size parameter was optimized for it. Note that the grey dotted line shows the minimum number of steps required to reach the goal state under the optimal greedy policy.</p><hr><h3 id="Experiment-s-Dyna-Q-agent-in-the-changing-maze-environment"><a href="#Experiment-s-Dyna-Q-agent-in-the-changing-maze-environment" class="headerlink" title="Experiment(s): Dyna-Q agent in the _changing_ maze environment"></a>Experiment(s): Dyna-Q agent in the _changing_ maze environment</h3><p>Great! Now let us see how Dyna-Q performs on the version of the maze in which a shorter path opens up after 3000 steps. The rest of the transition and reward dynamics remain the same. </p><p><img src="shortcut_env_after.png" alt="environment" width="800"></p><p>Before you proceed, take a moment to think about what you expect to see. Will Dyna-Q find the new, shorter path to the goal? If so, why? If not, why not?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment_with_state_visitations</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters, result_file_name)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_max_steps = exp_parameters[<span class="string">'num_max_steps'</span>]</span><br><span class="line">    planning_steps_all = agent_parameters[<span class="string">'planning_steps'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">"change_at_n"</span> : env_parameters[<span class="string">"change_at_n"</span>]&#125;                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  </span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"epsilon"</span>: agent_parameters[<span class="string">"epsilon"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    state_visits_before_change = np.zeros((len(planning_steps_all), num_runs, <span class="number">54</span>))  <span class="comment"># For saving the number of</span></span><br><span class="line">    state_visits_after_change = np.zeros((len(planning_steps_all), num_runs, <span class="number">54</span>))   <span class="comment">#     state-visitations </span></span><br><span class="line">    cum_reward_all = np.zeros((len(planning_steps_all), num_runs, num_max_steps))   <span class="comment"># For saving the cumulative reward</span></span><br><span class="line">    log_data = &#123;<span class="string">'planning_steps_all'</span> : planning_steps_all&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Planning steps : '</span>, planning_steps)</span><br><span class="line">        os.system(<span class="string">'sleep 1'</span>)          <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"planning_steps"</span>] = planning_steps  <span class="comment"># We pass the agent the information it needs. </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = run</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = run</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)  <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            num_steps = <span class="number">0</span></span><br><span class="line">            cum_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line"></span><br><span class="line">                state, _ = rl_glue.rl_start()  <span class="comment"># We start the experiment. We'll be collecting the </span></span><br><span class="line">                is_terminal = <span class="keyword">False</span>            <span class="comment"># state-visitation counts to visiualize the learned policy</span></span><br><span class="line">                <span class="keyword">if</span> num_steps &lt; env_parameters[<span class="string">"change_at_n"</span>]: </span><br><span class="line">                    state_visits_before_change[idx][run][state] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state_visits_after_change[idx][run][state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal <span class="keyword">and</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()  </span><br><span class="line">                    num_steps += <span class="number">1</span></span><br><span class="line">                    cum_reward += reward</span><br><span class="line">                    cum_reward_all[idx][run][num_steps] = cum_reward</span><br><span class="line">                    <span class="keyword">if</span> num_steps &lt; env_parameters[<span class="string">"change_at_n"</span>]:</span><br><span class="line">                        state_visits_before_change[idx][run][state] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        state_visits_after_change[idx][run][state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'state_visits_before'</span>] = state_visits_before_change</span><br><span class="line">    log_data[<span class="string">'state_visits_after'</span>] = state_visits_after_change</span><br><span class="line">    log_data[<span class="string">'cum_reward_all'</span>] = cum_reward_all</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cumulative_reward</span><span class="params">(data_all, item_key, y_key, y_axis_label, legend_prefix, title)</span>:</span></span><br><span class="line">    data_y_all = data_all[y_key]</span><br><span class="line">    items = data_all[item_key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(items):</span><br><span class="line">        plt.plot(np.mean(data_y_all[i], axis=<span class="number">0</span>), label=legend_prefix+str(item))</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=<span class="number">3000</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Timesteps'</span>)</span><br><span class="line">    plt.ylabel(y_axis_label, rotation=<span class="number">0</span>, labelpad=<span class="number">60</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>Did you notice that the environment changes after a fixed number of _steps_ and not episodes? </p><p>This is because the environment is separate from the agent, and the environment changes irrespective of the length of each episode (i.e., the number of environmental interactions per episode) that the agent perceives. And hence we are now plotting the data per step or interaction of the agent and the environment, in order to comfortably see the differences in the behaviours of the agents before and after the environment changes.  </p><p>Okay, now we will first plot the cumulative reward obtained by the agent per interaction with the environment, averaged over 10 runs of the experiment on this changing world. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">10</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">5</span>, <span class="number">10</span>, <span class="number">50</span>]      <span class="comment"># The list of planning_steps we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">dataq = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, <span class="string">"Dyna-Q_shortcut_steps"</span>)    </span><br><span class="line">plot_cumulative_reward(dataq, <span class="string">'planning_steps_all'</span>, <span class="string">'cum_reward_all'</span>, <span class="string">'Cumulative\nreward'</span>, <span class="string">'Planning steps = '</span>, <span class="string">'Dyna-Q : Varying planning_steps'</span>)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  5100%|██████████| 10/10 [00:10&lt;00:00,  1.08s/it]Planning steps :  10100%|██████████| 10/10 [00:16&lt;00:00,  1.70s/it]Planning steps :  50100%|██████████| 10/10 [01:19&lt;00:00,  7.99s/it]</code></pre><p><img src="output_34_6.png" alt="png"></p><p>We observe that the slope of the curves is almost constant. If the agent had discovered the shortcut and begun using it, we would expect to see an increase in the slope of the curves towards the later stages of training. This is because the agent can get to the goal state faster and get the positive reward. Note that the timestep at which the shortcut opens up is marked by the grey dotted line.</p><p>Note that this trend is constant across the increasing number of planning steps.</p><p>Now let’s check the heatmap of the state visitations of the agent with <code>planning_steps=10</code> during training, before and after the shortcut opens up after 3000 timesteps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_state_visitations</span><span class="params">(data, plot_titles, idx)</span>:</span></span><br><span class="line">    data_keys = [<span class="string">"state_visits_before"</span>, <span class="string">"state_visits_after"</span>]</span><br><span class="line">    positions = [<span class="number">211</span>,<span class="number">212</span>]</span><br><span class="line">    titles = plot_titles</span><br><span class="line">    wall_ends = [<span class="keyword">None</span>,<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line"></span><br><span class="line">        state_visits = data[data_keys[i]][idx]</span><br><span class="line">        average_state_visits = np.mean(state_visits, axis=<span class="number">0</span>)</span><br><span class="line">        grid_state_visits = np.rot90(average_state_visits.reshape((<span class="number">6</span>,<span class="number">9</span>)).T)</span><br><span class="line">        grid_state_visits[<span class="number">2</span>,<span class="number">1</span>:wall_ends[i]] = np.nan <span class="comment"># walls</span></span><br><span class="line">        <span class="comment">#print(average_state_visits.reshape((6,9)))</span></span><br><span class="line">        plt.subplot(positions[i])</span><br><span class="line">        plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">'gray'</span>, linewidth=<span class="number">1</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">        plt.text(<span class="number">3</span>+<span class="number">0.5</span>, <span class="number">0</span>+<span class="number">0.5</span>, <span class="string">'S'</span>, horizontalalignment=<span class="string">'center'</span>, verticalalignment=<span class="string">'center'</span>)</span><br><span class="line">        plt.text(<span class="number">8</span>+<span class="number">0.5</span>, <span class="number">5</span>+<span class="number">0.5</span>, <span class="string">'G'</span>, horizontalalignment=<span class="string">'center'</span>, verticalalignment=<span class="string">'center'</span>)</span><br><span class="line">        plt.title(titles[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        cm = plt.get_cmap()</span><br><span class="line">        cm.set_bad(<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">1.</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    cbar = plt.colorbar(cax=cax)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">plot_state_visitations(dataq, [<span class="string">'Dyna-Q : State visitations before the env changes'</span>, <span class="string">'Dyna-Q : State visitations after the env changes'</span>], <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="output_37_0.png" alt="png"></p><p>What do you observe?</p><p>The state visitation map looks almost the same before and after the shortcut opens. This means that the Dyna-Q agent hasn’t quite discovered and started exploiting the new shortcut.</p><p>Now let’s try increasing the exploration parameter $\epsilon$ to see if it helps the Dyna-Q agent discover the shortcut. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment_only_cumulative_reward</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_max_steps = exp_parameters[<span class="string">'num_max_steps'</span>]</span><br><span class="line">    epsilons = agent_parameters[<span class="string">'epsilons'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">"change_at_n"</span> : env_parameters[<span class="string">"change_at_n"</span>]&#125;                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  </span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"planning_steps"</span>: agent_parameters[<span class="string">"planning_steps"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    log_data = &#123;<span class="string">'epsilons'</span> : epsilons&#125; </span><br><span class="line">    cum_reward_all = np.zeros((len(epsilons), num_runs, num_max_steps))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> eps_idx, epsilon <span class="keyword">in</span> enumerate(epsilons):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Agent : Dyna-Q, epsilon : %f'</span> % epsilon)</span><br><span class="line">        os.system(<span class="string">'sleep 1'</span>)          <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"epsilon"</span>] = epsilon</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = run</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = run</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)  <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            num_steps = <span class="number">0</span></span><br><span class="line">            cum_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line"></span><br><span class="line">                rl_glue.rl_start()  <span class="comment"># We start the experiment</span></span><br><span class="line">                is_terminal = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal <span class="keyword">and</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line">                    reward, _, action, is_terminal = rl_glue.rl_step()  <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                    <span class="comment"># the reward, and action taken.</span></span><br><span class="line">                    num_steps += <span class="number">1</span></span><br><span class="line">                    cum_reward += reward</span><br><span class="line">                    cum_reward_all[eps_idx][run][num_steps] = cum_reward</span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'cum_reward_all'</span>] = cum_reward_all</span><br><span class="line">    <span class="keyword">return</span> log_data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : <span class="number">10</span>,</span><br><span class="line">    <span class="string">"epsilons"</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]    <span class="comment"># The list of epsilons we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">data = run_experiment_only_cumulative_reward(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_cumulative_reward(data, <span class="string">'epsilons'</span>, <span class="string">'cum_reward_all'</span>, <span class="string">'Cumulative\nreward'</span>, <span class="string">r'$\epsilon$ = '</span>, <span class="string">r'Dyna-Q : Varying $\epsilon$'</span>)</span><br></pre></td></tr></table></figure><pre><code>Agent : Dyna-Q, epsilon : 0.100000100%|██████████| 30/30 [00:52&lt;00:00,  1.75s/it]Agent : Dyna-Q, epsilon : 0.200000100%|██████████| 30/30 [00:49&lt;00:00,  1.65s/it]Agent : Dyna-Q, epsilon : 0.400000100%|██████████| 30/30 [00:50&lt;00:00,  1.69s/it]Agent : Dyna-Q, epsilon : 0.800000100%|██████████| 30/30 [00:52&lt;00:00,  1.74s/it]</code></pre><p><img src="output_40_8.png" alt="png"></p><p>What do you observe?</p><p>Increasing the exploration via the $\epsilon$-greedy strategy does not seem to be helping. In fact, the agent’s cumulative reward decreases because it is spending more and more time trying out the exploratory actions.</p><p>Can we do better…? </p><h2 id="Section-2-Dyna-Q"><a href="#Section-2-Dyna-Q" class="headerlink" title="Section 2: Dyna-Q+"></a>Section 2: Dyna-Q+</h2><p>The motivation behind Dyna-Q+ is to give a bonus reward for actions that haven’t been tried for a long time, since there is a greater chance that the dynamics for that actions might have changed.</p><p>In particular, if the modeled reward for a transition is $r$, and the transition has not been tried in $\tau(s,a)$ time steps, then planning updates are done as if that transition produced a reward of $r + \kappa \sqrt{ \tau(s,a)}$, for some small $\kappa$. </p><p>Let’s implement that!</p><p>Based on your <code>DynaQAgent</code>, create a new class <code>DynaQPlusAgent</code> to implement the aforementioned exploration heuristic. Additionally :</p><ol><li>actions that had never been tried before from a state should now be allowed to be considered in the planning step,</li><li>and the initial model for such actions is that they lead back to the same state with a reward of zero.</li></ol><p>At this point, you might want to refer to the video lectures and <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=188" target="_blank" rel="noopener">Section 8.3</a> of the RL textbook for a refresher on Dyna-Q+.</p><p>As usual, let’s break this down in pieces and do it one-by-one.</p><p>First of all, check out the <code>agent_init</code> method below. In particular, pay attention to the attributes which are new to <code>DynaQPlusAgent</code>â€“ state-visitation counts $\tau$ and the scaling parameter $\kappa$ â€“ because you shall be using them later. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynaQPlusAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                num_states (int): The number of states,</span></span><br><span class="line"><span class="string">                num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">                epsilon (float): The parameter for epsilon-greedy exploration,</span></span><br><span class="line"><span class="string">                step_size (float): The step-size,</span></span><br><span class="line"><span class="string">                discount (float): The discount factor,</span></span><br><span class="line"><span class="string">                planning_steps (int): The number of planning steps per environmental interaction</span></span><br><span class="line"><span class="string">                kappa (float): The scaling factor for the reward bonus</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                random_seed (int): the seed for the RNG used in epsilon-greedy</span></span><br><span class="line"><span class="string">                planning_random_seed (int): the seed for the RNG used in the planner</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, we get the relevant information from agent_info </span></span><br><span class="line">        <span class="comment"># Note: we use np.random.RandomState(seed) to set the two different RNGs</span></span><br><span class="line">        <span class="comment"># for the planner and the rest of the code</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.num_states = agent_info[<span class="string">"num_states"</span>]</span><br><span class="line">            self.num_actions = agent_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">"You need to pass both 'num_states' and 'num_actions' \</span></span><br><span class="line"><span class="string">                   in agent_info to initialize the action-value table"</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">"discount"</span>, <span class="number">0.95</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">"step_size"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">"epsilon"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.planning_steps = agent_info.get(<span class="string">"planning_steps"</span>, <span class="number">10</span>)</span><br><span class="line">        self.kappa = agent_info.get(<span class="string">"kappa"</span>, <span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">'random_seed'</span>, <span class="number">42</span>))</span><br><span class="line">        self.planning_rand_generator = np.random.RandomState(agent_info.get(<span class="string">'planning_random_seed'</span>, <span class="number">42</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, we initialize the attributes required by the agent, e.g., q_values, model, tau, etc.</span></span><br><span class="line">        <span class="comment"># The visitation-counts can be stored as a table as well, like the action values </span></span><br><span class="line">        self.q_values = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.tau = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.actions = list(range(self.num_actions))</span><br><span class="line">        self.past_action = <span class="number">-1</span></span><br><span class="line">        self.past_state = <span class="number">-1</span></span><br><span class="line">        self.model = &#123;&#125;</span><br></pre></td></tr></table></figure><p>Now first up, implement the <code>update_model</code> method. Note that this is different from Dyna-Q in the aforementioned way.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_model</span><span class="params">(self, past_state, past_action, state, reward)</span>:</span></span><br><span class="line">    <span class="string">"""updates the model </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        past_state  (int): s</span></span><br><span class="line"><span class="string">        past_action (int): a</span></span><br><span class="line"><span class="string">        state       (int): s'</span></span><br><span class="line"><span class="string">        reward      (int): r</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Recall that when adding a state-action to the model, if the agent is visiting the state</span></span><br><span class="line">    <span class="comment">#    for the first time, then the remaining actions need to be added to the model as well</span></span><br><span class="line">    <span class="comment">#    with zero reward and a transition into itself.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: do *not* update the visitation-counts here. We will do that in `agent_step`.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># (3 lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> past_state <span class="keyword">not</span> <span class="keyword">in</span> self.model:</span><br><span class="line">        self.model[past_state] = &#123;past_action : (state, reward)&#125;</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> self.actions:</span><br><span class="line">            <span class="keyword">if</span> action != past_action:</span><br><span class="line">                self.model[past_state][action] = (past_state, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.model[past_state][past_action] = (state, reward)</span><br></pre></td></tr></table></figure><h3 id="Test-update-model-1"><a href="#Test-update-model-1" class="headerlink" title="Test update_model()"></a>Test <code>update_model()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><p>Next, you will implement the <code>planning_step()</code> method. This will be very similar to the one you implemented in <code>DynaQAgent</code>, but here you will be adding the exploration bonus to the reward in the simulated transition.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">planning_step</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""performs planning, i.e. indirect RL.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The indirect RL step:</span></span><br><span class="line">    <span class="comment"># - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Query the model with this state-action pair for the predicted next state and reward.(~1 line)</span></span><br><span class="line">    <span class="comment"># - **Add the bonus to the reward** (~1 line)</span></span><br><span class="line">    <span class="comment"># - Update the action values with this simulated experience.                            (2~4 lines)</span></span><br><span class="line">    <span class="comment"># - Repeat for the required number of planning steps.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the update equation is different for terminal and non-terminal transitions. </span></span><br><span class="line">    <span class="comment"># To differentiate between a terminal and a non-terminal next state, assume that the model stores</span></span><br><span class="line">    <span class="comment"># the terminal state as a dummy state like -1</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Important: remember you have a random number generator 'planning_rand_generator' as </span></span><br><span class="line">    <span class="comment">#     a part of the class which you need to use as self.planning_rand_generator.choice()</span></span><br><span class="line">    <span class="comment">#     For the sake of reproducibility and grading, *do not* use anything else like </span></span><br><span class="line">    <span class="comment">#     np.random.choice() for performing search control.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.planning_steps):</span><br><span class="line">    </span><br><span class="line">        s = self.planning_rand_generator.choice(list(self.model.keys()))</span><br><span class="line">        a = self.planning_rand_generator.choice(list(self.model[s].keys()))</span><br><span class="line"></span><br><span class="line">        (next_s,r) = self.model[s][a]</span><br><span class="line">        r += self.kappa * np.sqrt(self.tau[s][a])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> next_s == <span class="number">-1</span>:</span><br><span class="line">            expect = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            expect = self.gamma * np.max(self.q_values[next_s])</span><br><span class="line">        self.q_values[s,a] += self.step_size * (r + expect - self.q_values[s,a])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-planning-step-1"><a href="#Test-planning-step-1" class="headerlink" title="Test planning_step()"></a>Test <code>planning_step()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test code for planning_step() ##</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"kappa"</span>: <span class="number">0.001</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">2</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">2</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.planning_step()</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;, </span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.10014142</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.00036373</span>, <span class="number">0</span>, <span class="number">0.00017321</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br></pre></td></tr></table></figure><p>Again, before you move on to implement the rest of the agent methods, here are the couple of helper functions that you’ve used in the previous assessments for choosing an action using an $\epsilon$-greedy policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">    <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q_values (Numpy array): the array of action values</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        action (int): an action with the highest value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    top = float(<span class="string">"-inf"</span>)</span><br><span class="line">    ties = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">        <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">            top = q_values[i]</span><br><span class="line">            ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">            ties.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action_egreedy</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""returns an action using an epsilon-greedy policy w.r.t. the current action-value function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Important: assume you have a random number generator 'rand_generator' as a part of the class</span></span><br><span class="line"><span class="string">                which you can use as self.rand_generator.choice() or self.rand_generator.rand()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (List): coordinates of the agent (two elements)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action taken w.r.t. the aforementioned epsilon-greedy policy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">        action = self.rand_generator.choice(self.actions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        values = self.q_values[state]</span><br><span class="line">        action = self.argmax(values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><p>Now implement the rest of the agent-related methods, namely <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>. Again, these will be very similar to the ones in the <code>DynaQAgent</code>, but you will have to think of a way to update the counts since the last visit.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># given the state, select the action using self.choose_action_egreedy(), </span></span><br><span class="line">    <span class="comment"># and save current state and action (~2 lines)</span></span><br><span class="line">    <span class="comment">### self.past_state = ?</span></span><br><span class="line">    <span class="comment">### self.past_action = ?</span></span><br><span class="line">    <span class="comment"># Note that the last-visit counts are not updated here.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = self.choose_action_egreedy(state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">            last step</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The action the agent is taking.</span></span><br><span class="line"><span class="string">    """</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update the last-visited counts (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Direct-RL step (1~3 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step (~1 line)</span></span><br><span class="line">    <span class="comment"># - `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment"># - Action Selection step (~1 line)</span></span><br><span class="line">    <span class="comment"># Save the current state and action before returning the action to be performed. (~2 lines)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.tau += <span class="number">1</span></span><br><span class="line">    self.tau[self.past_state,self.past_action] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward + self.gamma * np.max(self.q_values[state]) - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, state, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    action = self.choose_action_egreedy(state)</span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = action</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Called when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">            terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Again, add the same components you added in agent_step to augment Dyna-Q into Dyna-Q+</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.tau += <span class="number">1</span></span><br><span class="line">    self.tau[self.past_state,self.past_action] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward  - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, <span class="number">-1</span>, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-agent-start-agent-step-and-agent-end-1"><a href="#Test-agent-start-agent-step-and-agent-end-1" class="headerlink" title="Test agent_start(), agent_step(), and agent_end()"></a>Test <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>,</span><br><span class="line">              <span class="string">"kappa"</span>: <span class="number">0.001</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>) <span class="comment"># state</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.tau, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> agent.model == &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_tau = np.array([</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.tau == expected_tau)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.0191</span>, <span class="number">0.271</span>, <span class="number">0.0</span>, <span class="number">0.0191</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.000183847763</span>, <span class="number">0.000424264069</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;, </span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_tau = np.array([</span><br><span class="line">    [<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.tau == expected_tau)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.0191</span>, <span class="number">0.344083848</span>, <span class="number">0</span>, <span class="number">0.0444632051</span>],</span><br><span class="line">    [<span class="number">0.0191732051</span>, <span class="number">0.19</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.000183847763</span>, <span class="number">0.000424264069</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;<span class="number">0</span>: &#123;<span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>), <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">0</span>, <span class="number">0</span>), <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>)&#125;, <span class="number">2</span>: &#123;<span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">0</span>: (<span class="number">2</span>, <span class="number">0</span>), <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>)&#125;, <span class="number">1</span>: &#123;<span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>), <span class="number">0</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>)&#125;&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><h3 id="Experiment-Dyna-Q-agent-in-the-changing-environment"><a href="#Experiment-Dyna-Q-agent-in-the-changing-environment" class="headerlink" title="Experiment: Dyna-Q+ agent in the _changing_ environment"></a>Experiment: Dyna-Q+ agent in the _changing_ environment</h3><p>Okay, now we’re ready to test our Dyna-Q+ agent on the Shortcut Maze. As usual, we will average the results over 30 independent runs of the experiment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">50</span>]      </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQPlusAgent          <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">data_qplus = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, <span class="string">"Dyna-Q+"</span>)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  50100%|██████████| 30/30 [04:21&lt;00:00,  8.72s/it]</code></pre><p>Let’s compare the Dyna-Q and Dyna-Q+ agents with <code>planning_steps=50</code> each.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cumulative_reward_comparison</span><span class="params">(data1, data2)</span>:</span></span><br><span class="line"></span><br><span class="line">    cum_reward_q = data1[<span class="string">'cum_reward_all'</span>][<span class="number">2</span>]</span><br><span class="line">    cum_reward_qPlus = data2[<span class="string">'cum_reward_all'</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.plot(np.mean(cum_reward_qPlus, axis=<span class="number">0</span>), label=<span class="string">'Dyna-Q+'</span>)</span><br><span class="line">    plt.plot(np.mean(cum_reward_q, axis=<span class="number">0</span>), label=<span class="string">'Dyna-Q'</span>)</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=<span class="number">3000</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Timesteps'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Cumulative\nreward'</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">60</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.title(<span class="string">'Average performance of Dyna-Q and Dyna-Q+ agents in the Shortcut Maze\n'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">plot_cumulative_reward_comparison(dataq, data_qplus)</span><br></pre></td></tr></table></figure><p><img src="output_64_0.png" alt="png"></p><p>What do you observe? (For reference, your graph should look like <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=189" target="_blank" rel="noopener">Figure 8.5 in Chapter 8</a> of the RL textbook)</p><p>The slope of the curve increases for the Dyna-Q+ curve shortly after the shortcut opens up after 3000 steps, which indicates that the rate of receiving the positive reward increases. This implies that the Dyna-Q+ agent finds the shorter path to the goal.</p><p>To verify this, let us plot the state-visitations of the Dyna-Q+ agent before and after the shortcut opens up.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">plot_state_visitations(data_qplus, [<span class="string">'Dyna-Q+ : State visitations before the env changes'</span>, <span class="string">'Dyna-Q+ : State visitations after the env changes'</span>], <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="output_66_0.png" alt="png"></p><p>What do you observe?</p><p>Before the shortcut opens up, like Dyna-Q, the Dyna-Q+ agent finds the sole, long path to the goal. But because the Dyna-Q+ agent keeps exploring, it succeeds in discovering the shortcut once it opens up, which leads to the goal faster. So the bonus reward heuristic is effective in helping the agent explore and find changes in the environment without degrading the performance. </p><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations! You have:</p><ol><li>implemented Dyna-Q, a model-based approach to RL;</li><li>implemented Dyna-Q+, a variant of Dyna-Q with an exploration bonus that encourages exploration; </li><li>conducted scientific experiments to empirically validate the exploration/exploitation dilemma in the planning context on an environment that changes with time.</li></ol><p>Some points to ponder about:</p><ol><li>At what cost does Dyna-Q+ improve over Dyna-Q?</li><li>In general, what is the trade-off of using model-based methods like Dyna-Q over model-free methods like Q-learning?</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-Dyna-Q-and-Dyna-Q&quot;&gt;&lt;a href=&quot;#Assignment-Dyna-Q-and-Dyna-Q&quot; class=&quot;headerlink&quot; title=&quot;Assignment: Dyna-Q and Dyna-Q+&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Planning and learning with Tabular Methods</title>
    <link href="https://zhangruochi.com/Planning-and-learning-with-Tabular-Methods/2020/09/29/"/>
    <id>https://zhangruochi.com/Planning-and-learning-with-Tabular-Methods/2020/09/29/</id>
    <published>2020-09-29T08:09:45.000Z</published>
    <updated>2020-09-29T08:19:02.711Z</updated>
    
    <content type="html"><![CDATA[<p>We develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heuristic search, and methods that can be used without a model, such as Monte Carlo and temporal-difference methods. These are respectively called <strong>model-based</strong> and <strong>model-free</strong> reinforcement learning methods. Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning. Although there are real di↵erences between these two kinds of methods, there are also great similarities.</p><ul><li>All state-space planning methods involve computing value functions as a key intermediate step toward improving the policy</li><li>They compute value functions by updates or backup operations applied to simulated experience.</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions.</li><li>Distribution model produce a description of all possibilities and their probabilities. Sample model produce just one of the possibilities and their probabilities.</li><li>当给定一个 state 和一个 action 时，distribution model 可以生成所有可能的状态转移，而sample model只能给出一个可能的状态转移</li><li>当给定一个 state 和 Policy 时，distribution model 可以获得所有可能的 episode 并得到他们出现的概率，但 sample model 只能给出一个 episode</li></ul><p>总之，distribution model 比 sample model包含更多信息，但现实中往往更容易获得sample model。简单来说，distribution model 包含了所有状态的转移概率，但sample model更像是管中窥豹，可见一斑。在DP中，我们用到的是distribution model，而在MC中我们用到的是sample model。model 是对环境的一种表达方式，（不一定是真实或完全正确的），可以用来产生仿真经验（simulation experience）。</p><h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p>从Model中生成或提升Policy 的计算过程称为 Planning:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>注意本文讨论的Planning都是state space Planning，这种Planning有两个特点：</p><ul><li>通过计算values function 来进行Policy 提升</li><li>根据simulated experience来计算value function</li></ul><p>Planning（如DP） 和learning（如MC、TD）方法的核心都是用backing-up 更新公式计算value function 的估计值。区别在于Planning 所用经验是有模型生成的simulated exprience，而learning method使用的经验是由真实环境生成的real exprience。  但两者都满足上述state space Planning结构，这表示很多思想和算法可以相互借鉴，在应用中常常用 learning 中 value function 估计值的更新公式取代 Planning 中的 value function 估计值的更新公式。例如，我们可以将Q learning 和 planning 结合，得到random-sample one-step tabular Q-planning 方法：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%"></center><p>one-step tabular Q-learning最终会收敛到一个对应于真实环境的optimal Policy，而 random-sample one-step tabular Q-planning 则收敛到一个对应于model 的optimal Policy。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heur
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Q-Learning and Expected Sarsa </title>
    <link href="https://zhangruochi.com/Q-Learning-and-Expected-Sarsa/2020/09/29/"/>
    <id>https://zhangruochi.com/Q-Learning-and-Expected-Sarsa/2020/09/29/</id>
    <published>2020-09-29T02:52:00.000Z</published>
    <updated>2020-10-14T09:19:36.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Q-Learning-and-Expected-Sarsa"><a href="#Assignment-2-Q-Learning-and-Expected-Sarsa" class="headerlink" title="Assignment 2 - Q-Learning and Expected Sarsa"></a>Assignment 2 - Q-Learning and Expected Sarsa</h1><p>Welcome to Course 2 Programming Assignment 2. In this notebook, you will:</p><ol><li>Implement Q-Learning with $\epsilon$-greedy action selection</li><li>Implement Expected Sarsa with $\epsilon$-greedy action selection</li><li>Investigate how these two algorithms behave on Cliff World (described on page 132 of the textbook)</li></ol><p>We will provide you with the environment and infrastructure to run an experiment (called the experiment program in RL-Glue). This notebook will provide all the code you need to run your experiment and visualise learning performance.</p><p>This assignment will be graded automatically by comparing the behavior of your agent to our implementations of Expected Sarsa and Q-learning. The random seed will be set to avoid different behavior due to randomness. We will highlight the functions you have to use for generating random samples and the number of times these functions should be called. </p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>You will need the following libraries for this assignment. We are using:</p><ol><li>numpy: the fundamental package for scientific computing with Python.</li><li>scipy: a Python library for scientific and technical computing.</li><li>matplotlib: library for plotting graphs in Python.</li><li>RL-Glue: library for reinforcement learning experiments.</li></ol><p>DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> sem</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> cliffworld_env</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">'font.size'</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">'figure.figsize'</span>: [<span class="number">10</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure><h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>In this section you will implement and test a Q-Learning agent with $\epsilon$-greedy action selection (Section 6.5 in the textbook). </p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_init_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">"num_states"</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">"epsilon"</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">"step_size"</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">"discount"</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">"seed"</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action]  += self.step_size * (reward + self.discount * np.max(current_q) - \</span><br><span class="line">                                                                        self.q[self.prev_state][self.prev_action] )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">        <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        top = float(<span class="string">"-inf"</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p><p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">3</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = QLearningAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q == expected_values)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>,  <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.02</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>, <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.1</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure><h1 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h1><p>In this section you will implement an Expected Sarsa agent with $\epsilon$-greedy action selection (Section 6.6 in the textbook). </p><h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpectedSarsaAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_init_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">"num_states"</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">"epsilon"</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">"step_size"</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">"discount"</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">"seed"</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        expect = (<span class="number">1</span> - self.epsilon) * np.max(current_q) + self.epsilon * np.average(current_q)</span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward + self.discount * expect - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">        <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        top = float(<span class="string">"-inf"</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure><h3 id="Test-1"><a href="#Test-1" class="headerlink" title="Test"></a>Test</h3><p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p><p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">3</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = ExpectedSarsaAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q == expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.28</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure><h1 id="Solving-the-Cliff-World"><a href="#Solving-the-Cliff-World" class="headerlink" title="Solving the Cliff World"></a>Solving the Cliff World</h1><p>We described the Cliff World environment in the video “Expected Sarsa in the Cliff World” in Lesson 3. This is an undiscounted episodic task and thus we set $\gamma$=1. The agent starts in the bottom left corner of the gridworld below and takes actions that move it in the four directions. Actions that would move the agent off of the cliff incur a reward of -100 and send the agent back to the start state. The reward for all other transitions is -1. An episode terminates when the agent reaches the bottom right corner. </p><p><img src="cliffworld.png" alt="Drawing" style="width: 600px;"></p><p>Using the experiment program in the cell below we now compare the agents on the Cliff World environment and plot the sum of rewards during each episode for the two agents.</p><p>The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">"Q-learning"</span>: QLearningAgent,</span><br><span class="line">    <span class="string">"Expected Sarsa"</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125; <span class="comment"># Contains sum of rewards during episode</span></span><br><span class="line">all_state_visits = &#123;&#125; <span class="comment"># Contains state visit counts during the last 10 episodes</span></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">48</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.5</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">100</span> <span class="comment"># The number of runs</span></span><br><span class="line">num_episodes = <span class="number">100</span> <span class="comment"># The number of episodes in each run</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    all_reward_sums[algorithm] = []</span><br><span class="line">    all_state_visits[algorithm] = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">        rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">        reward_sums = []</span><br><span class="line">        state_visits = np.zeros(<span class="number">48</span>)</span><br><span class="line">        last_episode_total_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">            <span class="keyword">if</span> episode &lt; num_episodes - <span class="number">10</span>:</span><br><span class="line">                <span class="comment"># Runs an episode</span></span><br><span class="line">                rl_glue.rl_episode(<span class="number">10000</span>) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="comment"># Runs an episode while keeping track of visited states</span></span><br><span class="line">                state, action = rl_glue.rl_start()</span><br><span class="line">                state_visits[state] += <span class="number">1</span></span><br><span class="line">                is_terminal = <span class="keyword">False</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">                    state_visits[state] += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            reward_sums.append(rl_glue.rl_return() - last_episode_total_reward)</span><br><span class="line">            last_episode_total_reward = rl_glue.rl_return()</span><br><span class="line">            </span><br><span class="line">        all_reward_sums[algorithm].append(reward_sums)</span><br><span class="line">        all_state_visits[algorithm].append(state_visits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot results</span></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    plt.plot(np.mean(all_reward_sums[algorithm], axis=<span class="number">0</span>), label=algorithm)</span><br><span class="line">plt.xlabel(<span class="string">"Episodes"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Sum of\n rewards\n during\n episode"</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line">plt.ylim(<span class="number">-30</span>,<span class="number">0</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 100/100 [00:12&lt;00:00,  8.12it/s]100%|██████████| 100/100 [00:16&lt;00:00,  6.15it/s]</code></pre><p><img src="output_26_1.png" alt="png"></p><p>To see why these two agents behave differently, let’s inspect the states they visit most. Run the cell below to generate plots showing the number of timesteps that the agents spent in each state over the last 10 episodes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm, position <span class="keyword">in</span> [(<span class="string">"Q-learning"</span>, <span class="number">211</span>), (<span class="string">"Expected Sarsa"</span>, <span class="number">212</span>)]:</span><br><span class="line">    plt.subplot(position)</span><br><span class="line">    average_state_visits = np.array(all_state_visits[algorithm]).mean(axis=<span class="number">0</span>)</span><br><span class="line">    grid_state_visits = average_state_visits.reshape((<span class="number">4</span>,<span class="number">12</span>))</span><br><span class="line">    grid_state_visits[<span class="number">0</span>,<span class="number">1</span>:<span class="number">-1</span>] = np.nan</span><br><span class="line">    plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">'gray'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.title(algorithm)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    cm = plt.get_cmap()</span><br><span class="line">    cm.set_bad(<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">0.85</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    </span><br><span class="line">cbar = plt.colorbar(cax=cax)</span><br><span class="line">cbar.ax.set_ylabel(<span class="string">"Visits during\n the last 10\n episodes"</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">70</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_28_0.png" alt="png"></p><p>The Q-learning agent learns the optimal policy, one that moves along the cliff and reaches the goal in as few steps as possible. However, since the agent does not follow the optimal policy and uses $\epsilon$-greedy exploration, it occasionally falls off the cliff. The Expected Sarsa agent takes exploration into account and follows a safer path.</p><p>Previously we used a fixed step-size of 0.5 for the agents. What happens with other step-sizes? Does this difference in performance persist?</p><p>In the next experiment we will try 10 different step-sizes from 0.1 to 1.0 and compare the sum of rewards per episode averaged over the first 100 episodes (similar to the interim performance curves in Figure 6.3 of the textbook). Shaded regions show standard errors.</p><p>This cell takes around 10 minutes to run. The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">"Q-learning"</span>: QLearningAgent,</span><br><span class="line">    <span class="string">"Expected Sarsa"</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line">step_sizes = np.linspace(<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">10</span>)</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">48</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">30</span></span><br><span class="line">num_episodes = <span class="number">100</span></span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line"></span><br><span class="line">algorithms = [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]</span><br><span class="line">cross_product = list(product(algorithms, step_sizes, range(num_runs)))</span><br><span class="line"><span class="keyword">for</span> algorithm, step_size, run <span class="keyword">in</span> tqdm(cross_product):</span><br><span class="line">    <span class="keyword">if</span> (algorithm, step_size) <span class="keyword">not</span> <span class="keyword">in</span> all_reward_sums:</span><br><span class="line">        all_reward_sums[(algorithm, step_size)] = []</span><br><span class="line"></span><br><span class="line">    agent_info[<span class="string">"step_size"</span>] = step_size</span><br><span class="line">    agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">    rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    last_episode_total_reward = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>)</span><br><span class="line">    all_reward_sums[(algorithm, step_size)].append(rl_glue.rl_return()/num_episodes)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    algorithm_stds = np.array([sem(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    plt.plot(step_sizes, algorithm_means, marker=<span class="string">'o'</span>, linestyle=<span class="string">'solid'</span>, label=algorithm)</span><br><span class="line">    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">"Step-size"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Sum of\n rewards\n per episode"</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">50</span>)</span><br><span class="line">plt.xticks(step_sizes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 600/600 [01:38&lt;00:00,  6.08it/s]</code></pre><p><img src="output_30_1.png" alt="png"></p><p>Expected Sarsa shows an advantage over Q-learning in this problem across a wide range of step-sizes.</p><p>Congratulations! Now you have:</p><ul><li>implemented Q-Learning with $\epsilon$-greedy action selection</li><li>implemented Expected Sarsa with $\epsilon$-greedy action selection</li><li>investigated the behavior of these two algorithms on Cliff World</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Q-Learning-and-Expected-Sarsa&quot;&gt;&lt;a href=&quot;#Assignment-2-Q-Learning-and-Expected-Sarsa&quot; class=&quot;headerlink&quot; title=&quot;Assignme
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Policy Evaluation in Cliff Walking Environment</title>
    <link href="https://zhangruochi.com/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/"/>
    <id>https://zhangruochi.com/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/</id>
    <published>2020-09-28T08:04:11.000Z</published>
    <updated>2020-09-28T08:06:19.087Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-Policy-Evaluation-in-Cliff-Walking-Environment"><a href="#Assignment-Policy-Evaluation-in-Cliff-Walking-Environment" class="headerlink" title="Assignment: Policy Evaluation in Cliff Walking Environment"></a>Assignment: Policy Evaluation in Cliff Walking Environment</h1><p>Welcome to the Course 2 Module 2 Programming Assignment! In this assignment, you will implement one of the fundamental sample and bootstrapping based model free reinforcement learning agents for prediction. This is namely one that uses one-step temporal difference learning, also known as TD(0). The task is to design an agent for policy evaluation in the Cliff Walking environment. Recall that policy evaluation is the prediction problem where the goal is to accurately estimate the values of states given some policy.</p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ul><li>Implement parts of the Cliff Walking environment, to get experience specifying MDPs [Section 1].</li><li>Implement an agent that uses bootstrapping and, particularly, TD(0) [Section 2].</li><li>Apply TD(0) to estimate value functions for different policies, i.e., run policy evaluation experiments [Section 3].</li></ul><h2 id="The-Cliff-Walking-Environment"><a href="#The-Cliff-Walking-Environment" class="headerlink" title="The Cliff Walking Environment"></a>The Cliff Walking Environment</h2><p>The Cliff Walking environment is a gridworld with a discrete state space and discrete action space. The agent starts at grid cell S. The agent can move (deterministically) to the four neighboring cells by taking actions Up, Down, Left or Right. Trying to move out of the boundary results in staying in the same location. So, for example, trying to move left when at a cell on the leftmost column results in no movement at all and the agent remains in the same location. The agent receives -1 reward per step in most states, and -100 reward when falling off of the cliff. This is an episodic task; termination occurs when the agent reaches the goal grid cell G. Falling off of the cliff results in resetting to the start state, without termination.</p><p>The diagram below showcases the description above and also illustrates two of the policies we will be evaluating.</p><p><img src="cliffwalk.png" style="height:400px"></p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages."></a>Packages.</h2><p>We import the following libraries that are required for this assignment. We shall be using the following libraries:</p><ol><li>jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li><li>numpy: the fundamental package for scientific computing with Python.</li><li>matplotlib: the library for plotting graphs in Python.</li><li>RL-Glue: the library for reinforcement learning experiments.</li><li>BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.</li><li>Manager: the file allowing for visualization and testing.</li><li>itertools.product: the function that can be used easily to compute permutations.</li><li>tqdm.tqdm: Provides progress bars for visualizing the status of loops.</li></ol><p><strong>Please do not import other libraries</strong> — this will break the autograder.</p><p><strong>NOTE: For this notebook, there is no need to make any calls to methods of random number generators. Spurious or missing calls to random number generators may affect your results.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> Agent <span class="keyword">import</span> BaseAgent </span><br><span class="line"><span class="keyword">from</span> Environment <span class="keyword">import</span> BaseEnvironment  </span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> manager <span class="keyword">import</span> Manager</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure><h2 id="Section-1-Environment"><a href="#Section-1-Environment" class="headerlink" title="Section 1. Environment"></a>Section 1. Environment</h2><p>In the first part of this assignment, you will get to see how the Cliff Walking environment is implemented. You will also get to implement parts of it to aid your understanding of the environment and more generally how MDPs are specified. In particular, you will implement the logic for:</p><ol><li>Converting 2-dimensional coordinates to a single index for the state,</li><li>One of the actions (action up), and,</li><li>Reward and termination.</li></ol><p>Given below is an annotated diagram of the environment with more details that may help in completing the tasks of this part of the assignment. Note that we will be creating a more general environment where the height and width positions can be variable but the start, goal and cliff grid cells have the same relative positions (bottom left, bottom right and the cells between the start and goal grid cells respectively).</p><p><img src="cliffwalk-annotated.png" style="height:400px"></p><p>Once you have gone through the code and begun implementing solutions, it may be a good idea to come back here and see if you can convince yourself that the diagram above is an accurate representation of the code given and the code you have written.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty CliffWalkEnvironment class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CliffWalkEnvironment</span><span class="params">(BaseEnvironment)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># helper method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span><span class="params">(self, loc)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h2 id="env-init"><a href="#env-init" class="headerlink" title="env_init()"></a>env_init()</h2><p>The first function we add to the environment is the initialization function which is called once when an environment object is created. In this function, the grid dimensions and special locations (start and goal locations and the cliff locations) are stored for easy use later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_init</span><span class="params">(self, env_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the environment called when the experiment first starts.</span></span><br><span class="line"><span class="string">        Note:</span></span><br><span class="line"><span class="string">            Initialize a tuple with the reward, first state, boolean</span></span><br><span class="line"><span class="string">            indicating if it's terminal.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Note, we can setup the following variables later, in env_start() as it is equivalent. </span></span><br><span class="line">        <span class="comment"># Code is left here to adhere to the note above, but these variables are initialized once more</span></span><br><span class="line">        <span class="comment"># in env_start() [See the env_start() function below.]</span></span><br><span class="line">        </span><br><span class="line">        reward = <span class="keyword">None</span></span><br><span class="line">        state = <span class="keyword">None</span> <span class="comment"># See Aside</span></span><br><span class="line">        termination = <span class="keyword">None</span></span><br><span class="line">        self.reward_state_term = (reward, state, termination)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably </span></span><br><span class="line">        <span class="comment"># used with the term "state" for our purposes and for this assignment in particular. </span></span><br><span class="line">        <span class="comment"># A difference arises in the use of the terms when we have what is called Partial Observability where </span></span><br><span class="line">        <span class="comment"># the environment may return states that may not fully represent all the information needed to </span></span><br><span class="line">        <span class="comment"># predict values or make decisions (i.e., the environment is non-Markovian.)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the default height to 4 and width to 12 (as in the diagram given above)</span></span><br><span class="line">        self.grid_h = env_info.get(<span class="string">"grid_height"</span>, <span class="number">4</span>) </span><br><span class="line">        self.grid_w = env_info.get(<span class="string">"grid_width"</span>, <span class="number">12</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Now, we can define a frame of reference. Let positive x be towards the direction down and </span></span><br><span class="line">        <span class="comment"># positive y be towards the direction right (following the row-major NumPy convention.)</span></span><br><span class="line">        <span class="comment"># Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1 </span></span><br><span class="line">        <span class="comment"># and max y is then grid_w - 1. So, we have:</span></span><br><span class="line">        <span class="comment"># Starting location of agent is the bottom-left corner, (max x, min y). </span></span><br><span class="line">        self.start_loc = (self.grid_h - <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Goal location is the bottom-right corner. (max x, max y).</span></span><br><span class="line">        self.goal_loc = (self.grid_h - <span class="number">1</span>, self.grid_w - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The cliff will contain all the cells between the start_loc and goal_loc.</span></span><br><span class="line">        self.cliff = [(self.grid_h - <span class="number">1</span>, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, (self.grid_w - <span class="number">1</span>))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to </span></span><br><span class="line">        <span class="comment"># verify that your understanding of the above code is correct for the default case, i.e., where </span></span><br><span class="line">        <span class="comment"># height = 4 and width = 12.</span></span><br></pre></td></tr></table></figure><h2 id="Implement-state"><a href="#Implement-state" class="headerlink" title="Implement state()"></a><em>Implement</em> state()</h2><p>The agent location can be described as a two-tuple or coordinate (x, y) describing the agent’s position.<br>However, we can convert the (x, y) tuple into a single index and provide agents with just this integer.<br>One reason for this choice is that the spatial aspect of the problem is secondary and there is no need<br>for the agent to know about the exact dimensions of the environment.<br>From the agent’s viewpoint, it is just perceiving some states, accessing their corresponding values<br>in a table, and updating them. Both the coordinate (x, y) state representation and the converted coordinate representation are thus equivalent in this sense.</p><p>Given a grid cell location, the state() function should return the state; a single index corresponding to the location in the grid.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Example: Suppose grid_h is 2 and grid_w is 2. Then, we can write the grid cell two-tuple or coordinate</span><br><span class="line">states as follows (following the usual 0-index convention):</span><br><span class="line">|(0, 0) (0, 1)| |0 1|</span><br><span class="line">|(1, 0) (1, 1)| |2 3|</span><br><span class="line">Assuming row-major order as NumPy does,  we can flatten the latter to get a vector [0 1 2 3].</span><br><span class="line">So, if loc = (0, 0) we return 0. While, for loc = (1, 1) we return 3.</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [state]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Modify the return statement of this function to return a correct single index as </span></span><br><span class="line"><span class="comment"># the state (see the logic for this in the previous cell.)</span></span><br><span class="line"><span class="comment"># Lines: 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state</span><span class="params">(self, loc)</span>:</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> loc[<span class="number">0</span>] * <span class="number">12</span> + loc[<span class="number">1</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR STATE (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below corresponds to the annotated diagram for the environment</span></span><br><span class="line"><span class="comment">#       given previously and is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_state</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    coords_to_test = [(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">11</span>), (<span class="number">1</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">0</span>), (<span class="number">3</span>, <span class="number">9</span>), (<span class="number">3</span>, <span class="number">11</span>)]</span><br><span class="line">    true_states = [<span class="number">0</span>, <span class="number">11</span>, <span class="number">17</span>, <span class="number">36</span>, <span class="number">45</span>, <span class="number">47</span>]</span><br><span class="line">    output_states = [env.state(coords) <span class="keyword">for</span> coords <span class="keyword">in</span> coords_to_test]</span><br><span class="line">    <span class="keyword">assert</span>(output_states == true_states)</span><br><span class="line">test_state()</span><br></pre></td></tr></table></figure><h2 id="env-start"><a href="#env-start" class="headerlink" title="env_start()"></a>env_start()</h2><p>In env_start(), we initialize the agent location to be the start location and return the state corresponding to it as the first state for the agent to act upon. Additionally, we also set the reward and termination terms to be 0 and False respectively as they are consistent with the notion that there is no reward nor termination before the first action is even taken.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_start</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the episode starts, called before the</span></span><br><span class="line"><span class="string">    agent starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first state from the environment.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    reward = <span class="number">0</span></span><br><span class="line">    <span class="comment"># agent_loc will hold the current location of the agent</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br><span class="line">    <span class="comment"># state is the one dimensional state representation of the agent location.</span></span><br><span class="line">    state = self.state(self.agent_loc)</span><br><span class="line">    termination = <span class="keyword">False</span></span><br><span class="line">    self.reward_state_term = (reward, state, termination)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h2 id="Implement-env-step"><a href="#Implement-env-step" class="headerlink" title="Implement env_step()"></a><em>Implement</em> env_step()</h2><p>Once an action is taken by the agent, the environment must provide a new state, reward and termination signal. </p><p>In the Cliff Walking environment, agents move around using a 4-cell neighborhood called the Von Neumann neighborhood (<a href="https://en.wikipedia.org/wiki/Von_Neumann_neighborhood" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Von_Neumann_neighborhood</a>). Thus, the agent has 4 available actions at each state. Three of the actions have been implemented for you and your first task is to implement the logic for the fourth action (Action UP).</p><p>Your second task for this function is to implement the reward logic. Look over the environment description given earlier in this notebook if you need a refresher for how the reward signal is defined.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [env_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the code for action UP and implement the logic for reward and termination.</span></span><br><span class="line"><span class="comment"># Lines: ~7.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        action: The action taken by the agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (float, state, Boolean): a tuple of the reward, state,</span></span><br><span class="line"><span class="string">            and boolean indicating if it's terminal.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> action == <span class="number">0</span>: <span class="comment"># UP (Task 1)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Hint: Look at the code given for the other actions and think about the logic in them.</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] - <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &gt;= <span class="number">0</span>:</span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">1</span>: <span class="comment"># LEFT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &gt;= <span class="number">0</span>: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">2</span>: <span class="comment"># DOWN</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] + <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &lt; self.grid_h: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">3</span>: <span class="comment"># RIGHT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &lt; self.grid_w: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">raise</span> Exception(str(action) + <span class="string">" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!"</span>)</span><br><span class="line"></span><br><span class="line">    reward = <span class="number">-1</span></span><br><span class="line">    terminal = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Consider the initialization of reward and terminal variables above. Then, note the </span></span><br><span class="line">    <span class="comment"># conditional statements and comments given below and carefully ensure to set the variables reward </span></span><br><span class="line">    <span class="comment"># and terminal correctly for each case.</span></span><br><span class="line">    <span class="keyword">if</span> self.agent_loc == self.goal_loc: <span class="comment"># Reached Goal!</span></span><br><span class="line">        terminal = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">elif</span> self.agent_loc <span class="keyword">in</span> self.cliff: <span class="comment"># Fell into the cliff!</span></span><br><span class="line">        reward = <span class="number">-100</span></span><br><span class="line">        self.agent_loc = self.start_loc</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)</span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR ACTION UP (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is again limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_action_up</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">test_action_up()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR REWARD &amp; TERMINATION (10 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_reward</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">0</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-100</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">2</span>, <span class="number">11</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">11</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">True</span>)</span><br><span class="line">test_reward()</span><br></pre></td></tr></table></figure><h2 id="env-cleanup"><a href="#env-cleanup" class="headerlink" title="env_cleanup()"></a>env_cleanup()</h2><p>There is not much cleanup to do for the Cliff Walking environment. Here, we simply reset the agent location to be the start location in this function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Cleanup done after the environment ends"""</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br></pre></td></tr></table></figure><h2 id="Section-2-Agent"><a href="#Section-2-Agent" class="headerlink" title="Section 2. Agent"></a>Section 2. Agent</h2><p>In this second part of the assignment, you will be implementing the key updates for Temporal Difference Learning. There are two cases to consider depending on whether an action leads to a terminal state or not.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty TDAgent class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span><span class="params">(self)</span>:</span>        </span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h2 id="agent-init"><a href="#agent-init" class="headerlink" title="agent_init()"></a>agent_init()</h2><p>As we did with the environment, we first initialize the agent once when a TDAgent object is created. In this function, we create a random number generator, seeded with the seed provided in the agent_info dictionary to get reproducible results. We also set the policy, discount and step size based on the agent_info dictionary. Finally, with a convention that the policy is always specified as a mapping from states to actions and so is an array of size (# States, # Actions), we initialize a values array of shape (# States,) to zeros.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">    <span class="string">"""Setup for the agent called when the experiment first starts."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a random number generator with the provided seed to seed the agent for reproducibility.</span></span><br><span class="line">    self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Policy will be given, recall that the goal is to accurately estimate its corresponding value function. </span></span><br><span class="line">    self.policy = agent_info.get(<span class="string">"policy"</span>)</span><br><span class="line">    <span class="comment"># Discount factor (gamma) to use in the updates.</span></span><br><span class="line">    self.discount = agent_info.get(<span class="string">"discount"</span>)</span><br><span class="line">    <span class="comment"># The learning rate or step size parameter (alpha) to use in updates.</span></span><br><span class="line">    self.step_size = agent_info.get(<span class="string">"step_size"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize an array of zeros that will hold the values.</span></span><br><span class="line">    <span class="comment"># Recall that the policy can be represented as a (# States, # Actions) array. With the </span></span><br><span class="line">    <span class="comment"># assumption that this is the case, we can use the first dimension of the policy to</span></span><br><span class="line">    <span class="comment"># initialize the array for values.</span></span><br><span class="line">    self.values = np.zeros((self.policy.shape[<span class="number">0</span>],))</span><br></pre></td></tr></table></figure><h1 id="agent-start"><a href="#agent-start" class="headerlink" title="agent_start()"></a>agent_start()</h1><p>In agent_start(), we choose an action based on the initial state and policy we are evaluating. We also cache the state so that we can later update its value when we perform a Temporal Difference update. Finally, we return the action chosen so that the RL loop can continue and the environment can execute this action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># The policy can be represented as a (# States, # Actions) array. So, we can use </span></span><br><span class="line">    <span class="comment"># the second dimension here when choosing an action.</span></span><br><span class="line">    action = self.rand_generator.choice(range(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="Implement-agent-step"><a href="#Implement-agent-step" class="headerlink" title="Implement agent_step()"></a><em>Implement</em> agent_step()</h2><p>In agent_step(), the agent must:</p><ul><li>Perform an update to improve the value estimate of the previously visited state, and</li><li>Act based on the state provided by the environment.</li></ul><p>The latter of the two steps above has been implemented for you. Implement the former. Note that, unlike later in agent_end(), the episode has not yet ended in agent_step(). in other words, the previously observed state was not a terminal state.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step after the last action, i.e., where the agent ended up after the</span></span><br><span class="line"><span class="string">            last action</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action the agent is taking.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: We should perform an update with the last state given that we now have the reward and</span></span><br><span class="line">    <span class="comment"># next state. We break this into two steps. Recall for example that the Monte-Carlo update </span></span><br><span class="line">    <span class="comment"># had the form: V[S_t] = V[S_t] + alpha * (target - V[S_t]), where the target was the return, G_t.</span></span><br><span class="line">    target = reward + self.discount * self.values[state]</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Having updated the value for the last state, we now act based on the current </span></span><br><span class="line">    <span class="comment"># state, and set the last state to be current one as we will next be making an </span></span><br><span class="line">    <span class="comment"># update with it when agent_step is called next once the action we return from this function </span></span><br><span class="line">    <span class="comment"># is executed in the environment.</span></span><br><span class="line"></span><br><span class="line">    action = self.rand_generator.choice(range(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="Implement-agent-end"><a href="#Implement-agent-end" class="headerlink" title="Implement agent_end()"></a><em>Implement</em> agent_end()</h2><p>Implement the TD update for the case where an action leads to a terminal state.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_end]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Here too, we should perform an update with the last state given that we now have the </span></span><br><span class="line">    <span class="comment"># reward. Note that in this case, the action led to termination. Once more, we break this into </span></span><br><span class="line">    <span class="comment"># two steps, computing the target and the update itself that uses the target and the </span></span><br><span class="line">    <span class="comment"># current value estimate for the state whose value we are updating.</span></span><br><span class="line">    target = reward</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><h2 id="agent-cleanup"><a href="#agent-cleanup" class="headerlink" title="agent_cleanup()"></a>agent_cleanup()</h2><p>In cleanup, we simply reset the last state to be None to ensure that we are not storing any states past an episode.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Cleanup done after the agent ends."""</span></span><br><span class="line">    self.last_state = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><h2 id="agent-message"><a href="#agent-message" class="headerlink" title="agent_message()"></a>agent_message()</h2><p>agent_message() can generally be used to get different kinds of information about an RLGlue agent in the interaction loop of RLGlue. Here, we conditonally check for a message matching “get_values” and use it to retrieve the values table the agent has been updating over time.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">    <span class="string">"""A function used to pass information from the agent to the experiment.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        message: The message passed to the agent.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The response (or answer) to the message.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">"get_values"</span>:</span><br><span class="line">        <span class="keyword">return</span> self.values</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"TDAgent.agent_message(): Message not understood!"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR TD-UPDATES (20 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test belows serve as a good check in debugging your code for the TD updates. However, </span></span><br><span class="line"><span class="comment">#       as with the other tests, it is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_td_updates</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -1 and does not lead to a terminal state. This is in a simple two state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The first state's current value estimate is 0 while the second is 1.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>], [<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">"policy"</span>: np.array(policy_list), <span class="string">"discount"</span>: <span class="number">0.99</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>, <span class="number">1.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = <span class="number">-1</span></span><br><span class="line">    next_state = <span class="number">1</span></span><br><span class="line">    agent.agent_step(reward, next_state)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], <span class="number">-0.001</span>) <span class="keyword">and</span> np.isclose(agent.values[<span class="number">1</span>], <span class="number">1.</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -100 and lead to a terminal state. This is in a simple one state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The state's current value estimate is 0.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">"policy"</span>: np.array(policy_list), <span class="string">"discount"</span>: <span class="number">0.99</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = <span class="number">-100</span></span><br><span class="line">    next_state = <span class="number">0</span></span><br><span class="line">    agent.agent_end(reward)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], <span class="number">-10</span>))</span><br><span class="line">    </span><br><span class="line">test_td_updates()</span><br></pre></td></tr></table></figure><h2 id="Section-3-Policy-Evaluation-Experiments"><a href="#Section-3-Policy-Evaluation-Experiments" class="headerlink" title="Section 3. Policy Evaluation Experiments"></a>Section 3. Policy Evaluation Experiments</h2><p>Finally, in this last part of the assignment, you will get to see the TD policy evaluation algorithm in action by looking at the estimated values, the per state value error and after the experiment is complete, the Mean Squared Value Error curve vs. episode number, summarizing how the value error changed over time.</p><p>The code below runs one run of an experiment given env_info and agent_info dictionaries. A “manager” object is created for visualizations and is used in part for the autograder. By default, the run will be for 5000 episodes. The true_values_file is specified to compare the learned value function with the values stored in the true_values_file. Plotting of the learned value  function occurs by default after every 100 episodes. In addition, when true_values_file is specified, the value error per state and the root mean square value error will also be plotted.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No. </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(env_info, agent_info, </span></span></span><br><span class="line"><span class="function"><span class="params">                   num_episodes=<span class="number">5000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   experiment_name=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                   plot_freq=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   true_values_file=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                   value_error_threshold=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment</span><br><span class="line">    agent = TDAgent</span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line"></span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line">        <span class="keyword">if</span> episode % plot_freq == <span class="number">0</span>:</span><br><span class="line">            values = rl_glue.agent.agent_message(<span class="string">"get_values"</span>)</span><br><span class="line">            manager.visualize(values, episode)</span><br><span class="line"></span><br><span class="line">    values = rl_glue.agent.agent_message(<span class="string">"get_values"</span>)</span><br><span class="line">    <span class="keyword">if</span> true_values_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># Grading: The Manager will check that the values computed using your TD agent match </span></span><br><span class="line">        <span class="comment"># the true values (within some small allowance) across the states. In addition, it also</span></span><br><span class="line">        <span class="comment"># checks whether the root mean squared value error is close to 0.</span></span><br><span class="line">        manager.run_tests(values, value_error_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> values</span><br></pre></td></tr></table></figure><p>The cell below just runs a policy evaluation experiment with the determinstic optimal policy that strides just above the cliff. You should observe that the per state value error and RMSVE curve asymptotically go towards 0. The arrows in the four directions denote the probabilities of taking each action. This experiment is ungraded but should serve as a good test for the later experiments. The true values file provided for this experiment may help with debugging as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">"discount"</span>: <span class="number">1</span>, <span class="string">"step_size"</span>: <span class="number">0.01</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Optimal Policy that strides just along the cliff</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line"></span><br><span class="line">true_values_file = <span class="string">"optimal_policy_value_fn.npy"</span></span><br><span class="line">_ = run_experiment(env_info, agent_info, num_episodes=<span class="number">5000</span>, experiment_name=<span class="string">"Policy Evaluation on Optimal Policy"</span>,</span><br><span class="line">                   plot_freq=<span class="number">500</span>, true_values_file=true_values_file)</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The Safe Policy</span></span><br><span class="line"><span class="comment"># Hint: Fill in the array below (as done in the previous cell) based on the safe policy illustration </span></span><br><span class="line"><span class="comment"># in the environment diagram. This is the policy that strides as far as possible away from the cliff. </span></span><br><span class="line"><span class="comment"># We call it a "safe" policy because if the environment has any stochasticity, this policy would do a good job in </span></span><br><span class="line"><span class="comment"># keeping the agent from falling into the cliff (in contrast to the optimal policy shown before). </span></span><br><span class="line"><span class="comment"># BOILERPLATE:</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">24</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">12</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">11</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">policy[<span class="number">11</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">23</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH SAFE POLICY</span></span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line">v = run_experiment(env_info, agent_info,</span><br><span class="line">               experiment_name=<span class="string">"Policy Evaluation On Safe Policy"</span>,</span><br><span class="line">               num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">500</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A Near Optimal Stochastic Policy</span></span><br><span class="line"><span class="comment"># Now, we try a stochastic policy that deviates a little from the optimal policy seen above. </span></span><br><span class="line"><span class="comment"># This means we can get different results due to randomness.</span></span><br><span class="line"><span class="comment"># We will thus average the value function estimates we get over multiple runs. </span></span><br><span class="line"><span class="comment"># This can take some time, upto about 5 minutes from previous testing. </span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The autograder will compare . Re-run this cell upon making any changes.</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">"discount"</span>: <span class="number">1</span>, <span class="string">"step_size"</span>: <span class="number">0.01</span>&#125;</span><br><span class="line"></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line">agent_info.update(&#123;<span class="string">"step_size"</span>: <span class="number">0.01</span>&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH NEAR OPTIMAL STOCHASTIC POLICY (40 POINTS)</span></span><br><span class="line">arr = []</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(<span class="number">30</span>)):</span><br><span class="line">    env_info[<span class="string">'seed'</span>] = i</span><br><span class="line">    agent_info[<span class="string">'seed'</span>] = i</span><br><span class="line">    v = run_experiment(env_info, agent_info,</span><br><span class="line">                   experiment_name=<span class="string">"Policy Evaluation On Optimal Policy"</span>,</span><br><span class="line">                   num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">10000</span>)</span><br><span class="line">    arr.append(v)</span><br><span class="line">average_v = np.array(arr).mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations, you have completed assignment 2! In this assignment, we investigated a very useful concept for sample-based online learning: temporal difference. We particularly looked at the prediction problem where the goal is to find the value function corresponding to a given policy. In the next assignment, by learning the action-value function instead of the state-value function, you will get to see how temporal difference learning can be used in control as well.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-Policy-Evaluation-in-Cliff-Walking-Environment&quot;&gt;&lt;a href=&quot;#Assignment-Policy-Evaluation-in-Cliff-Walking-Environment&quot; clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Assignment 4: Chatbot</title>
    <link href="https://zhangruochi.com/Assignment-4-Chatbot/2020/09/28/"/>
    <id>https://zhangruochi.com/Assignment-4-Chatbot/2020/09/28/</id>
    <published>2020-09-28T05:50:01.000Z</published>
    <updated>2020-09-28T09:00:27.567Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-4-Chatbot"><a href="#Assignment-4-Chatbot" class="headerlink" title="Assignment 4: Chatbot"></a>Assignment 4: Chatbot</h1><p><img src="cbot.jpg" height="400" width="400"> </p><p>Welcome to the last assignment of Course 4. Before you get started, we want to congratulate you on getting here. It is your 16th programming assignment in this Specialization and we are very proud of you! In this assignment, you are going to use the <a href="https://arxiv.org/abs/2001.04451" target="_blank" rel="noopener">Reformer</a>, also known as the efficient Transformer, to generate a dialogue between two bots. You will feed conversations to your model and it will learn how to understand the context of each one. Not only will it learn how to answer questions but it will also know how to ask questions if it needs more info. For example, after a customer asks for a train ticket, the chatbot can ask what time the said customer wants to leave. You can use this concept to automate call centers, hotel receptions, personal trainers, or any type of customer service. By completing this assignment, you will:</p><ul><li>Understand how the Reformer works</li><li>Explore the <a href="https://arxiv.org/abs/1810.00278" target="_blank" rel="noopener">MultiWoz</a> dataset</li><li>Process the data to feed it into the model</li><li>Train your model</li><li>Generate a dialogue by feeding a question to the model</li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">Part 1:   Exploring the MultiWoz dataset</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li><li><a href="#2">Part 2:   Processing the data for Reformer inputs</a><ul><li><a href="#2.1">2.1   Tokenizing, batching with bucketing</a></li></ul></li><li><a href="#3">Part 3:   Reversible layers</a><ul><li><a href="#ex02">Exercise 02</a></li><li><a href="#ex03">Exercise 03</a></li><li><a href="#3.1">3.1   Reversible layers and randomness</a></li></ul></li><li><a href="#4">Part 4:   ReformerLM Training</a><ul><li><a href="#ex04">Exercise 04</a></li><li><a href="#ex05">Exercise 05</a></li></ul></li><li><a href="#5">Part 5:   Decode from a pretrained model</a><ul><li><a href="#ex06">Exercise 06</a></li></ul></li></ul><p><a name="1"></a></p><h1 id="Part-1-Exploring-the-MultiWoz-dataset"><a href="#Part-1-Exploring-the-MultiWoz-dataset" class="headerlink" title="Part 1:   Exploring the MultiWoz dataset"></a>Part 1:   Exploring the MultiWoz dataset</h1><p>You will start by exploring the MultiWoz dataset. The dataset you are about to use has more than 10,000 human annotated dialogues and spans multiple domains and topics. Some dialogues include multiple domains and others include single domains. In this section, you will load and explore this dataset, as well as develop a function to extract the dialogues.</p><p>Let’s first import the modules we will be using:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax   </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line">!pip list | grep trax</span><br></pre></td></tr></table></figure><p>Let’s also declare some constants we will be using in the exercises.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># filename of the MultiWOZ dialogue dataset</span></span><br><span class="line">DATA_FILE = <span class="string">'data.json'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data directory</span></span><br><span class="line">DATA_DIR = <span class="string">'./data'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dictionary where we will load the dialogue dataset</span></span><br><span class="line">DIALOGUE_DB = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary filename</span></span><br><span class="line">VOCAB_FILE = <span class="string">'en_32k.subword'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary file directory</span></span><br><span class="line">VOCAB_DIR = <span class="string">'data/vocabs'</span></span><br></pre></td></tr></table></figure><p>Let’s now load the MultiWOZ 2.1 dataset. We have already provided it for you in your workspace. It is in JSON format so we should load it as such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># help function to load a JSON file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_json</span><span class="params">(directory, file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">f'<span class="subst">&#123;directory&#125;</span>/<span class="subst">&#123;file&#125;</span>'</span>) <span class="keyword">as</span> file: </span><br><span class="line">        db = json.load(file)</span><br><span class="line">    <span class="keyword">return</span> db</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the dialogue data set into our dictionary</span></span><br><span class="line">DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)</span><br></pre></td></tr></table></figure><p>Let’s see how many dialogues we have in the dictionary. 1 key-value pair is one dialogue so we can just get the dictionary’s length.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The number of dialogues is: <span class="subst">&#123;len(DIALOGUE_DB)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>The number of dialogues is: 10438</code></pre><p>The dialogues are composed of multiple files and the filenames are used as keys in our dictionary. Those with multi-domain dialogues have “MUL” in their filenames while single domain dialogues have either “SNG” or “WOZ”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print 7 keys from the dataset to see the filenames</span></span><br><span class="line">print(list(DIALOGUE_DB.keys())[<span class="number">0</span>:<span class="number">7</span>])</span><br></pre></td></tr></table></figure><pre><code>[&#39;SNG01856.json&#39;, &#39;SNG0129.json&#39;, &#39;PMUL1635.json&#39;, &#39;MUL2168.json&#39;, &#39;SNG0073.json&#39;, &#39;SNG01445.json&#39;, &#39;MUL2105.json&#39;]</code></pre><p>As you can see from the cells above, there are 10,438 conversations, each in its own file.  You will train your model on all those conversations. Each file is also loaded into a dictionary and each has two keys which are the following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get keys of the fifth file in the list above</span></span><br><span class="line">print(DIALOGUE_DB[<span class="string">'SNG0073.json'</span>].keys())</span><br></pre></td></tr></table></figure><pre><code>dict_keys([&#39;goal&#39;, &#39;log&#39;])</code></pre><p>The <code>goal</code> also points to a dictionary and it contains several keys pertaining to the objectives of the conversation. For example below, we can see that the conversation will be about booking a taxi.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'goal'</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;taxi&#39;: {&#39;info&#39;: {&#39;leaveAt&#39;: &#39;17:15&#39;,   &#39;destination&#39;: &#39;pizza hut fen ditton&#39;,   &#39;departure&#39;: &quot;saint john&#39;s college&quot;},  &#39;reqt&#39;: [&#39;car type&#39;, &#39;phone&#39;],  &#39;fail_info&#39;: {}}, &#39;police&#39;: {}, &#39;hospital&#39;: {}, &#39;hotel&#39;: {}, &#39;attraction&#39;: {}, &#39;train&#39;: {}, &#39;message&#39;: [&quot;You want to book a &lt;span class=&#39;emphasis&#39;&gt;taxi&lt;/span&gt;. The taxi should go to &lt;span class=&#39;emphasis&#39;&gt;pizza hut fen ditton&lt;/span&gt; and should depart from &lt;span class=&#39;emphasis&#39;&gt;saint john&#39;s college&lt;/span&gt;&quot;,  &quot;The taxi should &lt;span class=&#39;emphasis&#39;&gt;leave after 17:15&lt;/span&gt;&quot;,  &quot;Make sure you get &lt;span class=&#39;emphasis&#39;&gt;car type&lt;/span&gt; and &lt;span class=&#39;emphasis&#39;&gt;contact number&lt;/span&gt;&quot;], &#39;restaurant&#39;: {}}</code></pre><p>The <code>log</code> on the other hand contains the dialog. It is a list of dictionaries and each element of this list contains several descriptions as well. Let’s look at an example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get first element of the log list</span></span><br><span class="line">DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;text&#39;: &quot;I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton.&quot;, &#39;metadata&#39;: {}, &#39;dialog_act&#39;: {&#39;Taxi-Inform&#39;: [[&#39;Dest&#39;, &#39;pizza hut fen ditton&#39;],   [&#39;Depart&#39;, &quot;saint john &#39;s college&quot;]]}, &#39;span_info&#39;: [[&#39;Taxi-Inform&#39;, &#39;Dest&#39;, &#39;pizza hut fen ditton&#39;, 11, 14],  [&#39;Taxi-Inform&#39;, &#39;Depart&#39;, &quot;saint john &#39;s college&quot;, 6, 9]]}</code></pre><p>For this assignment, we are only interested in the conversation which is in the <code>text</code> field.<br>The conversation goes back and forth between two persons. Let’s call them ‘Person 1’ and ‘Person 2’. This implies that<br>data[‘SNG0073.json’][‘log’][0][‘text’] is ‘Person 1’ and<br>data[‘SNG0073.json’][‘log’][1][‘text’] is ‘Person 2’ and so on. The even offsets are ‘Person 1’ and the odd offsets are ‘Person 2’.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">' Person 1: '</span>, DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>][<span class="string">'text'</span>])</span><br><span class="line">print(<span class="string">' Person 2: '</span>,DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">1</span>][<span class="string">'text'</span>])</span><br></pre></td></tr></table></figure><pre><code> Person 1:  I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton. Person 2:  What time do you want to leave and what time do you want to arrive by?</code></pre><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p>You will now implement the <code>get_conversation()</code> function that will extract the conversations from the dataset’s file.</p><p><strong>Instructions:</strong> Implement a function to extract conversations from the input file.<br>As described above, the conversation is in the <code>text</code> field in each of the elements in the <code>log</code> list of the file. If the log list has <code>x</code> number of elements, then the function will get the <code>text</code> entries of each of those elements. Your function should return the conversation, prepending each field with either ‘ Person 1: ‘ if ‘x’ is even or ‘ Person 2: ‘ if ‘x’ is odd. You can use the Python modulus operator ‘%’ to help select the even/odd entries. Important note: Do not print a newline character (i.e. <code>\n</code>) when generating the string. For example, in the code cell above, your function should output something like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person 1: I would like a taxi from Saint John&apos;s college to Pizza Hut Fen Ditton. Person 2: What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure><p>and <strong>not</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1:  I would like a taxi from Saint John&apos;s college to Pizza Hut Fen Ditton.</span><br><span class="line">Person 2:  What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_conversation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conversation</span><span class="params">(file, data_db)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file (string): filename of the dialogue file saved as json</span></span><br><span class="line"><span class="string">        data_db (dict): dialogue database</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: A string containing the 'text' fields of  data[file]['log'][x]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize empty string</span></span><br><span class="line">    result = <span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get length of file's log list</span></span><br><span class="line">    len_msg_log = len(data_db[file][<span class="string">'log'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set the delimiter strings</span></span><br><span class="line">    delimiter_1 = <span class="string">' Person 1: '</span></span><br><span class="line">    delimiter_2 = <span class="string">' Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over the file's log list</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len_msg_log):</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># get i'th element of file log list</span></span><br><span class="line">        cur_log = data_db[file][<span class="string">'log'</span>][i][<span class="string">'text'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check if i is even</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:                   </span><br><span class="line">            <span class="comment"># append the 1st delimiter string</span></span><br><span class="line">            result += delimiter_1</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="comment"># append the 2nd delimiter string</span></span><br><span class="line">            result += delimiter_2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the message text from the log</span></span><br><span class="line">        result += cur_log</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> w4_unittest</span><br><span class="line">w4_unittest.test_get_conversation(get_conversation)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file = <span class="string">'SNG01856.json'</span></span><br><span class="line">conversation = get_conversation(file, DIALOGUE_DB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print raw output</span></span><br><span class="line">print(conversation)</span><br></pre></td></tr></table></figure><pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</code></pre><p><strong>Expected Result:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&apos;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&apos;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.</span><br><span class="line">Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</span><br></pre></td></tr></table></figure></p><p>We can have a utility pretty print function just so we can visually follow the conversation more easily.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_conversation</span><span class="params">(conversation)</span>:</span></span><br><span class="line">    </span><br><span class="line">    delimiter_1 = <span class="string">'Person 1: '</span></span><br><span class="line">    delimiter_2 = <span class="string">'Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    split_list_d1 = conversation.split(delimiter_1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> sublist <span class="keyword">in</span> split_list_d1[<span class="number">1</span>:]:</span><br><span class="line">        split_list_d2 = sublist.split(delimiter_2)</span><br><span class="line">        print(colored(<span class="string">f'Person 1: <span class="subst">&#123;split_list_d2[<span class="number">0</span>]&#125;</span>'</span>, <span class="string">'red'</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> len(split_list_d2) &gt; <span class="number">1</span>:</span><br><span class="line">            print(colored(<span class="string">f'Person 2: <span class="subst">&#123;split_list_d2[<span class="number">1</span>]&#125;</span>'</span>, <span class="string">'green'</span>))</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">print_conversation(conversation)</span><br></pre></td></tr></table></figure><p>For this assignment, we will just use the outputs of the calls to <code>get_conversation</code> to train the model. But just to expound, there are also other information in the MultiWoz dataset that can be useful in other contexts. Each element of the log list has more information about it. For example, above, if you were to look at the other fields for the following, “am looking for a place to stay that has cheap price range it should be in a type of hotel”, you will get the following. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">'SNG01856.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;text&#39;: &#39;am looking for a place to to stay that has cheap price range it should be in a type of hotel&#39;, &#39;metadata&#39;: {}, &#39;dialog_act&#39;: {&#39;Hotel-Inform&#39;: [[&#39;Type&#39;, &#39;hotel&#39;], [&#39;Price&#39;, &#39;cheap&#39;]]}, &#39;span_info&#39;: [[&#39;Hotel-Inform&#39;, &#39;Type&#39;, &#39;hotel&#39;, 20, 20],  [&#39;Hotel-Inform&#39;, &#39;Price&#39;, &#39;cheap&#39;, 10, 10]]}</code></pre><p>The dataset also comes with hotel, hospital, taxi, train, police, and restaurant databases. For example, in case you need to call a doctor, or a hotel, or a taxi, this will allow you to automate the entire conversation. Take a look at the files accompanying the data set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the attractions file</span></span><br><span class="line">attraction_file = open(<span class="string">'data/attraction_db.json'</span>)</span><br><span class="line">attractions = json.load(attraction_file)</span><br><span class="line">print(attractions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;pool way, whitehill road, off newmarket road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;entrance fee&#39;: &#39;?&#39;, &#39;id&#39;: &#39;1&#39;, &#39;location&#39;: [52.208789, 0.154883], &#39;name&#39;: &#39;abbey pool and astroturf pitch&#39;, &#39;openhours&#39;: &#39;?&#39;, &#39;phone&#39;: &#39;01223902088&#39;, &#39;postcode&#39;: &#39;cb58nt&#39;, &#39;pricerange&#39;: &#39;?&#39;, &#39;type&#39;: &#39;swimmingpool&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hospital file</span></span><br><span class="line">hospital_file = open(<span class="string">'data/hospital_db.json'</span>)</span><br><span class="line">hospitals = json.load(hospital_file)</span><br><span class="line">print(hospitals[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;department&#39;: &#39;neurosciences critical care unit&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223216297&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hotel file</span></span><br><span class="line">hotel_file = open(<span class="string">'data/hotel_db.json'</span>)</span><br><span class="line">hotels = json.load(hotel_file)</span><br><span class="line">print(hotels[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;124 tenison road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;internet&#39;: &#39;yes&#39;, &#39;parking&#39;: &#39;no&#39;, &#39;id&#39;: &#39;0&#39;, &#39;location&#39;: [52.1963733, 0.1987426], &#39;name&#39;: &#39;a and b guest house&#39;, &#39;phone&#39;: &#39;01223315702&#39;, &#39;postcode&#39;: &#39;cb12dp&#39;, &#39;price&#39;: {&#39;double&#39;: &#39;70&#39;, &#39;family&#39;: &#39;90&#39;, &#39;single&#39;: &#39;50&#39;}, &#39;pricerange&#39;: &#39;moderate&#39;, &#39;stars&#39;: &#39;4&#39;, &#39;takesbookings&#39;: &#39;yes&#39;, &#39;type&#39;: &#39;guesthouse&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the police file</span></span><br><span class="line">police_file = open(<span class="string">'data/police_db.json'</span>)</span><br><span class="line">police = json.load(police_file)</span><br><span class="line">print(police[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;name&#39;: &#39;Parkside Police Station&#39;, &#39;address&#39;: &#39;Parkside, Cambridge&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223358966&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of a restuarant file</span></span><br><span class="line">restaurant_file = open(<span class="string">'data/restaurant_db.json'</span>)</span><br><span class="line">restaurants = json.load(restaurant_file)</span><br><span class="line">print(restaurants[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;Regent Street City Centre&#39;, &#39;area&#39;: &#39;centre&#39;, &#39;food&#39;: &#39;italian&#39;, &#39;id&#39;: &#39;19210&#39;, &#39;introduction&#39;: &#39;Pizza hut is a large chain with restaurants nationwide offering convenience pizzas pasta and salads to eat in or take away&#39;, &#39;location&#39;: [52.20103, 0.126023], &#39;name&#39;: &#39;pizza hut city centre&#39;, &#39;phone&#39;: &#39;01223323737&#39;, &#39;postcode&#39;: &#39;cb21ab&#39;, &#39;pricerange&#39;: &#39;cheap&#39;, &#39;type&#39;: &#39;restaurant&#39;}</code></pre><p>For more information about the multiwoz 2.1 data set, please run the cell below to read the <code>ReadMe.txt</code> file. Feel free to open any other file to explore it. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data/README'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    print(file.read())</span><br></pre></td></tr></table></figure><pre><code>###########################################################################################################  Copyright Cambridge Dialogue Systems Group, 2018 ###########################################################################################################Dataset contains the following files:1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have &quot;MUL&quot; in their names. Single domain dialogues have either &quot;SNG&quot; or &quot;WOZ&quot; in their names.2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.7. police_db.json: the Cambridge police station information.8. taxi_db.json: slot-value list for taxi domain.9. valListFile.txt: list of dialogues for validation.10. testListFile.txt: list of dialogues for testing.11. system_acts.json:  There are 6 domains (&#39;Booking&#39;, &#39;Restaurant&#39;, &#39;Hotel&#39;, &#39;Attraction&#39;, &#39;Taxi&#39;, &#39;Train&#39;) and 1 dummy domain (&#39;general&#39;).  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. &#39;Hotel-inform&#39; means it is an &#39;inform&#39; act in the Hotel domain.  Dialogue acts which cannot take slots, e.g., &#39;good bye&#39;, are defined under the &#39;general&#39; domain.  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.  If a dialogue act takes no slots, e.g., dialogue act &#39;offer booking&#39; for an utterance &#39;would you like to take a reservation?&#39;, its slot-value pair is [&#39;none&#39;, &#39;none&#39;]  There are four types of values:  1) If a slot takes a binary value, e.g., &#39;has Internet&#39; or &#39;has park&#39;, the value is either &#39;yes&#39; or &#39;no&#39;.  2) If a slot is under the act &#39;request&#39;, e.g., &#39;request&#39; about &#39;area&#39;, the value is expressed as &#39;?&#39;.  3) The value that appears in the utterance e.g., the name of a restaurant.  4) If for some reason the turn does not have an annotation then it is labeled as &quot;No Annotation.&quot;12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. </code></pre><p>As you can see, there are many other aspects of the MultiWoz dataset. Nonetheless, you’ll see that even with just the conversations, your model will still be able to generate useful responses. This concludes our exploration of the dataset. In the next section, we will do some preprocessing before we feed it into our model for training.</p><p><a name="2"></a></p><h1 id="Part-2-Processing-the-data-for-Reformer-inputs"><a href="#Part-2-Processing-the-data-for-Reformer-inputs" class="headerlink" title="Part 2:   Processing the data for Reformer inputs"></a>Part 2:   Processing the data for Reformer inputs</h1><p>You will now use the <code>get_conversation()</code> function to process the data. The Reformer expects inputs of this form: </p><p><strong>Person 1: Why am I so happy? Person 2: Because you are learning NLP Person 1: … Person 2: …*</strong></p><p>And the conversation keeps going with some text. As you can see ‘Person 1’ and ‘Person 2’ act as delimiters so the model automatically recognizes the person and who is talking. It can then come up with the corresponding text responses for each person. Let’s proceed to process the text in this fashion for the Reformer. First, let’s grab all the conversation strings from all dialogue files and put them in a list.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the keys are the file names</span></span><br><span class="line">all_files = DIALOGUE_DB.keys()</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize empty list</span></span><br><span class="line">untokenized_data = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># loop over all files</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> all_files:</span><br><span class="line">    <span class="comment"># this is the graded function you coded</span></span><br><span class="line">    <span class="comment"># returns a string delimited by Person 1 and Person 2</span></span><br><span class="line">    result = get_conversation(file, DIALOGUE_DB)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># append to the list</span></span><br><span class="line">    untokenized_data.append(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the first element to check if it's the same as the one we got before</span></span><br><span class="line">print(untokenized_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</code></pre><p>Now let us split the list to a train and eval dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle the list we generated above</span></span><br><span class="line">random.shuffle(untokenized_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a cutoff (5% of the total length for this assignment)</span></span><br><span class="line"><span class="comment"># convert to int because we will use it as a list index</span></span><br><span class="line">cut_off = int(len(untokenized_data) * <span class="number">.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># slice the list. the last elements after the cut_off value will be the eval set. the rest is for training. </span></span><br><span class="line">train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'number of conversations in the data set: <span class="subst">&#123;len(untokenized_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'number of conversations in train set: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'number of conversations in eval set: <span class="subst">&#123;len(eval_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>number of conversations in the data set: 10438number of conversations in train set: 9917number of conversations in eval set: 521</code></pre><p><a name="2.1"></a></p><h2 id="2-1-Tokenizing-batching-with-bucketing"><a href="#2-1-Tokenizing-batching-with-bucketing" class="headerlink" title="2.1   Tokenizing, batching with bucketing"></a>2.1   Tokenizing, batching with bucketing</h2><p>We can now proceed in generating tokenized batches of our data. Let’s first define a utility generator function to yield elements from our data sets:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stream</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># loop over the entire data</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="comment"># get a random element</span></span><br><span class="line">        d = random.choice(data)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># yield a tuple pair of identical values </span></span><br><span class="line">        <span class="comment"># (i.e. our inputs to the model will also be our targets during training)</span></span><br><span class="line">        <span class="keyword">yield</span> (d, d)</span><br></pre></td></tr></table></figure><p>Now let’s define our data pipeline for tokenizing and batching our data. As in the previous assignments, we will bucket by length and also have an upper bound on the token length.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trax allows us to use combinators to generate our data pipeline</span></span><br><span class="line">data_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># randomize the stream</span></span><br><span class="line">    trax.data.Shuffle(),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># tokenize the data</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=VOCAB_DIR,</span><br><span class="line">                       vocab_file=VOCAB_FILE),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># filter too long sequences</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># bucket by length</span></span><br><span class="line">    trax.data.BucketByLength(boundaries=[<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>],</span><br><span class="line">                             batch_sizes=[<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,   <span class="number">2</span>, <span class="number">1</span>]),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add loss weights but do not add it to the padding tokens (i.e. 0)</span></span><br><span class="line">    trax.data.AddLossWeights(id_to_mask=<span class="number">0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply the data pipeline to our train and eval sets</span></span><br><span class="line">train_stream = data_pipeline(stream(train_data))</span><br><span class="line">eval_stream = data_pipeline(stream(eval_data))</span><br></pre></td></tr></table></figure><p>Peek into the train stream.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the stream generators will yield (input, target, weights). let's just grab the input for inspection</span></span><br><span class="line">inp, _, _ = next(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the shape. format is (batch size, token length)</span></span><br><span class="line">print(<span class="string">"input shape: "</span>, inp.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># detokenize the first element</span></span><br><span class="line">print(trax.data.detokenize(inp[<span class="number">0</span>], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))</span><br></pre></td></tr></table></figure><pre><code>input shape:  (4, 512) Person 1: I need a place to stay that has free wifi.  Person 2: There are 32 options in Cambridge, what price range are you looking for? Person 1: I&#39;m looking for something in the cheap price range, but I need it to have a 4 star rating. I don&#39;t need any parking though. Person 2: Again I have many to choose from that meet those criteria. Would you like a suggestion? Person 1: Ok, yes, if you could suggest one that comes with free parking that would be great! Person 2: I will book it for you,is there anything else I can do for you ? Person 1: I also need a Vietnamese restaurant. Person 2: My apologies it appears that I forgot to book your lodging. I recommend Alexander Bed and Breakfast, would you like me to book it for you? Person 1: Oh yes, please do. I need it for 8 people and 5 nights, beginning friday Person 2: You are booked with the reference number E9100B48. I can help you with the Vietnamese restaurant now. Do you have an area in mind? Person 1: I just want the restaurant to be in the same price range as my hotel Person 2: There is one cheap vietnamese restaurant in town. It is thanh binh. Do you want to book? Person 1: No, just provide me with the address and area for that restaurant if you could Person 2: The restaurant is located at 17 Magdalene Street City Centre in the West.  Can I help you with anything else? Person 1: Yes, will you book me a taxi to the restaurant from the hotel, please Person 2: And what time would you like that taxi? Person 1: I would like to leave the hotel by 22:15. Person 2: Your taxi service was book with a red volkswagen. The contact number is 07797935179 in case you need to contact them. Person 1: Thank you, that will be all. Person 2: You are welcome enjoy your meal. Have a good evenening</code></pre><p><a name="3"></a></p><h1 id="Part-3-Reversible-layers"><a href="#Part-3-Reversible-layers" class="headerlink" title="Part 3:   Reversible layers"></a>Part 3:   Reversible layers</h1><p>When running large deep models, you will often run out of memory as each layer allocates memory to store activations for use in backpropagation. To save this resource, you need to be able to recompute these activations during the backward pass without storing them during the forward pass. Take a look first at the leftmost diagram below. </p><p><img src="reversible2.PNG" height="400" width="600"></p><p>This is how the residual networks are implemented in the standard Transformer. It follows that, given <code>F()</code> is Attention and <code>G()</code> is Feed-forward(FF).<br>: </p><p>\begin{align}<br>\mathrm{y}_\mathrm{a} &amp;= \mathrm{x} + \mathrm{F}\left(\mathrm{x}\right)\tag{1} \\<br>\mathrm{y}_{b}&amp;=\mathrm{y}_{a}+\mathrm{G}\left(\mathrm{y}_{a}\right)\tag{2}\\<br>\end{align}</p><p>As you can see, it requires that $\mathrm{x}$ and $\mathrm{y}_{a}$ be saved so it can be used during backpropagation. We want to avoid this to conserve memory and this is where reversible residual connections come in. They are shown in the middle and rightmost diagrams above. The key idea is that we will start with two copies of the input to the model and at each layer we will only update one of them. The activations that we <em>don’t</em> update are the ones that will be used to compute the residuals. </p><p>Now in this reversible set up you get the following instead: </p><p>\begin{align}<br>\mathrm{y}_{1}&amp;=\mathrm{x}_{1}+\mathrm{F}\left(\mathrm{x}_{2}\right)\tag{3}\\<br>\mathrm{y}_{2}&amp;=\mathrm{x}_{2}+\mathrm{G}\left(\mathrm{y}_{1}\right)\tag{4}\\<br>\end{align}<br>To recover $\mathrm{(x_1,x_2)}$ from $\mathrm{(y_1, y_2)}$ </p><p>\begin{align}<br>\mathrm{x}_{2}&amp;=\mathrm{y}_{2}-\mathrm{G}\left(\mathrm{y}_{1}\right)\tag{5}\\<br>\mathrm{x}_{1}&amp;=\mathrm{y}_{1}-\mathrm{F}\left(\mathrm{x}_{2}\right)\tag{6}\\<br>\end{align}</p><p>With this configuration, we’re now able to run the network fully in reverse. You’ll notice that during the backward pass, $\mathrm{x2}$ and $\mathrm{x1}$ can be recomputed based solely on the values of $\mathrm{y2}$ and $\mathrm{y1}$. No need to save it during the forward pass.</p><p><a name="ex02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> You will implement the <code>reversible_layer_forward</code> function using equations 3 and 4 above. This function takes in the input vector <code>x</code> and the functions <code>f</code> and <code>g</code> and returns the concatenation of $y_1 and y_2$. For this exercise, we will be splitting <code>x</code> before going through the reversible residual steps$\mathrm{^1}$.  We can then use those two vectors for the <code>reversible_layer_reverse</code> function. Utilize <code>np.concatenate()</code> to form the output being careful to match the axis of the <code>np.split()</code>.</p><p>$\mathrm{^1}$<em>Take note that this is just for demonstrating the concept in this exercise and there are other ways of processing the input. As you’ll see in the Reformer architecture later, the initial input (i.e. <code>x</code>) can instead be duplicated instead of split.</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_forward</span><span class="params">(x, f, g)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        x (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by 'x', f and g</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    x1, x2 = np.split(x, <span class="number">2</span>, axis=<span class="number">-1</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y1 using equation 3</span></span><br><span class="line">    y1 = x1 + f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y2 using equation 4</span></span><br><span class="line">    y2 = x2 + g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray</span></span><br><span class="line">    y = np.concatenate((y1,y2), axis = <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_forward(reversible_layer_forward)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p>You will now implement the <code>reversible_layer_reverse</code> function  which is possible because at every time step you have $x_1$ and $x_2$ and $y_2$ and $y_1$, along with the function <code>f</code>, and <code>g</code>. Where <code>f</code> is the attention and <code>g</code> is the feedforward. This allows you to compute equations 5 and 6.</p><p><strong>Instructions:</strong> Implement the <code>reversible_layer_reverse</code>. Your function takes in the output vector from  <code>reversible_layer_forward</code> and functions f and g. Using equations 5 and 6 above, it computes the inputs to the layer,  $x_1$ and $x_2$.  The output, x, is the concatenation of  $x_1, x_2$. Utilize <code>np.concatenate()</code>  to form the output being careful to match the axis of the <code>np.split()</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_reverse</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_reverse</span><span class="params">(y, f, g)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        y (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix of the form of 'y'</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix of the form of 'y'</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by 'y', f and g</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    y1, y2 = np.split(y, <span class="number">2</span>, axis=<span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x2 using equation 5</span></span><br><span class="line">    x2 = y2 - g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x1 using equation 6</span></span><br><span class="line">    x1 = y1 - f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate x1 and x2 along the depth dimension</span></span><br><span class="line">    x = np.concatenate((x1,x2),axis = <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_reverse(reversible_layer_reverse)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: assert at the end can be used in grading as well</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + <span class="number">2</span></span><br><span class="line">g = <span class="keyword">lambda</span> x: x * <span class="number">3</span></span><br><span class="line">input_vector = np.random.uniform(size=(<span class="number">32</span>,))</span><br><span class="line"></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector)</span><br></pre></td></tr></table></figure><p><a name="3.1"></a></p><h2 id="3-1-Reversible-layers-and-randomness"><a href="#3-1-Reversible-layers-and-randomness" class="headerlink" title="3.1   Reversible layers and randomness"></a>3.1   Reversible layers and randomness</h2><p>This is why we were learning about fastmath’s random functions and keys in Course 3 Week 1. Utilizing the same key, <code>trax.fastmath.random.uniform()</code> will return the same values. This is required for the backward pass to return the correct layer inputs when random noise is introduced in the layer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Layers like dropout have noise, so let's simulate it here:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + np.random.uniform(size=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that the above doesn't work any more:</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="keyword">not</span> np.allclose(reversed_vector, input_vector)  <span class="comment"># Fails!!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It failed because the noise when reversing used a different random seed.</span></span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">27686</span></span><br><span class="line">rng = trax.fastmath.random.get_prng(random_seed)</span><br><span class="line">f = <span class="keyword">lambda</span> x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that it works now as the same rng is used on forward and reverse.</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector,  atol=<span class="number">1e-07</span>)</span><br></pre></td></tr></table></figure><p><a name="4"></a></p><h1 id="Part-4-ReformerLM-Training"><a href="#Part-4-ReformerLM-Training" class="headerlink" title="Part 4:   ReformerLM Training"></a>Part 4:   ReformerLM Training</h1><p>You will now proceed to training your model. Since you have already know the two main components that differentiates it from the standard Transformer, LSH in Course 1 and reversible layers above, you can just use the pre-built model already implemented in Trax. It will have this architecture:</p><p><img src="Reformer.jpg"></p><p>Similar to the Transformer you learned earlier, you want to apply an attention and feed forward layer to your inputs. For the Reformer, we improve the memory efficiency by using <strong>reversible decoder blocks</strong> and you can picture its implementation in Trax like below:</p><p><img src="ReversibleDecoder.png"></p><p>You can see that it takes the initial inputs <code>x1</code> and <code>x2</code> and does the first equation of the reversible networks you learned in Part 3. As you’ve also learned, the reversible residual has two equations for the forward-pass so doing just one of them will just constitute half of the reversible decoder block. Before doing the second equation (i.e. second half of the reversible residual), it first needs to swap the elements to take into account the stack semantics in Trax. It simply puts <code>x2</code> on top of the stack so it can be fed to the add block of the half-residual layer. It then swaps the two outputs again so it can be fed to the next layer of the network. All of these arrives at the two equations in Part 3 and it can be used to recompute the activations during the backward pass.</p><p>These are already implemented for you in Trax and in the following exercise, you’ll get to practice how to call them to build your network.</p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement a wrapper function that returns a Reformer Language Model. You can use Trax’s <a href="https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.reformer.reformer.ReformerLM" target="_blank" rel="noopener">ReformerLM</a> to do this quickly. It will have the same architecture as shown above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM</span><span class="params">(vocab_size=<span class="number">33000</span>, n_layers=<span class="number">2</span>, mode=<span class="string">'train'</span>, attention_type=tl.SelfAttention)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        vocab_size (int): size of the vocabulary</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers</span></span><br><span class="line"><span class="string">        mode (string): setting of the model which can be 'train', 'eval', or 'predict' </span></span><br><span class="line"><span class="string">        attention_type(class): attention class to use </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        model (ReformerLM): a reformer language model implemented in Trax</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    <span class="comment"># initialize an instance of Trax's ReformerLM class</span></span><br><span class="line">    model = trax.models.reformer.ReformerLM( </span><br><span class="line">        <span class="comment"># set vocab size</span></span><br><span class="line">        vocab_size = vocab_size,</span><br><span class="line">        <span class="comment"># set number of layers</span></span><br><span class="line">        n_layers = n_layers,</span><br><span class="line">        <span class="comment"># set mode</span></span><br><span class="line">        mode = mode,</span><br><span class="line">        <span class="comment"># set attention type</span></span><br><span class="line">        attention_type = attention_type</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># display the model</span></span><br><span class="line">temp_model = ReformerLM(<span class="string">'train'</span>)</span><br><span class="line">print(str(temp_model))</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> temp_model</span><br></pre></td></tr></table></figure><pre><code>Serial[  ShiftRight(1)  Embedding_train_512  Dropout  PositionalEncoding  Dup_out2  ReversibleSerial_in2_out2[    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm      ]      SelfAttention    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm        Dense_2048        Dropout        FastGelu        Dense_512        Dropout      ]    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm      ]      SelfAttention    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm        Dense_2048        Dropout        FastGelu        Dense_512        Dropout      ]    ]    ReversibleSwap_in2_out2  ]  Concatenate_in2  LayerNorm  Dropout  Dense_train  LogSoftmax]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_ReformerLM(ReformerLM)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><p><a name="ex05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>You will now write a function that takes in your model and trains it. </p><p><strong>Instructions:</strong> Implement the <code>training_loop</code> below to train the neural network above. Here is a list of things you should do:</p><ul><li>Create <code>TrainTask</code> and <code>EvalTask</code></li><li>Create the training loop <code>trax.supervised.training.Loop</code></li><li>Pass in the following depending to train_task :<ul><li><code>labeled_data=train_gen</code></li><li><code>loss_layer=tl.CrossEntropyLoss()</code></li><li><code>optimizer=trax.optimizers.Adam(0.01)</code></li><li><code>lr_schedule=lr_schedule</code></li><li><code>n_steps_per_checkpoint=10</code>  </li></ul></li></ul><p>You will be using your CrossEntropyLoss loss function with Adam optimizer. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam" target="_blank" rel="noopener">trax</a> documentation to get a full understanding. </p><ul><li>Pass in the following to eval_task:<ul><li><code>labeled_data=eval_gen</code></li><li><code>metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]</code></li></ul></li></ul><p>This function should return a <code>training.Loop</code> object. To read more about this check the <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop" target="_blank" rel="noopener">docs</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span><span class="params">(ReformerLM, train_gen, eval_gen, output_dir = <span class="string">"./model/"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you are building</span></span><br><span class="line"><span class="string">        train_gen (generator): train data generator.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Validation generator. </span></span><br><span class="line"><span class="string">        output_dir (string): Path to save the model output. Defaults to './model/'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop for the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># use the warmup_and_rsqrt_decay learning rate schedule</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(</span><br><span class="line">        n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the train task</span></span><br><span class="line">    train_task = training.TrainTask(            </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = train_gen,</span><br><span class="line">        <span class="comment"># loss layer</span></span><br><span class="line">        loss_layer = tl.CrossEntropyLoss(),</span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line">        optimizer=trax.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">        <span class="comment"># lr_schedule</span></span><br><span class="line">        lr_schedule=lr_schedule,</span><br><span class="line">        <span class="comment"># n_steps</span></span><br><span class="line">        n_steps_per_checkpoint=<span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define the eval task</span></span><br><span class="line">    eval_task = training.EvalTask(                      </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = eval_gen,</span><br><span class="line">        metrics = [tl.CrossEntropyLoss(), tl.Accuracy()]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    loop = training.Loop(ReformerLM(mode=<span class="string">'train'</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: Use the train task and eval task for grading train_model</span></span><br><span class="line">test_loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">train_task = test_loop._task</span><br><span class="line">eval_task = test_loop._eval_task</span><br><span class="line"></span><br><span class="line">print(train_task)</span><br><span class="line">print(eval_task)</span><br></pre></td></tr></table></figure><pre><code>&lt;trax.supervised.training.TrainTask object at 0x7fd4ddf95dd0&gt;&lt;trax.supervised.training.EvalTask object at 0x7fd4dc2a2f50&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_tasks(train_task, eval_task)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we will now test your function</span></span><br><span class="line">!rm -f model/model.pkl.gz</span><br><span class="line">loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>Step      1: Ran 1 train steps in 58.71 secsStep      1: train CrossEntropyLoss |  10.41530514Step      1: eval  CrossEntropyLoss |  10.41272354Step      1: eval          Accuracy |  0.00000000Step     10: Ran 9 train steps in 163.46 secsStep     10: train CrossEntropyLoss |  10.25675583Step     10: eval  CrossEntropyLoss |  9.94296360Step     10: eval          Accuracy |  0.11201393</code></pre><p><strong>Approximate Expected output:</strong>  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Step      1: Ran 1 train steps in 55.73 secs</span><br><span class="line">Step      1: train CrossEntropyLoss |  10.41907787</span><br><span class="line">Step      1: eval  CrossEntropyLoss |  10.41005802</span><br><span class="line">Step      1: eval          Accuracy |  0.00000000</span><br><span class="line"></span><br><span class="line">Step     10: Ran 9 train steps in 108.21 secs</span><br><span class="line">Step     10: train CrossEntropyLoss |  10.15449715</span><br><span class="line">Step     10: eval  CrossEntropyLoss |  9.63478279</span><br><span class="line">Step     10: eval          Accuracy |  0.16350447</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">&lt;a name=&quot;5&quot;&gt;&lt;/a&gt;</span><br><span class="line"># Part 5:   Decode from a pretrained model</span><br><span class="line"></span><br><span class="line">We will now proceed on decoding using the model architecture you just implemented. As in the previous weeks, we will be giving you a pretrained model so you can observe meaningful output during inference. You will be using the [autoregressive_sample_stream()](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample_stream) decoding method from Trax to do fast inference. Let&apos;s define a few parameters to initialize our model.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention</span><br><span class="line">def attention(*args, **kwargs):</span><br><span class="line">    # number of input positions to remember in a cache when doing fast inference. </span><br><span class="line">    kwargs[&apos;predict_mem_len&apos;] = 120</span><br><span class="line">    # number of input elements to drop once the fast inference input cache fills up.</span><br><span class="line">    kwargs[&apos;predict_drop_len&apos;] = 120</span><br><span class="line">    # return the attention layer with the parameters defined above</span><br><span class="line">    return tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"># define the model using the ReformerLM function you implemented earlier.</span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=33000,</span><br><span class="line">    n_layers=6,</span><br><span class="line">    mode=&apos;predict&apos;,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.</span><br><span class="line">shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)</span><br></pre></td></tr></table></figure><p>We can now initialize our model from a file containing the pretrained weights. We will save this starting state so we can reset the model state when we generate a new conversation. This will become clearer in the <code>generate_dialogue()</code> function later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize from file</span></span><br><span class="line">model.init_from_file(<span class="string">'chatbot_model1.pkl.gz'</span>,</span><br><span class="line">                     weights_only=<span class="keyword">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save the starting state</span></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure><p>Let’s define a few utility functions as well to help us tokenize and detokenize. We can use the <a href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize" target="_blank" rel="noopener">tokenize()</a> and <a href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize" target="_blank" rel="noopener">detokenize()</a> from <code>trax.data.tf_inputs</code> to do this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentence, vocab_file, vocab_dir)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span><span class="params">(tokens, vocab_file, vocab_dir)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)</span><br></pre></td></tr></table></figure><p>We are now ready to define our decoding function. This will return a generator that yields that next symbol output by the model. It will be able to predict the next words by just feeding it a starting sentence.</p><p><a name="ex06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement the function below to return a generator that predicts the next word of the conversation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM_output_gen</span><span class="params">(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create input tokens using the the tokenize function</span></span><br><span class="line">    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add batch dimension to array. Convert from (n,) to (x, n) where </span></span><br><span class="line">    <span class="comment"># x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)</span></span><br><span class="line">    input_tokens_with_batch = np.expand_dims(input_tokens, axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># call the autoregressive_sample_stream function from trax</span></span><br><span class="line">    output_gen = trax.supervised.decoding.autoregressive_sample_stream( </span><br><span class="line">        <span class="comment"># model</span></span><br><span class="line">        ReformerLM,</span><br><span class="line">        <span class="comment"># inputs will be the tokens with batch dimension</span></span><br><span class="line">        inputs = input_tokens_with_batch,</span><br><span class="line">        <span class="comment"># temperature</span></span><br><span class="line">        temperature = temperature</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output_gen</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">WEIGHTS_FROM_FILE = ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'weights'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    WEIGHTS_FROM_FILE = pickle.load(file)</span><br><span class="line"></span><br><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    kwargs[<span class="string">'predict_mem_len'</span>] = <span class="number">120</span></span><br><span class="line">    kwargs[<span class="string">'predict_drop_len'</span>] = <span class="number">120</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">test_model = ReformerLM(vocab_size=<span class="number">5</span>, n_layers=<span class="number">1</span>, mode=<span class="string">'predict'</span>, attention_type=attention)</span><br><span class="line"></span><br><span class="line">test_output_gen = ReformerLM_output_gen(test_model, <span class="string">"test"</span>, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_model.init_weights_and_state(shape11)</span><br><span class="line"></span><br><span class="line">test_model.weights = WEIGHTS_FROM_FILE</span><br><span class="line"></span><br><span class="line">output = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    output.append(next(test_output_gen)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> test_model </span><br><span class="line"><span class="keyword">del</span> WEIGHTS_FROM_FILE</span><br><span class="line"><span class="keyword">del</span> test_output_gen</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><pre><code>[1, 0, 4, 3, 0, 4]</code></pre><p><strong><em>Expected value:</em></strong></p><p>[1, 0, 4, 3, 0, 4]</p><p>Great! Now you will be able to see the model in action. The utility function below will call the generator you just implemented and will just format the output to be easier to read. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    kwargs[<span class="string">'predict_mem_len'</span>] = <span class="number">120</span>  <span class="comment"># max length for predictions</span></span><br><span class="line">    kwargs[<span class="string">'predict_drop_len'</span>] = <span class="number">120</span>  <span class="comment"># never drop old stuff</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=<span class="number">33000</span>,</span><br><span class="line">    n_layers=<span class="number">6</span>,</span><br><span class="line">    mode=<span class="string">'predict'</span>,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.init_from_file(<span class="string">'chatbot_model1.pkl.gz'</span>,</span><br><span class="line">                     weights_only=<span class="keyword">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_dialogue</span><span class="params">(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        model_state (np.array): initial state of the model before decoding</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        max_len (int): maximum number of tokens to generate </span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    """</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the delimiters we used during training</span></span><br><span class="line">    delimiter_1 = <span class="string">'Person 1: '</span> </span><br><span class="line">    delimiter_2 = <span class="string">'Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize detokenized output</span></span><br><span class="line">    sentence = <span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># token counter</span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output tokens. we insert a ': ' for formatting</span></span><br><span class="line">    result = [tokenize(<span class="string">': '</span>, vocab_file=vocab_file, vocab_dir=vocab_dir)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># reset the model state when starting a new dialogue</span></span><br><span class="line">    ReformerLM.state = model_state</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calls the output generator implemented earlier</span></span><br><span class="line">    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print the starting sentence</span></span><br><span class="line">    print(start_sentence.split(delimiter_2)[<span class="number">0</span>].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.</span></span><br><span class="line">    <span class="keyword">for</span> o <span class="keyword">in</span> output:</span><br><span class="line">        </span><br><span class="line">        result.append(o)</span><br><span class="line">        </span><br><span class="line">        sentence = detokenize(np.concatenate(result, axis=<span class="number">0</span>), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> sentence.endswith(delimiter_1):</span><br><span class="line">            sentence = sentence.split(delimiter_1)[<span class="number">0</span>]</span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;delimiter_2&#125;</span><span class="subst">&#123;sentence&#125;</span>'</span>)</span><br><span class="line">            sentence = <span class="string">''</span></span><br><span class="line">            result.clear()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> sentence.endswith(delimiter_2):</span><br><span class="line">            sentence = sentence.split(delimiter_2)[<span class="number">0</span>]</span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;delimiter_1&#125;</span><span class="subst">&#123;sentence&#125;</span>'</span>)</span><br><span class="line">            sentence = <span class="string">''</span></span><br><span class="line">            result.clear()</span><br><span class="line"></span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> counter &gt; max_len:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>We can now feed in different starting sentences and see how the model generates the dialogue. You can even input your own starting sentence. Just remember to ask a question that covers the topics in the Multiwoz dataset so you can generate a meaningful conversation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Are there theatres in town? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Are there theatres in town?Person 2: : There are 4 theatres in town. Do you have a preference on area? Person 1: No, I don&#39;t care. Which one do you recommend? Person 2: I would recommend the Mumford Theatre. Would you like more information on it? Person 1: Yes, could I get the postcode and phone number please? Person 2: The phone number is 08451962320 and the postcode is cb11pt. The phone number is 084519/ 15/15 - would you like to book a table? </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Is there a hospital nearby? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Is there a hospital nearby?Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need anything else? Person 1: No, that&#39;s all I need. Thanks. Person 2: You&#39;re welcome. Have a good day.Good bye.Person 1: Thanks again. Goodbye. Person 2: You&#39;re welcome. Have a good day.Good bye.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Can you book a taxi? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Can you book a taxi?Person 2: : I sure can. Where are you going? Person 1: I&#39;m going to be picked up from the city centre north b and b. Person 2: I have booked you a grey volkswagen. The contact number is 0783212843. Person 1: Thank you. That&#39;s all I need. Person 2: Thank you for using our services. Have a great day!k you.Good bye.Person 1: Actually, I&#39;ry about there. </code></pre><p><strong>Congratulations! You just wrapped up the final assignment of this course and the entire specialization!</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-4-Chatbot&quot;&gt;&lt;a href=&quot;#Assignment-4-Chatbot&quot; class=&quot;headerlink&quot; title=&quot;Assignment 4: Chatbot&quot;&gt;&lt;/a&gt;Assignment 4: Chatbot&lt;/h1
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
</feed>
