<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-04-15T00:25:53.860Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Fourier Transform</title>
    <link href="https://zhangruochi.com/Fourier-Transform/2020/04/14/"/>
    <id>https://zhangruochi.com/Fourier-Transform/2020/04/14/</id>
    <published>2020-04-15T00:24:47.000Z</published>
    <updated>2020-04-15T00:25:53.860Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Fourier-Transform"><a href="#Fourier-Transform" class="headerlink" title="Fourier Transform"></a>Fourier Transform</h2><p>The Fourier transform (FT) decomposes a function (often a function of time, or a signal) into its constituent frequencies.</p><p>One motivation for the Fourier transform comes from the study of Fourier series. In the study of <strong>Fourier series</strong>, complicated but periodic functions are written as the sum of simple waves mathematically represented by sines and cosines. The Fourier transform is an extension of the Fourier series that results when the period of the represented function is lengthened and allowed to approach infinity.</p><p>The Fourier transform of a function f is traditionally denoted $\hat{f}$, by adding a circumflex to the symbol of the function. There are several common conventions for defining the Fourier transform of an integrable function</p><script type="math/tex; mode=display">\hat{f}(\xi) = \int_{-\infty}^{\infty} f(x)\ e^{-2\pi i x \xi}\,dx</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>If we want to describe a signal, we need three things :</p><ul><li>The <strong>frequency</strong> of the signal which shows, how many occurrences in the period we have.</li><li><strong>Amplitude</strong> which shows the height of the signal or in other terms the strength of the signal.</li><li><strong>Phase shift</strong> as to where does the signal starts.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy.fft <span class="keyword">import</span> fft</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_wave</span> <span class="params">(freq, amp, T, sr)</span>:</span></span><br><span class="line"></span><br><span class="line">    time = np.arange(<span class="number">0</span>,T,<span class="number">1</span>/sr)</span><br><span class="line">    </span><br><span class="line">    X = amp*np.sin(<span class="number">2</span>*np.pi*freq*time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> time,X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">"seaborn"</span>)</span><br><span class="line">plt.rcParams[<span class="string">"xtick.labelsize"</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">"ytick.labelsize"</span>] = <span class="number">14</span></span><br><span class="line"></span><br><span class="line">f, axarr = plt.subplots(<span class="number">2</span>, figsize=(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">sr=<span class="number">50</span> <span class="comment">#in Hz</span></span><br><span class="line"></span><br><span class="line">x,y   = gen_wave(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,sr)</span><br><span class="line">x,y2   = gen_wave(<span class="number">5</span>,<span class="number">3</span>,<span class="number">1</span>,sr)</span><br><span class="line"></span><br><span class="line">y = y + y2</span><br><span class="line"></span><br><span class="line">axarr[<span class="number">0</span>].plot(x, y)</span><br><span class="line"></span><br><span class="line">n = len(y) </span><br><span class="line">p = fft(y) <span class="comment"># take the fourier transform </span></span><br><span class="line"></span><br><span class="line">mag = np.sqrt(p.real**<span class="number">2</span> + p.imag**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">mag = mag * <span class="number">2</span> / n</span><br><span class="line"></span><br><span class="line">mag = mag[<span class="number">0</span>:math.ceil((n)/<span class="number">2.0</span>)]</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, len(mag), <span class="number">1.0</span>) * (sr / n)</span><br><span class="line"></span><br><span class="line">axarr[<span class="number">1</span>].bar(x, mag, color=<span class="string">'b'</span>)</span><br><span class="line">axarr[<span class="number">1</span>].xaxis.set_ticks(np.arange(min(x), max(x)+<span class="number">1</span>, <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_2_0.png" alt="png"></p><h2 id="Decompose-the-cord"><a href="#Decompose-the-cord" class="headerlink" title="Decompose the cord"></a>Decompose the cord</h2><p>A special case is the expression of a musical chord in terms of the volumes and frequencies of its constituent notes.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(C) 2018 Nikolay Manchev</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This work is licensed under the Creative Commons Attribution 4.0 International</span></span><br><span class="line"><span class="string">License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> wavfile</span><br><span class="line"></span><br><span class="line">fs, snd = wavfile.read(<span class="string">"output.wav"</span>)</span><br><span class="line"></span><br><span class="line">snd = snd / (<span class="number">2.</span>**<span class="number">15</span>)</span><br><span class="line">s1 = snd[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line">plt.style.use(<span class="string">"seaborn"</span>)</span><br><span class="line"></span><br><span class="line">time = np.arange(<span class="number">0</span>, s1.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">time = (time / fs) * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">plt.plot(time, s1, color=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'Amplitude'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Time (ms)'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xticks(fontsize=<span class="number">14</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_5_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(C) 2018 Nikolay Manchev</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This work is licensed under the Creative Commons Attribution 4.0 International</span></span><br><span class="line"><span class="string">License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> wavfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy.fft <span class="keyword">import</span> fft</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">"xtick.labelsize"</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">"ytick.labelsize"</span>] = <span class="number">14</span></span><br><span class="line">plt.style.use(<span class="string">"seaborn"</span>)</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">800</span></span><br><span class="line">fs, snd = wavfile.read(<span class="string">"output.wav"</span>)</span><br><span class="line">y = snd[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">n = len(y) </span><br><span class="line">p = fft(y) </span><br><span class="line"></span><br><span class="line">mag = np.sqrt(p.real**<span class="number">2</span> + p.imag**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">mag = mag * <span class="number">2</span> / n</span><br><span class="line"></span><br><span class="line">mag = mag[<span class="number">0</span>:math.ceil((n)/<span class="number">2.0</span>)]</span><br><span class="line"></span><br><span class="line">freq = np.arange(<span class="number">0</span>, len(mag), <span class="number">1.0</span>) * (fs / n)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> threshold != <span class="number">0</span>:</span><br><span class="line">    print(np.unique(np.rint(freq[np.in1d(mag, mag[mag&gt;threshold])])))</span><br><span class="line">    mag[mag&lt;threshold]=threshold</span><br><span class="line"></span><br><span class="line">plt.plot(freq/<span class="number">1000</span>, mag, color=<span class="string">'b'</span>)</span><br><span class="line">plt.xticks(np.arange(min(freq/<span class="number">1000</span>), max(freq/<span class="number">1000</span>)+<span class="number">1</span>, <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>[329. 330. 415. 555.]</code></pre><p><img src="output_6_1.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Fourier-Transform&quot;&gt;&lt;a href=&quot;#Fourier-Transform&quot; class=&quot;headerlink&quot; title=&quot;Fourier Transform&quot;&gt;&lt;/a&gt;Fourier Transform&lt;/h2&gt;&lt;p&gt;The Fourie
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Signal Processing" scheme="https://zhangruochi.com/tags/Signal-Processing/"/>
    
  </entry>
  
  <entry>
    <title>PCA from scratch</title>
    <link href="https://zhangruochi.com/PCA-from-scratch/2020/04/14/"/>
    <id>https://zhangruochi.com/PCA-from-scratch/2020/04/14/</id>
    <published>2020-04-14T17:32:41.000Z</published>
    <updated>2020-04-14T17:33:44.799Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_arrow</span><span class="params">(ax, start, stop)</span>:</span></span><br><span class="line">    ax.annotate(<span class="string">''</span>, xytext=start, xy=stop,                 </span><br><span class="line">                arrowprops=dict(facecolor=<span class="string">'red'</span>, width=<span class="number">2.0</span>))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr_vars</span><span class="params">( start=<span class="number">-10</span>, stop=<span class="number">10</span>, step=<span class="number">0.5</span>, mu=<span class="number">0</span>, sigma=<span class="number">3</span>, func=lambda x: x )</span>:</span></span><br><span class="line">    x = np.arange(start, stop, step)    </span><br><span class="line">    e = np.random.normal(mu, sigma, x.size)</span><br><span class="line">    y = np.zeros(x.size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> range(x.size):</span><br><span class="line">        y[ind] = func(x[ind]) + e[ind]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (x,y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">(x1,x2) = corr_vars(start=<span class="number">2</span>, stop=<span class="number">4</span>, step=<span class="number">0.2</span>, sigma=<span class="number">2</span>, func=<span class="keyword">lambda</span> x: <span class="number">2</span>*math.sin(x))</span><br><span class="line"></span><br><span class="line">A = np.column_stack((x1,x2))</span><br><span class="line"></span><br><span class="line">Aorig = A</span><br><span class="line"></span><br><span class="line">A</span><br></pre></td></tr></table></figure><pre><code>array([[ 2.        , -1.68093609],       [ 2.2       ,  2.30235361],       [ 2.4       ,  3.65699797],       [ 2.6       ,  0.52613067],       [ 2.8       ,  2.63261787],       [ 3.        ,  1.3106777 ],       [ 3.2       ,  0.32561105],       [ 3.4       , -2.65116887],       [ 3.6       , -1.26403255],       [ 3.8       , -0.71371289]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">A = (A-np.mean(A,axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the original matrix</span></span><br><span class="line">f, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">7</span>,<span class="number">4</span>))</span><br><span class="line">ax1.scatter(Aorig[:,<span class="number">0</span>], Aorig[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Original data"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the centered data</span></span><br><span class="line">ax2.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax2.set_title(<span class="string">"Centered data"</span>)</span><br><span class="line">ax2.grid(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">ax1.axhline(<span class="number">0</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">ax1.axvline(<span class="number">0</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">ax2.axhline(<span class="number">0</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">ax2.axvline(<span class="number">0</span>, color=<span class="string">"blue"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">-2</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>(-4, 5)</code></pre><p><img src="output_2_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">S = np.dot(A.T,A)/(A.shape[<span class="number">0</span>]<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"The covariance matrix is:"</span>)</span><br><span class="line">print(S,<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure><pre><code>The covariance matrix is:[[ 0.36666667 -0.55248919] [-0.55248919  4.18798554]] </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">V = np.array([[<span class="number">-1</span>],[<span class="number">0</span>]])</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">ax1.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Vector [-1,1]"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.plot([<span class="number">0</span>,V[<span class="number">0</span>]],[<span class="number">0</span>,V[<span class="number">1</span>]],c=<span class="string">'r'</span>)</span><br><span class="line">plot_arrow(ax1, (<span class="number">0</span>,<span class="number">0</span>),(V[<span class="number">0</span>],V[<span class="number">1</span>]))</span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-5</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Vector slope:  [-0.](-5, 5)</code></pre><p><img src="output_4_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">ax1.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Vector [-1,1]"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.plot([<span class="number">0</span>,V[<span class="number">0</span>]],[<span class="number">0</span>,V[<span class="number">1</span>]],c=<span class="string">'r'</span>)</span><br><span class="line">plot_arrow(ax1, (<span class="number">0</span>,<span class="number">0</span>),(V[<span class="number">0</span>],V[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-5</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Vector slope:  [-1.50678871](-5, 5)</code></pre><p><img src="output_5_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">ax1.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Vector [-1,1]"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.plot([<span class="number">0</span>,V[<span class="number">0</span>]],[<span class="number">0</span>,V[<span class="number">1</span>]],c=<span class="string">'r'</span>)</span><br><span class="line">plot_arrow(ax1, (<span class="number">0</span>,<span class="number">0</span>),(V[<span class="number">0</span>],V[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-5</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Vector slope:  [-5.72313052](-5, 5)</code></pre><p><img src="output_6_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">ax1.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Vector [-1,1]"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.plot([<span class="number">0</span>,V[<span class="number">0</span>]],[<span class="number">0</span>,V[<span class="number">1</span>]],c=<span class="string">'r'</span>)</span><br><span class="line">plot_arrow(ax1, (<span class="number">0</span>,<span class="number">0</span>),(V[<span class="number">0</span>],V[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-5</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Vector slope:  [-6.94911232](-5, 5)</code></pre><p><img src="output_7_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"The slope of the vector converges to the direction of greatest variance:\n"</span>)</span><br><span class="line"></span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>The slope of the vector converges to the direction of greatest variance:Vector slope:  [-7.0507464]Vector slope:  [-7.0577219]Vector slope:  [-7.05819391]Vector slope:  [-7.05822582]Vector slope:  [-7.05822798]Vector slope:  [-7.05822813]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://en.wikipedia.org/wiki/Eigenvalue_algorithm</span></span><br><span class="line"></span><br><span class="line">l_1 = (S.trace() + np.sqrt(pow(S.trace(),<span class="number">2</span>) - <span class="number">4</span>*np.linalg.det(S))) / <span class="number">2</span></span><br><span class="line">l_2 = (S.trace() - np.sqrt(pow(S.trace(),<span class="number">2</span>) - <span class="number">4</span>*np.linalg.det(S))) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"The eigenvalues are:"</span>)</span><br><span class="line">print(<span class="string">"L1:"</span>,l_1)</span><br><span class="line">print(<span class="string">"L2:"</span>,l_2)</span><br></pre></td></tr></table></figure><pre><code>The eigenvalues are:L1: 4.266261447240239L2: 0.28839076171131417</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cayley-Hamilton theorem</span></span><br><span class="line"><span class="comment"># (A - 位1I )(A - 位2I ) = (A - 位2I )(A - 位1I ) = 0</span></span><br><span class="line"></span><br><span class="line">A1 = S - l_1 * np.identity(<span class="number">2</span>)</span><br><span class="line">A2 = S - l_2 * np.identity(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">E1 = A2[:,<span class="number">1</span>]</span><br><span class="line">E2 = A1[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">E1 = E1 / np.linalg.norm(E1)</span><br><span class="line">E2 = E2 / np.linalg.norm(E2)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"The eigenvectors are:"</span>)</span><br><span class="line">print(<span class="string">"E1:"</span>, E1)</span><br><span class="line">print(<span class="string">"E2:"</span>, E2)</span><br></pre></td></tr></table></figure><pre><code>The eigenvectors are:E1: [-0.14027773  0.9901122 ]E2: [-0.9901122  -0.14027773]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">E = np.column_stack((E1,E2))</span><br><span class="line"></span><br><span class="line">E</span><br></pre></td></tr></table></figure><pre><code>array([[-0.14027773, -0.9901122 ],       [ 0.9901122 , -0.14027773]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">evals, evecs = np.linalg.eigh(S)</span><br><span class="line"></span><br><span class="line">print(evals)</span><br><span class="line">print(evecs)</span><br></pre></td></tr></table></figure><pre><code>[0.28839076 4.26626145][[-0.9901122  -0.14027773] [-0.14027773  0.9901122 ]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2, ax3) = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax1.scatter(Aorig[:,<span class="number">0</span>],Aorig[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Original Data"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.set_aspect(<span class="string">'equal'</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Aorig[:,<span class="number">0</span>],Aorig[:,<span class="number">1</span>])</span><br><span class="line">ax2.set_title(<span class="string">"E1"</span>)</span><br><span class="line">ax2.grid(<span class="keyword">True</span>)</span><br><span class="line">plot_arrow(ax2, np.mean(Aorig,axis=<span class="number">0</span>), np.mean(Aorig,axis=<span class="number">0</span>) + np.dot(Aorig, E).std(axis=<span class="number">0</span>).mean() * E1)</span><br><span class="line">ax2.set_aspect(<span class="string">'equal'</span>)               </span><br><span class="line"></span><br><span class="line">ax3.scatter(Aorig[:,<span class="number">0</span>],Aorig[:,<span class="number">1</span>])</span><br><span class="line">ax3.set_title(<span class="string">"E1 and E2"</span>)</span><br><span class="line">ax3.grid(<span class="keyword">True</span>)</span><br><span class="line">plot_arrow(ax3, np.mean(Aorig,axis=<span class="number">0</span>), np.mean(Aorig,axis=<span class="number">0</span>) + np.dot(Aorig, E).std(axis=<span class="number">0</span>).mean() * E1)</span><br><span class="line">plot_arrow(ax3, np.mean(Aorig,axis=<span class="number">0</span>), np.mean(Aorig,axis=<span class="number">0</span>) + np.dot(Aorig, E).std(axis=<span class="number">0</span>).mean() * E2)</span><br><span class="line">ax3.set_aspect(<span class="string">'equal'</span>)               </span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">0</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>(-4, 5)</code></pre><p><img src="output_13_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">F1 = np.dot(A, E1)</span><br><span class="line">F2 = np.dot(A, E2)</span><br><span class="line"></span><br><span class="line">F = np.column_stack((F1, F2))</span><br><span class="line">F</span><br></pre></td></tr></table></figure><pre><code>array([[-1.97812455,  1.18924584],       [ 1.93772363,  0.43245658],       [ 3.25091797,  0.04440771],       [ 0.12295254,  0.28557622],       [ 2.18055566, -0.20793946],       [ 0.84363103, -0.22052313],       [-0.15975102, -0.28036266],       [-3.13515266, -0.06080918],       [-1.78978762, -0.45341595],       [-1.27296497, -0.72863598]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pca = decomposition.PCA(n_components=<span class="number">2</span>)</span><br><span class="line">print(pca.fit_transform(A))</span><br></pre></td></tr></table></figure><pre><code>[[-1.97812455  1.18924584] [ 1.93772363  0.43245658] [ 3.25091797  0.04440771] [ 0.12295254  0.28557622] [ 2.18055566 -0.20793946] [ 0.84363103 -0.22052313] [-0.15975102 -0.28036266] [-3.13515266 -0.06080918] [-1.78978762 -0.45341595] [-1.27296497 -0.72863598]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Unsupervised Learning" scheme="https://zhangruochi.com/tags/Unsupervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>DBSCAN Clustering</title>
    <link href="https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/"/>
    <id>https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/</id>
    <published>2020-04-14T05:42:51.000Z</published>
    <updated>2020-04-14T05:54:17.806Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Intution"><a href="#Intution" class="headerlink" title="Intution"></a>Intution</h3><p>Partitioning methods (K-means, PAM clustering) and hierarchical clustering work for finding <strong>spherical-shaped</strong> clusters or convex clusters. In other words, they are suitable only for compact and well-separated clusters. Moreover, they are also severely affected by the presence of <strong>noise</strong> and <strong>outliers</strong> in the data.</p><p>Real life data may contain irregularities, like:</p><ul><li>Clusters can be of arbitrary shape such as those shown in the figure below.</li><li>Data may contain noise.</li></ul><p>The figure below shows a data set containing nonconvex clusters and outliers/noises. Given such data, k-means algorithm has difficulties for identifying these clusters with arbitrary shapes.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%"></center><h3 id="DBSCAN-algorithm-requires-two-parameters"><a href="#DBSCAN-algorithm-requires-two-parameters" class="headerlink" title="DBSCAN algorithm requires two parameters"></a>DBSCAN algorithm requires two parameters</h3><ol><li><strong>eps</strong>: It defines the neighborhood around a data point i.e. if the distance between two points is lower or equal to <code>eps</code> then they are considered as neighbors. If the eps value is chosen too small then large part of the data will be considered as outliers. If it is chosen very large then the clusters will merge and majority of the data points will be in the same clusters. One way to find the eps value is based on the k-distance graph.</li><li><strong>MinPts</strong>: Minimum number of neighbors (data points) within <code>eps</code> radius. Larger the dataset, the larger value of MinPts must be chosen. As a general rule, the minimum MinPts can be derived from the number of dimensions D in the dataset as, MinPts &gt;= D+1. The minimum value of MinPts must be chosen at least 3.</li></ol><h3 id="In-this-algorithm-we-have-3-types-of-data-points"><a href="#In-this-algorithm-we-have-3-types-of-data-points" class="headerlink" title="In this algorithm, we have 3 types of data points."></a>In this algorithm, we have 3 types of data points.</h3><ul><li><strong>Core Point</strong>: A point is a core point if it has more than MinPts points within eps.</li><li><strong>Border Point</strong>: A point which has fewer than MinPts within eps but it is in the neighborhood of a core point.</li><li><strong>Noise or outlier</strong>: A point which is not a core point or border point.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center><h4 id="DBSCAN-algorithm-can-be-abstracted-in-the-following-steps"><a href="#DBSCAN-algorithm-can-be-abstracted-in-the-following-steps" class="headerlink" title="DBSCAN algorithm can be abstracted in the following steps"></a>DBSCAN algorithm can be abstracted in the following steps</h4><ul><li>Find all the neighbor points within eps and identify the core points or visited with more than MinPts neighbors.</li><li>For each core point if it is not already assigned to a cluster, create a new cluster.</li><li>Find recursively all its density connected points and assign them to the same cluster as the core point.<ul><li>A point a and b are said to be density connected if there exist a point c which has a sufficient number of points in its neighbors and both the points a and b are within the eps distance. This is a chaining process. So, if b is neighbor of c, c is neighbor of d, d is neighbor of e, which in turn is neighbor of a implies that b is neighbor of a.</li></ul></li><li>Iterate through the remaining unvisited points in the dataset. Those points that do not belong to any cluster are noise.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Intution&quot;&gt;&lt;a href=&quot;#Intution&quot; class=&quot;headerlink&quot; title=&quot;Intution&quot;&gt;&lt;/a&gt;Intution&lt;/h3&gt;&lt;p&gt;Partitioning methods (K-means, PAM clustering)
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Clustering" scheme="https://zhangruochi.com/tags/Clustering/"/>
    
  </entry>
  
  <entry>
    <title>SparkML Examples</title>
    <link href="https://zhangruochi.com/SparkML-Examples/2020/04/14/"/>
    <id>https://zhangruochi.com/SparkML-Examples/2020/04/14/</id>
    <published>2020-04-14T04:58:54.000Z</published>
    <updated>2020-04-14T05:59:09.690Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/IBM/coursera/raw/master/hmp.parquet</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a dataframe out of it</span></span><br><span class="line">df = spark.read.parquet(<span class="string">'hmp.parquet'</span>)</span><br><span class="line"><span class="comment"># two class</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">'df'</span>)</span><br><span class="line">df_two_class = spark.sql(<span class="string">"select * from df where class in ('Use_telephone','Standup_chair')"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train test split</span></span><br><span class="line">splits = df_two_class.randomSplit([<span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line">df_train = splits[<span class="number">0</span>]</span><br><span class="line">df_test = splits[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># transformer</span></span><br><span class="line">indexer = StringIndexer(inputCol=<span class="string">"class"</span>, outputCol=<span class="string">"label"</span>)</span><br><span class="line">encoder = OneHotEncoder(inputCol=<span class="string">"label"</span>, outputCol=<span class="string">"labelVec"</span>)</span><br><span class="line">vectorAssembler = VectorAssembler(inputCols=[<span class="string">"x"</span>,<span class="string">"y"</span>,<span class="string">"z"</span>],</span><br><span class="line">                                  outputCol=<span class="string">"features"</span>)</span><br><span class="line">normalizer = MinMaxScaler(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"features_norm"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># modeling</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LinearSVC</span><br><span class="line">lsvc = LinearSVC(maxIter=<span class="number">10</span>, regParam=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## pipeline </span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,lsvc])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># fit and predict</span></span><br><span class="line">model = pipeline.fit(df_train)</span><br><span class="line">prediction = model.transform(df_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate model</span></span><br><span class="line">evaluator = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"rawPrediction"</span>)</span><br><span class="line">evaluator.evaluate(prediction)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># testing</span></span><br><span class="line">prediction = model.transform(df_test)</span><br><span class="line">evaluator = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"rawPrediction"</span>)</span><br><span class="line">evaluator.evaluate(prediction)</span><br></pre></td></tr></table></figure><h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a dataframe out of it</span></span><br><span class="line">df = spark.read.parquet(<span class="string">'hmp.parquet'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># register a corresponding query table</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">'df'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line">vectorAssembler = VectorAssembler(inputCols=[<span class="string">"x"</span>,<span class="string">"y"</span>,<span class="string">"z"</span>],</span><br><span class="line">                                  outputCol=<span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.clustering <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">kmeans = KMeans().setK(<span class="number">13</span>).setSeed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline(stages=[vectorAssembler, kmeans])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df.createOrReplaceTempView(<span class="string">'df'</span>)</span><br><span class="line">df = spark.sql(<span class="string">"select * from df where class in ('Brush_teeth','Climb_stairs')"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = pipeline.fit(df)</span><br><span class="line"></span><br><span class="line">wssse = model.stages[<span class="number">1</span>].computeCost(vectorAssembler.transform(df))</span><br><span class="line">print(<span class="string">"Within Set Sum of Squared Errors = "</span> + str(wssse))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Supervised-Learning&quot;&gt;&lt;a href=&quot;#Supervised-Learning&quot; class=&quot;headerlink&quot; title=&quot;Supervised Learning&quot;&gt;&lt;/a&gt;Supervised Learning&lt;/h3&gt;&lt;figu
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Pipeline" scheme="https://zhangruochi.com/categories/AI-Workflow/Pipeline/"/>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning Pipeline</title>
    <link href="https://zhangruochi.com/Machine-Learning-Pipeline/2020/04/13/"/>
    <id>https://zhangruochi.com/Machine-Learning-Pipeline/2020/04/13/</id>
    <published>2020-04-13T20:12:30.000Z</published>
    <updated>2020-04-13T23:11:28.964Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scikit-learn-Pipeline"><a href="#Scikit-learn-Pipeline" class="headerlink" title="Scikit-learn Pipeline"></a>Scikit-learn Pipeline</h2><h3 id="Pipeline-1"><a href="#Pipeline-1" class="headerlink" title="Pipeline 1"></a>Pipeline 1</h3><p>In most machine learning projects the data that you have to work with is unlikely to be in the ideal format for producing the best performing model. There are quite often a number of transformational steps such as encoding categorical variables, feature scaling and normalisation that need to be performed. Scikit-learn has built in functions for most of these commonly used transformations in its <code>preprocessing</code> package.</p><p>However, in a typical machine learning workflow you will need to apply all these transformations at <code>least twice</code>. Once when training the model and again on any new data you want to predict on. Of course you could write a function to apply them and reuse that but you would still need to run this first and then call the model separately. Scikit-learn pipelines are a tool to simplify this process. They have several key benefits:</p><ul><li>They make your workflow much easier to read and understand.</li><li>They enforce the implementation and order of steps in your project.</li><li>These in turn make your work much more reproducible.</li></ul><p>Before building the pipeline I am splitting the training data into a train and test set so that I can validate the performance of the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = train.drop(<span class="string">'Target'</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = train[<span class="string">'Target'</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>The first step in building the pipeline is to define each <code>transformer type</code>. The convention here is generally to create transformers for the different variable types.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, OneHotEncoder</span><br><span class="line"></span><br><span class="line">numeric_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">'imputer'</span>, SimpleImputer(strategy=<span class="string">'median'</span>)),</span><br><span class="line">    (<span class="string">'scaler'</span>, StandardScaler())])</span><br><span class="line"></span><br><span class="line">categorical_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">'imputer'</span>, SimpleImputer(strategy=<span class="string">'constant'</span>, fill_value=<span class="string">'missing'</span>)),</span><br><span class="line">    (<span class="string">'onehot'</span>, OneHotEncoder(handle_unknown=<span class="string">'ignore'</span>))])</span><br></pre></td></tr></table></figure><p>Next we use the <code>ColumnTransformer</code> to apply the transformations to the correct columns in the dataframe. Before building this I have stored lists of the numeric and categorical columns using the pandas dtype method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">numeric_features = train.select_dtypes(include=[<span class="string">'int64'</span>, <span class="string">'float64'</span>]).columns</span><br><span class="line"></span><br><span class="line">categorical_features = train.select_dtypes(include=[<span class="string">'object'</span>]).drop([<span class="string">'Loan_Status'</span>], axis=<span class="number">1</span>).columns</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">'num'</span>, numeric_transformer, numeric_features),</span><br><span class="line">        (<span class="string">'cat'</span>, categorical_transformer, categorical_features)])</span><br></pre></td></tr></table></figure><p>The next step is to create a pipeline that combines the preprocessor created above with a classifier. In this case I have used a simple RandomForestClassifier to start with.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rf = Pipeline(steps=[(<span class="string">'preprocessor'</span>, preprocessor),</span><br><span class="line">                      (<span class="string">'classifier'</span>, RandomForestClassifier())])</span><br></pre></td></tr></table></figure><p>Fitting the classifier</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rf.fit(X_train, y_train)</span><br><span class="line">y_pred = rf.predict(X_test)</span><br></pre></td></tr></table></figure><p>A pipeline can also be used during the model selection process. The following example code loops through a number of scikit-learn classifiers applying the transformations and training the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, log_loss</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC, NuSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line">classifiers = [</span><br><span class="line">    KNeighborsClassifier(<span class="number">3</span>),</span><br><span class="line">    SVC(kernel=<span class="string">"rbf"</span>, C=<span class="number">0.025</span>, probability=<span class="keyword">True</span>),</span><br><span class="line">    NuSVC(probability=<span class="keyword">True</span>),</span><br><span class="line">    DecisionTreeClassifier(),</span><br><span class="line">    RandomForestClassifier(),</span><br><span class="line">    AdaBoostClassifier(),</span><br><span class="line">    GradientBoostingClassifier()</span><br><span class="line">    ]</span><br><span class="line"><span class="keyword">for</span> classifier <span class="keyword">in</span> classifiers:</span><br><span class="line">    pipe = Pipeline(steps=[(<span class="string">'preprocessor'</span>, preprocessor),</span><br><span class="line">                      (<span class="string">'classifier'</span>, classifier)])</span><br><span class="line">    pipe.fit(X_train, y_train)   </span><br><span class="line">    print(classifier)</span><br><span class="line">    print(<span class="string">"model score: %.3f"</span> % pipe.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>The pipeline can also be used in grid search to find the best performing parameters. To do this you first need to create a parameter grid for your chosen model. One important thing to note is that you need to <code>append</code> the name that you have given the classifier part of your pipeline to each parameter name. In my code above I have called this classifier so I have added <code>classifier__</code> to each parameter. Next I created a grid search object which includes the original pipeline. When I then call fit, the transformations are applied to the data, before a cross-validated grid-search is performed over the parameter grid.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">param_grid = &#123; </span><br><span class="line">    <span class="string">'classifier__n_estimators'</span>: [<span class="number">200</span>, <span class="number">500</span>],</span><br><span class="line">    <span class="string">'classifier__max_features'</span>: [<span class="string">'auto'</span>, <span class="string">'sqrt'</span>, <span class="string">'log2'</span>],</span><br><span class="line">    <span class="string">'classifier__max_depth'</span> : [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">    <span class="string">'classifier__criterion'</span> :[<span class="string">'gini'</span>, <span class="string">'entropy'</span>]&#125;</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">CV = GridSearchCV(rf, param_grid, n_jobs= <span class="number">1</span>)</span><br><span class="line">                  </span><br><span class="line">CV.fit(X_train, y_train)  </span><br><span class="line">print(CV.best_params_)    </span><br><span class="line">print(CV.best_score_)</span><br></pre></td></tr></table></figure><h3 id="Pipeline-2"><a href="#Pipeline-2" class="headerlink" title="Pipeline 2"></a>Pipeline 2</h3><p>The example below demonstrates the pipeline defined with four steps:</p><ul><li>Feature Extraction with Principal Component Analysis (3 features)</li><li>Feature Extraction with Statistical Selection (6 features)</li><li>Feature Union</li><li>Learn a Logistic Regression Model</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> FeatureUnion</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">url = <span class="string">"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>, <span class="string">'class'</span>]</span><br><span class="line">dataframe = read_csv(url, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:,<span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line">Y = array[:,<span class="number">8</span>]</span><br><span class="line"><span class="comment"># create feature union</span></span><br><span class="line">features = []</span><br><span class="line">features.append((<span class="string">'pca'</span>, PCA(n_components=<span class="number">3</span>)))</span><br><span class="line">features.append((<span class="string">'select_best'</span>, SelectKBest(k=<span class="number">6</span>)))</span><br><span class="line">feature_union = FeatureUnion(features)</span><br><span class="line"><span class="comment"># create pipeline</span></span><br><span class="line">estimators = []</span><br><span class="line">estimators.append((<span class="string">'feature_union'</span>, feature_union))</span><br><span class="line">estimators.append((<span class="string">'logistic'</span>, LogisticRegression()))</span><br><span class="line">model = Pipeline(estimators)</span><br><span class="line"><span class="comment"># evaluate pipeline</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">kfold = KFold(n_splits=<span class="number">10</span>, random_state=seed)</span><br><span class="line">results = cross_val_score(model, X, Y, cv=kfold)</span><br><span class="line">print(results.mean())</span><br></pre></td></tr></table></figure><h3 id="Building-Scikit-Learn-transformers"><a href="#Building-Scikit-Learn-transformers" class="headerlink" title="Building Scikit-Learn transformers"></a>Building Scikit-Learn transformers</h3><p>Scikit-Learns API uses <code>duck typing</code>: if you want to write your own custom estimators (including transformers and predictors), you only need to implement the right methods, you dont have to inherit from any particular class.</p><p>For example, all <strong>estimators</strong> must implement a <code>fit()</code> method, and <code>get_params()</code> and <code>set_params()</code> methods. All <strong>transformers</strong> must also implement <code>transform()</code> and <code>fit_transform()</code> methods. All <strong>predictors</strong> must implement a <code>predict()</code> method. And so on.</p><p>The most basic implementation of the <code>fit_transform()</code> method is just this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.fit(X, y).transform(X, y)</span><br></pre></td></tr></table></figure></p><p>You dont have to inherit from the <code>TransformerMixin</code> class, but thats what you get if you do: if you implement the <code>fit()</code> method and the <code>transform()</code> method, it gives you the <code>fit_transform()</code> method for free, just like the above.</p><p>Similarly, the BaseEstimator class just gives you the <code>get_params()</code> and <code>set_params()</code> methods for free. By default, <code>get_params()</code> does some introspection to get the parameters of the constructor <strong>init</strong>(), and it assumes that the class has corresponding instance variables. For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyEstimator</span><span class="params">(BaseEstimator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a, b=<span class="number">2</span>)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m = MyEstimator(1, 2)</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m.get_params()</span></span><br><span class="line"><span class="comment"># &#123;'a': 1, 'b': 2&#125;</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m.set_params(a=5, b=10)</span></span><br><span class="line"><span class="comment"># MyEstimator(a=5, b=10)</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m.a</span></span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m.b</span></span><br><span class="line"><span class="comment"># 10</span></span><br></pre></td></tr></table></figure><p>Lets say I have a lot of text and I want to extract certain data from it. Im going to build a featurizer that takes a list of functions, calls each function with our text, and returns the results of all functions as a feature vector.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longest_run_of_capitol_letters_feature</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""Find the longest run of capitol letters and return their length."""</span></span><br><span class="line">    runs = sorted(re.findall(<span class="string">r"[A-Z]+"</span>, text), key=len)</span><br><span class="line">    <span class="keyword">if</span> runs:</span><br><span class="line">        <span class="keyword">return</span> len(runs[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">percent_character_feature</span><span class="params">(char)</span>:</span></span><br><span class="line">    <span class="string">"""Return percentage of text that is a particular char compared to total</span></span><br><span class="line"><span class="string">    text length."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feature_fn</span><span class="params">(text)</span>:</span></span><br><span class="line">        periods = text.count(char)</span><br><span class="line">        <span class="keyword">return</span> periods / len(text)</span><br><span class="line">    <span class="keyword">return</span> feature_fn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FunctionFeaturizer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *featurizers)</span>:</span></span><br><span class="line">        self.featurizers = featurizers</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="string">"""All SciKit-Learn compatible transformers and classifiers have the</span></span><br><span class="line"><span class="string">        same interface. `fit` always returns the same object."""</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Given a list of original data, return a list of feature vectors."""</span></span><br><span class="line">        fvs = []</span><br><span class="line">        <span class="keyword">for</span> datum <span class="keyword">in</span> X:</span><br><span class="line">            fv = [f(datum) <span class="keyword">for</span> f <span class="keyword">in</span> self.featurizers]</span><br><span class="line">            fvs.append(fv)</span><br><span class="line">        <span class="keyword">return</span> np.array(fvs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">sms_featurizer = FunctionFeaturizer(longest_run_of_capitol_letters_feature,</span><br><span class="line">                                    percent_character_feature(<span class="string">"."</span>))</span><br><span class="line"><span class="comment"># sms_featurizer.transform(sms_data[:10])</span></span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(sms_data, sms_results)</span><br><span class="line"></span><br><span class="line">pipe = make_pipeline(sms_featurizer, DecisionTreeClassifier())</span><br><span class="line">pipe.fit(X_train, y_train)</span><br><span class="line">pipe.score(X_test, y_test)</span><br><span class="line"><span class="comment"># =&gt; 0.91385498923187369</span></span><br></pre></td></tr></table></figure><h2 id="SparkML-Pipeline"><a href="#SparkML-Pipeline" class="headerlink" title="SparkML Pipeline"></a>SparkML Pipeline</h2><h3 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a dataframe out of it</span></span><br><span class="line">df = spark.read.parquet(<span class="string">'hmp.parquet'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># register a corresponding query table</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">'df'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">indexer = StringIndexer(inputCol=<span class="string">"class"</span>, outputCol=<span class="string">"classIndex"</span>)</span><br><span class="line">indexed = indexer.fit(df).transform(df)</span><br><span class="line">indexed.show()</span><br><span class="line">indexed.select(<span class="string">'classIndex'</span>).distinct().show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoder, StringIndexer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">encoder = OneHotEncoder(inputCol=<span class="string">"classIndex"</span>, outputCol=<span class="string">"categoryVec"</span>)</span><br><span class="line">encoded = encoder.transform(indexed)</span><br><span class="line">encoded.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line">vectorAssembler = VectorAssembler(inputCols=[<span class="string">"x"</span>,<span class="string">"y"</span>,<span class="string">"z"</span>],</span><br><span class="line">                                  outputCol=<span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For your special case that has string instead of doubles you should cast them first.</span></span><br><span class="line"><span class="comment"># expr = [col(c).cast("Double").alias(c) </span></span><br><span class="line"><span class="comment">#         for c in vectorAssembler.getInputCols()]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># df2 = df2.select(*expr)</span></span><br><span class="line">features_vectorized = vectorAssembler.transform(encoded)</span><br><span class="line">features_vectorized.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line">min_max_scaler = MinMaxScaler(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"features_norm"</span>)</span><br><span class="line">min_max_scaler_model = min_max_scaler.fit(features_vectorized)</span><br><span class="line">normalized_data = min_max_scaler_model.transform(features_vectorized)</span><br><span class="line">normalized_data.show()</span><br><span class="line"></span><br><span class="line">df_train = normalized_data.drop(<span class="string">"source"</span>).drop(<span class="string">"class"</span>).drop(<span class="string">"classIndex"</span>).drop(<span class="string">"features"</span>).drop(<span class="string">"x"</span>).drop(<span class="string">"y"</span>).drop(<span class="string">"z"</span>)</span><br><span class="line"></span><br><span class="line">df_train.show()</span><br></pre></td></tr></table></figure><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, min_max_scaler_model])</span><br><span class="line">model = pipeline.fit(df)</span><br><span class="line">prediction = model.transform(df)</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf" target="_blank" rel="noopener">https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Scikit-learn-Pipeline&quot;&gt;&lt;a href=&quot;#Scikit-learn-Pipeline&quot; class=&quot;headerlink&quot; title=&quot;Scikit-learn Pipeline&quot;&gt;&lt;/a&gt;Scikit-learn Pipeline&lt;/
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Pipeline" scheme="https://zhangruochi.com/categories/AI-Workflow/Pipeline/"/>
    
    
  </entry>
  
  <entry>
    <title>Feedback loops</title>
    <link href="https://zhangruochi.com/Feedback-loops/2020/04/07/"/>
    <id>https://zhangruochi.com/Feedback-loops/2020/04/07/</id>
    <published>2020-04-07T19:04:27.000Z</published>
    <updated>2020-04-07T19:04:27.602Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Spark Fundamental</title>
    <link href="https://zhangruochi.com/Spark-Fundamental/2020/04/06/"/>
    <id>https://zhangruochi.com/Spark-Fundamental/2020/04/06/</id>
    <published>2020-04-07T02:12:46.000Z</published>
    <updated>2020-04-07T03:10:27.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h1><h2 id="Work-with-the-SparkContext-object"><a href="#Work-with-the-SparkContext-object" class="headerlink" title="Work with the SparkContext object"></a>Work with the SparkContext object</h2><p>The Spark driver application uses the SparkContext object to allow a programming interface to interact with the driver application. The SparkContext object tells Spark how and where to access a cluster.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line">conf = SparkConf().setAppName(appName).setMaster(master)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure><h2 id="Work-with-Resilient-Distributed-Datasets"><a href="#Work-with-Resilient-Distributed-Datasets" class="headerlink" title="Work with Resilient Distributed Datasets"></a>Work with Resilient Distributed Datasets</h2><p>Spark uses an abstraction for working with data called a Resilient Distributed Dataset (RDD). An RDD is a collection of elements that can be operated on in parallel. RDDs are immutable, so you cant update the data in them. To update data in an RDD, you must create a new RDD. In Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. When working with RDDs, the Spark driver application automatically distributes the work across the cluster.<br>You can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.<br>You can run these types of methods on RDDs:</p><ul><li><strong>Actions</strong>: query the data and return values</li><li><strong>Transformations</strong>: manipulate data values and return pointers to new RDDs.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a Python collection of the numbers 1 - 10</span></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Put the collection into an RDD named x_nbr_rdd using the parallelize method</span></span><br><span class="line">x_nbr_rdd = sc.parallelize(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the first element in the RDD</span></span><br><span class="line"><span class="comment"># Each number in the collection is in a different element in the RDD. Because the first() method returned a value, it is an action.</span></span><br><span class="line">x_nbr_rdd.first()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now view the first five elements in the RDD</span></span><br><span class="line">x_nbr_rdd.take(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create another RDD</span></span><br><span class="line">y = [<span class="string">"Hello Human"</span>, <span class="string">"My Name is Spark"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_str_rdd = sc.parallelize(y)</span></span><br><span class="line">y_str_rdd.take(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="Manipulate-data-in-RDDs"><a href="#Manipulate-data-in-RDDs" class="headerlink" title="Manipulate data in RDDs"></a>Manipulate data in RDDs</h2><p>Remember that to manipulate data, you use transformation functions.<br>Here are some common Python transformation functions that youll be using in this notebook:</p><ul><li><code>map(func)</code>: returns a new RDD with the results of running the specified function on each element</li><li><code>filter(func)</code>: returns a new RDD with the elements for which the specified function returns true</li><li><code>distinct([numTasks]))</code>: returns a new RDD that contains the distinct elements of the source RDD</li><li><code>flatMap(func)</code>: returns a new RDD by first running the specified function on all elements, returning 0 or more results for each original element, and then flattening the results into individual elements<br>You can also create functions that run a single expression and dont have a name with the Python lambda keyword. For example, this function returns the sum of its arguments: <code>lambda a , b : a + b</code>.</li></ul><h3 id="Update-numeric-values"><a href="#Update-numeric-values" class="headerlink" title="Update numeric values"></a>Update numeric values</h3><p>Run the <code>map()</code> function with the lambda keyword to replace each element, X, in your first RDD (the one that has numeric values) with X+1. Because RDDs are <strong>immutable</strong>, you need to specify a new RDD name.</p><p><strong>Be careful</strong> with the collect method! It returns all elements of the RDD to the driver. Returning a large data set might be not be very useful. No-one wants to scroll through a million rows!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_nbr_rdd_2 = x_nbr_rdd.map(<span class="keyword">lambda</span> x: x+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now look at the elements of the new RDD</span></span><br><span class="line">x_nbr_rdd_2.collect()</span><br></pre></td></tr></table></figure><h3 id="Add-numbers-in-an-array"><a href="#Add-numbers-in-an-array" class="headerlink" title="Add numbers in an array"></a>Add numbers in an array</h3><p>An array of values is a common data format where multiple values are contained in one element. You can manipulate the individual values if you split them up into separate elements.<br>Create an array of numbers by including quotation marks around the whole set of numbers. If you omit the quotation marks, you get a collection of numbers instead of an array.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = [<span class="string">"1,2,3,4,5,6,7,8,9,10"</span>]</span><br><span class="line"></span><br><span class="line">y_rd = sc.parallelize(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the values at commas and add values in the positions 2 and 9 in the array. Keep in mind that an array starts with position 0. Use a backslash character, \, to break the line of code for clarity.</span></span><br><span class="line">Sum_rd = y_rd.map(<span class="keyword">lambda</span> y: y.split(<span class="string">","</span>)).\</span><br><span class="line">map(<span class="keyword">lambda</span> y: (int(y[<span class="number">2</span>])+int(y[<span class="number">9</span>])))</span><br><span class="line"></span><br><span class="line">Sum_rd.first()</span><br></pre></td></tr></table></figure><h3 id="Split-and-count-text-strings"><a href="#Split-and-count-text-strings" class="headerlink" title="Split and count text strings"></a>Split and count text strings</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Words = [<span class="string">"Hello Human. I'm Spark and I love running analysis on data."</span>]</span><br><span class="line">words_rd = sc.parallelize(Words)</span><br><span class="line">words_rd.first()</span><br><span class="line"></span><br><span class="line">Words_rd2 = words_rd.map(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br><span class="line">Words_rd2.first()</span><br><span class="line"></span><br><span class="line">Words_rd2.count()</span><br></pre></td></tr></table></figure><h3 id="Count-words-with-a-pair-RDD"><a href="#Count-words-with-a-pair-RDD" class="headerlink" title="Count words with a pair RDD"></a>Count words with a pair RDD</h3><p>A common way to count the number of instances of words in an RDD is to create a pair RDD. A pair RDD converts each word into a key-value pair: the word is the key and the number 1 is the value. Because the values are all 1, when you add the values for a particular word, you get the number of instances of that word.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">z = [<span class="string">"First,Line"</span>, <span class="string">"Second,Line"</span>, <span class="string">"and,Third,Line"</span>]</span><br><span class="line">z_str_rdd = sc.parallelize(z)</span><br><span class="line">z_str_rdd.first()</span><br><span class="line"></span><br><span class="line">z_str_rdd_split_flatmap = z_str_rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">","</span>))</span><br><span class="line">z_str_rdd_split_flatmap.collect()</span><br><span class="line"></span><br><span class="line">countWords = z_str_rdd_split_flatmap.map(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>))</span><br><span class="line">countWords.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># [('First', 1),</span></span><br><span class="line"><span class="comment">#  ('Line', 1),</span></span><br><span class="line"><span class="comment">#  ('Second', 1),</span></span><br><span class="line"><span class="comment">#  ('Line', 1),</span></span><br><span class="line"><span class="comment">#  ('and', 1),</span></span><br><span class="line"><span class="comment">#  ('Third', 1),</span></span><br><span class="line"><span class="comment">#  ('Line', 1)]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line">countWords2 = countWords.reduceByKey(add)</span><br><span class="line">countWords2.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># [('Second', 1), ('Line', 3), ('First', 1), ('and', 1), ('Third', 1)]</span></span><br></pre></td></tr></table></figure><h3 id="Filter-data"><a href="#Filter-data" class="headerlink" title="Filter data"></a>Filter data</h3><p>The filter command creates a new RDD from another RDD based on a filter criteria. The filter syntax is:<br><code>.filter(lambda line: &quot;Filter Criteria Value&quot; in line)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">words_rd3 = z_str_rdd_split_flatmap.filter(<span class="keyword">lambda</span> line: <span class="string">"Second"</span> <span class="keyword">in</span> line) </span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"The count of words "</span> + str(words_rd3.first()))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Is: "</span> + str(words_rd3.count()))</span><br></pre></td></tr></table></figure><h1 id="Querying-data"><a href="#Querying-data" class="headerlink" title="Querying data"></a>Querying data</h1><h2 id="Enable-SQL-processing"><a href="#Enable-SQL-processing" class="headerlink" title="Enable SQL processing"></a>Enable SQL processing</h2><p>The preferred method to enable SQL processing with Spark 2.0 is to use the new SparkSession object, but you can also create a SQLContext object.<br>Use the predefined Spark Context, sc, which contains the connection information for Spark, to create an SQLContext:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line">sqlContext = SQLContext(sc)</span><br></pre></td></tr></table></figure><h2 id="Create-a-DataFrame"><a href="#Create-a-DataFrame" class="headerlink" title="Create a DataFrame"></a>Create a DataFrame</h2><p>Instead of creating an RDD to read the file, youll create a Spark DataFrame. Unlike an RDD, a DataFrame creates a <code>schema</code> around the data, which supplies the necessary structure for SQL queries. A self-describing format like JSON is ideal for DataFrames, but many other file types are supported, including text (CSV) and Parquet.<br>Create a DataFrame:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example1_df = sqlContext.read.json(<span class="string">"world_bank.json.gz"</span>)</span><br></pre></td></tr></table></figure></p><h2 id="Create-a-table"><a href="#Create-a-table" class="headerlink" title="Create a table"></a>Create a table</h2><p>SQL statements must be run against a table. Create a table thats a pointer to the DataFrame:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example1_df.registerTempTable(<span class="string">"world_bank"</span>)</span><br></pre></td></tr></table></figure><h2 id="Run-SQL-queries"><a href="#Run-SQL-queries" class="headerlink" title="Run SQL queries"></a>Run SQL queries</h2><p>You must define a new DataFrame for the results of the SQL query and put the SQL statement inside the sqlContext.sql() method.<br>Run the following cell to select all columns from the table and print information about the resulting DataFrame and schema of the data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">temp_df =  sqlContext.sql(<span class="string">"select * from world_bank"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (type(temp_df))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"*"</span> * <span class="number">20</span>)</span><br><span class="line"><span class="keyword">print</span> (temp_df)</span><br></pre></td></tr></table></figure><h3 id="Display-query-results-with-a-pandas-DataFrame"><a href="#Display-query-results-with-a-pandas-DataFrame" class="headerlink" title="Display query results with a pandas DataFrame"></a>Display query results with a pandas DataFrame</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">sqlContext.sql(<span class="string">"select id, borrower from world_bank limit 2"</span>).toPandas()</span><br></pre></td></tr></table></figure><h3 id="Run-a-group-by-query"><a href="#Run-a-group-by-query" class="headerlink" title="Run a group by query"></a>Run a group by query</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    regionname ,</span></span><br><span class="line"><span class="string">    count(*) as project_count</span></span><br><span class="line"><span class="string">from world_bank</span></span><br><span class="line"><span class="string">group by regionname </span></span><br><span class="line"><span class="string">order by count(*) desc</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">sqlContext.sql(query).toPandas()</span><br></pre></td></tr></table></figure><h3 id="Run-a-subselect-query"><a href="#Run-a-subselect-query" class="headerlink" title="Run a subselect query"></a>Run a subselect query</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">select * from</span></span><br><span class="line"><span class="string">    (select</span></span><br><span class="line"><span class="string">        regionname ,</span></span><br><span class="line"><span class="string">        count(*) as project_count</span></span><br><span class="line"><span class="string">    from world_bank</span></span><br><span class="line"><span class="string">    group by regionname </span></span><br><span class="line"><span class="string">    order by count(*) desc) table_alias</span></span><br><span class="line"><span class="string">limit 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">sqlContext.sql(query).toPandas()</span><br></pre></td></tr></table></figure><h2 id="Convert-RDDs-to-DataFrames"><a href="#Convert-RDDs-to-DataFrames" class="headerlink" title="Convert RDDs to DataFrames"></a>Convert RDDs to DataFrames</h2><p>If you want to run SQL queries on an existing RDD, you must convert the RDD to a DataFrame. The main difference between RDDs and DataFrames is whether the columns are named.<br>Youll create an RDD and then convert it to a DataFrame in two different ways:</p><ul><li>Apply a schema</li><li>Create rows with named columns</li></ul><h3 id="Create-a-simple-RDD"><a href="#Create-a-simple-RDD" class="headerlink" title="Create a simple RDD"></a>Create a simple RDD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">data_e2 = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    random_int = int(random.random() * <span class="number">10</span>)</span><br><span class="line">    data_e2.append([x, random_int, random_int^<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">rdd_example2 = sc.parallelize(data_e2)</span><br><span class="line"><span class="keyword">print</span> (rdd_example2.collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># [[1, 1, 3], [2, 3, 1], [3, 1, 3], [4, 8, 10], [5, 0, 2]]</span></span><br></pre></td></tr></table></figure><h3 id="Apply-a-schema"><a href="#Apply-a-schema" class="headerlink" title="Apply a schema"></a>Apply a schema</h3><p>Youll use the <code>StructField</code> method to create a schema object thats based on a string, apply the schema to the RDD to create a DataFrame, and then create a table to run SQL queries on.<br>Define your schema columns as a string:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">schemaString = <span class="string">"ID VAL1 VAL2"</span></span><br></pre></td></tr></table></figure><p>Assign header information with the StructField method and create the schema with the <code>StructType</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fields = [StructField(field_name, StringType(), <span class="keyword">True</span>) <span class="keyword">for</span> field_name <span class="keyword">in</span> schemaString.split()]</span><br><span class="line">schema = StructType(fields)</span><br></pre></td></tr></table></figure><p>Apply the schema to the RDD with the createDataFrame method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schemaExample = sqlContext.createDataFrame(rdd_example2, schema)</span><br></pre></td></tr></table></figure><p>Register the DataFrame as a table<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Register the DataFrame as a table</span></span><br><span class="line">schemaExample.registerTempTable(<span class="string">"example2"</span>)</span><br></pre></td></tr></table></figure></p><p>View the data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (schemaExample.collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># [Row(ID='1', VAL1='1', VAL2='3'), Row(ID='2', VAL1='3', VAL2='1'), Row(ID='3', VAL1='1', VAL2='3'), Row(ID='4', VAL1='8', VAL2='10'), Row(ID='5', VAL1='0', VAL2='2')]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can reference the columns names in DataFrames</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> schemaExample.take(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">print</span> (row.ID, row.VAL1, row.VAL2)</span><br></pre></td></tr></table></figure><p>Run a simple SQL query</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.sql(<span class="string">"select * from example2"</span>).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="20%" height="20%"></center><h3 id="Create-rows-with-named-columns"><a href="#Create-rows-with-named-columns" class="headerlink" title="Create rows with named columns"></a>Create rows with named columns</h3><p>Youll create an RDD with named columns and then convert it to a DataFrame and a table.<br>Create a new RDD and specify the names of the columns with the map method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line">rdd_example3 = rdd_example2.map(<span class="keyword">lambda</span> x: Row(id=x[<span class="number">0</span>], val1=x[<span class="number">1</span>], val2=x[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (rdd_example3.collect()) </span><br><span class="line"></span><br><span class="line"><span class="comment"># [Row(id=1, val1=1, val2=3), Row(id=2, val1=3, val2=1), Row(id=3, val1=1, val2=3), Row(id=4, val1=8, val2=10), Row(id=5, val1=0, val2=2)]</span></span><br></pre></td></tr></table></figure><p>Convert rdd_example3 to a DataFrame and register an associated table</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_example3 = rdd_example3.toDF()</span><br><span class="line">df_example3.registerTempTable(<span class="string">"example3"</span>)</span><br></pre></td></tr></table></figure><p>Run a simple SQL query</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.sql(<span class="string">"select * from example3"</span>).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="20%" height="20%"></center><h3 id="Join-tables"><a href="#Join-tables" class="headerlink" title="Join tables"></a>Join tables</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Join tables example2 and example3 on the ID column:</span></span><br><span class="line"></span><br><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    *</span></span><br><span class="line"><span class="string">from</span></span><br><span class="line"><span class="string">    example2 e2</span></span><br><span class="line"><span class="string">inner join example3 e3 on</span></span><br><span class="line"><span class="string">    e2.ID = e3.id</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (sqlContext.sql(query).toPandas())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternatively, you can join DataFrames with a Python command instead of an SQL query:</span></span><br><span class="line"></span><br><span class="line">df_example4 = df_example3.join(schemaExample, schemaExample[<span class="string">"ID"</span>] == df_example3[<span class="string">"id"</span>] )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df_example4.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row)</span><br></pre></td></tr></table></figure><h2 id="Create-SQL-functions"><a href="#Create-SQL-functions" class="headerlink" title="Create SQL functions"></a>Create SQL functions</h2><p>You can create functions that run in SQL queries.<br>First, create a Python function and test it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_function</span><span class="params">(v)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> int(v * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#test the function</span></span><br><span class="line"><span class="keyword">print</span> (simple_function(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>Next, register the function as an SQL function with the registerFunction method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.registerFunction(<span class="string">"simple_function"</span>, simple_function)</span><br></pre></td></tr></table></figure><p>Now run the function in an SQL Statement:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    ID,</span></span><br><span class="line"><span class="string">    VAL1,</span></span><br><span class="line"><span class="string">    VAL2,</span></span><br><span class="line"><span class="string">    simple_function(VAL1) as s_VAL1,</span></span><br><span class="line"><span class="string">    simple_function(VAL2) as s_VAL2</span></span><br><span class="line"><span class="string">from</span></span><br><span class="line"><span class="string"> example2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">sqlContext.sql(query).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center><p>The values in the VAL1 and VAL2 columns look like strings (10 characters instead of a number multiplied by 10). Thats because string is the default data type for columns in Spark DataFrames.</p><h2 id="Convert-a-pandas-DataFrame-to-a-Spark-DataFrame"><a href="#Convert-a-pandas-DataFrame-to-a-Spark-DataFrame" class="headerlink" title="Convert a pandas DataFrame to a Spark DataFrame"></a>Convert a pandas DataFrame to a Spark DataFrame</h2><p>Although pandas DataFrames display data in a friendlier format, Spark DataFrames can be faster and more scalable.<br>Youll get a new data set, create a pandas DataFrame for it, and then convert the pandas DataFrame to a Spark DataFrame.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pandas_df = pd.read_csv(<span class="string">"./GoSales_Tx.csv"</span>)</span><br><span class="line">pandas_df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the pandas DataFrame to a Spark DataFrame with the createDataFrame method. Remember using the createDataFrame method to convert an RDD to a Spark DataFrame</span></span><br><span class="line">spark_df = sqlContext.createDataFrame(pandas_df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Register the Spark DataFrame as a table</span></span><br><span class="line">spark_df.registerTempTable(<span class="string">"gosales_tx"</span>)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">"select * from gosales_tx limit 10"</span>).collect()</span><br></pre></td></tr></table></figure><h1 id="Spark-machine-learning"><a href="#Spark-machine-learning" class="headerlink" title="Spark machine learning"></a>Spark machine learning</h1><p>The Spark machine learning library makes practical machine learning scalable and easy. The library consists of common machine learning algorithms and utilities, including classification, regression, clustering, collaborative filtering (this notebook!), dimensionality reduction, lower-level optimization primitives, and higher-level pipeline APIs.<br>The library has two packages:</p><ul><li>spark.mllib contains the original API that handles data in RDDs. Its in maintenance mode, but fully supported.</li><li>spark.ml contains a newer API for constructing ML pipelines. It handles data in DataFrames. Its being actively enhanced.</li></ul><h2 id="Alternating-least-squares-algorithm"><a href="#Alternating-least-squares-algorithm" class="headerlink" title="Alternating least squares algorithm"></a>Alternating least squares algorithm</h2><p>The alternating least squares (ALS) algorithm provides collaborative filtering between customers and products to find products that the customers might like, based on their previous purchases or ratings.<br>The ALS algorithm creates a matrix of all customers versus all products. Most cells in the matrix are empty, which means the customer hasnt bought that product. The ALS algorithm then fills in the probability of customers buying products that they havent bought yet, based on similarities between customer purchases and similarities between products. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between fixing the customer factors and solving for product factors and fixing the product factors and solving for customer factors.<br>You dont, however, need to understand how the ALS algorithm works to use it! Spark machine learning algorithms have default values that work well in most cases.</p><h2 id="Get-the-data"><a href="#Get-the-data" class="headerlink" title="Get the data"></a>Get the data</h2><p>The data set contains the transactions of an online retailer of gift items for the period from 01/12/2010 to 09/12/2011. Many of the customers are wholesalers.<br>Youll be using a slightly modified version of UCIs Online Retail Data Set.<br>Heres a glimpse of the data:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%"></center><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm <span class="string">'OnlineRetail.csv.gz'</span> -f</span><br><span class="line">wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">loadRetailData = sc.textFile(<span class="string">"OnlineRetail.csv.gz"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> loadRetailData.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country</span></span><br><span class="line"><span class="comment"># 536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/10 8:26,2.55,17850,United Kingdom</span></span><br><span class="line"><span class="comment"># 536365,71053,WHITE METAL LANTERN,6,12/1/10 8:26,3.39,17850,United Kingdom</span></span><br><span class="line"><span class="comment"># 536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/10 8:26,2.75,17850,United Kingdom</span></span><br><span class="line"><span class="comment"># 536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/10 8:26,3.39,17850,United Kingdom</span></span><br></pre></td></tr></table></figure><h2 id="Prepare-and-shape-the-data"><a href="#Prepare-and-shape-the-data" class="headerlink" title="Prepare and shape the data"></a>Prepare and shape the data</h2><p>Its been said that preparing and shaping data is 80% of a data scientists job. Having the right data in the right format is critical for getting accurate results.<br>To get the data ready, complete these tasks:</p><ul><li>Format the data</li><li>Clean the data</li><li>Create a DataFrame</li><li>Remove unneeded columns</li></ul><h3 id="Format-the-data"><a href="#Format-the-data" class="headerlink" title="Format the data"></a>Format the data</h3><p>Remove the header from the RDD and split the string in each row with a comma:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">header = loadRetailData.first()</span><br><span class="line">loadRetailData = loadRetailData.filter(<span class="keyword">lambda</span> line: line != header).\</span><br><span class="line">                            map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> loadRetailData.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ['536365', '85123A', 'WHITE HANGING HEART T-LIGHT HOLDER', '6', '12/1/10 8:26', '2.55', '17850', 'United Kingdom']</span></span><br><span class="line"><span class="comment"># ['536365', '71053', 'WHITE METAL LANTERN', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']</span></span><br><span class="line"><span class="comment"># ['536365', '84406B', 'CREAM CUPID HEARTS COAT HANGER', '8', '12/1/10 8:26', '2.75', '17850', 'United Kingdom']</span></span><br><span class="line"><span class="comment"># ['536365', '84029G', 'KNITTED UNION FLAG HOT WATER BOTTLE', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']</span></span><br><span class="line"><span class="comment"># ['536365', '84029E', 'RED WOOLLY HOTTIE WHITE HEART.', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']</span></span><br></pre></td></tr></table></figure><h3 id="Clean-the-data"><a href="#Clean-the-data" class="headerlink" title="Clean the data"></a>Clean the data</h3><p>Remove the rows that have incomplete data. Keep only the rows that meet the following criteria:</p><ul><li>The purchase quantity is greater than 0</li><li>The customer ID not equal to 0</li><li>The stock code is not blank after you remove non-numeric characters</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">loadRetailData = loadRetailData.filter(<span class="keyword">lambda</span> l: int(l[<span class="number">3</span>]) &gt; <span class="number">0</span>\</span><br><span class="line">                                <span class="keyword">and</span> len(re.sub(<span class="string">"\D"</span>, <span class="string">""</span>, l[<span class="number">1</span>])) != <span class="number">0</span> \</span><br><span class="line">                                <span class="keyword">and</span> len(l[<span class="number">6</span>]) != <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (loadRetailData.take(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [['536365', '85123A', 'WHITE HANGING HEART T-LIGHT HOLDER', '6', '12/1/10 8:26', '2.55', '17850', 'United Kingdom'], ['536365', '71053', 'WHITE METAL LANTERN', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']]</span></span><br></pre></td></tr></table></figure><h3 id="Create-a-DataFrame-1"><a href="#Create-a-DataFrame-1" class="headerlink" title="Create a DataFrame"></a>Create a DataFrame</h3><p>First, create an SQLContext and map each line to a row:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext, Row</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Convert each line to a Row.</span></span><br><span class="line">loadRetailData = loadRetailData.map(<span class="keyword">lambda</span> l: Row(inv=int(l[<span class="number">0</span>]),\</span><br><span class="line">                                    stockCode=int(re.sub(<span class="string">"\D"</span>, <span class="string">""</span>, l[<span class="number">1</span>])),\</span><br><span class="line">                                    description=l[<span class="number">2</span>],\</span><br><span class="line">                                    quant=int(l[<span class="number">3</span>]),\</span><br><span class="line">                                    invDate=l[<span class="number">4</span>],\</span><br><span class="line">                                    price=float(l[<span class="number">5</span>]),\</span><br><span class="line">                                    custId=int(l[<span class="number">6</span>]),\</span><br><span class="line">                                    country=l[<span class="number">7</span>]))</span><br></pre></td></tr></table></figure><p>Create a DataFrame and show the inferred schema:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">retailDf = sqlContext.createDataFrame(loadRetailData)</span><br><span class="line"><span class="keyword">print</span> (retailDf.printSchema())</span><br><span class="line"></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- country: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- custId: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- description: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- inv: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- invDate: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- price: double (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- quant: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- stockCode: long (nullable = true)</span></span><br></pre></td></tr></table></figure></p><p>Register the DataFrame as a table so that you can run SQL queries on it and show the first two rows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">retailDf.registerTempTable(<span class="string">"retailPurchases"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"SELECT * FROM retailPurchases limit 2"</span>).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%"></center><h3 id="Remove-unneeded-columns"><a href="#Remove-unneeded-columns" class="headerlink" title="Remove unneeded columns"></a>Remove unneeded columns</h3><p>The only columns you need are custId, stockCode, and a new column, purch, which has a value of 1 to indicate that the customer purchased the product:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string">SELECT </span></span><br><span class="line"><span class="string">    custId, stockCode, 1 as purch</span></span><br><span class="line"><span class="string">FROM </span></span><br><span class="line"><span class="string">    retailPurchases </span></span><br><span class="line"><span class="string">group </span></span><br><span class="line"><span class="string">    by custId, stockCode"""</span></span><br><span class="line">retailDf = sqlContext.sql(query)</span><br><span class="line">retailDf.registerTempTable(<span class="string">"retailDf"</span>)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">"select * from retailDf limit 10"</span>).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="40%" height="40%"></center><h2 id="Split-the-data-into-three-sets"><a href="#Split-the-data-into-three-sets" class="headerlink" title="Split the data into three sets"></a>Split the data into three sets</h2><p>Youll split the data into three sets:</p><ul><li>a testing data set (10% of the data)</li><li>a cross-validation data set (10% of the data)</li><li>a training data set (80% of the data)</li></ul><p>Split the data randomly and create a DataFrame for each data set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">testDf, cvDf, trainDf = retailDf.randomSplit([<span class="number">.1</span>,<span class="number">.1</span>,<span class="number">.8</span>],<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"trainDf count: "</span>, trainDf.count(), <span class="string">" example: "</span>)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> trainDf.take(<span class="number">2</span>): <span class="keyword">print</span> (row)</span><br><span class="line"><span class="keyword">print</span> ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cvDf count: "</span>, cvDf.count(), <span class="string">" example: "</span>)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> cvDf.take(<span class="number">2</span>): <span class="keyword">print</span> (row)</span><br><span class="line"><span class="keyword">print</span> ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"testDf count: "</span>, testDf.count(), <span class="string">" example: "</span>)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> testDf.take(<span class="number">2</span>): <span class="keyword">print</span> (row)</span><br><span class="line"><span class="keyword">print</span> ()</span><br></pre></td></tr></table></figure><h2 id="Build-recommendation-models"><a href="#Build-recommendation-models" class="headerlink" title="Build recommendation models"></a>Build recommendation models</h2><p>Machine learning algorithms have standard parameters and hyperparameters. Standard parameters specify data and options. Hyperparameters control the performance of the algorithm.<br>The ALS algorithm has these hyperparameters:</p><ul><li>The rank hyperparameter represents the number of features. The default value of rank is 10.</li><li>The maxIter hyperparameter represents the number of iterations to run the least squares computation. The default value of maxIter is 10.<br>Use the training DataFrame to train three models with the ALS algorithm with different values for the rank and maxIter hyperparameters. Assign the userCol, itemCol, and ratingCol parameters to the appropriate data columns. Set the implicitPrefs parameter to true so that the algorithm can predict latent factors.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.recommendation <span class="keyword">import</span> ALS</span><br><span class="line"></span><br><span class="line">als1 = ALS(rank=<span class="number">3</span>, maxIter=<span class="number">15</span>,userCol=<span class="string">"custId"</span>,itemCol=<span class="string">"stockCode"</span>,ratingCol=<span class="string">"purch"</span>,implicitPrefs=<span class="keyword">True</span>)</span><br><span class="line">model1 = als1.fit(trainDf)</span><br><span class="line"></span><br><span class="line">als2 = ALS(rank=<span class="number">15</span>, maxIter=<span class="number">3</span>,userCol=<span class="string">"custId"</span>,itemCol=<span class="string">"stockCode"</span>,ratingCol=<span class="string">"purch"</span>,implicitPrefs=<span class="keyword">True</span>)</span><br><span class="line">model2 = als2.fit(trainDf)</span><br><span class="line"></span><br><span class="line">als3 = ALS(rank=<span class="number">15</span>, maxIter=<span class="number">15</span>,userCol=<span class="string">"custId"</span>,itemCol=<span class="string">"stockCode"</span>,ratingCol=<span class="string">"purch"</span>,implicitPrefs=<span class="keyword">True</span>)</span><br><span class="line">model3 = als3.fit(trainDf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"The models are trained"</span>)</span><br></pre></td></tr></table></figure><h2 id="Test-the-models"><a href="#Test-the-models" class="headerlink" title="Test the models"></a>Test the models</h2><p>First, test the three models on the cross-validation data set, and then on the testing data set.<br>Youll know the model is accurate when the prediction values for products that the customers have already bought are close to 1.</p><h3 id="Clean-the-cross-validation-data-set"><a href="#Clean-the-cross-validation-data-set" class="headerlink" title="Clean the cross validation data set"></a>Clean the cross validation data set</h3><p>Remove any of the customers or products in the cross-validation data set that are not in the training data set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> UserDefinedFunction</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> BooleanType</span><br><span class="line">customers = set(trainDf.rdd.map(<span class="keyword">lambda</span> line: line.custId).collect())</span><br><span class="line">stock = set(trainDf.rdd.map(<span class="keyword">lambda</span> line: line.stockCode).collect())</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (cvDf.count())</span><br><span class="line">cvDf = cvDf.rdd.filter(<span class="keyword">lambda</span> line: line.stockCode <span class="keyword">in</span> stock <span class="keyword">and</span>\</span><br><span class="line">                                           line.custId <span class="keyword">in</span> customers).toDF()</span><br><span class="line"><span class="keyword">print</span> (cvDf.count())</span><br></pre></td></tr></table></figure><h3 id="Run-the-models-on-the-cross-validation-data-set"><a href="#Run-the-models-on-the-cross-validation-data-set" class="headerlink" title="Run the models on the cross-validation data set"></a>Run the models on the cross-validation data set</h3><p>Run the model with the cross-validation DataFrame by using the transform function and print the first two rows of each set of predictions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">predictions1 = model1.transform(cvDf)</span><br><span class="line">predictions2 = model2.transform(cvDf)</span><br><span class="line">predictions3 = model3.transform(cvDf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (predictions1.take(<span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (predictions2.take(<span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (predictions3.take(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [Row(custId=14606, stockCode=20735, purch=1, prediction=0.02294829487800598), Row(custId=16464, stockCode=20735, purch=1, prediction=0.00998256541788578)]</span></span><br><span class="line"><span class="comment"># [Row(custId=14606, stockCode=20735, purch=1, prediction=0.0441482812166214), Row(custId=16464, stockCode=20735, purch=1, prediction=0.004716672468930483)]</span></span><br><span class="line"><span class="comment"># [Row(custId=14606, stockCode=20735, purch=1, prediction=0.10467907041311264), Row(custId=16464, stockCode=20735, purch=1, prediction=0.0019032559357583523)]</span></span><br></pre></td></tr></table></figure><h3 id="Calculate-the-accuracy-for-each-model"><a href="#Calculate-the-accuracy-for-each-model" class="headerlink" title="Calculate the accuracy for each model"></a>Calculate the accuracy for each model</h3><p>Youll use the mean squared error calculation to determine accuracy by comparing the prediction values for products to the actual purchase values. Remember that if a customer purchased a product, the value in the purch column is 1. The mean squared error calculation measures the average of the squares of the errors between what is estimated and the existing data. The lower the mean squared error value, the more accurate the model.<br>For all predictions, subtract the prediction from the actual purchase value (1), square the result, and calculate the mean of all of the squared differences:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">meanSquaredError1 = predictions1.rdd.map(<span class="keyword">lambda</span> line: (line.purch - line.prediction)**<span class="number">2</span>).mean()</span><br><span class="line">meanSquaredError2 = predictions2.rdd.map(<span class="keyword">lambda</span> line: (line.purch - line.prediction)**<span class="number">2</span>).mean()</span><br><span class="line">meanSquaredError3 = predictions3.rdd.map(<span class="keyword">lambda</span> line: (line.purch - line.prediction)**<span class="number">2</span>).mean()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Mean squared error = %.4f for our first model'</span> % meanSquaredError1)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Mean squared error = %.4f for our second model'</span> % meanSquaredError2)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Mean squared error = %.4f for our third model'</span> % meanSquaredError3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean squared error = 0.7393 for our first model</span></span><br><span class="line"><span class="comment"># Mean squared error = 0.7011 for our second model</span></span><br><span class="line"><span class="comment"># Mean squared error = 0.6683 for our third model</span></span><br></pre></td></tr></table></figure><p>The third model (model3) has the lowest mean squared error value, so its the most accurate.<br>Notice that of the three models, model3 has the highest values for the hyperparameters. At this point you might be tempted to run the model with even higher values for rank and maxIter. However, you might not get better results. Increasing the values of the hyperparameters increases the time for the model to run. Also, you dont want to overfit the model so that it exactly fits the original data. In that case, you wouldnt get any recommendations! For best results, keep the values of the hyperparameters close to the defaults.</p><h3 id="Confirm-the-best-model"><a href="#Confirm-the-best-model" class="headerlink" title="Confirm the best model"></a>Confirm the best model</h3><p>Now run model3 on the testing data set to confirm that its the best model. You want to make sure that the model is not over-matched to the cross-validation data. Its possible for a model to match one subset of the data well but not another. If the values of the mean squared error for the testing data set and the cross-validation data set are close, then youve confirmed that the model works for all the data.<br>Clean the testing data set, run model3 on the testing data set, and calculate the mean squared error:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">filteredTestDf = testDf.rdd.filter(<span class="keyword">lambda</span> line: line.stockCode <span class="keyword">in</span> stock <span class="keyword">and</span>\</span><br><span class="line">                                              line.custId <span class="keyword">in</span> customers).toDF()</span><br><span class="line">predictions4 = model3.transform(filteredTestDf)</span><br><span class="line">meanSquaredError4 = predictions4.rdd.map(<span class="keyword">lambda</span> line: (line.purch - line.prediction)**<span class="number">2</span>).mean()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Mean squared error = %.4f for our best model'</span> % meanSquaredError4)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean squared error = 0.6693 for our best model</span></span><br><span class="line"><span class="comment"># That's pretty close. The model works for all the data.</span></span><br></pre></td></tr></table></figure><h2 id="Implement-the-model"><a href="#Implement-the-model" class="headerlink" title="Implement the model"></a>Implement the model</h2><p>Use the best model to predict which products a specific customer might be interested in purchasing.</p><h3 id="Create-a-DataFrame-for-the-customer-and-all-products"><a href="#Create-a-DataFrame-for-the-customer-and-all-products" class="headerlink" title="Create a DataFrame for the customer and all products"></a>Create a DataFrame for the customer and all products</h3><p>Create a DataFrame in which each row has the customer ID (15544) and a product ID</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line"></span><br><span class="line">stock15544 = set(trainDf.filter(trainDf[<span class="string">'custId'</span>] == <span class="number">15544</span>).rdd.map(<span class="keyword">lambda</span> line: line.stockCode).collect())</span><br><span class="line"></span><br><span class="line">userItems = trainDf.select(<span class="string">"stockCode"</span>).distinct().\</span><br><span class="line">            withColumn(<span class="string">'custId'</span>, lit(<span class="number">15544</span>)).\</span><br><span class="line">            rdd.filter(<span class="keyword">lambda</span> line: line.stockCode <span class="keyword">not</span> <span class="keyword">in</span> stock15544).toDF()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> userItems.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row.stockCode, row.custId)</span><br></pre></td></tr></table></figure><h3 id="Rate-each-product"><a href="#Rate-each-product" class="headerlink" title="Rate each product"></a>Rate each product</h3><p>Run the transform function to create a prediction value for each product:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">userItems = model3.transform(userItems)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> userItems.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row.stockCode, row.custId, row.prediction)</span><br></pre></td></tr></table></figure><h3 id="Find-the-top-recommendations"><a href="#Find-the-top-recommendations" class="headerlink" title="Find the top recommendations"></a>Find the top recommendations</h3><p>Print the top five product recommendations</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">userItems.registerTempTable(<span class="string">"predictions"</span>)</span><br><span class="line">query = <span class="string">"select * from predictions order by prediction desc limit 5"</span></span><br><span class="line"></span><br><span class="line">sqlContext.sql(query).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="40%" height="40%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Basic-concepts&quot;&gt;&lt;a href=&quot;#Basic-concepts&quot; class=&quot;headerlink&quot; title=&quot;Basic concepts&quot;&gt;&lt;/a&gt;Basic concepts&lt;/h1&gt;&lt;h2 id=&quot;Work-with-the-Spa
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Spark" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Stock Prices Series Problems</title>
    <link href="https://zhangruochi.com/Stock-Prices-Series-Problems/2020/04/05/"/>
    <id>https://zhangruochi.com/Stock-Prices-Series-Problems/2020/04/05/</id>
    <published>2020-04-06T00:34:38.000Z</published>
    <updated>2020-04-06T01:09:25.855Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Buy-and-sell-once"><a href="#Buy-and-sell-once" class="headerlink" title="Buy and sell once"></a>Buy and sell once</h2><h3 id="Leetcode-121-Best-Time-to-Buy-and-Sell-Stock"><a href="#Leetcode-121-Best-Time-to-Buy-and-Sell-Stock" class="headerlink" title="Leetcode 121. Best Time to Buy and Sell Stock"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock/" target="_blank" rel="noopener">Leetcode 121</a>. Best Time to Buy and Sell Stock</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Say you have an array for which the ith element is the price of a given stock on day i.</span><br><span class="line"></span><br><span class="line">If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.</span><br><span class="line"></span><br><span class="line">Note that you cannot sell a stock before you buy one.</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: [7,1,5,3,6,4]</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5.</span><br><span class="line">             Not 7-1 = 6, as selling price needs to be larger than buying price.</span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: In this case, no transaction is done, i.e. max profit = 0.</span><br></pre></td></tr></table></figure><h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p>The max profit of current state only relevent to the previous minimium price. So we can use one variable to record the previous minimium price and update the current max profit.</p><p>The state transfer function is: </p><script type="math/tex; mode=display">MaxProfit_{i} = Prices[i] - MinimumPrice</script><script type="math/tex; mode=display">MinimumPrice = min(MinimumPrice, Prices[i])</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> prices:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        mini = prices[<span class="number">0</span>]</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> prices[<span class="number">1</span>:]:</span><br><span class="line">            res = max(res, p - mini)</span><br><span class="line">            mini = min(mini, p)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h2 id="Buy-and-sell-multiple-times"><a href="#Buy-and-sell-multiple-times" class="headerlink" title="Buy and sell multiple times"></a>Buy and sell multiple times</h2><h3 id="Leetocde-122-Best-Time-to-Buy-and-Sell-Stock-II"><a href="#Leetocde-122-Best-Time-to-Buy-and-Sell-Stock-II" class="headerlink" title="Leetocde 122  Best Time to Buy and Sell Stock II"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/" target="_blank" rel="noopener">Leetocde 122</a>  Best Time to Buy and Sell Stock II</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Say you have an array for which the ith element is the price of a given stock on day i.</span><br><span class="line"></span><br><span class="line">Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).</span><br><span class="line"></span><br><span class="line">Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: [7,1,5,3,6,4]</span><br><span class="line">Output: 7</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4.</span><br><span class="line">             Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3.</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: [1,2,3,4,5]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4.</span><br><span class="line">             Note that you cannot buy on day 1, buy on day 2 and sell them later, as you are</span><br><span class="line">             engaging multiple transactions at the same time. You must sell before buying again.</span><br><span class="line"></span><br><span class="line">Example 3:</span><br><span class="line"></span><br><span class="line">Input: [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: In this case, no transaction is done, i.e. max profit = 0.</span><br></pre></td></tr></table></figure><h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><ol><li>Mathemetical View</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><p>If we analyze the graph, we notice that the points of interest are the consecutive valleys and peaks. For example, in the above case, if we skip $peak_i$ and $vally_j$, trying to obtain more profit by considering points with more difference in heights, the net profit obtained will always be <strong>lesser than</strong> the one obtained by including them, Since:</p><script type="math/tex; mode=display">C <= A + B</script><p>Therefore, we can find all the preak and valley pairs and calculate the total Profit:</p><script type="math/tex; mode=display">TotalProfit = \sum_{i}( height(peak_i) - height(valley_i))</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(prices)):</span><br><span class="line">            <span class="keyword">if</span> prices[i] &gt; prices[i<span class="number">-1</span>]:</span><br><span class="line">                res += (prices[i] - prices[i<span class="number">-1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><ol><li>Dynamic Programming</li></ol><p>At the end of the $i_{th}$ day, we maintain <code>cash</code>, the maximum profit we could have if we did not have a share of stock, and hold, the maximum profit we could have if we owned a share of stock.</p><p>The intution is the state of today is only relevent to the yesterday. The State of yesterday can be expressed as :</p><script type="math/tex; mode=display">State_{yesterday} = [Profit, cash] \quad or \quad [Profit, hold]</script><p>Therefore , we can use two variables <code>cash</code>, <code>hold</code> to represent the above two states. The state transfer functions are:</p><ol><li>keep the same as day i-1, or sell from hold status at day i-1<script type="math/tex; mode=display">cash =  max(cash, hold + prices[i])</script></li><li>keep the same as day i-1, or buy from cash status at day i-1<script type="math/tex; mode=display">hold = max(hold, cash-prices[i])</script></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> prices:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># the maximum profit we could have if we did not have a share of stock</span></span><br><span class="line">        cash = <span class="number">0</span></span><br><span class="line">        <span class="comment"># the maximum profit we could have if we owned a share of stock.</span></span><br><span class="line">        hold = -prices[<span class="number">0</span>] </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(prices)):</span><br><span class="line">            cash = max(cash, hold+prices[i])</span><br><span class="line">            hold = max(hold, cash-prices[i])</span><br><span class="line">            </span><br><span class="line">            print(cash, hold)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> cash</span><br></pre></td></tr></table></figure><h3 id="Leetcode-714-Best-Time-to-Buy-and-Sell-Stock-with-Transaction-Fee"><a href="#Leetcode-714-Best-Time-to-Buy-and-Sell-Stock-with-Transaction-Fee" class="headerlink" title="Leetcode 714. Best Time to Buy and Sell Stock with Transaction Fee"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/" target="_blank" rel="noopener">Leetcode 714</a>. Best Time to Buy and Sell Stock with Transaction Fee</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Your are given an array of integers prices, for which the i-th element is the price of a given stock on day i; and a non-negative integer fee representing a transaction fee.</span><br><span class="line"></span><br><span class="line">You may complete as many transactions as you like, but you need to pay the transaction fee for each transaction. You may not buy more than 1 share of a stock at a time (ie. you must sell the stock share before you buy again.)</span><br><span class="line"></span><br><span class="line">Return the maximum profit you can make.</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: prices = [1, 3, 2, 8, 4, 9], fee = 2</span><br><span class="line">Output: 8</span><br><span class="line">Explanation: The maximum profit can be achieved by:</span><br><span class="line">Buying at prices[0] = 1</span><br><span class="line">Selling at prices[3] = 8</span><br><span class="line">Buying at prices[4] = 4</span><br><span class="line">Selling at prices[5] = 9</span><br><span class="line">The total profit is ((8 - 1) - 2) + ((9 - 4) - 2) = 8.</span><br><span class="line">Note:</span><br><span class="line"></span><br><span class="line">0 &lt; prices.length &lt;= 50000.</span><br><span class="line">0 &lt; prices[i] &lt; 50000.</span><br><span class="line">0 &lt;= fee &lt; 50000.</span><br></pre></td></tr></table></figure><h3 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h3><ol><li>Dynamic Programming</li></ol><p>See explanation above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int], fee: int)</span> -&gt; int:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># the maximum profit we could have if we did not have a share of stock</span></span><br><span class="line">        cash = <span class="number">0</span></span><br><span class="line">        <span class="comment"># the maximum profit we could have if we owned a share of stock.</span></span><br><span class="line">        hold = -prices[<span class="number">0</span>] </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(prices)):</span><br><span class="line">            cash = max(cash,hold+prices[i]-fee)</span><br><span class="line">            hold = max(hold, cash-prices[i])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> cash</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Buy-and-sell-once&quot;&gt;&lt;a href=&quot;#Buy-and-sell-once&quot; class=&quot;headerlink&quot; title=&quot;Buy and sell once&quot;&gt;&lt;/a&gt;Buy and sell once&lt;/h2&gt;&lt;h3 id=&quot;Leetc
      
    
    </summary>
    
    
      <category term="Data Structure and Algorithm" scheme="https://zhangruochi.com/categories/Data-Structure-and-Algorithm/"/>
    
    
  </entry>
  
  <entry>
    <title>Deploy Models with TensorFlow Serving and Flask</title>
    <link href="https://zhangruochi.com/Deploy-Models-with-TensorFlow-Serving-and-Flask/2020/04/03/"/>
    <id>https://zhangruochi.com/Deploy-Models-with-TensorFlow-Serving-and-Flask/2020/04/03/</id>
    <published>2020-04-03T14:22:09.000Z</published>
    <updated>2020-04-07T18:04:14.788Z</updated>
    
    <content type="html"><![CDATA[<p>A hands-on project from coursera course <a href="https://www.coursera.org/learn/deploy-models-tensorflow-serving-flask/home/welcome" target="_blank" rel="noopener">Deploy Models with TensorFlow Serving and Flask</a></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Course Certificate</div></center><ol><li>Write a front view handle image uplaod</li></ol><blockquote><p>base.html</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends "bootstrap/base.html" %&#125;</span><br><span class="line">&#123;% block title %&#125;Dog vs Cat&#123;% endblock %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% block content %&#125;</span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><blockquote><p>index.html </p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends "base.html" %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% block content %&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Upload File<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">form</span> <span class="attr">method</span>=<span class="string">"post"</span> <span class="attr">enctype</span>=<span class="string">"multipart/form-data"</span> <span class="attr">class</span>=<span class="string">"form-inline"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"file"</span> <span class="attr">name</span>=<span class="string">"file"</span> <span class="attr">class</span>=<span class="string">"btn btn-default"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"Upload"</span>, <span class="attr">class</span>=<span class="string">"btn btn-default"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><ol><li>Use docker to deployment tensorflow serving.</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 8502:8501 --name=pets -v <span class="string">"/home/models/pets/:/models/pets/1"</span> -e MODEL_NAME=pets tensorflow/serving</span><br></pre></td></tr></table></figure><p>This will copy the model from <code>/home/models/pets/</code> which in your desktop to the path <code>/models/pets/1</code> in docker</p><p>The port <code>8501</code> is defined by docker and you can change the <code>8502</code> to any port you can you used.</p><ol><li>Use <code>Flask</code> to process HTTP Requests and do model inference in docker.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, redirect, url_for, render_template</span><br><span class="line"><span class="keyword">from</span> flask_bootstrap <span class="keyword">import</span> Bootstrap</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">Bootstrap(app)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Constants</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">MODEL_URI = <span class="string">'http://localhost:8502/v1/models/pets:predict'</span></span><br><span class="line">OUTPUT_DIR = <span class="string">'static'</span></span><br><span class="line">CLASSES = [<span class="string">'Cat'</span>, <span class="string">'Dog'</span>]</span><br><span class="line">SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Utility functions</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_filename</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(random.choices(string.ascii_lowercase, k=<span class="number">20</span>)) + <span class="string">'.jpg'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prediction</span><span class="params">(image_path)</span>:</span></span><br><span class="line">    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(SIZE, SIZE))</span><br><span class="line">    image = tf.keras.preprocessing.image.img_to_array(image)</span><br><span class="line">    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)</span><br><span class="line">    image = np.expand_dims(image, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    data = json.dumps(&#123;<span class="string">'instances'</span>: image.tolist() &#125;)</span><br><span class="line">    response = requests.post(MODEL_URI, data = data.encode())</span><br><span class="line">    result = json.loads(response.text)</span><br><span class="line">    prediction = result[<span class="string">'predictions'</span>][<span class="number">0</span>]</span><br><span class="line">    class_name = CLASSES[int(prediction &gt; <span class="number">0.5</span>)]</span><br><span class="line">    <span class="keyword">return</span> class_name</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Routes</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="meta">@app.route('/', methods=['GET', 'POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> request.method == <span class="string">'POST'</span>:</span><br><span class="line">        uploaded_file = request.files[<span class="string">'file'</span>]</span><br><span class="line">        <span class="keyword">if</span> uploaded_file.filename != <span class="string">''</span>:</span><br><span class="line">            <span class="keyword">if</span> uploaded_file.filename[<span class="number">-3</span>:] <span class="keyword">in</span> [<span class="string">'jpg'</span>, <span class="string">'png'</span>]:</span><br><span class="line">                image_path = os.path.join(OUTPUT_DIR, generate_filename())</span><br><span class="line">                uploaded_file.save(image_path)</span><br><span class="line">                class_name = get_prediction(image_path)</span><br><span class="line">                result = &#123;</span><br><span class="line">                    <span class="string">'class_name'</span>: class_name,</span><br><span class="line">                    <span class="string">'path_to_image'</span>: image_path,</span><br><span class="line">                    <span class="string">'size'</span>: SIZE</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> render_template(<span class="string">'show.html'</span>, result=result)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(debug=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><ol><li>Rendering results in template</li></ol><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends "base.html" %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% block content %&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Predicted Class: &#123;&#123; result.class_name &#125;&#125;<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"&#123;&#123; result.path_to_image &#125;&#125;"</span> <span class="attr">class</span>=<span class="string">"img-rounded"</span> <span class="attr">width</span>=<span class="string">"&#123;&#123; result.size &#125;&#125;"</span> <span class="attr">height</span>=<span class="string">"auto"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span> = <span class="string">"&#123;&#123; url_for('index') &#125;&#125;"</span> <span class="attr">class</span>=<span class="string">"btn btn-default"</span>&gt;</span>Go Back<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;A hands-on project from coursera course &lt;a href=&quot;https://www.coursera.org/learn/deploy-models-tensorflow-serving-flask/home/welcome&quot; targ
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Deployment" scheme="https://zhangruochi.com/categories/AI-Workflow/Deployment/"/>
    
    
  </entry>
  
  <entry>
    <title>Programming Language: New Types, Pattern Matching, Tail Recursion</title>
    <link href="https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/03/29/"/>
    <id>https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/03/29/</id>
    <published>2020-03-29T04:09:05.000Z</published>
    <updated>2020-04-03T14:21:55.457Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Conceptual-Ways-to-Build-New-Types"><a href="#Conceptual-Ways-to-Build-New-Types" class="headerlink" title="Conceptual Ways to Build New Types"></a>Conceptual Ways to Build New Types</h2><p>To create a compound type, there are really only three essential building blocks. Any decent programming language provides these building blocks in some way:</p><ul><li><strong>Each-of</strong>: A compound type t describes values that contain each of values of type t1, t2, , and tn. Tuples are an example: int * bool describes values that contain an int and a bool. A Java class with fields is also an each-of sort of thing.</li><li><strong>One-of</strong>: A compound type t describes values that contain a value of one of the types t1, t2, , or tn. For a type that contains an int or a bool in ML, we need <code>datatype bindings</code>. In object-oriented languages with classes like Java, one-of types are achieved with subclassing, but that is a topic for much later in the course.</li><li><strong>Self-reference</strong>: A compound type t may refer to itself in its definition in order to describe recursive data structures like lists and trees. This is useful in combination with each-of and one-of types. For example, int list describes values that either contain nothing or contain an int and another int list. </li></ul><h2 id="Records-Another-Approach-to-Each-of-Types"><a href="#Records-Another-Approach-to-Each-of-Types" class="headerlink" title="Records: Another Approach to Each-of Types"></a>Records: Another Approach to <strong>Each-of</strong> Types</h2><p>Record types are each-of types where each component is a <code>named field</code>.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;foo : <span class="built_in">int</span>, bar : <span class="built_in">int</span>*<span class="built_in">bool</span>, baz : <span class="built_in">bool</span>*<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><p>In ML, we do not have to declare that we want a record type with particular field names and field types  we just write down a record expression and the type-checker gives it the right type.</p><p>Now that we know how to build record values, we need a way to access their pieces. For now, we will use <code>#foo e</code> where <code>foo</code> is a field name. </p><h3 id="The-truth-of-tuple"><a href="#The-truth-of-tuple" class="headerlink" title="The truth of tuple"></a>The truth of tuple</h3><p>In fact, this is how ML actually defines tuples: A tuple is a record. That is, all the syntax for tuples is just a convenient way to write down and use records. The REPL just always uses the tuple syntax where possible, so if you evaluate {2=1+2, 1=3+4} it will print the result as (7,3). Using the tuple syntax is better style, but we did not need to give tuples their own semantics: we can instead use the another way of writing rules above and then reuse the semantics for records.</p><p>This is the first of many examples we will see of <code>syntactic sugar</code>. We say, Tuples are just syntactic sugar for records with fields named 1, 2, , n.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> z = (<span class="number">3</span>,<span class="number">7</span>) : <span class="built_in">int</span> * <span class="built_in">int</span></span><br><span class="line"><span class="keyword">val</span> z = &#123;<span class="number">1</span>=<span class="number">3</span>,<span class="number">3</span>=<span class="number">7</span>&#125; : &#123;<span class="number">1</span>:<span class="built_in">int</span>, <span class="number">3</span>:<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="Datatype-Bindings-Our-Own-One-of-Types"><a href="#Datatype-Bindings-Our-Own-One-of-Types" class="headerlink" title="Datatype Bindings: Our Own One-of Types"></a>Datatype Bindings: Our Own <strong>One-of</strong> Types</h2><p>We now introduce datatype bindings, our third kind of binding after variable bindings and function bindings.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">datatype mytype = TwoInts of int * int</span><br><span class="line">                | Str of string</span><br><span class="line">                | Pizza</span><br></pre></td></tr></table></figure><p>Roughly, this defines a new type where values have an int * int or a string or nothing. Any value will also be <code>tagged</code> with information that lets us know which variant it is: These tags, which we will call constructors, are <code>TwoInts</code>, <code>Str</code>, and <code>Pizza</code>.</p><p>A constructor is two different things. First, it is either a function for creating values of the new type (if the variant has of t for some type t) or it is actually a value of the new type (otherwise). In our example, TwoInts is a function of type int*int -&gt; mytype, Str is a function of type string-&gt;mytype, and Pizza is a value of type mytype. Second, we use constructors in case-expressions as described further below.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> mytype = <span class="type">TwoInts</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span> </span><br><span class="line">                | <span class="type">Str</span> <span class="keyword">of</span> <span class="built_in">string</span> </span><br><span class="line">                | <span class="type">Pizza</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a = <span class="type">Str</span> <span class="string">"hi"</span></span><br><span class="line"><span class="keyword">val</span> b = <span class="type">Str</span></span><br><span class="line"><span class="keyword">val</span> c = <span class="type">Pizza</span></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">TwoInts</span>(<span class="number">1</span>+<span class="number">2</span>,<span class="number">3</span>+<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> e = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">(* val a = Str "hi" : mytype</span></span><br><span class="line"><span class="comment">val b = fn : string -&gt; mytype</span></span><br><span class="line"><span class="comment">val c = Pizza : mytype</span></span><br><span class="line"><span class="comment">val d = TwoInts (3,7) : mytype</span></span><br><span class="line"><span class="comment">val e = Str "hi" : mytype *)</span></span><br></pre></td></tr></table></figure><h2 id="How-ML-Provides-Access-to-Datatype-Values-Case-Expressions"><a href="#How-ML-Provides-Access-to-Datatype-Values-Case-Expressions" class="headerlink" title="How ML Provides Access to Datatype Values: Case Expressions"></a>How ML Provides Access to Datatype Values: Case Expressions</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Conceptual-Ways-to-Build-New-Types&quot;&gt;&lt;a href=&quot;#Conceptual-Ways-to-Build-New-Types&quot; class=&quot;headerlink&quot; title=&quot;Conceptual Ways to Build
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>Bayesian networks</title>
    <link href="https://zhangruochi.com/Bayesian-networks/2020/03/23/"/>
    <id>https://zhangruochi.com/Bayesian-networks/2020/03/23/</id>
    <published>2020-03-23T20:03:48.000Z</published>
    <updated>2020-03-23T22:48:28.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Probabilistic-modeling-with-Bayesian-networks"><a href="#Probabilistic-modeling-with-Bayesian-networks" class="headerlink" title="Probabilistic modeling with Bayesian networks"></a>Probabilistic modeling with Bayesian networks</h2><p>Directed graphical models (a.k.a. Bayesian networks) are a family of probability distributions that admit a compact parametrization that can be naturally described using a directed graph.</p><p>The general idea behind this parametrization is surprisingly simple. Recall that by the chain rule, we can write any probability $p$ as:</p><script type="math/tex; mode=display">p(x_1, x_2, \dotsc, x_n) = p(x_1) p(x_2 \mid x_1) \cdots p(x_n \mid x_{n-1}, \dotsc, x_2, x_1).</script><p>A <strong>compact</strong> Bayesian network is a distribution in which each factor on the right hand side depends only on a small number of <em>ancestor variables</em> $x_{A_i}$:</p><script type="math/tex; mode=display">p(x_i \mid x_{i-1}, \dotsc, x_1) = p(x_i \mid x_{A_i}).</script><p>For example, in a model with five variables, we may choose to approximate the factor $p(x_5 \mid x_4, x_3, x_2, x_1)$ with $p(x_5 \mid x_4, x_3)$. In this case, we write $x_{A_5} = \{x_4, x_3\}$.</p><h2 id="Graphical-representation"><a href="#Graphical-representation" class="headerlink" title="Graphical representation"></a>Graphical representation</h2><p>As an example, consider a model of a students grade <script type="math/tex">g</script> on an exam. This grade depends on the exams difficulty $d$ and the students intelligence $i$; it also affects the quality $l$ of the reference letter from the professor who taught the course. The students intelligence $i$ affects the SAT score $s$ as well. Each variable is binary, except for $g$, which takes 3 possible values.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bayes net model describing the performance of a student on an exam. The distribution can be represented a product of conditional probability distributions specified by tables. The form of these distributions is described by edges in the graph.</div></center><p>The joint probability distribution over the 5 variables naturally factorizes as follows:</p><script type="math/tex; mode=display">p(l, g, i, d, s) = p(l \mid g)\, p(g \mid i, d)\, p(i)\, p(d)\, p(s \mid i).</script><p>The graphical representation of this distribution is a DAG that visually specifies how random variables depend on each other. The graph clearly indicates that the letter depends on the grade, which in turn depends on the students intelligence and the difficulty of the exam.</p><p>Another way to interpret directed graphs is in terms of stories for how the data was generated. In the above example, to determine the quality of the reference letter, we may first sample an intelligence level and an exam difficulty; then, a students grade is sampled given these parameters; finally, the recommendation letter is generated based on that grade.</p><h2 id="Formal-definition"><a href="#Formal-definition" class="headerlink" title="Formal definition."></a>Formal definition.</h2><p>Formally, a Bayesian network is a directed graph $G = (V,E)$ together with</p><ul><li>A random variable $x_i$ for each node $i \in V$.</li><li>One conditional probability distribution (CPD) $p(x_i \mid x_{A_i})$ per node, specifying the probability of $x_i$ conditioned on its parents values.</li></ul><p>Thus, a Bayesian network defines a probability distribution $p$. Conversely, we say that a probability $p$ <strong>factorizes</strong> over a DAG $G$ if it can be decomposed into a product of factors, as specified by $G$.</p><p>It is not hard to see that a probability represented by a Bayesian network will be valid: clearly, it will be non-negative and one can show using an induction argument (and using the fact that the CPDs are valid probabilities) that the sum over all variable assignments will be one. Conversely, we can also show by counter-example that when <script type="math/tex">G</script> contains cycles, its associated probability may not sum to one.</p><h2 id="The-dependencies-of-a-Bayes-net"><a href="#The-dependencies-of-a-Bayes-net" class="headerlink" title="The dependencies of a Bayes net"></a>The dependencies of a Bayes net</h2><p>To summarize, Bayesian networks represent probability distributions that can be formed via products of smaller, local conditional probability distributions (one for each variable). By expressing a probability in this form, we are introducing into our model assumptions that certain variables are independent.</p><p>This raises the question: which independence assumptions are we exactly making by using a Bayesian network model with a given structure described by $G$? This question is important for two reasons: we should know precisely what model assumptions we are making (and whether they are correct); also, this information will help us design more efficient inference algorithms later on.</p><p>Let us use the notation $I(p)$ to denote the set of all independencies that hold for a joint distribution $p$. For example, if $p(x,y) = p(x) p(y)$, then we say that $x \perp y \in I(p)$.</p><h3 id="Independencies-described-by-directed-graphs"><a href="#Independencies-described-by-directed-graphs" class="headerlink" title="Independencies described by directed graphs"></a>Independencies described by directed graphs</h3><p>It turns out that a Bayesian network $p$ very elegantly describes many independencies in $I(p)$; these independencies can be recovered from the graph by looking at three types of structures.</p><p>For simplicity, lets start by looking at a Bayes net $G$ with three nodes: $A$, $B$, and $C$. In this case, <script type="math/tex">G</script> essentially has only three possible structures, each of which leads to different independence assumptions. The interested reader can easily prove these results using a bit of algebra.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bayesian networks over three variables, encoding different types of dependencies: cascade (a,b), common parent (c), and v-structure (d).</div></center><ul><li><p><strong>Common parent.</strong> If $G$ is of the form $A \leftarrow B \rightarrow C$, and $B$ is observed, then $A \perp C \mid B$. However, if $B$ is unobserved, then $A \not\perp C$. Intuitively this stems from the fact that $B$ contains all the information that determines the outcomes of $A$ and $C$; once it is observed, there is nothing else that affects these variables outcomes.</p></li><li><p><strong>Cascade.</strong>: If $G$ equals $A \rightarrow B \rightarrow C$, and $B$ is again observed, then, again $A \perp C \mid B$. However, if $B$ is unobserved, then $A \not\perp C$. Here, the intuition is again that $B$ holds all the information that determines the outcome of $C$; thus, it does not matter what value $A$ takes.</p></li><li><strong>V-structure.</strong> (also known as <em>explaining away</em>): If $G$ is $A \rightarrow C \leftarrow B$, then knowing $C$ couples $A$ and $B$. In other words, $A \perp B$ if $C$ is unobserved, but $A \not\perp B \mid C$ if $C$ is observed.</li></ul><p>The latter case requires additional explanation. Suppose that $C$ is a Boolean variable that indicates whether our lawn is wet one morning; $A$ and $B$ are two explanations for it being wet: either it rained (indicated by $A$), or the sprinkler turned on (indicated by $B$). If we know that the grass is wet ($C$ is true) and the sprinkler didnt go on ($B$ is false), then the probability that $A$ is true must be one, because that is the only other possible explanation. Hence, $A$ and $B$ are not independent given $C$.</p><p>These structures clearly describe the independencies encoded by a three-variable Bayesian net. </p><h3 id="d-separation"><a href="#d-separation" class="headerlink" title="$d$-separation"></a>$d$-separation</h3><p>We can extend them to general networks by applying them recursively over any larger graph. This leads to a notion called $d$-separation (where $d$ stands for directed).</p><p>Let $Q$, $W$, and $O$ be three sets of nodes in a Bayesian Network $G$. We say that $Q$ and $W$ are $d$-separated given $O$ (<em>i.e.</em> the variables $O$ are observed) if $Q$ and $W$ are not connected by an <em>active path</em>. An undirected path in $G$ is called <em>active</em> given observed variables $O$ if for every consecutive triple of variables $X,Y,Z$ on the path, one of the following holds:</p><ul><li>$X \leftarrow Y \leftarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \rightarrow Y \rightarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \leftarrow Y \rightarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \rightarrow Y \leftarrow Z$, and $Y$ or any of its descendants are observed.</li></ul><p>In other words: A trail $X1, \cdots, X_n$ is active given Z if:</p><ul><li>for any v-structure we have that $X_i$ or one of its descendants<br>$\in$ Z</li><li>no other $X_i$ is in Z</li></ul><p>In this example, $X_1$ and $X_6$ are $d$-separated given $X_2, X_3$.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="7.png" width="70%" height="70%"></center><p>However, $X_2, X_3$ are not $d$-separated given $X_1, X_6$. There is an active pass which passed through the V-structure created when $X_6$ is observed.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="8.png" width="70%" height="70%"></center><p>For example, in the graph below, $X_1$ and $X_6$ are $d$-separated given $X_2, X_3$. However, $X_2, X_3$ are not $d$-separated given $X_1, X_6$, because we can find an active path $(X_2, X_6, X_5, X_3)$</p><p>The notion of $d$-separation is useful, because it lets us describe a large fraction of the dependencies that hold in our model. Let $I(G) = \{(X \perp Y \mid Z) : \text{$X,Y$ are $d$-sep given $Z$}\}$ be a set of variables that are $d$-separated in $G$.</p><h4 id="Two-theorem"><a href="#Two-theorem" class="headerlink" title="Two theorem"></a>Two theorem</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="9.png" width="70%" height="70%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="10.png" width="70%" height="70%"></center><h3 id="I-map"><a href="#I-map" class="headerlink" title="$I$-map"></a>$I$-map</h3><blockquote><p>If $p$ factorizes over $G$, then $I(G) \subseteq I(p)$. In this case, we say that $G$ is an $I$-map (independence map) for $p$.</p></blockquote><p>In other words, all the independencies encoded in $G$ are sound: variables that are $d$-separated in $G$ are truly independent in $p$. However, the converse is not true: a distribution may factorize over $G$, yet have independencies that are not captured in $G$.</p><p>In a way this is almost a trivial statement. If $p(x,y) = p(x)p(y)$, then this distribution still factorizes over the graph $y \rightarrow x$, since we can always write it as $p(x,y) = p(x\mid y)p(y)$ with a CPD $p(x\mid y)$ in which the probability of $x$ does not actually vary with $y$. However, we can construct a graph that matches the structure of $p$ by simply removing that unnecessary edge.</p><h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="11.png" width="70%" height="70%"></center><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Two equivalent views of graph structure:</p><ul><li><strong>Factorization</strong>: $G$ allows $P$ to be represented</li><li><strong>I-map</strong>: Independencies encoded by G hold in P<ul><li>If $P$ factorizes over a graph $G$, we can read from the graph independencies that must hold in $P$ (an independency map)</li></ul></li></ul><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="12.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Naive Bayes Probabilistic Graphical Model</div></center><p>From the graph,</p><script type="math/tex; mode=display">(x_i \perp x_j | c) \quad \text{for all} \quad x_i, x_j</script><p>Then, we can get</p><script type="math/tex; mode=display">P(C, x_i, \cdots, x_n) = P(c)\prod_{i=1}^{n}P(x_i | C)</script><p>Therefore, the raito of two class is:</p><script type="math/tex; mode=display">\frac{P(C = c^{1} | x_i, \cdots, x_n)}{P(C = c^{2} | x_i, \cdots, x_n)} = \frac{P(C = c^{1})}{P(C = c^{2})}\prod_{i=1}^{n}\frac{P(x_i | C = c^{1})}{P( x_i | C = c^{2})}</script><p>Indtroduce <code>Bernoulli</code>(or others) to calcute probabilities</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="13.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bernoulli Naive Bayes for Text</div></center><ul><li>Simple approach for classification <ul><li>Computationally efficient</li><li>Easy to construct</li></ul></li><li>Surprisingly effective in domains with many <strong>weakly</strong> relevant features</li><li>Strong independence assumptions reduce performance when many features are strongly correlated</li></ul><h2 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h2><h4 id="Calculate-the-number-of-parameters-of-a-distribution-model"><a href="#Calculate-the-number-of-parameters-of-a-distribution-model" class="headerlink" title="Calculate the number of parameters of a distribution model"></a>Calculate the number of parameters of a distribution model</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">The number of parameters</div></center><h4 id="Inter-causal-reasoning"><a href="#Inter-causal-reasoning" class="headerlink" title="Inter-causal reasoning"></a>Inter-causal reasoning</h4><center>    <img style="border-radius: 0.1em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%">    <img style="border-radius: 0.1em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Inter-causal reasoning</div></center><h4 id="Independencies-in-a-graph"><a href="#Independencies-in-a-graph" class="headerlink" title="Independencies in a graph"></a>Independencies in a graph</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Independencies in a graph</div></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://ermongroup.github.io/cs228-notes/" target="_blank" rel="noopener">https://ermongroup.github.io/cs228-notes/</a></li><li>Course note from Coursera course <a href="https://www.coursera.org/learn/probabilistic-graphical-models/" target="_blank" rel="noopener">Probabilistic graphical models</a> lectured by Daphne Koller</li></ul>]]></content>
    
    <summary type="html">
    
      how do we choose a probability distribution to model some interesting aspect of the world?
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Probabilistic Graphical Models" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Probabilistic-Graphical-Models/"/>
    
    
  </entry>
  
  <entry>
    <title>Fucking distributions</title>
    <link href="https://zhangruochi.com/Fucking-distributions/2020/03/22/"/>
    <id>https://zhangruochi.com/Fucking-distributions/2020/03/22/</id>
    <published>2020-03-22T07:44:30.000Z</published>
    <updated>2020-03-22T07:50:53.670Z</updated>
    
    <content type="html"><![CDATA[<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="overview.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">overview</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">import</span> operator <span class="keyword">as</span> op</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> scipy.special <span class="keyword">as</span> sps</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><h2 id="Uniform-distribution-continuous"><a href="#Uniform-distribution-continuous" class="headerlink" title="Uniform distribution(continuous)"></a>Uniform distribution(continuous)</h2><ul><li>Uniform distribution has same probaility value on [a, b], easy probability.</li></ul><script type="math/tex; mode=display">f(x)=\begin{cases}  \frac{1}{b - a} & \mathrm{for}\ a \le x \le b, \\[8pt]  0 & \mathrm{for}\ x<a\ \mathrm{or}\ x>b  \end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uniform</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line">    y = [<span class="number">1</span> / (b-a) <span class="keyword">if</span> a &lt;= val <span class="keyword">and</span> val &lt;= b <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> val <span class="keyword">in</span> x]</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-100</span>,<span class="number">100</span>)</span><br><span class="line"><span class="keyword">for</span> dis <span class="keyword">in</span> [(<span class="number">-50</span>, <span class="number">50</span>), (<span class="number">10</span>, <span class="number">20</span>)]:</span><br><span class="line">    a, b = dis[<span class="number">0</span>], dis[<span class="number">1</span>]</span><br><span class="line">    x, y, u, s = uniform(x, a, b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_5_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">15</span>, density=<span class="keyword">True</span>)</span><br><span class="line">plt.plot(bins, np.ones_like(bins), linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x11eae0cc0&gt;]</code></pre><p><img src="output_6_1.png" alt="png"></p><h2 id="Bernoulli-distribution-discrete"><a href="#Bernoulli-distribution-discrete" class="headerlink" title="Bernoulli distribution(discrete)"></a>Bernoulli distribution(discrete)</h2><ul><li>Bernoulli distribution is not considered about prior probability P(X). Therefore, if we optimize to the maximum likelihood, we will be vulnerable to overfitting.</li><li>We use binary cross entropy to classify binary classification. It has same form like taking a negative log of the bernoulli distribution.</li></ul><script type="math/tex; mode=display">f(k;p) = \begin{cases}   p & \text{if }k=1, \\   q = 1-p & \text {if } k = 0. \end{cases}</script><ul><li>For Logistic Regression<script type="math/tex; mode=display">p=p(y|x,\theta)=p_{1}^{y_{i}}\ast p_{0}^{1-y_{i}}</script><script type="math/tex; mode=display">max \sum_{i=1}^{m}({y_{i}\log{p_{1}}+(1-y_{i})\log{p_{0})}}</script></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bernoulli</span><span class="params">(p, k)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> p <span class="keyword">if</span> k <span class="keyword">else</span> <span class="number">1</span> - p</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">100</span></span><br><span class="line">p = <span class="number">0.6</span></span><br><span class="line">x = np.arange(n_experiment)</span><br><span class="line">y = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n_experiment):</span><br><span class="line">    pick = bernoulli(p, k=bool(random.getrandbits(<span class="number">1</span>)))</span><br><span class="line">    y.append(pick)</span><br><span class="line"></span><br><span class="line">u, s = np.mean(y), np.std(y)</span><br><span class="line">plt.scatter(x, y, label=<span class="string">r'$p=%.2f,\ \mu=%.2f,\ \sigma=%.2f$'</span> % (p,u, s))</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_8_0.png" alt="png"></p><h2 id="Binomial-distribution-discrete"><a href="#Binomial-distribution-discrete" class="headerlink" title="Binomial distribution(discrete)"></a>Binomial distribution(discrete)</h2><ul><li>Binomial distribution with parameters <strong>n</strong> and <strong>p</strong> is the discrete probability distribution of the number of successes in a sequence of n independent experiments.</li><li>Binomial distribution is distribution considered prior probaility by specifying the number to be picked in advance.</li></ul><script type="math/tex; mode=display">f(k,n,p) = \Pr(k;n,p) = \Pr(X = k) = \binom{n}{k}p^k(1-p)^{n-k}</script><p>for k = 0, 1, 2, , n, where</p><script type="math/tex; mode=display">\binom{n}{k} =\frac{n!}{k!(n-k)!}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">const</span><span class="params">(n, r)</span>:</span></span><br><span class="line">    r = min(r, n-r)</span><br><span class="line">    numer = reduce(op.mul, range(n, n-r, <span class="number">-1</span>), <span class="number">1</span>)</span><br><span class="line">    denom = reduce(op.mul, range(<span class="number">1</span>, r+<span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> numer / denom</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binomial</span><span class="params">(n, p)</span>:</span></span><br><span class="line">    q = <span class="number">1</span> - p</span><br><span class="line">    y = [const(n, k) * (p ** k) * (q ** (n-k)) <span class="keyword">for</span> k <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">return</span> y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">0.5</span>, <span class="number">20</span>), (<span class="number">0.7</span>, <span class="number">40</span>), (<span class="number">0.5</span>, <span class="number">40</span>)]:</span><br><span class="line">    p, n_experiment = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line">    x = np.arange(n_experiment)</span><br><span class="line">    y, u, s = binomial(n_experiment, p)</span><br><span class="line">    plt.scatter(x, y, label=<span class="string">r'$n_&#123;experiment&#125;=%d,\ p=%.2f,\ \mu=%.2f,\ \sigma=%.2f$'</span> % (n_experiment,p, u, s))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_10_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.binomial(<span class="number">10</span>, <span class="number">0.8</span>, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><h2 id="Multi-Bernoulli-distribution-Categorical-distribution-discrete"><a href="#Multi-Bernoulli-distribution-Categorical-distribution-discrete" class="headerlink" title="Multi-Bernoulli distribution, Categorical distribution(discrete)"></a>Multi-Bernoulli distribution, Categorical distribution(discrete)</h2><ul><li>Multi-bernoulli called categorical distribution, is a probability expanded more than 2.</li><li><strong>cross entopy</strong> has same form like taking a negative log of the Multi-Bernoulli distribution.</li></ul><script type="math/tex; mode=display">f(x\mid \boldsymbol{p} ) = \prod_{i=1}^k p_i^{[x=i]}</script><p>where $[x = i]$ evaluates to 1 if $x = i$, 0 otherwise. There are various advantages of this formulation</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical</span><span class="params">(p, k)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> p[k]</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">100</span></span><br><span class="line">p = [<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.7</span>]</span><br><span class="line">x = np.arange(n_experiment)</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n_experiment):</span><br><span class="line">    pick = categorical(p, k = random.randint(<span class="number">0</span>, len(p) - <span class="number">1</span>))</span><br><span class="line">    y.append(pick)</span><br><span class="line"></span><br><span class="line">u, s = np.mean(y), np.std(y)</span><br><span class="line">plt.scatter(x, y, label=<span class="string">r'$p=[0.2, 0.1, 0.7],\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt="png"></p><h2 id="Multinomial-distribution-discrete"><a href="#Multinomial-distribution-discrete" class="headerlink" title="Multinomial distribution(discrete)"></a>Multinomial distribution(discrete)</h2><ul><li>The multinomial distribution has the same relationship with the categorical distribution as the relationship between Bernoull and Binomial.</li><li>For example, it models the probability of counts for each side of a <strong>k-sided</strong> die rolled n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of <strong>any particular combination</strong> of numbers of successes for the various categories.</li><li>When k is 2 and n is 1, the multinomial distribution is the Bernoulli distribution. When k is 2 and n is bigger than 1, it is the binomial distribution. When k is bigger than 2 and n is 1, it is the categorical distribution.</li></ul><script type="math/tex; mode=display">\begin{align}f(x_1,\ldots,x_k;n,p_1,\ldots,p_k) & {} = \Pr(X_1 = x_1 \text{ and } \dots \text{ and } X_k = x_k) \\& {} = \begin{cases} { \displaystyle {n! \over x_1!\cdots x_k!}p_1^{x_1}\times\cdots\times p_k^{x_k}}, \quad &\text{when } \sum_{i=1}^k x_i=n \\  \\0 & \text{otherwise,} \end{cases}\end{align}</script><p>for non-negative integers $x_1, \cdots, x_k$.</p><p>The probability mass function can be expressed using the gamma function as:</p><script type="math/tex; mode=display">f(x_1,\dots, x_{k}; p_1,\ldots, p_k) = \frac{\Gamma(\sum_i x_i + 1)}{\prod_i \Gamma(x_i+1)} \prod_{i=1}^k p_i^{x_i}.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> reduce(op.mul, range(<span class="number">1</span>, n + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">const</span><span class="params">(n, a, b, c)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        return n! / a! b! c!, where a+b+c == n</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span>  a + b + c == n</span><br><span class="line"></span><br><span class="line">    numer = factorial(n)</span><br><span class="line">    denom = factorial(a) * factorial(b) * factorial(c)</span><br><span class="line">    <span class="keyword">return</span> numer / denom</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multinomial</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param x : list, sum(x) should be `n`</span></span><br><span class="line"><span class="string">    :param n : number of trial</span></span><br><span class="line"><span class="string">    :param p: list, sum(p) should be `1`</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># get all a,b,c where a+b+c == n, a&lt;b&lt;c</span></span><br><span class="line">    ls = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(j, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> i + j + k == n:</span><br><span class="line">                    ls.append([i, j, k])</span><br><span class="line"></span><br><span class="line">    y = [const(n, l[<span class="number">0</span>], l[<span class="number">1</span>], l[<span class="number">2</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> ls]</span><br><span class="line">    x = np.arange(len(y))</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_experiment <span class="keyword">in</span> [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>]:</span><br><span class="line">    x, y, u, s = multinomial(n_experiment)</span><br><span class="line">    plt.scatter(x, y, label=<span class="string">r'$trial=%d$'</span> % (n_experiment))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_15_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.multinomial(<span class="number">20</span>, [<span class="number">1</span>/<span class="number">6.</span>]*<span class="number">6</span>, size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>array([[1, 1, 2, 3, 8, 5],       [2, 4, 3, 3, 6, 2],       [1, 6, 3, 2, 3, 5],       [5, 3, 4, 4, 2, 2],       [3, 8, 4, 2, 0, 3],       [2, 4, 1, 5, 1, 7],       [6, 3, 2, 4, 3, 2],       [8, 2, 1, 1, 4, 4],       [3, 6, 4, 1, 4, 2],       [3, 2, 3, 3, 6, 3]])</code></pre><h2 id="Beta-distribution-continuous"><a href="#Beta-distribution-continuous" class="headerlink" title="Beta distribution(continuous)"></a>Beta distribution(continuous)</h2><ul><li>Beta distribution is conjugate to the binomial and Bernoulli distributions.</li><li>Using conjucation, we can get the posterior distribution more easily using the prior distribution we know.</li><li>Uniform distiribution is same when beta distribution met special case(alpha=1, beta=1).</li></ul><script type="math/tex; mode=display">\begin{align}f(x;\alpha,\beta) & = \mathrm{constant}\cdot x^{\alpha-1}(1-x)^{\beta-1} \\[3pt]& = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\displaystyle \int_0^1 u^{\alpha-1} (1-u)^{\beta-1}\, du} \\[6pt]& = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, x^{\alpha-1}(1-x)^{\beta-1} \\[6pt]& = \frac{1}{B(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1}\end{align}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beta</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line"></span><br><span class="line">    gamma = gamma_function(a + b) / \</span><br><span class="line">            (gamma_function(a) * gamma_function(b))</span><br><span class="line">    y = gamma * (x ** (a - <span class="number">1</span>)) * ((<span class="number">1</span> - x) ** (b - <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">3</span>), (<span class="number">5</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">5</span>)]:</span><br><span class="line">    a, b = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x in [0, 1], trial is 1/0.001 = 1000</span></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.001</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = beta(x, a=a, b=b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \alpha=%d,\ \beta=%d$'</span> % (u, s, a, b))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.beta(<span class="number">2</span>, <span class="number">5</span>, size=<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_19_0.png" alt="png"></p><h2 id="Gamma-distribution-continuous"><a href="#Gamma-distribution-continuous" class="headerlink" title="Gamma distribution(continuous)"></a>Gamma distribution(continuous)</h2><ul><li><p>Gamma distribution will be beta distribution, if $\frac{Gamma(a,1)}{Gamma(a,1) + Gamma(b,1)}$ is same with $Beta(a,b)$.</p></li><li><p>The exponential distribution and chi-squared distribution are special cases of the gamma distribution.</p></li></ul><p>A random variable X that is gamma-distributed with shape 伪 and rate 尾 is denoted:</p><script type="math/tex; mode=display">X \sim \Gamma(\alpha, \beta) \equiv \operatorname{Gamma}(\alpha,\beta)</script><p>The corresponding probability density function in the shape-rate parametrization is:</p><script type="math/tex; mode=display">\begin{align}f(x;\alpha,\beta) & = \frac{ \beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)} \quad \text{ for } x > 0 \quad \alpha, \beta > 0, \\[6pt]\end{align}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line">    c = (b ** a) / gamma_function(a)</span><br><span class="line">    y = c * (x ** (a - <span class="number">1</span>)) * np.exp(-b * x)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>)]:</span><br><span class="line">    a, b = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">20</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = gamma(x, a=a, b=b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \alpha=%d,\ \beta=%d$'</span> % (u, s, a, b))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_21_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a, b = <span class="number">2.</span>, <span class="number">2.</span></span><br><span class="line">s = np.random.gamma(a, b, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">50</span>, density=<span class="keyword">True</span>)</span><br><span class="line">y = bins**(a<span class="number">-1</span>)*(np.exp(-bins/b) /</span><br><span class="line">                      (sps.gamma(a)*b**b))</span><br><span class="line">plt.plot(bins, y, linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x12cb80c50&gt;]</code></pre><p><img src="output_22_1.png" alt="png"></p><h2 id="Dirichlet-distribution-continuous"><a href="#Dirichlet-distribution-continuous" class="headerlink" title="Dirichlet distribution(continuous)"></a>Dirichlet distribution(continuous)</h2><ul><li>Dirichlet distribution is conjugate to the MultiNomial distributions. Dirichlet甯涔涓涓涓澶椤瑰甯浼肩跺芥板锛寰扮楠甯浠舵涓涓Dirichlet甯</li><li>If k=2, it will be Beta distribution.</li></ul><script type="math/tex; mode=display">f \left(x_1,\ldots, x_{K}; \alpha_1,\ldots, \alpha_K \right) = \frac{1}{\mathrm{B}(\boldsymbol\alpha)} \prod_{i=1}^K x_i^{\alpha_i - 1}</script><p>where $\{x_k\}_{k=1}^{k=K}$ belong to the standard $K-1$ simplex, or in other words: </p><script type="math/tex; mode=display">\sum_{i=1}^{K} x_i=1 \mbox{ and } x_i \ge 0 \mbox{ for all } i \in [1,K]</script><p>The normalizing constant is the multivariate beta function, which can be expressed in terms of the gamma function</p><script type="math/tex; mode=display">\mathrm{B}(\boldsymbol\alpha) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^K \alpha_i\right)},\qquad\boldsymbol{\alpha}=(\alpha_1,\ldots,\alpha_K).</script><blockquote><p>Dirichlet甯浠ョ甯涔涓甯濡浣瑙ｈヨ锛浠浠ュ涓句釜渚瀛锛璁炬浠涓涓楠板锛舵锛涓{1,2,3,4,5,6}板ㄦ浠浜10000娆℃风瀹楠锛寰扮瀹楠缁㈠虹颁{2000,2000,2000,2000,1000,1000}娆★濡ㄦ涓㈠虹扮娆℃颁璇楠绘扮姣间及璁¤涓㈠虹扮姒锛浠寰板㈠虹扮姒锛涓{0.2,0.2,0.2,0.2,0.1,0.1}板锛浠杩涓婊¤冻锛浠宠10000娆¤楠锛姣娆¤楠涓浠芥烽板10000娆°浠崇ラ锛楠板㈠虹版涓{0.2,0.2,0.2,0.2,0.1,0.1}姒澶灏锛璇翠瀹涓娆¤楠缁璁″扮姒涓{0.1, 0.1, 0.2, 0.2, 0.2, 0.2}杩蜂锛杩锋浠灏卞ㄦ楠板㈠虹版甯杩风甯涔涓甯杩蜂涓甯灏辨Dirichlet甯 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalization</span><span class="params">(x, s)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :return: normalizated list, where sum(x) == s</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> [(i * s) / sum(x) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampling</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> normalization([random.randint(<span class="number">1</span>, <span class="number">100</span>),</span><br><span class="line">            random.randint(<span class="number">1</span>, <span class="number">100</span>), random.randint(<span class="number">1</span>, <span class="number">100</span>)], s=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beta_function</span><span class="params">(alpha)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param alpha: list, len(alpha) is k</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    numerator = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> alpha:</span><br><span class="line">        numerator *= gamma_function(a)</span><br><span class="line">    denominator = gamma_function(sum(alpha))</span><br><span class="line">    <span class="keyword">return</span> numerator / denominator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dirichlet</span><span class="params">(x, a, n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param x: list of [x[1,...,K], x[1,...,K], ...], shape is (n_trial, K)</span></span><br><span class="line"><span class="string">    :param a: list of coefficient, a_i &gt; 0</span></span><br><span class="line"><span class="string">    :param n: number of trial</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    c = (<span class="number">1</span> / beta_function(a))</span><br><span class="line">    y = [c * (xn[<span class="number">0</span>] ** (a[<span class="number">0</span>] - <span class="number">1</span>)) * (xn[<span class="number">1</span>] ** (a[<span class="number">1</span>] - <span class="number">1</span>))</span><br><span class="line">         * (xn[<span class="number">2</span>] ** (a[<span class="number">2</span>] - <span class="number">1</span>)) <span class="keyword">for</span> xn <span class="keyword">in</span> x]</span><br><span class="line">    x = np.arange(n)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">1200</span></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">6</span>, <span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>), (<span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>), (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)]:</span><br><span class="line">    alpha = list(ls)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># random samping [x[1,...,K], x[1,...,K], ...], shape is (n_trial, K)</span></span><br><span class="line">    <span class="comment"># each sum of row should be one.</span></span><br><span class="line">    x = [sampling() <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>, n_experiment + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    x, y, u, s = dirichlet(x, alpha, n=n_experiment)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\alpha=(%d,%d,%d)$'</span> % (ls[<span class="number">0</span>], ls[<span class="number">1</span>], ls[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_24_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.dirichlet((<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.5</span>), <span class="number">20</span>).transpose()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s.shape</span><br></pre></td></tr></table></figure><pre><code>(3, 20)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">0</span>])</span><br><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">1</span>], left=s[<span class="number">0</span>], color=<span class="string">'g'</span>)</span><br><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">2</span>], left=s[<span class="number">0</span>]+s[<span class="number">1</span>], color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;BarContainer object of 20 artists&gt;</code></pre><p><img src="output_27_1.png" alt="png"></p><h2 id="Exponential-distribution-continuous"><a href="#Exponential-distribution-continuous" class="headerlink" title="Exponential distribution(continuous)"></a>Exponential distribution(continuous)</h2><ul><li>Exponential distribution is special cases of the gamma distribution when alpha is 1.</li></ul><script type="math/tex; mode=display">f(x;\lambda) = \begin{cases}\lambda e^{-\lambda x} & x \ge 0, \\0 & x < 0.\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exponential</span><span class="params">(x, lamb)</span>:</span></span><br><span class="line">    y = lamb * np.exp(-lamb * x)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lamb <span class="keyword">in</span> [<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">1.5</span>]:</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">20</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = exponential(x, lamb=lamb)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \lambda=%d$'</span> % (u, s, lamb))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_29_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.exponential(scale = <span class="number">0.5</span>, size=<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_30_0.png" alt="png"></p><h2 id="Gaussian-distribution-continuous"><a href="#Gaussian-distribution-continuous" class="headerlink" title="Gaussian distribution(continuous)"></a>Gaussian distribution(continuous)</h2><script type="math/tex; mode=display">f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    u = x.mean()</span><br><span class="line">    s = x.std()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># divide [x.min(), x.max()] by n</span></span><br><span class="line">    x = np.linspace(x.min(), x.max(), n)</span><br><span class="line"></span><br><span class="line">    a = ((x - u) ** <span class="number">2</span>) / (<span class="number">2</span> * (s ** <span class="number">2</span>))</span><br><span class="line">    y = <span class="number">1</span> / (s * np.sqrt(<span class="number">2</span> * np.pi)) * np.exp(-a)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, x.mean(), x.std()</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-100</span>, <span class="number">100</span>) <span class="comment"># define range of x</span></span><br><span class="line">x, y, u, s = gaussian(x, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_32_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mu, sigma = <span class="number">0</span>, <span class="number">0.1</span> <span class="comment"># mean and standard deviation</span></span><br><span class="line">s = np.random.default_rng().normal(mu, sigma, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">30</span>, density=<span class="keyword">True</span>)</span><br><span class="line">plt.plot(bins, <span class="number">1</span>/(sigma * np.sqrt(<span class="number">2</span> * np.pi)) *</span><br><span class="line">         np.exp( - (bins - mu)**<span class="number">2</span> / (<span class="number">2</span> * sigma**<span class="number">2</span>) ),</span><br><span class="line">         linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x11122d4e0&gt;]</code></pre><p><img src="output_33_1.png" alt="png"></p><h2 id="Poisson-distribution"><a href="#Poisson-distribution" class="headerlink" title="Poisson distribution"></a>Poisson distribution</h2><ul><li>ㄤ涓堕存靛浜浠跺钩娆℃版浠娉惧甯</li></ul><script type="math/tex; mode=display">\!f(k; \lambda)= \Pr(X = k)= \frac{\lambda^k e^{-\lambda}}{k!},</script><ul><li>e is Eulers number (e = 2.71828)</li><li>k! is the factorial of k.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.poisson(<span class="number">5</span>, <span class="number">10000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">14</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_35_0.png" alt="png"></p><h2 id="Chi-squared-distribution-continuous"><a href="#Chi-squared-distribution-continuous" class="headerlink" title="Chi-squared distribution(continuous)"></a>Chi-squared distribution(continuous)</h2><ul><li>Chi-square distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables.</li><li>Chi-square distribution is special case of Beta distribution</li></ul><p>If $Z_1, \cdots, Z_k$ are independent, standard normal random variables, then the sum of their squares,</p><script type="math/tex; mode=display">Q\ = \sum_{i=1}^k Z_i^2 ,</script><p>is distributed according to the chi-square distribution with k degrees of freedom. This is usually denoted as</p><script type="math/tex; mode=display">Q\ \sim\ \chi^2(k)\ \ \text{or}\ \ Q\ \sim\ \chi^2_k .</script><p>The chi-square distribution has one parameter: a positive integer k that specifies the number of degrees of freedom (the number of $Z_i$ s).</p><p>The probability density function (pdf) of the chi-square distribution is</p><script type="math/tex; mode=display">f(x;\,k) =\begin{cases}  \dfrac{x^{\frac k 2 -1} e^{-\frac x 2}}{2^{\frac k 2} \Gamma\left(\frac k 2 \right)},  & x > 0; \\ 0, & \text{otherwise}.\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chi_squared</span><span class="params">(x, k)</span>:</span></span><br><span class="line"></span><br><span class="line">    c = <span class="number">1</span> / (<span class="number">2</span> ** (k/<span class="number">2</span>)) * gamma_function(k//<span class="number">2</span>)</span><br><span class="line">    y = c * (x ** (k/<span class="number">2</span> - <span class="number">1</span>)) * np.exp(-x /<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>]:</span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, _, _ = chi_squared(x, k)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$k=%d$'</span> % (k))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_37_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.chisquare(<span class="number">4</span>,<span class="number">10000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_38_0.png" alt="png"></p><h2 id="Student-t-distribution-continuous"><a href="#Student-t-distribution-continuous" class="headerlink" title="Student-t distribution(continuous)"></a>Student-t distribution(continuous)</h2><ul><li>Definition</li></ul><p>Let $X_1, \cdots, X_n$ be independent and identically distributed as $N(\mu, \sigma^2)$, i.e. this is a sample of size $n$ from a normally distributed population with expected mean value $\mu$ and variance $\sigma^{2}$</p><p>Let</p><script type="math/tex; mode=display">\bar X = \frac 1 n \sum_{i=1}^n X_i</script><p>be the sample mean and let</p><script type="math/tex; mode=display">S^2 = \frac 1 {n-1} \sum_{i=1}^n (X_i - \bar X)^2</script><p>be the (Bessel-corrected) sample variance. </p><p>Then the random variable</p><script type="math/tex; mode=display">\frac{ \bar X - \mu} {S /\sqrt{n}}</script><p>has a standard normal distribution</p><p>Students t-distribution has the probability density function given by</p><script type="math/tex; mode=display">f(t) = \frac{\Gamma(\frac{\nu+1}{2})} {\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})} \left(1+\frac{t^2}{\nu} \right)^{\!-\frac{\nu+1}{2}}</script><ul><li>$\nu$ is the number of degrees of freedom </li><li>$\Gamma$ is the gamma function.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">student_t</span><span class="params">(x, freedom, n)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># divide [x.min(), x.max()] by n</span></span><br><span class="line">    x = np.linspace(x.min(), x.max(), n)</span><br><span class="line"></span><br><span class="line">    c = gamma_function((freedom + <span class="number">1</span>) // <span class="number">2</span>) \</span><br><span class="line">        / np.sqrt(freedom * np.pi) * gamma_function(freedom // <span class="number">2</span>)</span><br><span class="line">    y = c * (<span class="number">1</span> + x**<span class="number">2</span> / freedom) ** (-((freedom + <span class="number">1</span>) / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> freedom <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>]:</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">-10</span>, <span class="number">10</span>) <span class="comment"># define range of x</span></span><br><span class="line">    x, y, _, _ = student_t(x, freedom=freedom, n=<span class="number">10000</span>)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$v=%d$'</span> % (freedom))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_40_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Suppose the daily energy intake for 11 women in kilojoules (kJ) is:</span></span><br><span class="line"></span><br><span class="line">intake = np.array([<span class="number">5260.</span>, <span class="number">5470</span>, <span class="number">5640</span>, <span class="number">6180</span>, <span class="number">6390</span>, <span class="number">6515</span>, <span class="number">6805</span>, <span class="number">7515</span>, \</span><br><span class="line">                    <span class="number">7515</span>, <span class="number">8230</span>, <span class="number">8770</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## Does their energy intake deviate systematically from the recommended value of 7725 kJ?</span></span><br><span class="line"><span class="comment">## We have 10 degrees of freedom, so is the sample mean within 95% of the recommended value?</span></span><br><span class="line"></span><br><span class="line">s = np.random.standard_t(<span class="number">10</span>, size=<span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Calculate the t statistic, setting the ddof parameter to the unbiased value so the divisor in the standard deviation will be degrees of freedom, N-1.</span></span><br><span class="line"></span><br><span class="line">t = (np.mean(intake)<span class="number">-7725</span>)/(intake.std(ddof=<span class="number">1</span>)/np.sqrt(len(intake)))</span><br><span class="line"></span><br><span class="line">h = plt.hist(s, bins=<span class="number">100</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_41_0.png" alt="png"></p><p>So the p-value is about 0.009, which says the null hypothesis has a probability of about 99% of being true.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(s&lt;t) / float(len(s))</span><br></pre></td></tr></table></figure><pre><code>0.0086</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/graykode/distribution-is-all-you-need" target="_blank" rel="noopener">https://github.com/graykode/distribution-is-all-you-need</a></li><li><a href="https://blog.csdn.net/deropty/article/details/50266309" target="_blank" rel="noopener">https://blog.csdn.net/deropty/article/details/50266309</a></li></ul>]]></content>
    
    <summary type="html">
    
      Understanding distributions
    
    </summary>
    
    
      <category term="Math" scheme="https://zhangruochi.com/categories/Math/"/>
    
      <category term="Statistics" scheme="https://zhangruochi.com/categories/Math/Statistics/"/>
    
    
  </entry>
  
  <entry>
    <title>Gaussian Mixed Model Introduction</title>
    <link href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/"/>
    <id>https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/</id>
    <published>2020-03-15T04:44:46.000Z</published>
    <updated>2020-03-15T07:08:58.707Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Gaussian-Mixture-Models-楂娣峰妯″"><a href="#Gaussian-Mixture-Models-楂娣峰妯″" class="headerlink" title="Gaussian Mixture Models(楂娣峰妯″)"></a>Gaussian Mixture Models(楂娣峰妯″)</h2><p>楂妯″虫ｆ甯锛楂娣峰妯″灏辨涓姝ｆ甯锛姣涓涓姝ｆ甯浠ｈ〃涓涓绫诲锛浠ュK-means寰锛楂娣峰妯″涔浠ョㄦュｇ绫诲</p><h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><h3 id="Jensens-inequality"><a href="#Jensens-inequality" class="headerlink" title="Jensens inequality"></a>Jensens inequality</h3><p>For any concave function, we have</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Property of concave function</div></center><script type="math/tex; mode=display">f(\alpha a + (1-\alpha)b) \geq \alpha f(a) + (1 - \alpha) f(b)</script><p>Then, we have:</p><script type="math/tex; mode=display">f(\mathbb{E}_{p(t)}t) \geq \mathbb{E}_{p(t)}f(t)</script><h3 id="KullbackLeibler-divergence"><a href="#KullbackLeibler-divergence" class="headerlink" title="KullbackLeibler divergence"></a>KullbackLeibler divergence</h3><script type="math/tex; mode=display">\mathcal K \mathcal L (q || p) = \int q(x) log\frac{q(x)}{p(x)}dx</script><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><script type="math/tex; mode=display">max_{\theta} \prod_{i=1}^{N} p(x_i | \theta) = \prod_{i = 1}^{N} (\pi_1 \mathcal{N} (x_i | \mu_1, \mathbb{E_1}) + \cdots )</script><script type="math/tex; mode=display">\text{subject to} \qquad \pi_1 + \pi_2 + \pi_3 = 1; \pi_k \geq 0; k = 1,2,3</script><h3 id="Introducing-latent-variable"><a href="#Introducing-latent-variable" class="headerlink" title="Introducing latent variable"></a>Introducing latent variable</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Latent Variable</div></center><script type="math/tex; mode=display">p(t=c| \theta) = \pi_c</script><script type="math/tex; mode=display">p(x | t = c , \theta) = \mathcal N ( x | \mu_c,\mathbb{E_c} )</script><h3 id="General-form-of-Expectation-Maximization"><a href="#General-form-of-Expectation-Maximization" class="headerlink" title="General form of Expectation Maximization"></a>General form of Expectation Maximization</h3><script type="math/tex; mode=display">p(x_i | \theta) = \sum_{c=1}^{3}p(x_i | t_i = c , \theta) p(t_i = c | \theta)</script><h2 id="烘姝ラ"><a href="#烘姝ラ" class="headerlink" title="烘姝ラ"></a>烘姝ラ</h2><h3 id="姒瑙搴"><a href="#姒瑙搴" class="headerlink" title="姒瑙搴"></a>姒瑙搴</h3><ol><li>濮$\theta^{old}$</li><li>E step:  $\theta^{old}$璁＄锋瀵瑰姒甯锛虫楠姒锛$p(Z|X,\theta^{old})$跺璁＄瀹ㄦ版瀵规颁技跺瑰楠姒锛瀹$\theta$芥:<script type="math/tex; mode=display">Q(\theta, \theta^{old}) = \sum_{Z}p(Z|X, \theta^{old})ln p(X,Z|\theta)</script></li><li>M step: 澶уQ芥,寰$\theta^{new}$</li><li>ヤ舵缁杩浠ｃ</li></ol><h3 id="绋搴瑙搴"><a href="#绋搴瑙搴" class="headerlink" title="绋搴瑙搴"></a>绋搴瑙搴</h3><ol><li>娴涓绫诲锛㈡涓楂甯;</li><li>瀵规涓涓楂甯锛虹跺煎瑰樊杩琛璧;</li><li>瀵规涓涓锋锛璁＄跺ㄥ涓楂甯涓姒;<script type="math/tex; mode=display">f(x)=\frac{1 }{\sqrt\times\sigma}e^{\frac{1}{2}(\frac{x-\mu}{\sigma})^2}</script></li><li>瀵规涓涓楂甯锛姣涓涓锋瀵硅ラ甯璐＄浠ョ卞朵姒琛ㄧず锛濡姒澶у琛ㄧず璐＄澶э涔浜躲杩锋锋瀵硅ラ甯璐＄浣涓烘ヨ＄煎瑰樊涔夸唬跺煎瑰樊;</li><li>澶3~4村版涓涓楂甯煎瑰樊舵;</li><li>褰楂娣峰妯″瑰肩淮板ぇ浜涓缁存讹ㄨ＄跺杩瑕璁＄瑰樊锛宠涓缁村害涔寸镐宠.</li></ol><blockquote><p>抽杩妯″ヨ＄版笺杩存板拔煎ヨ╂兼澶у杩涓杩绋浠ヤ杩浠ｇ村颁袱娆¤凯浠ｄ腑板甯稿涓烘璇ヨ绋k-means绠娉璁缁杩绋寰镐技锛k-means涓存扮被涓蹇ヨ╃澶у锛锛涓杩ㄨ楂妯″涓锛浠瑕舵存颁袱涓帮甯煎宸</p></blockquote><h2 id="GMM-VS-KMeans"><a href="#GMM-VS-KMeans" class="headerlink" title="GMM VS KMeans"></a>GMM VS KMeans</h2><p>KMeans 灏锋扮诲舵杩绫讳腑蹇ㄧ绨锛涔灏辨姣涓锋版灞浜绨姒跺1瀵规KMeans锛楂娣峰涓涔澶ㄤ锛锋瑰浜绨姒涓跺1锛灞浜涓绨涓姒笺楂娣峰妯″璁炬锋规K涓楂甯娣峰</p><h2 id="Implementing-the-EM-Expectation-Maximization-algorithm-for-Gaussian-mixture-models"><a href="#Implementing-the-EM-Expectation-Maximization-algorithm-for-Gaussian-mixture-models" class="headerlink" title="Implementing the EM(Expectation Maximization) algorithm for Gaussian mixture models"></a>Implementing the EM(Expectation Maximization) algorithm for Gaussian mixture models</h2><h3 id="Log-likelihood"><a href="#Log-likelihood" class="headerlink" title="Log likelihood"></a>Log likelihood</h3><p>We provide a function to calculate log likelihood for mixture of Gaussians. The log likelihood quantifies the probability of observing a given set of data under a particular setting of the parameters in our model. We will use this to assess convergence of our EM algorithm; specifically, we will keep looping through EM update steps until the log likehood ceases to increase at a certain rate.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span><span class="params">(Z)</span>:</span></span><br><span class="line">    <span class="string">""" Compute log(\sum_i exp(Z_i)) for some array Z."""</span></span><br><span class="line">    <span class="keyword">return</span> np.max(Z) + np.log(np.sum(np.exp(Z - np.max(Z))))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loglikelihood</span><span class="params">(data, weights, means, covs)</span>:</span></span><br><span class="line">    <span class="string">""" Compute the loglikelihood of the data for a Gaussian mixture model with the given parameters. """</span></span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    ll = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        </span><br><span class="line">        Z = np.zeros(num_clusters)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute (x-mu)^T * Sigma^&#123;-1&#125; * (x-mu)</span></span><br><span class="line">            delta = np.array(d) - means[k]</span><br><span class="line">            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute loglikelihood contribution for this data point and this cluster</span></span><br><span class="line">            Z[k] += np.log(weights[k])</span><br><span class="line">            Z[k] -= <span class="number">1</span>/<span class="number">2.</span> * (num_dim * np.log(<span class="number">2</span>*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Increment loglikelihood contribution of this data point across all clusters</span></span><br><span class="line">        ll += log_sum_exp(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> ll</span><br></pre></td></tr></table></figure><h3 id="E-step-assign-cluster-responsibilities-given-current-parameters"><a href="#E-step-assign-cluster-responsibilities-given-current-parameters" class="headerlink" title="E-step: assign cluster responsibilities, given current parameters"></a>E-step: assign cluster responsibilities, given current parameters</h3><p>The first step in the EM algorithm is to compute cluster responsibilities. Let $r_{ik}$ denote the responsibility of cluster $k$ for data point $i$. Note that cluster responsibilities are fractional parts: Cluster responsibilities for a single data point $i$ should sum to 1.</p><script type="math/tex; mode=display">r_{i1} + r_{i2} + \ldots + r_{iK} = 1</script><p>To figure how much a cluster is responsible for a given data point, we compute the likelihood of the data point under the  particular cluster assignment, multiplied by the weight of the cluster. For data point $i$ and cluster $k$, this quantity is</p><script type="math/tex; mode=display">r_{ik} \propto \pi_k N(x_i | \mu_k, \Sigma_k)</script><p>where $N(x_i | \mu_k, \Sigma_k)$ is the Gaussian distribution for cluster $k$ (with mean $\mu_k$ and covariance $\Sigma_k$).</p><p>We used $\propto$ because the quantity $N(x_i | \mu_k, \Sigma_k)$ is not yet the responsibility we want. To ensure that all responsibilities over each data point add up to 1, we add the normalization constant in the denominator:</p><script type="math/tex; mode=display">r_{ik} = \frac{\pi_k N(x_i | \mu_k, \Sigma_k)}{\sum_{k=1}^{K} \pi_k N(x_i | \mu_k, \Sigma_k)}.</script><p>Complete the following function that computes $r_{ik}$ for all data points $i$ and clusters $k$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_responsibilities</span><span class="params">(data, weights, means, covariances)</span>:</span></span><br><span class="line">    <span class="string">'''E-step: compute responsibilities, given the current parameters'''</span></span><br><span class="line">    num_data = len(data)</span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    resp = np.zeros((num_data, num_clusters))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update resp matrix so that resp[i,k] is the responsibility of cluster k for data point i.</span></span><br><span class="line">    <span class="comment"># Hint: To compute likelihood of seeing data point i given cluster k, use multivariate_normal.pdf.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            resp[i, k] = weights[k]*multivariate_normal.pdf(data[i], mean=means[k], cov=covariances[k])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add up responsibilities over each data point and normalize</span></span><br><span class="line">    row_sums = resp.sum(axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">    resp = resp / row_sums</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure><h3 id="M-step-Update-parameters-given-current-cluster-responsibilities"><a href="#M-step-Update-parameters-given-current-cluster-responsibilities" class="headerlink" title="M-step: Update parameters, given current cluster responsibilities"></a>M-step: Update parameters, given current cluster responsibilities</h3><p>Once the cluster responsibilities are computed, we update the parameters (weights, means, and covariances) associated with the clusters.</p><p><strong>Computing soft counts</strong>. Before updating the parameters, we first compute what is known as soft counts. The soft count of a cluster is the sum of all cluster responsibilities for that cluster:</p><script type="math/tex; mode=display">N^{\text{soft}}_k = r_{1k} + r_{2k} + \ldots + r_{Nk} = \sum_{i=1}^{N} r_{ik}</script><p>where we loop over data points. Note that, unlike k-means, we must loop over every single data point in the dataset. This is because all clusters are represented in all data points, to a varying degree.</p><p>We provide the function for computing the soft counts:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_soft_counts</span><span class="params">(resp)</span>:</span></span><br><span class="line">    <span class="comment"># Compute the total responsibility assigned to each cluster, which will be useful when </span></span><br><span class="line">    <span class="comment"># implementing M-steps below. In the lectures this is called N^&#123;soft&#125;</span></span><br><span class="line">    counts = np.sum(resp, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> counts</span><br></pre></td></tr></table></figure><p><strong>Updating weights.</strong> The cluster weights show us how much each cluster is represented over all data points. The weight of cluster $k$ is given by the ratio of the soft count $N^{\text{soft}}_{k}$ to the total number of data points $N$:</p><script type="math/tex; mode=display">\hat{\pi}_k = \frac{N^{\text{soft}}_{k}}{N}</script><p>Notice that $N$ is equal to the sum over the soft counts $N^{\text{soft}}_{k}$ of all clusters.</p><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_weights</span><span class="params">(counts)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    weights = [<span class="number">0.</span>] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update the weight for cluster k using the M-step update rule for the cluster weight, \hat&#123;\pi&#125;_k.</span></span><br><span class="line">        <span class="comment"># HINT: compute # of data points by summing soft counts.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        weights[k] = counts[k] / np.sum(counts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure><p><strong>Updating means</strong>. The mean of each cluster is set to the <a href="https://en.wikipedia.org/wiki/Weighted_arithmetic_mean" target="_blank" rel="noopener">weighted average</a> of all data points, weighted by the cluster responsibilities:</p><script type="math/tex; mode=display">\hat{\mu}_k = \frac{1}{N_k^{\text{soft}}} \sum_{i=1}^N r_{ik}x_i</script><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_means</span><span class="params">(data, resp, counts)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    num_data = len(data)</span><br><span class="line">    means = [np.zeros(len(data[<span class="number">0</span>]))] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update means for cluster k using the M-step update rule for the mean variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable means[k] to be our estimate for \hat&#123;\mu&#125;_k.</span></span><br><span class="line">        weighted_sum = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            weighted_sum += data[i] * resp[i][k]</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        means[k] = weighted_sum / counts[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> means</span><br></pre></td></tr></table></figure><p><strong>Updating covariances</strong>.  The covariance of each cluster is set to the weighted average of all <a href="https://people.duke.edu/~ccc14/sta-663/LinearAlgebraReview.html" target="_blank" rel="noopener">outer products</a>, weighted by the cluster responsibilities:</p><script type="math/tex; mode=display">\hat{\Sigma}_k = \frac{1}{N^{\text{soft}}_k}\sum_{i=1}^N r_{ik} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T</script><p>The outer product in this context refers to the matrix product</p><script type="math/tex; mode=display">(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T.</script><p>Letting $(x_i - \hat{\mu}_k)$ to be $d \times 1$ column vector, this product is a $d \times d$ matrix. Taking the weighted average of all outer products gives us the covariance matrix, which is also $d \times d$.</p><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_covariances</span><span class="params">(data, resp, counts, means)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    num_data = len(data)</span><br><span class="line">    covariances = [np.zeros((num_dim,num_dim))] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update covariances for cluster k using the M-step update rule for covariance variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable covariances[k] to be the estimate for \hat&#123;\Sigma&#125;_k.</span></span><br><span class="line">        weighted_sum = np.zeros((num_dim, num_dim))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE (Hint: Use np.outer on the data[i] and this cluster's mean)</span></span><br><span class="line">            weighted_sum += resp[i][k]*np.outer(data[i] - means[k], data[i] - means[k])</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        covariances[k] = weighted_sum / counts[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> covariances</span><br></pre></td></tr></table></figure><h3 id="The-EM-algorithm"><a href="#The-EM-algorithm" class="headerlink" title="The EM algorithm"></a>The EM algorithm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SOLUTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EM</span><span class="params">(data, init_means, init_covariances, init_weights, maxiter=<span class="number">1000</span>, thresh=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make copies of initial parameters, which we will update during each iteration</span></span><br><span class="line">    means = init_means[:]</span><br><span class="line">    covariances = init_covariances[:]</span><br><span class="line">    weights = init_weights[:]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Infer dimensions of dataset and the number of clusters</span></span><br><span class="line">    num_data = len(data)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize some useful variables</span></span><br><span class="line">    resp = np.zeros((num_data, num_clusters))</span><br><span class="line">    ll = loglikelihood(data, weights, means, covariances)</span><br><span class="line">    ll_trace = [ll]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(maxiter):</span><br><span class="line">        <span class="keyword">if</span> it % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Iteration %s"</span> % it)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># E-step: compute responsibilities</span></span><br><span class="line">        resp = compute_responsibilities(data, weights, means, covariances)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># M-step</span></span><br><span class="line">        <span class="comment"># Compute the total responsibility assigned to each cluster, which will be useful when </span></span><br><span class="line">        <span class="comment"># implementing M-steps below. In the lectures this is called N^&#123;soft&#125;</span></span><br><span class="line">        counts = compute_soft_counts(resp)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the weight for cluster k using the M-step update rule for the cluster weight, \hat&#123;\pi&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        weights = compute_weights(counts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update means for cluster k using the M-step update rule for the mean variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable means[k] to be our estimate for \hat&#123;\mu&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        means = compute_means(data, resp, counts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update covariances for cluster k using the M-step update rule for covariance variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable covariances[k] to be the estimate for \hat&#123;\Sigma&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        covariances = compute_covariances(data, resp, counts, means)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the loglikelihood at this iteration</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        ll_latest = loglikelihood(data, weights, means, covariances)</span><br><span class="line">        ll_trace.append(ll_latest)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check for convergence in log-likelihood and store</span></span><br><span class="line">        <span class="keyword">if</span> (ll_latest - ll) &lt; thresh <span class="keyword">and</span> ll_latest &gt; -np.inf:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        ll = ll_latest</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">5</span> != <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Iteration %s"</span> % it)</span><br><span class="line">    </span><br><span class="line">    out = &#123;<span class="string">'weights'</span>: weights, <span class="string">'means'</span>: means, <span class="string">'covs'</span>: covariances, <span class="string">'loglik'</span>: ll_trace, <span class="string">'resp'</span>: resp&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p><strong>Reference from <a href="https://zhuanlan.zhihu.com/p/29538307" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29538307</a></strong><br><strong>Reference from <a href="https://zhuanlan.zhihu.com/p/31103654" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31103654</a></strong><br><strong>Reference from coursera course Machine Learning Foundation from University of Washington</strong>  </p>]]></content>
    
    <summary type="html">
    
      Gaussian Mixture Models Introduction
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Machine Learning Algorithm" scheme="https://zhangruochi.com/tags/Machine-Learning-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Lifelong Learning</title>
    <link href="https://zhangruochi.com/Lifelong-Learning/2020/03/14/"/>
    <id>https://zhangruochi.com/Lifelong-Learning/2020/03/14/</id>
    <published>2020-03-14T06:28:06.000Z</published>
    <updated>2020-03-14T07:42:44.810Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Three-problem-need-to-solve-when-realize-lifelong-learnig"><a href="#Three-problem-need-to-solve-when-realize-lifelong-learnig" class="headerlink" title="Three problem need to solve when realize lifelong learnig"></a>Three problem need to solve when realize lifelong learnig</h2><ol><li>Knowledge Retention</li><li>Knowledge Transfer</li><li>Model Expansion</li></ol><h2 id="Knowledge-Retention"><a href="#Knowledge-Retention" class="headerlink" title="Knowledge Retention"></a>Knowledge Retention</h2><p>When a network learn a new task. It has the inclination to forget the skills it has learned. The phenomenon that the model will forget the previous skills is called <strong>Catastrophic Forgetting</strong>.</p><p>The reason for resulting the catastrophic forgetting is not the models size which has no enough capacity to learn the new skills. To prove this we can make the model to learn a multi-task problem and get a good results.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><p>The ideas to solve knowledge retention</p><h3 id="Elastic-Weight-Consolidation"><a href="#Elastic-Weight-Consolidation" class="headerlink" title="Elastic Weight Consolidation"></a>Elastic Weight Consolidation</h3><script type="math/tex; mode=display">L^{\prime}(\theta) = L(\theta) + \lambda \sum_{i} b_i(\theta_i - \theta_i^{b})^2</script><ul><li>$L^{\prime}(\theta)$ : the loss to be optimized</li><li>$L(\theta)$ : the loss of current task</li><li>$\theta_i$ : the parameters to be learning</li><li>$\theta_i^{b}$ : the parameters leaned from previous tasks</li><li>$b_i$ how important the parameter is</li></ul><p>the idea of EWC is: learning the new parameters which are not far from the previous parameters</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">EWC</div></center><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="How-to-set-b-i"><a href="#How-to-set-b-i" class="headerlink" title="How to set $b_i$ ?"></a>How to set $b_i$ ?</h4><ul><li>small 2nd derivative -&gt; set $b_i$ to small or large</li><li>large 2nd derivative -&gt;  set $b_i$ to small</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">important of parameters</div></center><h3 id="Generating-Data"><a href="#Generating-Data" class="headerlink" title="Generating Data"></a>Generating Data</h3><p>Conducting multi-task learning by generating pseudo-data using generative model. </p><p>We know the multi-task learning is a good way to solve life long task(sometimes it is the upper bound). If a new task come, we can regard it as a multi-task problem combined with previous tasks and build a model to solve it. But the premise of dong this is we have the data of previous tasks. In reality, we can not store all the dataset of previous tasks. Therefore, We can use generative model to generate previous dataset.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Generating Data</div></center><h2 id="Knowledge-Transfer"><a href="#Knowledge-Transfer" class="headerlink" title="Knowledge Transfer"></a>Knowledge Transfer</h2><p>The difference of knowledge transfer between lifelong learning with transfer learning is that <code>transfer learning</code> is just concentrate on new task while the lifelong learning shuild consider the catastrophic forgetting</p><h3 id="Gredient-Episodic-Memory"><a href="#Gredient-Episodic-Memory" class="headerlink" title="Gredient Episodic Memory"></a>Gredient Episodic Memory</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>The idea of GEM is: When we update the parameters by gredient descent we can find a direction which can benifits the previous tasks and new tasks to update. The disadvantages of GEM is we need store a little bit of data of previous tasks.</p><h2 id="Model-Expansion"><a href="#Model-Expansion" class="headerlink" title="Model Expansion"></a>Model Expansion</h2><h3 id="Progressive-Neural-Networks"><a href="#Progressive-Neural-Networks" class="headerlink" title="Progressive Neural Networks"></a>Progressive Neural Networks</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>An example of model expansion is the <code>Progressive Neural Networks</code> proposed in 2016. We fix the parameters after learning some tasks, and then train the new task. We build a new model and use the output of previous task as the input of new task. However, there is a disadvantage that you can not train too many new tasks because it will cause a lot of load.</p><h3 id="Expert-Gate"><a href="#Expert-Gate" class="headerlink" title="Expert Gate"></a>Expert Gate</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="7.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>Exper Gates method: We still have one model for each task. For example, we have three tasks, and the fourth task is similar to the first one (we use Gate to determine which new task is similar to the old one), then we will use the model of the first task as the fourth Initialization of each task model, this has formed a certain migration effect. However, this method is still a task corresponding to a model, which still causes a lot of load on storage.</p><h3 id="Tasknomy"><a href="#Tasknomy" class="headerlink" title="Tasknomy"></a>Tasknomy</h3><p>The order of learning tasks is just like the order of our textbooks, which has a great impact on the final results. (This is a optimal order for the learning tasks).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Three-problem-need-to-solve-when-realize-lifelong-learnig&quot;&gt;&lt;a href=&quot;#Three-problem-need-to-solve-when-realize-lifelong-learnig&quot; clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Transfer Learning" scheme="https://zhangruochi.com/tags/Transfer-Learning/"/>
    
      <category term="Lifelong Learning" scheme="https://zhangruochi.com/tags/Lifelong-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Object Detection Summary</title>
    <link href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/"/>
    <id>https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</id>
    <published>2020-03-06T20:53:08.000Z</published>
    <updated>2020-03-09T16:58:07.340Z</updated>
    
    <content type="html"><![CDATA[<h2 id="轰REGION-PROPOSAL规"><a href="#轰REGION-PROPOSAL规" class="headerlink" title="轰REGION PROPOSAL规"></a>轰REGION PROPOSAL规</h2><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><h4 id="姝ラ"><a href="#姝ラ" class="headerlink" title="姝ラ"></a>姝ラ</h4><p>R-CNN绠娉姝ラ:</p><ol><li>Selective Search规浠涓寮惧涓绾2K涓哄锛 </li><li>棣褰涓涓虹涓灏哄革瀵规涓哄锛浣跨ㄦ繁搴缃缁瑰锛 </li><li>灏虹瑰ユ涓绫荤SVM 绫诲锛ゅ灞浜璇ョ被锛 </li><li>浣跨ㄥ褰ㄧ簿缁淇姝ｅ妗浣缃涓虹妫娴棰琛￠㈢Н锛璁稿浼煎纭妫娴缁锛寰寰涓哄妗涓澶纭锛瀵艰撮㈢Н寰灏</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">R-CNN Architecture</div></center><h3 id="SPP-NET"><a href="#SPP-NET" class="headerlink" title="SPP-NET"></a>SPP-NET</h3><p>Kaiming He瀵规や烘硅锛轰SPP-net锛ㄧО锛Spatial Pyramid Pooling锛锛澶х硅瑕灏捐ヤ娆★灏卞浠ュ版涓哄瑰</p><p>R-CNN涓锛哄瑕缁杩褰㈢缉撅浠ラ搴CNN杈ワ浠ラ杩淇圭缁缁锛浣垮浠绘澶у剧借借ュCNN涓Kaiming Heㄨ烘涓轰SPP缁ラ搴浠讳澶у剧杈ャSPP-net瀵R-CNN澶х硅灏辨瑰姝ラゅ浜淇癸朵妯″浠跺R-CNN涓枫瑰涓瑕姣涓哄界杩CNN锛瑕灏村剧杈ュCNN灏卞浠ヤ锛ROI瑰存ヤ瑰捐峰R-CNN告锛搴楂浜24~102</p><p>SPP-Net绠娉姝ラ:</p><ol><li>棣杩╂ф绱锛瀵瑰妫娴剧杩琛绱㈠2000涓绐ｃ杩涓姝ュR-CNN涓枫</li><li>瑰舵点杩涓姝ュ氨R-CNN澶х哄浜锛杩涓姝ラょ蜂浣濡涓锛村寰妫娴剧锛杈CNN涓锛杩琛涓娆℃х瑰锛寰feature maps锛跺feature maps涓惧板涓妗哄(ROI)锛瀵瑰涓妗ㄩ瀛濉绌洪存锛哄哄垮害瑰R-CNN杈ョ姣涓妗锛跺ㄨCNN锛涓SPP-Net瑕涓娆″规村剧杩琛瑰锛搴浼澶уぇ</li><li>涓姝ヤR-CNN涓凤SVM绠娉杩琛瑰绫昏</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">SPP Architecture</div></center><h3 id="FAST-R-CNN"><a href="#FAST-R-CNN" class="headerlink" title="FAST R-CNN"></a>FAST R-CNN</h3><p>FAST R-CNN绠娉姝ラわ</p><ol><li>杩selective searchregion proposal锛姣寮剧澶х害2000涓RoI锛 </li><li>Fast-RCNN村剧CNN锛杩琛瑰锛region proposal灏CNN涓灞风Нfeature map涓锛 </li><li>杩RoI pooling灞锛涔浠ョО涓哄灞SPP layer锛浣垮姣涓寤鸿绐ｇ哄澶уfeature map锛 </li><li>缁х画缁杩涓や釜ㄨュ锛FC锛寰扮瑰瑰缁卞FC灞锛寰颁袱涓杈哄锛绗涓涓绫伙浣跨softmax锛绗浜涓姣涓绫荤bounding box褰╃Softmax Loss锛㈡绫绘锛Smooth L1 Loss锛㈡杈规褰锛瀵瑰绫绘杈规褰锛Bounding Box Regression锛璁缁</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Fast-RCNN Architecture</div></center><h3 id="FASTER-R-CNN"><a href="#FASTER-R-CNN" class="headerlink" title="FASTER R-CNN"></a>FASTER R-CNN</h3><p>变Fast R-CNN浠舵轰Selective Search规region proposal锛Selective Search规region proposal璁＄娉GPU杩琛锛娉GPU楂搴骞惰杩绠藉锛浠ユ浣涓2000涓哄锛涔浜㈡繁搴瀛涔澶Faster-RCNN = RPN锛哄缃缁锛+ Fast-RCNN锛RPN缃缁浠ｆFast-RCNN涓Selective SearchFaster-RCNN稿炽</p><p>FASTER R-CNN 绠娉姝ラ:</p><ol><li>杈ユ璇惧锛 </li><li>灏村剧杈CNN锛杩琛瑰锛 </li><li>RPN寤鸿绐ｏproposals锛锛姣寮剧300涓寤鸿绐ｏ </li><li>寤鸿绐ｆ灏CNN涓灞风Нfeature map涓锛 </li><li>杩RoI pooling灞浣挎涓RoI哄灏哄哥feature map锛 </li><li>╃Softmax Loss锛㈡绫绘锛Smooth L1 Loss锛㈡杈规褰锛瀵瑰绫绘杈规褰锛Bounding Box Regression锛璁缁</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Faster-RCNN Architecture</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="frcnn.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Faster-RCNN Architecture2</div></center><h4 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h4><p>璋anchors锛瀹涓灏辨涓缁rpn/generate_anchors.py╁舰存ヨ琛浣demo涓generate_anchors.py浠ュ颁互涓杈猴</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[ -84.  -40.   99.   55.]</span><br><span class="line"> [-176.  -88.  191.  103.]</span><br><span class="line"> [-360. -184.  375.  199.]</span><br><span class="line"> [ -56.  -56.   71.   71.]</span><br><span class="line"> [-120. -120.  135.  135.]</span><br><span class="line"> [-248. -248.  263.  263.]</span><br><span class="line"> [ -36.  -80.   51.   95.]</span><br><span class="line"> [ -80. -168.   95.  183.]</span><br><span class="line"> [-168. -344.  183.  359.]]</span><br></pre></td></tr></table></figure><p>朵腑姣琛4涓 $(x_1, y_1, x_2, y_2)$ 琛ㄧ╁舰<code>宸涓</code><code>充</code>瑙瑰9涓╁舰辨3绉褰㈢讹垮芥涓哄ぇ绾涓 $\frac{width}{height}  \in \{1:1, 1:2, 2:1\}$涓绉锛濡俱瀹涓杩anchors灏卞ヤ妫娴涓甯哥ㄥ扮澶灏哄害规</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Anchor</div></center><p>Conv layers璁＄峰feature maps锛涓烘涓涓归介澶杩9绉anchors浣涓哄濮妫娴妗杩峰峰妫娴妗寰涓纭锛涓ㄦ蹇锛㈣2娆bounding box regression浠ヤ慨姝ｆ娴妗浣缃</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Anchor2</div></center><p>璁惧conv5 feature map涓姣涓逛k涓anchor锛榛璁k=9锛锛姣涓anhcor瑕positivenegative锛浠ユ涓圭256d feature杞涓cls=2k scores锛姣涓anchor芥(x, y, w, h)瀵瑰4涓绉婚锛浠reg=4k coordinates. 琛ュ涓癸ㄩanchors垮昏缁澶澶浜锛璁缁绋搴浼ㄥanchors涓洪128涓postive anchors+128涓negative anchors杩琛璁缁</p><h4 id="Bounding-box-regression"><a href="#Bounding-box-regression" class="headerlink" title="Bounding box regression"></a>Bounding box regression</h4><p>瀵逛绐ｄ浣跨ㄥ缁村 (x,y,w,h) 琛ㄧず锛琛ㄧず绐ｇ涓蹇瑰瀹介瀵逛涓撅绾㈣茬妗A浠ｈ〃濮positive Anchors锛缁胯茬妗G浠ｈ〃GT锛浠瀵绘句绉崇郴锛浣垮杈ュ濮anchor A缁杩灏寰颁涓璺瀹绐G存ヨ褰绐G锛</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="8.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bouding Box Regression</div></center><ul><li>缁瀹 anchor $A = (A_x, A_y, A_w, A_h)$  $GT =( G_x, G_y, G_w, G_h)$</li><li>瀵绘句绉 F, 浣垮: <script type="math/tex; mode=display">F(A_x, A_y, A_w, A_h)$ = ( G_x^{\prime}, G_y^{\prime}, G_w^{\prime}, G_h^{\prime})</script><script type="math/tex; mode=display">( G_x^{\prime}, G_y^{\prime}, G_w^{\prime}, G_h^{\prime}) \approx ( G_x, G_y, G_w, G_h)</script></li></ul><p>ｄ缁杩浣绉F戒10涓anchor A涓G锛 姣杈绠璺灏辨:</p><ol><li>骞崇Щ</li></ol><script type="math/tex; mode=display">G_x^{\prime} = A_w \dot d_x(A) + A_x</script><script type="math/tex; mode=display">G_y^{\prime} = A_h \dot d_y(A) + A_y</script><ol><li>缂╂</li></ol><script type="math/tex; mode=display">G_w^{\prime} = A_w \dot exp(d_w(A))</script><script type="math/tex; mode=display">G_h^{\prime} = A_h \dot exp(d_h(A))</script><p>瑕瀛涔 $d_x(A)$, $d_y(A)$, $d_w(A)$, $d_h(A)$ 杩涓褰杈ョanchor A涓GT稿樊杈灏讹浠ヨや负杩绉㈡涓绉绾挎у锛 ｄ灏卞浠ョㄧ嚎у褰ュ缓妯″圭ｈ琛寰璋</p><p> faster RCNN涓锛positive anchor涓ground truth涔寸骞崇Щ$(t_x, t_y)$涓灏哄害瀛$(t_w, t_h)$濡涓:</p><script type="math/tex; mode=display">t_x = \frac{G_x - x_a}{W_a}</script><script type="math/tex; mode=display">t_y = \frac{G_y - y_a}{h_a}</script><script type="math/tex; mode=display">t_w = log(\frac{G_w}{W_a})</script><script type="math/tex; mode=display">t_h = log(\frac{h}{h_a})</script><p>瀵逛璁缁bouding box regression缃缁褰锛杈ユX(cnn feature), label 涓杩板昂搴㈠瀛锛璁缁ㄨfetaure X′欢涓锛褰缃缁杈哄氨姣涓Anchor骞崇Щ㈠昂搴$(t_x, t_y, t_w, t_h)$ 剧跺冲ㄦヤ慨姝Anchor浣缃浜</p><h4 id="Faster-RCNN-璁缁"><a href="#Faster-RCNN-璁缁" class="headerlink" title="Faster RCNN  璁缁"></a>Faster RCNN  璁缁</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="train.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Training</div></center><p>Faster R-CNN璁缁锛ㄥ凡缁璁缁濂界model锛濡VGG_CNN_M_1024锛VGG锛ZF锛虹涓缁х画杩琛璁缁瀹涓璁缁杩绋涓6涓姝ラわ</p><ol><li>ㄥ凡缁璁缁濂界model涓锛璁缁RPN缃缁</li><li>╃ㄦラ1涓璁缁濂界RPN缃缁</li><li>绗涓娆¤缁Fast RCNN缃缁</li><li>绗浜璁缁RPN缃缁</li><li>娆″╃ㄦラ4涓璁缁濂界RPN缃缁</li><li>绗浜娆¤缁Fast RCNN缃缁</li></ol><h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>YOLO稿冲氨╃ㄦ村句涓虹缁杈ワ存ュㄨ哄褰bounding box浣缃bounding box灞绫诲</p><p>YOLO绠娉姝ラ:</p><ol><li>棣锛灏涓骞惧SS涓缃硷grid cell锛锛濡涓╀涓蹇藉ㄨ涓缃间腑锛杩涓缃煎氨璐璐ｉ娴杩涓╀<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="yolo.png" width="50%" height="50%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">YOLO</div></center></li><li>姣涓缃艰棰娴B涓bounding box锛姣涓bounding boxや瑕褰韬浣缃涔澶锛杩瑕甯棰娴涓涓confidence杩涓confidence浠ｈ〃浜棰娴box涓object缃淇″害杩涓box棰娴澶杩涓ら淇℃</li><li>姣涓bounding box瑕棰娴(x, y, w, h)confidence5涓硷姣涓cell杩瑕棰娴涓涓绫诲淇℃锛璁颁负C绫汇SS涓缃硷姣涓缃艰棰娴B涓bounding box杩瑕棰娴C涓categories杈哄氨SS(5B+C)涓涓tensor </li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="yolo_arch.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">YOLO Architecture</div></center><h3 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>浠ユ楂妯″舵搴锛灏杩锛 </p><h4 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h4><p>棣448448杈ㄧImageNet版finetune浣跨缁搴楂杈ㄧ杈ワ跺灏璇ョ缁ㄤ妫娴浠诲finetune锛 </p><h4 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h4><p>婚やYOLOㄨュ锛ㄥ哄妗锛anchor boxes锛ラ娴bounding boxes锛Faster RCNN娉锛YOLO2涔灏璇ㄥ楠妗锛anchor锛ㄦ涓grid棰璁惧涓缁涓澶у瀹介姣杈规锛ヨ翠釜惧涓浣缃澶绉灏哄害锛杩浜楠妗浣涓洪瀹涔哄ㄧ缁缃缁涓灏妫娴朵腑瀛ㄥ硅薄锛浠ュ寰璋杈规浣缃</p><h4 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters**"></a>Dimension Clusters**</h4><p>Anchor boxes杩k-meansㄨ缁涓瀛寰锛骞朵浣瀹涔浜扮璺绂诲寮锛浣跨k-means峰anchor boxesラ娴bounding boxes璁╂ā村规瀛涔濡浣棰娴bounding boxes锛绫荤娉瑕╁浣璁＄涓や釜杈规涔寸璺绂烩锛瀵逛甯哥ㄧ娆у璺绂伙澶ц竟妗浼浜х村ぇ璇宸锛浣浠冲杈规IOU浠ワYOLO2ㄨ绫绘堕ㄤ互涓寮ヨ＄涓や釜杈规涔寸璺绂烩 </p><script type="math/tex; mode=display">d(box, centroid) = 1 - IOU(box, centroid)</script><h4 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h4><p>翠Faster RCNN楠妗规锛ㄨ缁╂舵碉朵缃棰娴瀹规涓绋冲朵缃棰娴寮涓猴</p><script type="math/tex; mode=display">x = (t_x * w_a) + x_a</script><script type="math/tex; mode=display">y = (t_y * h_a) + y_a</script><p>朵腑,(x,y)棰娴杈规涓蹇锛$x_a$, $y_a$楠妗锛anchor锛涓蹇瑰锛(w_a,h_a)楠妗锛anchor锛瀹藉楂$t_x$,$t_y$瑕瀛涔灏哄害㈠瀛<br>变$t_x$,$t_y$娌℃绾锛姝ら娴杈规涓蹇藉虹板ㄤ换浣浣缃锛璁缁╂舵典瀹规绋冲YOLO璋翠棰娴寮锛灏棰娴杈规涓蹇绾ㄧ瑰gird缃煎</p><script type="math/tex; mode=display">b_x = \sigma(t_x) + c_x</script><script type="math/tex; mode=display">b_y = \sigma(t_y) + c_y</script><script type="math/tex; mode=display">b_w  = p_w e^{t_w}</script><script type="math/tex; mode=display">b_h  = p_h e^{t_h}</script><script type="math/tex; mode=display">Pr(object)\dot IOU(b,object) = \sigma(t_o)</script><p>朵腑锛$b_X,b_y,b_w,b_h$棰娴杈规涓蹇瀹介,$Pr(object)\dot IOU(b,object) = \sigma(t_o)$ 棰娴杈规缃淇″害锛YOLO1存ラ娴缃淇″害硷杩瀵归娴$t_o$杩琛$\sigma$㈠浣涓虹疆淇″害笺$c_x$,$c_y$褰缃煎乏涓瑙板惧宸涓瑙璺绂伙瑕灏缃煎ぇ灏褰涓锛充护涓涓缃肩瀹=1锛楂=1$p_w,p_h$楠妗瀹藉楂</p><p><strong>see explanantion in <a href="https://www.jianshu.com/p/86b8208f634f" target="_blank" rel="noopener">https://www.jianshu.com/p/86b8208f634f</a></strong></p><h4 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h4><p>YOLOv2杩娣诲涓涓pass through layer锛灏涓涓风Н瑰剧淇℃璧锋ワ瀵硅薄妫娴涓寸涓涓棰惧涓瀵硅薄浼澶ф灏锛杈ュ惧缁杩澶灞缃缁瑰锛杈虹瑰句腑锛姣濡YOLO2涓杈416<em>416缁杩风Н缃缁涓锋杈烘13</em>13锛锛杈灏瀵硅薄界瑰宸茬涓剧宠蹇界ユ浜涓轰村ソ妫娴轰浜姣杈灏瀵硅薄锛杈虹瑰鹃瑕淇涓浜寸淇℃</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="pass.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">pass through</div></center><h4 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h4><p>YOLOv2缃缁ㄥ颁风Н灞姹灞锛姝ゅ浠ヨ琛ㄦ璋磋ュ惧灏哄革浣甯YOLOv2瀵逛涓灏哄稿惧妫娴芥杈濂界椴妫э姝ゅ浜瀵规ц缁杩绉绛ヨYOLOv2缃缁涓寰涓瀛瀵逛灏哄哥惧杈ラ借棰娴寰寰濂斤杩崇涓涓缃缁浠ヨ浠讳杈ㄧ妫娴浠诲★ㄧ缁璁缁濂戒锛ㄤ娇ㄦ跺瑕规姹锛淇圭缁杈ュ惧灏哄革widthheight硷冲</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="box.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">YOLO v2 bounding box regression</div></center><h3 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="yolov3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">YOLO v3 Architecture</div></center><ul><li><strong>DBL</strong>: 濡1宸涓瑙绀猴涔灏辨浠ｇ涓Darknetconv2d_BN_Leaky锛yolo_v3烘缁浠躲灏辨风Н+BN+Leaky relu瀵逛v3ヨ达BNleaky relu宸茬风Н灞涓绂荤ㄥ浜(涓灞风Нゅ)锛卞浜灏缁浠躲</li><li><strong>resn</strong>锛n浠ｈ〃板锛res1锛res2,  ,res8绛绛锛琛ㄧず杩涓res_block澶灏涓res_unit杩yolo_v3澶х浠讹yolo_v3寮濮翠ResNet娈宸缁锛浣跨ㄨ绉缁浠ヨ╃缁缁存繁(浠v2darknet-19涓v3darknet-53锛娌℃娈宸缁)瀵逛res_block瑙ｉ锛浠ュㄥ1充瑙磋帮跺烘缁浠朵DBL</li><li><strong>concat</strong>锛寮兼ャ灏darknet涓村㈢涓灞涓疯琛兼ャ兼ョ浣娈宸灞add浣涓涓风锛兼ヤ╁寮缁村害锛add存ョ稿涓浼瀵艰村缁村害瑰</li></ul><h4 id="澶灏哄害瑰"><a href="#澶灏哄害瑰" class="headerlink" title="澶灏哄害瑰"></a>澶灏哄害瑰</h4><ul><li>YOLOv3杈轰3涓涓灏哄害瑰撅ㄥ浠涓㈣琛棰娴╀妗杩翠瑰瀛濉缃缁锛FPN, feature pyramid networks锛锛ㄥ灏哄害ュ逛澶у杩琛妫娴锛瓒绮剧grid cell灏卞浠ユ娴鸿绮剧╀</li><li>3涓灏哄害瑰撅<ul><li>灏哄害y1锛灏哄害1feature map风Н存ュbox淇℃</li><li>灏哄害y2锛瀵瑰昂搴1杈虹风Н杩琛涓凤跺灏哄害2feature map稿锛缁杩风Н杈box淇℃锛翠釜feature map澶у稿逛灏哄害1╁ぇ浜涓ゅ</li><li>灏哄害y3锛y2</li></ul></li><li>3涓灏哄害瑰炬繁搴芥255锛杈归跨瑙寰13:26:52</li><li>ㄤ澶灏哄害瑰锛浠ヨ竟妗伴瑕姣涔澶寰澶.浠ヨュ惧涓416x416涓轰(13x13+26x26+52x52)x3=10647锛瑕姣v213x13x5村</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="10.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">YOLO v3 Architecture</div></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://blog.csdn.net/v1_vivian/article/details/73275259" target="_blank" rel="noopener">https://blog.csdn.net/v1_vivian/article/details/73275259</a></li><li><a href="https://blog.csdn.net/zhang_can/article/details/79490735" target="_blank" rel="noopener">https://blog.csdn.net/zhang_can/article/details/79490735</a></li><li><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31426458</a></li><li><a href="https://zhuanlan.zhihu.com/p/47575929" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47575929</a></li><li><a href="https://blog.csdn.net/leviopku/article/details/82660381" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/82660381</a></li><li><a href="https://www.jianshu.com/p/86b8208f634f" target="_blank" rel="noopener">https://www.jianshu.com/p/86b8208f634f</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;轰REGION-PROPOSAL规&quot;&gt;&lt;a href=&quot;#轰REGION-PROPOSAL规&quot; class=&quot;headerlink&quot; title=&quot;轰REGION PROPOSAL规&quot;&gt;&lt;/a&gt;轰REGION PROPOSAL规&lt;/h2&gt;&lt;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>AI - Search II</title>
    <link href="https://zhangruochi.com/AI-Search-II/2020/03/05/"/>
    <id>https://zhangruochi.com/AI-Search-II/2020/03/05/</id>
    <published>2020-03-05T16:44:39.000Z</published>
    <updated>2020-03-06T01:05:44.631Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><p>Definition: search problem</p><ul><li>$S_{start}$: staring state</li><li>Actions(s): possible actions</li><li>Cons(s,a): action cost</li><li>Succ(s,a): successor</li><li>IsEnd(s): reached end state?</li></ul><p>Objective: find the minimum cost path from $S_start$ to an $s$ that satisfying IsEnd(s).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransportationProblem</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N)</span>:</span></span><br><span class="line">        <span class="comment"># N = number of blocks</span></span><br><span class="line">        self.N = N</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startState</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEnd</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> state == self.N</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">succAndCost</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="comment"># return list of (action, newState, cost) triples</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">if</span> state+<span class="number">1</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'walk'</span>, state+<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> state*<span class="number">2</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'tram'</span>, state*<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="Learning-problem"><a href="#Learning-problem" class="headerlink" title="Learning problem"></a>Learning problem</h2><p>Now suppose we dont know what the costs are, but we observe someone getting from 1 to n via some sequence of walking and tram-taking. Can we figure out what the costs are? This is the goal of learning.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="learning.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Learning Problem</div></center><p>Lets cast the problem as predicting an output y given an input x. Here, the input x is the search problem (visualized as a search tree) without the costs provided. The output y is the desired solution path. The question is what the costs should be set to so that y is actually the minimum cost path of the resulting search problem.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="algo.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Structured Perceptron</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.setrecursionlimit(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Model (search problem)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransportationProblem</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N, weights)</span>:</span></span><br><span class="line">        <span class="comment"># N = number of blocks</span></span><br><span class="line">        <span class="comment"># weights = weights of different actions</span></span><br><span class="line">        self.N = N</span><br><span class="line">        self.weights = weights</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startState</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEnd</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> state == self.N</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">succAndCost</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="comment"># return list of (action, newState, cost) triples</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">if</span> state+<span class="number">1</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'walk'</span>, state+<span class="number">1</span>, self.weights[<span class="string">'walk'</span>]))</span><br><span class="line">        <span class="keyword">if</span> state*<span class="number">2</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'tram'</span>, state*<span class="number">2</span>, self.weights[<span class="string">'tram'</span>]))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamicProgramming</span><span class="params">(problem)</span>:</span></span><br><span class="line">    cache = &#123;&#125; <span class="comment"># state -&gt; futureCost(state), action, newState, cost</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">futureCost</span><span class="params">(state)</span>:</span></span><br><span class="line">        <span class="comment"># Base case</span></span><br><span class="line">        <span class="keyword">if</span> problem.isEnd(state):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">in</span> cache: <span class="comment"># Exponential savings</span></span><br><span class="line">            <span class="keyword">return</span> cache[state][<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Actually doing work</span></span><br><span class="line">        result = min((cost+futureCost(newState), action, newState, cost) \</span><br><span class="line">                <span class="keyword">for</span> action, newState, cost <span class="keyword">in</span> problem.succAndCost(state))</span><br><span class="line">        cache[state] = result</span><br><span class="line">        <span class="keyword">return</span> result[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    state = problem.startState()</span><br><span class="line">    totalCost = futureCost(state)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Recover history</span></span><br><span class="line">    history = []</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> problem.isEnd(state):</span><br><span class="line">        _, action, newState, cost = cache[state]</span><br><span class="line">        history.append((action, newState, cost))</span><br><span class="line">        state = newState</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (futureCost(problem.startState()), history)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### Main</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(N, weights)</span>:</span></span><br><span class="line">    <span class="comment"># f(x)</span></span><br><span class="line">    <span class="comment"># Input (x): N (number of blocks)</span></span><br><span class="line">    <span class="comment"># Output (y): path (sequence of actions)</span></span><br><span class="line">    problem = TransportationProblem(N, weights)</span><br><span class="line">    totalCost, history = dynamicProgramming(problem)</span><br><span class="line">    <span class="keyword">return</span> [action <span class="keyword">for</span> action, newState, cost <span class="keyword">in</span> history]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateExamples</span><span class="params">()</span>:</span></span><br><span class="line">    trueWeights = &#123;<span class="string">'walk'</span>: <span class="number">1</span>, <span class="string">'tram'</span>: <span class="number">5</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> [(N, predict(N, trueWeights)) <span class="keyword">for</span> N <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">30</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">structuredPerceptron</span><span class="params">(examples)</span>:</span></span><br><span class="line">    weights = &#123;<span class="string">'walk'</span>: <span class="number">0</span>, <span class="string">'tram'</span>: <span class="number">0</span>&#125;</span><br><span class="line">    iteration = <span class="number">100</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(iteration):</span><br><span class="line">        numMistakes = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> N, trueActions <span class="keyword">in</span> examples:</span><br><span class="line">            <span class="comment"># Make a prediction</span></span><br><span class="line">            predActions = predict(N, weights)</span><br><span class="line">            <span class="keyword">if</span> predActions != trueActions:</span><br><span class="line">                numMistakes += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Update weights</span></span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> trueActions:</span><br><span class="line">                weights[action] -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> predActions:</span><br><span class="line">                weights[action] += <span class="number">1</span></span><br><span class="line">        print(<span class="string">'Iteration &#123;&#125;, numMistakes = &#123;&#125;, weights = &#123;&#125;'</span>.format(t, numMistakes, weights))</span><br><span class="line">        <span class="keyword">if</span> numMistakes == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">examples = generateExamples()</span><br><span class="line">print(<span class="string">'Training dataset:'</span>)</span><br><span class="line"><span class="keyword">for</span> example <span class="keyword">in</span> examples:</span><br><span class="line">    print(<span class="string">'  '</span>, example)</span><br><span class="line">structuredPerceptron(examples)</span><br></pre></td></tr></table></figure><h2 id="A-Star-Search"><a href="#A-Star-Search" class="headerlink" title="A Star Search"></a>A Star Search</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="aStar.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Exploring states</div></center><p><strong>UCS</strong>: explore states in order of PastCost(s)<br><strong>A start</strong>: explore in order of PastCost(s) + h(s)</p><pre><code>- A heuristic h(s) is any estimate of FutureCost(s).</code></pre><ul><li>First, some terminology: PastCost(s) is the minimum cost from the start state to s, and FutureCost(s) is the minimum cost from s to an end state. Without loss of generality, we can just assume we have one end state. (If we have multiple ones, create a new official goal state which is the successor of all the original end states.)</li><li>Recall that UCS explores states in order of PastCost(s). Itd be nice if we could explore states in order of PastCost(s) + FutureCost(s), which would definitely take the end state into account, but computing FutureCost(s) would be as expensive as solving the original problem.</li><li>A star relies on a heuristic h(s), which is an estimate of FutureCost(s). For A star to work, h(s) must satisfy some conditions, but for now, just think of h(s) as an approximation. We will soon show that A star will explore states in order of PastCost(s) + h(s). This is nice, because now states which are estimated (by h(s)) to be really far away from the end state will be explored later, even if their PastCost(s) is small.</li></ul><h3 id="F-G-H"><a href="#F-G-H" class="headerlink" title="F = G + H"></a>F = G + H</h3><p>One important aspect of A star is f = g + h. The f, g, and h variables are in our Node class and get calculated every time we create a new node. Quickly Ill go over what these variables mean.</p><ul><li>F is the total cost of the node.</li><li>G is the distance between the current node and the start node.</li><li>H is the heuristic  estimated distance from the current node to the end node.</li></ul><h3 id="Consistent-heuristics"><a href="#Consistent-heuristics" class="headerlink" title="Consistent heuristics"></a>Consistent heuristics</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="consistent.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Consistent heuristics</div></center><ul><li>We need h(s) to be consistent, which means two things. First, the modified edge costs are non-negative (this is the main property). This is important for UCS to find the minimum cost path (remember that UCS only works when all the edge costs are non-negative).</li><li>Second, h(send) = 0, which is just saying: be reasonable. The minimum cost from the end state to the end state is trivially 0, so just use 0.</li></ul><h3 id="Correctness-of-A"><a href="#Correctness-of-A" class="headerlink" title="Correctness of A*"></a>Correctness of A*</h3><p><strong>If h is consistent, A star returns the minimum cost path.</strong></p><h3 id="Admissibility"><a href="#Admissibility" class="headerlink" title="Admissibility"></a>Admissibility</h3><p>A heuristic h(s) is admissible if </p><script type="math/tex; mode=display">h(s) \leq FutureCost(s)</script><p>Theorem: <strong>consistency implies admissibility</strong></p><blockquote><p>If a heuristic h(s) is consistent, then h(s) is admissible.</p></blockquote><h3 id="Relaxation"><a href="#Relaxation" class="headerlink" title="Relaxation"></a>Relaxation</h3><p>With an arbitrary configuration of walls, we cant compute FutureCost(s) except by doing search. However, if we just relaxed the original problem by removing the walls, then we can compute FutureCost(s) in closed form: its just the Manhattan distance between $s$ and $s_{end}$. Specifically, ManhattanDistance((r1, c1), (r2, c2)) = |r1  r2| + |c1  c2|.</p><ul><li>More formally, we define a relaxed search problem as one where the relaxed edge costs are no larger than the original edge costs.</li><li>The relaxed heuristic is simply the future cost of the relaxed search problem, which by design should be efficiently computable.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="algo2.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">A star Search Algorithm</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq, collections, re, sys, time, os, random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data structure for supporting uniform cost search.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PriorityQueue</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.DONE = <span class="number">-100000</span></span><br><span class="line">        self.heap = []</span><br><span class="line">        self.priorities = &#123;&#125;  <span class="comment"># Map from state to priority</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        state, newPriority = node.position, node.f</span><br><span class="line">        oldPriority = self.priorities.get(state)</span><br><span class="line">        <span class="keyword">if</span> oldPriority == <span class="keyword">None</span> <span class="keyword">or</span> newPriority &lt; oldPriority:</span><br><span class="line">            self.priorities[state] = newPriority</span><br><span class="line">            heapq.heappush(self.heap, node)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeMin</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> len(self.heap) &gt; <span class="number">0</span>:</span><br><span class="line">            node = heapq.heappop(self.heap)</span><br><span class="line">            priority, state = node.f, node.position</span><br><span class="line">            <span class="keyword">if</span> self.priorities[state] == self.DONE: </span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment"># Outdated priority, skip</span></span><br><span class="line">            self.priorities[state] = self.DONE</span><br><span class="line">            <span class="keyword">return</span> node</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""A node class for A* Pathfinding"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent=None, position=None)</span>:</span></span><br><span class="line">        self.parent = parent</span><br><span class="line">        self.position = position</span><br><span class="line"></span><br><span class="line">        self.g = <span class="number">0</span></span><br><span class="line">        self.h = <span class="number">0</span></span><br><span class="line">        self.f = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.position == other.position</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.f &lt; other.f</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Node (position: &#123;&#125;, g: &#123;&#125;, h: &#123;&#125;, f: &#123;&#125;)"</span>.format(self.position, self.g, self.h, self.f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">astar</span><span class="params">(maze, start, end)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a list of tuples as a path from the given start to the given end in the given maze"""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create start and end node</span></span><br><span class="line">    start_node = Node(<span class="keyword">None</span>, start)</span><br><span class="line">    start_node.g = start_node.h = start_node.f = <span class="number">0</span></span><br><span class="line">    end_node = Node(<span class="keyword">None</span>, end)</span><br><span class="line">    end_node.g = end_node.h = end_node.f = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize both open and closed list</span></span><br><span class="line">    frontier = PriorityQueue()</span><br><span class="line">    frontier.update(start_node)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop until you find the end</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get the current node</span></span><br><span class="line">        current_node = frontier.removeMin()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Found the goal</span></span><br><span class="line">        <span class="keyword">if</span> current_node == end_node:</span><br><span class="line">            path = []</span><br><span class="line">            current = current_node</span><br><span class="line">            <span class="keyword">while</span> current <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                path.append(current.position)</span><br><span class="line">                current = current.parent</span><br><span class="line">            <span class="keyword">return</span> path[::<span class="number">-1</span>] <span class="comment"># Return reversed path</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate children</span></span><br><span class="line">        children = []</span><br><span class="line">        <span class="keyword">for</span> new_position <span class="keyword">in</span> [(<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>), (<span class="number">-1</span>, <span class="number">-1</span>), (<span class="number">-1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">1</span>, <span class="number">1</span>)]: <span class="comment"># Adjacent squares</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Get node position</span></span><br><span class="line">            node_position = (current_node.position[<span class="number">0</span>] + new_position[<span class="number">0</span>], current_node.position[<span class="number">1</span>] + new_position[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Make sure within range</span></span><br><span class="line">            <span class="keyword">if</span> node_position[<span class="number">0</span>] &gt; (len(maze) - <span class="number">1</span>) <span class="keyword">or</span> node_position[<span class="number">0</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> node_position[<span class="number">1</span>] &gt; (len(maze[len(maze)<span class="number">-1</span>]) <span class="number">-1</span>) <span class="keyword">or</span> node_position[<span class="number">1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Make sure walkable terrain</span></span><br><span class="line">            <span class="keyword">if</span> maze[node_position[<span class="number">0</span>]][node_position[<span class="number">1</span>]] != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create new node</span></span><br><span class="line">            new_node = Node(current_node, node_position)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create the f, g, and h values</span></span><br><span class="line">            new_node.g = current_node.g + <span class="number">1</span></span><br><span class="line">            new_node.h = ((new_node.position[<span class="number">0</span>] - end_node.position[<span class="number">0</span>]) ** <span class="number">2</span>) + ((new_node.position[<span class="number">1</span>] - end_node.position[<span class="number">1</span>]) ** <span class="number">2</span>)</span><br><span class="line">            new_node.f = new_node.g + new_node.h</span><br><span class="line"></span><br><span class="line">            frontier.update(new_node)</span><br><span class="line">         </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    maze = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    start = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    end = (<span class="number">7</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">    path = astar(maze, start, end)</span><br><span class="line">    print(path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># [(0, 0), (1, 1), (2, 2), (3, 3), (4, 3), (5, 4), (6, 5), (7, 6)]</span></span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://medium.com/@nicholas.w.swift/easy-a-star-pathfinding-7e6689c7f7b2" target="_blank" rel="noopener">https://medium.com/@nicholas.w.swift/easy-a-star-pathfinding-7e6689c7f7b2</a></li><li>CS221 course note and ppt <a href="https://stanford-cs221.github.io/autumn2019/" target="_blank" rel="noopener">https://stanford-cs221.github.io/autumn2019/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Review&quot;&gt;&lt;a href=&quot;#Review&quot; class=&quot;headerlink&quot; title=&quot;Review&quot;&gt;&lt;/a&gt;Review&lt;/h2&gt;&lt;p&gt;Definition: search problem&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S_{start}$: s
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
    
  </entry>
  
  <entry>
    <title>Programming Language: Summary</title>
    <link href="https://zhangruochi.com/Programming-Language-Summary/2020/02/27/"/>
    <id>https://zhangruochi.com/Programming-Language-Summary/2020/02/27/</id>
    <published>2020-02-27T21:51:07.000Z</published>
    <updated>2020-02-27T22:37:59.218Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ML-Expressions-and-Variable-Bindings"><a href="#ML-Expressions-and-Variable-Bindings" class="headerlink" title="ML Expressions and Variable Bindings"></a>ML Expressions and Variable Bindings</h2><p>An ML program is a sequence of <strong>bindings</strong>. Each binding gets type-checked and then (assuming it type-checks) evaluated. What type (if any) a binding has depends on a <strong>static environment</strong>, which is roughly the types of the preceding bindings in the file. How a binding is evaluated depends on a <strong>dynamic</strong> environment, which is roughly the values of the preceding bindings in the file. When we just say environment, we usually mean dynamic environment. Sometimes context is used as a synonym for static environment.</p><p>There are several kinds of bindings, but for now lets consider only a variable binding, which in ML has this syntax :</p><center> val x = e; </center><p>Here, val is a keyword, x can be any variable, and e can be any expression. We now know a variable bindings <strong>syntax</strong> (how to write it), but we still need to know its <strong>semantics</strong> (how it type-checks and evaluates). Mostly this depends on the expression e. To type-check a variable binding, we use the current static environment (the types of preceding bindings) to type-check e (which will depend on what kind of expression it is) and produce a <code>new static environment</code> that is the current static environment except with x having type t where t is the type of e.  Evaluation is analogous: To evaluate a variable binding, we use the current dynamic environment (the values of preceding bindings) to evaluate e (which will depend on what kind of expression it is) and produce a <code>new dynamic environment</code> that is the current environment except with x having the value v where v is the result of evaluating e.</p><p>Whenever you learn a new construct in a programming language, you should ask these three questions: </p><ul><li>What is the syntax? </li><li>What are the type-checking rules? </li><li>What are the evaluation rules?</li></ul><p>for example:</p><h3 id="Addition"><a href="#Addition" class="headerlink" title="Addition:"></a>Addition:</h3><ul><li>Syntax: e1+e2 where e1 and e2 are expressions</li><li>Type-checking: type int but only if e1 and e2 have type int in the same static environment, else does not type-check</li><li>Evaluation: evaluate e1 to v1 and e2 to v2 in the same dynamic environment and then produce the sum of v1 and v2</li></ul><h2 id="Function-Bindings"><a href="#Function-Bindings" class="headerlink" title="Function Bindings"></a>Function Bindings</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fun pow (x:int, y:int) = (* correct only for y &gt;= 0 *)</span><br><span class="line">    if y=0</span><br><span class="line">    then 1</span><br><span class="line">    else x * pow(x,y-1)</span><br></pre></td></tr></table></figure><h3 id="Syntax"><a href="#Syntax" class="headerlink" title="Syntax:"></a>Syntax:</h3><center> fun x0 (x1 : t1, ..., xn : tn) = e</center><h3 id="Type-checking"><a href="#Type-checking" class="headerlink" title="Type-checking:"></a>Type-checking:</h3><p>To type-check a function binding, we type-check the body e in a static environment that (in addition to alltheearlierbindings)maps x1 to t1,, xn to tn and x0 to t1 <em>  </em> tn -&gt; t. Because x0 is in the environment, we can make recursive function calls, i.e., a function definition can use itself. The syntax of a function type is <code>argument types -&gt; result type</code> where the argument types are separated by * (which just happens to be the same character used in expressions for multiplication). For the function binding to type-check, the body e must have the type t, i.e., the result type of x0. That makes sense given the evaluation rules below because the result of a function call is the result of evaluating e.</p><p>But what, exactly, is t - we never wrote it down? It can be any type, and it is up to the type-checker (part of the language implementation) to figure out what t should be such that using it for the result type of x0 makes, everything work out.</p><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation:"></a>Evaluation:</h3><p>The evaluation rule for a function binding is trivial: A function is a value  we simply add x0 to the environment as a function that can be called later. As expected for recursion, x0 is in the dynamic environment in the function body and for subsequent bindings (but not, unlike in say Java, for preceding bindings, so the order you define functions is very important).</p><h2 id="Pairs-and-Other-Tuples"><a href="#Pairs-and-Other-Tuples" class="headerlink" title="Pairs and Other Tuples"></a>Pairs and Other Tuples</h2><p>Tuples: <strong>fixed</strong> number of pieces that may have <code>different types</code></p><h3 id="Build"><a href="#Build" class="headerlink" title="Build:"></a>Build:</h3><ul><li>Syntax: <code>(e1,e2)</code></li><li>Evaluation: Evaluate e1 to v1 and e2 to v2; result is (v1,v2)<ul><li>A pair of values is a value</li></ul></li><li>Type-checking: If e1 has type ta and e2 has type tb, then the<br>pair expression has type ta * tb <ul><li>A new kind of type</li></ul></li></ul><h3 id="Access"><a href="#Access" class="headerlink" title="Access:"></a>Access:</h3><ul><li>Syntax: <code>#1 e</code> and <code>#2 e</code></li><li>Evaluation: Evaluate e to a pair of values and return first or<br>second piece<ul><li>Example: If e is a variable x, then look up x in environment</li></ul></li><li>Type-checking: If e has type ta * tb, then #1 e has type ta and #2 e has type tb</li></ul><h2 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h2><p>Lists can have <strong>any number</strong> of elements,but all list elements have the <strong>same type</strong></p><h3 id="Build-1"><a href="#Build-1" class="headerlink" title="Build"></a>Build</h3><ul><li>The empty list is a value: []</li><li>In general, a list of values is a value; elements separated by commas:<center> [v1,v2,...,vn]</center></li><li>If e1 evaluates to v and e2 evaluates to a list [v1,,vn], then e1::e2 evaluates to [v,,vn] <center> e1::e2</center></li></ul><h3 id="Access-1"><a href="#Access-1" class="headerlink" title="Access"></a>Access</h3><p>Until we learn pattern-matching, we will use three standard-library functions</p><ul><li>null e evaluates to true if and only if e evaluates to []</li><li>If e evaluates to [v1,v2,,vn] then <code>hd</code> e evaluates to v1 (raise exception if e evaluates to [])</li><li>If e evaluates to [v1,v2,,vn] then <code>tl</code> e evaluates to [v2,,vn]  (raise exception if e evaluates to [])</li></ul><h2 id="Let-expressions"><a href="#Let-expressions" class="headerlink" title="Let-expressions"></a>Let-expressions</h2><p>Let-expressions are an absolutely crucial feature that allows for local variables in a very simple, general, and flexible way. Let-expressions are crucial for style and for efficiency. A let-expression lets us have <strong>local variables</strong>. In fact, it lets us have local bindings of any sort, including function bindings. Because it is a kind of expression, it can appear anywhere an expression can.</p><ul><li>Syntax: let b1 b2 bn in e end<ul><li>Each bi is any binding and e is any expression</li></ul></li><li>Type-checking: Type-check each bi and e in a static environment that includes the previous bindings. Type of whole let-expression is the type of e.</li><li>Evaluation: Evaluate each bi and e in a dynamic environment that includes the previous bindings.<br>Result of whole let-expression is result of evaluating e.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fun good_max (xs : int list) = if null xs</span><br><span class="line">    then 0 (* note: bad style; see below *)</span><br><span class="line">    else if null (tl xs)</span><br><span class="line">    then hd xs</span><br><span class="line">    else</span><br><span class="line">(* for style, could also use a let-binding for hd xs *) </span><br><span class="line">    let val tl_ans = good_max(tl xs)</span><br><span class="line">    in</span><br><span class="line">        if hd xs &gt; tl_ans then hd xs</span><br><span class="line">        else tl_ans</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><h2 id="Options"><a href="#Options" class="headerlink" title="Options"></a>Options</h2><p>The previous example does not properly handle the empty list  it returns 0. This is bad style because 0 is really not the maximum value of 0 numbers. There is no good answer, but we should deal with this case reasonably. One possibility is to raise an exception; you can learn about ML exceptions on your own if you are interested before we discuss them later in the course. Instead, lets change the return type to either return the maximum number or indicate the input list was empty so there is no maximum. Given the constructs we have, we could code this up by return an int list, using [] if the input was the empty list and a list with one integer (the maximum) if the input list was not empty.</p><p>The ML library has <code>options</code> which are a precise description: an option value has either 0 or 1 thing: <code>NONE</code> is an option value carrying nothing whereas SOME e evaluates e to a value v and becomes the option carrying the one value v. The type of NONE is a option and the type of SOME e is t option if e has type t.</p><h3 id="Building"><a href="#Building" class="headerlink" title="Building:"></a>Building:</h3><ul><li><code>NONE</code> has type a option (much like [] has type a list)</li><li><code>SOME</code> e has type t option if e has type t (much like e::[])</li></ul><h3 id="Access-2"><a href="#Access-2" class="headerlink" title="Access:"></a>Access:</h3><ul><li><code>isSome</code> has type a option -&gt; bool</li><li><code>valOf</code> has type a option -&gt; a (exception if given NONE)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fun better_max (xs : int list) = if null xs</span><br><span class="line">    then NONE</span><br><span class="line">    else</span><br><span class="line">        let val tl_ans = better_max(tl xs)</span><br><span class="line">        in </span><br><span class="line">            if isSome tl_ans andalso valOf tl_ans &gt; hd xs</span><br><span class="line">            then tl_ans</span><br><span class="line">            else SOME (hd xs)</span><br><span class="line">        end</span><br></pre></td></tr></table></figure><h2 id="Boolean-operations"><a href="#Boolean-operations" class="headerlink" title="Boolean operations"></a>Boolean operations</h2><ul><li>e1 <code>andalso</code> e2</li><li>e1 <code>orelse</code> e2</li><li><code>not</code> e1</li></ul><h2 id="Comparisons"><a href="#Comparisons" class="headerlink" title="Comparisons"></a>Comparisons</h2><p>= &lt;&gt; &gt; &lt; &gt;= &lt;=</p><ul><li><blockquote><p>&lt; &gt;= &lt;= can be used with real, but not 1 int and 1 real</p></blockquote></li><li>= &lt;&gt; can be used with any equality type but not with real</li></ul><h2 id="Lack-of-Mutation-and-Benefits-Thereof"><a href="#Lack-of-Mutation-and-Benefits-Thereof" class="headerlink" title="Lack of Mutation and Benefits Thereof"></a>Lack of Mutation and Benefits Thereof</h2><p>In ML, there is no way to change the contents of a binding, a tuple, or a list. If x maps to some value like the list of pairs [(3,4),(7,9)] in some environment, then x will forever map to that list in that environment. There is no assignment statement that changes x to map to a different list. (You can introduce a new binding that <strong>shadows</strong> x, but that will not affect any code that looks up the <code>original</code> x in an environment.)</p><p>For a final example, the following Java is the key idea behind an actual security hole in an important (and subsequently fixed) Java library. Suppose we are maintaining permissions for who is allowed to access something like a file on the disk. It is fine to let everyone see who has permission, but clearly only those that do have permission can actually use the resource. Consider this wrong code (some parts omitted if not relevant):</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProtectedResource</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> Resource theResource = ...; </span><br><span class="line"><span class="keyword">private</span> String[] allowedUsers = ...; </span><br><span class="line"><span class="keyword">public</span> String[] getAllowedUsers() &#123;</span><br><span class="line">      <span class="keyword">return</span> allowedUsers;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">currentUser</span><span class="params">()</span> </span>&#123; ... &#125; </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">useTheResource</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; allowedUsers.length; i++) &#123; </span><br><span class="line">        <span class="keyword">if</span>(currentUser().equals(allowedUsers[i])) &#123;</span><br><span class="line">    ... <span class="comment">// access allowed: use it return;</span></span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalAccessException();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Can you find the problem? Here it is: </p><p><code>getAllowedUsers</code> returns an alias to the allowedUsers array, so any user can gain access by doing <code>getAllowedUsers()[0] = currentUser().</code></p><h2 id="The-Pieces-of-a-Programming-Language"><a href="#The-Pieces-of-a-Programming-Language" class="headerlink" title="The Pieces of a Programming Language"></a>The Pieces of a Programming Language</h2><p>Now that we have learned enough ML to write some simple functions and programs with it, we can list the essential pieces necessary for defining and learning any programming language:</p><ul><li><strong>Syntax</strong>: How do you write the various parts of the language?</li><li><strong>Semantics</strong>: What do the various language features mean? For example, how are expressions evaluated?</li><li><strong>Idioms</strong>: What are the common approaches to using the language features to express computations?</li><li><strong>Libraries</strong>: What has already been written for you? How do you do things you could not do without library support (like access files)?</li><li><strong>Tools</strong>: What is available for manipulating programs in the language (compilers, read-eval-print loops, debuggers, )</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Course note from <a href="https://www.coursera.org/learn/programming-languages/home/week/2" target="_blank" rel="noopener">Programming Language</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ML-Expressions-and-Variable-Bindings&quot;&gt;&lt;a href=&quot;#ML-Expressions-and-Variable-Bindings&quot; class=&quot;headerlink&quot; title=&quot;ML Expressions and V
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorBoard</title>
    <link href="https://zhangruochi.com/TensorBoard/2020/02/22/"/>
    <id>https://zhangruochi.com/TensorBoard/2020/02/22/</id>
    <published>2020-02-22T18:34:48.000Z</published>
    <updated>2020-04-13T23:23:53.957Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TensorBoard-with-Fashion-MNIST"><a href="#TensorBoard-with-Fashion-MNIST" class="headerlink" title="TensorBoard with Fashion MNIST"></a>TensorBoard with Fashion MNIST</h1><p>In this weeks exercise you will train a convolutional neural network to classify images of the Fashion MNIST dataset and you will use TensorBoard to explore how its confusion matrix evolves over time.</p><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the TensorBoard notebook extension.</span></span><br><span class="line">%load_ext tensorboard</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"TensorFlow version: "</span>, tf.__version__)</span><br></pre></td></tr></table></figure><pre><code>TensorFlow version:  2.0.0</code></pre><h2 id="Load-the-Fashion-MNIST-Dataset"><a href="#Load-the-Fashion-MNIST-Dataset" class="headerlink" title="Load the Fashion-MNIST Dataset"></a>Load the Fashion-MNIST Dataset</h2><p>We are going to use a CNN to classify images in the the <a href="https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/" target="_blank" rel="noopener">Fashion-MNIST</a> dataset. This dataset consist of 70,000 grayscale images of fashion products from 10 categories, with 7,000 images per category. The images have a size of $28\times28$ pixels.</p><p>First, we load the data. Even though these are really images, we will load them as NumPy arrays and not as binary image objects. The data is already divided into training and testing sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the data.</span></span><br><span class="line">train_images = np.load(<span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/train_images.npy"</span>)</span><br><span class="line">train_labels = np.load(<span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/train_labels.npy"</span>)</span><br><span class="line"></span><br><span class="line">test_images = np.load(<span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/test_images.npy"</span>)</span><br><span class="line">test_labels = np.load(<span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/test_labels.npy"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The labels of the images are integers representing classes.</span></span><br><span class="line"><span class="comment"># Here we set the Names of the integer classes, i.e., 0 -&gt; T-short/top, 1 -&gt; Trouser, etc.</span></span><br><span class="line">class_names = [<span class="string">'T-shirt/top'</span>, <span class="string">'Trouser'</span>, <span class="string">'Pullover'</span>, <span class="string">'Dress'</span>, <span class="string">'Coat'</span>,</span><br><span class="line">               <span class="string">'Sandal'</span>, <span class="string">'Shirt'</span>, <span class="string">'Sneaker'</span>, <span class="string">'Bag'</span>, <span class="string">'Ankle boot'</span>]</span><br></pre></td></tr></table></figure><h2 id="Format-the-Images"><a href="#Format-the-Images" class="headerlink" title="Format the Images"></a>Format the Images</h2><p><code>train_images</code> is a NumPy array with shape <code>(60000, 28, 28)</code> and <code>test_images</code> is a NumPy array with shape <code>(10000, 28, 28)</code>. However, our model expects arrays with shape <code>(batch_size, height, width, channels)</code> . Therefore, we must reshape our NumPy arrays to also include the number of color channels. Since the images are grayscale, we will set <code>channels</code> to <code>1</code>. We will also normalize the values of our NumPy arrays to be in the range <code>[0,1]</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pre-process images</span></span><br><span class="line">train_images = train_images.reshape(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">train_images = train_images / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">test_images = test_images.reshape(<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">test_images = test_images / <span class="number">255.0</span></span><br></pre></td></tr></table></figure><h2 id="Build-the-Model"><a href="#Build-the-Model" class="headerlink" title="Build the Model"></a>Build the Model</h2><p>We will build a simple CNN and compile it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the model</span></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><h2 id="Plot-Confusion-Matrix"><a href="#Plot-Confusion-Matrix" class="headerlink" title="Plot Confusion Matrix"></a>Plot Confusion Matrix</h2><p>When training a classifier, its often useful to see the <a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener">confusion matrix</a>. The confusion matrix gives you detailed knowledge of how your classifier is performing on test data.</p><p>In the cell below, we will define a function that returns a Matplotlib figure containing the plotted confusion matrix.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(cm, class_names)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns a matplotlib figure containing the plotted confusion matrix.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">       cm (array, shape = [n, n]): a confusion matrix of integer classes</span></span><br><span class="line"><span class="string">       class_names (array, shape = [n]): String names of the integer classes</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    plt.imshow(cm, interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</span><br><span class="line">    plt.title(<span class="string">"Confusion matrix"</span>)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    tick_marks = np.arange(len(class_names))</span><br><span class="line">    plt.xticks(tick_marks, class_names, rotation=<span class="number">45</span>)</span><br><span class="line">    plt.yticks(tick_marks, class_names)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Normalize the confusion matrix.</span></span><br><span class="line">    cm = np.around(cm.astype(<span class="string">'float'</span>) / cm.sum(axis=<span class="number">1</span>)[:, np.newaxis], decimals=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use white text if squares are dark; otherwise black.</span></span><br><span class="line">    threshold = cm.max() / <span class="number">2.</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> itertools.product(range(cm.shape[<span class="number">0</span>]), range(cm.shape[<span class="number">1</span>])):</span><br><span class="line">        color = <span class="string">"white"</span> <span class="keyword">if</span> cm[i, j] &gt; threshold <span class="keyword">else</span> <span class="string">"black"</span></span><br><span class="line">        plt.text(j, i, cm[i, j], horizontalalignment=<span class="string">"center"</span>, color=color)</span><br><span class="line">        </span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.ylabel(<span class="string">'True label'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Predicted label'</span>)</span><br><span class="line">    <span class="keyword">return</span> figure</span><br></pre></td></tr></table></figure><h2 id="TensorBoard-Callback"><a href="#TensorBoard-Callback" class="headerlink" title="TensorBoard Callback"></a>TensorBoard Callback</h2><p>We are now ready to train the CNN and regularly log the confusion matrix during the process. In the cell below, you will create a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard" target="_blank" rel="noopener">Keras TensorBoard callback</a> to log basic metrics.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Clear logs prior to logging data.</span></span><br><span class="line">!rm -rf logs/image</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create log directory</span></span><br><span class="line">logdir = <span class="string">"logs/image/"</span> + datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Define a TensorBoard callback. Use the log_dir parameter</span></span><br><span class="line"><span class="comment"># to specify the path to the directory where you want to save the</span></span><br><span class="line"><span class="comment"># log files to be parsed by TensorBoard.</span></span><br><span class="line">tensorboard_callback = keras.callbacks.TensorBoard(log_dir = logdir, histogram_freq = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">file_writer_cm = tf.summary.create_file_writer(logdir + <span class="string">'/cm'</span>)</span><br></pre></td></tr></table></figure><pre><code>rm: cannot remove &#39;logs/image/20200222-182126/cm&#39;: Directory not empty</code></pre><h2 id="Convert-Matplotlib-Figure-to-PNG"><a href="#Convert-Matplotlib-Figure-to-PNG" class="headerlink" title="Convert Matplotlib Figure to PNG"></a>Convert Matplotlib Figure to PNG</h2><p>Unfortunately, the Matplotlib file format cannot be logged as an image, but the PNG file format can be logged. So, you will create a helper function that takes a Matplotlib figure and converts it to PNG format so it can be written. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_to_image</span><span class="params">(figure)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts the matplotlib plot specified by 'figure' to a PNG image and</span></span><br><span class="line"><span class="string">    returns it. The supplied figure is closed and inaccessible after this call.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    buf = io.BytesIO()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Use plt.savefig to save the plot to a PNG in memory.</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    plt.savefig(buf, format=<span class="string">'png'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Closing the figure prevents it from being displayed directly inside</span></span><br><span class="line">    <span class="comment"># the notebook.</span></span><br><span class="line">    plt.close(figure)</span><br><span class="line">    buf.seek(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Use tf.image.decode_png to convert the PNG buffer</span></span><br><span class="line">    <span class="comment"># to a TF image. Make sure you use 4 channels.</span></span><br><span class="line">    image = tf.image.decode_png(buf.getvalue(), channels=<span class="number">4</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Use tf.expand_dims to add the batch dimension</span></span><br><span class="line">    image = tf.expand_dims(image, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><p>In the cell below, you will define a function that calculates the confusion matrix.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_confusion_matrix</span><span class="params">(epoch, logs)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Use the model to predict the values from the test_images.</span></span><br><span class="line">    test_pred_raw = model.predict(test_images)    </span><br><span class="line">    test_pred = np.argmax(test_pred_raw, axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Calculate the confusion matrix using sklearn.metrics</span></span><br><span class="line">    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)</span><br><span class="line">    </span><br><span class="line">    figure = plot_confusion_matrix(cm, class_names=class_names)</span><br><span class="line">    cm_image = plot_to_image(figure)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Log the confusion matrix as an image summary.</span></span><br><span class="line">    <span class="keyword">with</span> file_writer_cm.as_default():</span><br><span class="line">        tf.summary.image(<span class="string">"Confusion Matrix"</span>, cm_image, step=epoch)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the per-epoch callback.</span></span><br><span class="line">cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)</span><br></pre></td></tr></table></figure><h2 id="Running-TensorBoard"><a href="#Running-TensorBoard" class="headerlink" title="Running TensorBoard"></a>Running TensorBoard</h2><p>The next step will be to run the code shown below to render the TensorBoard. Unfortunately, TensorBoard cannot be rendered within the Coursera environment. Therefore, we wont run the code below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start TensorBoard.</span></span><br><span class="line">%tensorboard --logdir logs/image</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the classifier.</span></span><br><span class="line">model.fit(train_images,</span><br><span class="line">          train_labels,</span><br><span class="line">          epochs=<span class="number">5</span>,</span><br><span class="line">          verbose=<span class="number">0</span>, <span class="comment"># Suppress chatty output</span></span><br><span class="line">          callbacks=[tensorboard_callback, cm_callback],</span><br><span class="line">          validation_data=(test_images, test_labels))</span><br></pre></td></tr></table></figure><p>However, you are welcome to download the notebook and run the above code locally on your machine or in Googles Colab to see TensorBoard in action. Below are some example screenshots that you should see when executing the code:</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;TensorBoard-with-Fashion-MNIST&quot;&gt;&lt;a href=&quot;#TensorBoard-with-Fashion-MNIST&quot; class=&quot;headerlink&quot; title=&quot;TensorBoard with Fashion MNIST&quot;&gt;
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Visualization" scheme="https://zhangruochi.com/categories/AI-Workflow/Visualization/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow hub</title>
    <link href="https://zhangruochi.com/Tensorflow-hub/2020/02/22/"/>
    <id>https://zhangruochi.com/Tensorflow-hub/2020/02/22/</id>
    <published>2020-02-22T07:48:22.000Z</published>
    <updated>2020-04-07T18:05:24.249Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Exporting-an-MNIST-Classifier-in-SavedModel-Format"><a href="#Exporting-an-MNIST-Classifier-in-SavedModel-Format" class="headerlink" title="Exporting an MNIST Classifier in SavedModel Format"></a>Exporting an MNIST Classifier in SavedModel Format</h1><p>In this exercise, we will learn on how to create models for TensorFlow Hub. You will be tasked with performing the following tasks:</p><ul><li>Creating a simple MNIST classifier and evaluating its accuracy.</li><li>Exporting it into SavedModel.</li><li>Hosting the model as TF Hub Module.</li><li>Importing this TF Hub Module to be used with Keras Layers.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub</span><br><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br></pre></td></tr></table></figure><h2 id="Create-an-MNIST-Classifier"><a href="#Create-an-MNIST-Classifier" class="headerlink" title="Create an MNIST Classifier"></a>Create an MNIST Classifier</h2><p>We will start by creating a class called <code>MNIST</code>. This class will load the MNIST dataset, preprocess the images from the dataset, and build a CNN based classifier. This class will also have some methods to train, test, and save our model. </p><p>In the cell below, fill in the missing code and create the following Keras <code>Sequential</code> model:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">lambda (Lambda)              (None, 28, 28, 1)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d (Conv2D)              (None, 28, 28, 8)         80        </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)            (None, 14, 14, 16)        1168      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)            (None, 7, 7, 32)          4640      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (None, 1568)              0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, 128)               200832    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 10)                1290      </span><br><span class="line">=================================================================</span><br></pre></td></tr></table></figure><p>Notice that we are using a <code>tf.keras.layers.Lambda</code> layer at the beginning of our model. <code>Lambda</code> layers are used to wrap arbitrary expressions as a <code>Layer</code> object:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Lambda(expression)</span><br></pre></td></tr></table></figure><p>The <code>Lambda</code> layer exists so that arbitrary TensorFlow functions can be used when constructing <code>Sequential</code> and Functional API models. <code>Lambda</code> layers are best suited for simple operations. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNIST</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, export_path, buffer_size=<span class="number">1000</span>, batch_size=<span class="number">32</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_rate=<span class="number">1e-3</span>, epochs=<span class="number">10</span>)</span>:</span></span><br><span class="line">        self._export_path = export_path</span><br><span class="line">        self._buffer_size = buffer_size</span><br><span class="line">        self._batch_size = batch_size</span><br><span class="line">        self._learning_rate = learning_rate</span><br><span class="line">        self._epochs = epochs</span><br><span class="line">    </span><br><span class="line">        self._build_model()</span><br><span class="line">        self.train_dataset, self.test_dataset = self._prepare_dataset()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Function to preprocess the images.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_fn</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Cast x to tf.float32 using the tf.cast() function.</span></span><br><span class="line">        <span class="comment"># You should also normalize the values of x to be in the range [0, 1].</span></span><br><span class="line">        x = tf.cast(x, tf.float32) / <span class="number">255.0</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Build the model according to the model summary shown above.</span></span><br><span class="line">        self._model = tf.keras.models.Sequential([</span><br><span class="line">            tf.keras.layers.Input(shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>), dtype=tf.uint8),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Use a Lambda layer to use the self.preprocess_fn function</span></span><br><span class="line">            <span class="comment"># defined above to preprocess the images.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.Lambda(self.preprocess_fn),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a Conv2D layer with 8 filters, a kernel size of 3</span></span><br><span class="line">            <span class="comment"># and padding='same'.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.Conv2D(filters = <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">"same"</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a MaxPool2D() layer. Use default values.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.MaxPool2D(),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a Conv2D layer with 16 filters, a kernel size of 3</span></span><br><span class="line">            <span class="comment"># and padding='same'.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.Conv2D(filters = <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">"same"</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a MaxPool2D() layer. Use default values.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.MaxPool2D(),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a Conv2D layer with 32 filters, a kernel size of 3</span></span><br><span class="line">            <span class="comment"># and padding='same'.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.Conv2D(filters = <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">"same"</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create the Flatten and Dense layers as described in the </span></span><br><span class="line">            <span class="comment"># model summary shown above.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            </span><br><span class="line">            tf.keras.layers.Flatten(),</span><br><span class="line">            tf.keras.layers.Dense(<span class="number">128</span>),</span><br><span class="line">            tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">"softmax"</span>)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Define the optimizer, loss function and metrics.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use the tf.keras.optimizers.Adam optimizer and set the</span></span><br><span class="line">        <span class="comment"># learning rate to self._learning_rate.</span></span><br><span class="line">        optimizer_fn = tf.keras.optimizers.Adam(learning_rate=self._learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use sparse_categorical_crossentropy as your loss function.</span></span><br><span class="line">        loss_fn = <span class="string">"sparse_categorical_crossentropy"</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the metrics to accuracy.</span></span><br><span class="line">        metrics_list = [<span class="string">"accuracy"</span>]</span><br><span class="line">     </span><br><span class="line">        <span class="comment"># Compile the model.</span></span><br><span class="line">        self._model.compile(optimizer_fn, loss=loss_fn, metrics=metrics_list)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prepare_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        filePath = <span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2"</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Load the MNIST dataset using tfds.load(). Make sure to use</span></span><br><span class="line">        <span class="comment"># the argument data_dir=filePath. You should load the images as well</span></span><br><span class="line">        <span class="comment"># as their corresponding labels and load both the test and train splits.</span></span><br><span class="line">        dataset = tfds.load(<span class="string">'mnist'</span>, split=[<span class="string">'train'</span>, <span class="string">'test'</span>], data_dir = filePath, as_supervised=<span class="keyword">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Extract the 'train' and 'test' splits from the dataset above.</span></span><br><span class="line">        train_dataset, test_dataset = dataset</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> train_dataset, test_dataset</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Shuffle and batch the self.train_dataset. Use self._buffer_size</span></span><br><span class="line">        <span class="comment"># as the shuffling buffer and self._batch_size as the batch size for batching. </span></span><br><span class="line">        dataset_tr = self.train_dataset.shuffle(self._buffer_size).batch(self._batch_size)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         images, labels = next(iter(dataset_tr)).items()</span></span><br><span class="line"><span class="comment">#         print(images[1].shape) # (32, 28, 28, 1)</span></span><br><span class="line">             </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Train the model for specified number of epochs.</span></span><br><span class="line">        self._model.fit(dataset_tr, epochs=self._epochs)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Batch the self.test_dataset. Use a batch size of 32.</span></span><br><span class="line">        dataset_te = self.test_dataset.batch(<span class="number">32</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Evaluate the dataset</span></span><br><span class="line">        results = self._model.evaluate(dataset_te)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Print the metric values on which the model is being evaluated on.</span></span><br><span class="line">        <span class="keyword">for</span> name, value <span class="keyword">in</span> zip(self._model.metrics_names, results):</span><br><span class="line">            print(<span class="string">"%s: %.3f"</span> % (name, value))</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">export_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Save the model.</span></span><br><span class="line">        tf.saved_model.save(self._model, self._export_path)</span><br></pre></td></tr></table></figure><h2 id="Train-Evaluate-and-Save-the-Model"><a href="#Train-Evaluate-and-Save-the-Model" class="headerlink" title="Train, Evaluate, and Save the Model"></a>Train, Evaluate, and Save the Model</h2><p>We will now use the <code>MNIST</code> class we created above to create an <code>mnist</code> object. When creating our <code>mnist</code> object we will use a dictionary to pass our training parameters. We will then call the <code>train</code> and <code>export_model</code> methods to train and save our model, respectively. Finally, we call the <code>test</code> method to evaluate our model after training. </p><p><strong>NOTE:</strong> It will take about 12 minutes to train the model for 5 epochs.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the training parameters.</span></span><br><span class="line">args = &#123;<span class="string">'export_path'</span>: <span class="string">'./saved_model'</span>,</span><br><span class="line">        <span class="string">'buffer_size'</span>: <span class="number">1000</span>,</span><br><span class="line">        <span class="string">'batch_size'</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">        <span class="string">'epochs'</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the mnist object. </span></span><br><span class="line">mnist = MNIST(**args)</span><br><span class="line">print(mnist._model.summary())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model.</span></span><br><span class="line">mnist.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the model.</span></span><br><span class="line">mnist.export_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the trained MNIST model.</span></span><br><span class="line">mnist.test()</span><br></pre></td></tr></table></figure><pre><code>WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /tf/week2/../tmp2. Using currently defined version 1.0.0.Model: &quot;sequential_2&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================lambda_1 (Lambda)            (None, 28, 28, 1)         0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 28, 28, 8)         80        _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 14, 14, 16)        1168      _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 7, 7, 16)          0         _________________________________________________________________conv2d_5 (Conv2D)            (None, 7, 7, 32)          4640      _________________________________________________________________flatten_1 (Flatten)          (None, 1568)              0         _________________________________________________________________dense_1 (Dense)              (None, 128)               200832    _________________________________________________________________dense_2 (Dense)              (None, 10)                1290      =================================================================Total params: 208,010Trainable params: 208,010Non-trainable params: 0_________________________________________________________________NoneEpoch 1/51875/1875 [==============================] - 135s 72ms/step - loss: 0.1548 - accuracy: 0.9532548 - accuracy: 0.95Epoch 2/5 563/1875 [========&gt;.....................] - ETA: 1:36 - loss: 0.0868 - accuracy: 0.9733</code></pre><h2 id="Create-a-Tarball"><a href="#Create-a-Tarball" class="headerlink" title="Create a Tarball"></a>Create a Tarball</h2><p>The <code>export_model</code> method saved our model in the TensorFlow SavedModel format in the <code>./saved_model</code> directory. The SavedModel format saves our model and its weights in various files and directories. This makes it difficult to distribute our model. Therefore, it is convenient to create a single compressed file that contains all the files and folders of our model. To do this, we will use the <code>tar</code> archiving program to create a tarball (similar to a Zip file) that contains our SavedModel.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tarball from the SavedModel.</span></span><br><span class="line">!tar -cz -f module.tar.gz -C ./saved_model .</span><br></pre></td></tr></table></figure><h2 id="Inspect-the-Tarball"><a href="#Inspect-the-Tarball" class="headerlink" title="Inspect the Tarball"></a>Inspect the Tarball</h2><p>We can uncompress our tarball to make sure it has all the files and folders from our SavedModel.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inspect the tarball.</span></span><br><span class="line">!tar -tf module.tar.gz</span><br></pre></td></tr></table></figure><pre><code>././variables/./variables/variables.data-00001-of-00002./variables/variables.data-00000-of-00002./variables/variables.index./saved_model.pb./assets/</code></pre><h2 id="Simulate-Server-Conditions"><a href="#Simulate-Server-Conditions" class="headerlink" title="Simulate Server Conditions"></a>Simulate Server Conditions</h2><p>Once we have verified our tarball, we can now simulate server conditions. In a normal scenario, we will fetch our TF Hub module from a remote server using the modules handle. However, since this notebook cannot host the server, we will instead point the module handle to the directory where our SavedModel is stored. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!rm -rf ./module</span><br><span class="line">!mkdir -p module</span><br><span class="line">!tar xvzf module.tar.gz -C ./module</span><br></pre></td></tr></table></figure><pre><code>././variables/./variables/variables.data-00001-of-00002./variables/variables.data-00000-of-00002./variables/variables.index./saved_model.pb./assets/</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the module handle.</span></span><br><span class="line">MODULE_HANDLE = <span class="string">'./module'</span></span><br></pre></td></tr></table></figure><h2 id="Load-the-TF-Hub-Module"><a href="#Load-the-TF-Hub-Module" class="headerlink" title="Load the TF Hub Module"></a>Load the TF Hub Module</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Load the TF Hub module using the hub.load API.</span></span><br><span class="line">model = hub.load(MODULE_HANDLE)</span><br></pre></td></tr></table></figure><h2 id="Test-the-TF-Hub-Module"><a href="#Test-the-TF-Hub-Module" class="headerlink" title="Test the TF Hub Module"></a>Test the TF Hub Module</h2><p>We will now test our TF Hub module with images from the <code>test</code> split of the MNIST dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">filePath = <span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Load the MNIST 'test' split using tfds.load().</span></span><br><span class="line"><span class="comment"># Make sure to use the argument data_dir=filePath. You</span></span><br><span class="line"><span class="comment"># should load the images along with their corresponding labels.</span></span><br><span class="line"></span><br><span class="line">dataset = tfds.load(<span class="string">'mnist'</span>, split=tfds.Split.TEST, data_dir = filePath, as_supervised=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Batch the dataset using a batch size of 32.</span></span><br><span class="line">test_dataset = dataset.batch(<span class="number">32</span>)</span><br></pre></td></tr></table></figure><pre><code>WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /tf/week2/../tmp2. Using currently defined version 1.0.0.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the TF Hub module for a single batch of data</span></span><br><span class="line"><span class="keyword">for</span> batch_data <span class="keyword">in</span> test_dataset.take(<span class="number">1</span>):</span><br><span class="line">    outputs = model(batch_data[<span class="number">0</span>])</span><br><span class="line">    outputs = np.argmax(outputs, axis=<span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">'Predicted Labels:'</span>, outputs)</span><br><span class="line">    print(<span class="string">'True Labels:     '</span>, batch_data[<span class="number">1</span>].numpy())</span><br></pre></td></tr></table></figure><pre><code>Predicted Labels: [6 2 3 7 2 2 3 4 7 6 6 9 2 0 9 6 2 0 6 5 1 4 8 1 9 8 4 0 0 5 8 4]True Labels:      [6 2 3 7 2 2 3 4 7 6 6 9 2 0 9 6 8 0 6 5 1 4 8 1 9 8 4 0 0 5 2 4]</code></pre><p>We can see that the model correctly predicts the labels for most images in the batch. </p><h2 id="Evaluate-the-Model-Using-Keras"><a href="#Evaluate-the-Model-Using-Keras" class="headerlink" title="Evaluate the Model Using Keras"></a>Evaluate the Model Using Keras</h2><p>In the cell below, you will integrate the TensorFlow Hub module into the high level Keras API.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Integrate the TensorFlow Hub module into a Keras</span></span><br><span class="line"><span class="comment"># sequential model. You should use a hub.KerasLayer and you </span></span><br><span class="line"><span class="comment"># should make sure to use the correct values for the output_shape,</span></span><br><span class="line"><span class="comment"># and input_shape parameters. You should also use tf.uint8 for</span></span><br><span class="line"><span class="comment"># the dtype parameter.</span></span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    hub.KerasLayer(model, output_shape=[<span class="number">10</span>], input_shape=[<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>], </span><br><span class="line">                           dtype=tf.uint8)</span><br><span class="line">]) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compile the model.</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'sparse_categorical_crossentropy'</span>, </span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate the model on the test_dataset.</span></span><br><span class="line">results = model.evaluate(test_dataset)</span><br></pre></td></tr></table></figure><pre><code>313/313 [==============================] - 27s 88ms/step - loss: 0.0605 - accuracy: 0.9824</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the metric values on which the model is being evaluated on.</span></span><br><span class="line"><span class="keyword">for</span> name, value <span class="keyword">in</span> zip(model.metrics_names, results):</span><br><span class="line">    print(<span class="string">"%s: %.3f"</span> % (name, value))</span><br></pre></td></tr></table></figure><pre><code>loss: 0.061accuracy: 0.982</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Exporting-an-MNIST-Classifier-in-SavedModel-Format&quot;&gt;&lt;a href=&quot;#Exporting-an-MNIST-Classifier-in-SavedModel-Format&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Deployment" scheme="https://zhangruochi.com/categories/AI-Workflow/Deployment/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow Serving</title>
    <link href="https://zhangruochi.com/Tensorflow-Serving/2020/02/21/"/>
    <id>https://zhangruochi.com/Tensorflow-Serving/2020/02/21/</id>
    <published>2020-02-22T04:37:47.000Z</published>
    <updated>2020-04-07T18:05:41.993Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Train-Your-Own-Model-and-Serve-It-With-TensorFlow-Serving"><a href="#Train-Your-Own-Model-and-Serve-It-With-TensorFlow-Serving" class="headerlink" title="Train Your Own Model and Serve It With TensorFlow Serving"></a>Train Your Own Model and Serve It With TensorFlow Serving</h1><p>In this notebook, you will train a neural network to classify images of handwritten digits from the <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a> dataset. You will then save the trained model, and serve it using <a href="https://www.tensorflow.org/tfx/guide/serving" target="_blank" rel="noopener">TensorFlow Serving</a>.</p><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    %tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> tempfile</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\u2022 Using TensorFlow Version:"</span>, tf.__version__)</span><br></pre></td></tr></table></figure><p>  Using TensorFlow Version: 2.2.0-dev20200217</p><h2 id="Import-the-MNIST-Dataset"><a href="#Import-the-MNIST-Dataset" class="headerlink" title="Import the MNIST Dataset"></a>Import the MNIST Dataset</h2><p>The <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a> dataset contains 70,000 grayscale images of the digits 0 through 9. The images show individual digits at a low resolution (28 by 28 pixels). </p><p>Even though these are really images, we will load them as NumPy arrays and not as binary image objects.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br></pre></td></tr></table></figure><pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz11493376/11490434 [==============================] - 1s 0us/step</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Scale the values of the arrays below to be between 0.0 and 1.0.</span></span><br><span class="line">train_images = train_images / <span class="number">255.0</span></span><br><span class="line">test_images =  test_images / <span class="number">255.0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_images.shape, test_images.shape</span><br></pre></td></tr></table></figure><pre><code>((60000, 28, 28), (10000, 28, 28))</code></pre><p>In the cell below use the <code>.reshape</code> method to resize the arrays to the following sizes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_images.shape: (<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">test_images.shape: (<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Reshape the arrays below.</span></span><br><span class="line">train_images = train_images.reshape((*train_images.shape,<span class="number">1</span>))</span><br><span class="line">test_images =  test_images.reshape((*test_images.shape,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'\ntrain_images.shape: &#123;&#125;, of &#123;&#125;'</span>.format(train_images.shape, train_images.dtype))</span><br><span class="line">print(<span class="string">'test_images.shape: &#123;&#125;, of &#123;&#125;'</span>.format(test_images.shape, test_images.dtype))</span><br></pre></td></tr></table></figure><pre><code>train_images.shape: (60000, 28, 28, 1), of float64test_images.shape: (10000, 28, 28, 1), of float64</code></pre><h2 id="Look-at-a-Sample-Image"><a href="#Look-at-a-Sample-Image" class="headerlink" title="Look at a Sample Image"></a>Look at a Sample Image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">idx = <span class="number">42</span></span><br><span class="line"></span><br><span class="line">plt.imshow(test_images[idx].reshape(<span class="number">28</span>,<span class="number">28</span>), cmap=plt.cm.binary)</span><br><span class="line">plt.title(<span class="string">'True Label: &#123;&#125;'</span>.format(test_labels[idx]), fontdict=&#123;<span class="string">'size'</span>: <span class="number">16</span>&#125;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_14_0.png" alt="png"></p><h2 id="Build-a-Model"><a href="#Build-a-Model" class="headerlink" title="Build a Model"></a>Build a Model</h2><p>In the cell below build a <code>tf.keras.Sequential</code> model that can be used to classify the images of the MNIST dataset. Feel free to use the simplest possible CNN. Make sure your model has the correct <code>input_shape</code> and the correct number of output units.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a model.</span></span><br><span class="line">model =  tf.keras.Sequential([</span><br><span class="line">    </span><br><span class="line">        tf.keras.layers.Conv2D(input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>), filters=<span class="number">8</span>, kernel_size=<span class="number">3</span>,</span><br><span class="line">                               strides=<span class="number">2</span>, activation=<span class="string">'relu'</span>, name=<span class="string">'Conv1'</span>),</span><br><span class="line">        tf.keras.layers.Flatten(),</span><br><span class="line">        tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax, name=<span class="string">'Softmax'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================Conv1 (Conv2D)               (None, 13, 13, 8)         80        _________________________________________________________________flatten (Flatten)            (None, 1352)              0         _________________________________________________________________Softmax (Dense)              (None, 10)                13530     =================================================================Total params: 13,610Trainable params: 13,610Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="Train-the-Model"><a href="#Train-the-Model" class="headerlink" title="Train the Model"></a>Train the Model</h2><p>In the cell below configure your model for training using the <code>adam</code> optimizer, <code>sparse_categorical_crossentropy</code> as the loss, and <code>accuracy</code> for your metrics. Then train the model for the given number of epochs, using the <code>train_images</code> array.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Configure the model for training.</span></span><br><span class="line">model.compile(optimizer = <span class="string">"adam"</span>, loss = <span class="string">"sparse_categorical_crossentropy"</span>, metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Train the model.</span></span><br><span class="line">history = model.fit(train_images, train_labels,</span><br><span class="line">                    batch_size = <span class="number">16</span>,</span><br><span class="line">                    epochs = epochs,</span><br><span class="line">                    validation_data= [test_images, test_labels],</span><br><span class="line">                    verbose = <span class="number">1</span></span><br><span class="line">                   )</span><br></pre></td></tr></table></figure><pre><code>Train on 60000 samples, validate on 10000 samplesEpoch 1/560000/60000 [==============================] - 8s 127us/sample - loss: 0.3098 - accuracy: 0.9120 - val_loss: 0.1723 - val_accuracy: 0.9489Epoch 2/560000/60000 [==============================] - 7s 121us/sample - loss: 0.1511 - accuracy: 0.9569 - val_loss: 0.1145 - val_accuracy: 0.9667Epoch 3/560000/60000 [==============================] - 7s 122us/sample - loss: 0.1103 - accuracy: 0.9680 - val_loss: 0.0939 - val_accuracy: 0.9720Epoch 4/560000/60000 [==============================] - 7s 121us/sample - loss: 0.0901 - accuracy: 0.9737 - val_loss: 0.0895 - val_accuracy: 0.9739Epoch 5/560000/60000 [==============================] - 7s 121us/sample - loss: 0.0780 - accuracy: 0.9763 - val_loss: 0.0787 - val_accuracy: 0.9758</code></pre><h2 id="Evaluate-the-Model"><a href="#Evaluate-the-Model" class="headerlink" title="Evaluate the Model"></a>Evaluate the Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Evaluate the model on the test images.</span></span><br><span class="line">results_eval = model.evaluate(test_images, test_labels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> metric, value <span class="keyword">in</span> zip(model.metrics_names, results_eval):</span><br><span class="line">    print(metric + <span class="string">': &#123;:.3&#125;'</span>.format(value))</span><br></pre></td></tr></table></figure><pre><code>10000/10000 [==============================] - 0s 39us/sample - loss: 0.0787 - accuracy: 0.9758loss: 0.0787accuracy: 0.976</code></pre><h2 id="Save-the-Model"><a href="#Save-the-Model" class="headerlink" title="Save the Model"></a>Save the Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">MODEL_DIR = <span class="string">"digits_model"</span></span><br><span class="line"></span><br><span class="line">version = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">export_path = os.path.join(MODEL_DIR, str(version))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.isdir(export_path):</span><br><span class="line">    print(<span class="string">'\nAlready saved a model, cleaning up\n'</span>)</span><br><span class="line">    !rm -r &#123;export_path&#125;</span><br><span class="line"></span><br><span class="line">model.save(export_path, save_format=<span class="string">"tf"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nexport_path = &#123;&#125;'</span>.format(export_path))</span><br><span class="line">!ls -l &#123;export_path&#125;</span><br></pre></td></tr></table></figure><h2 id="Examine-Your-Saved-Model"><a href="#Examine-Your-Saved-Model" class="headerlink" title="Examine Your Saved Model"></a>Examine Your Saved Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!saved_model_cli show --dir &#123;export_path&#125; --all</span><br></pre></td></tr></table></figure><pre><code>MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs:signature_def[&#39;__saved_model_init_op&#39;]:  The given SavedModel SignatureDef contains the following input(s):  The given SavedModel SignatureDef contains the following output(s):    outputs[&#39;__saved_model_init_op&#39;] tensor_info:        dtype: DT_INVALID        shape: unknown_rank        name: NoOp  Method name is: signature_def[&#39;serving_default&#39;]:  The given SavedModel SignatureDef contains the following input(s):    inputs[&#39;Conv1_input&#39;] tensor_info:        dtype: DT_FLOAT        shape: (-1, 28, 28, 1)        name: serving_default_Conv1_input:0  The given SavedModel SignatureDef contains the following output(s):    outputs[&#39;Softmax&#39;] tensor_info:        dtype: DT_FLOAT        shape: (-1, 10)        name: StatefulPartitionedCall:0  Method name is: tensorflow/serving/predictWARNING:tensorflow:From /Users/ZRC/miniconda3/envs/tryit/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1809: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.Defined Functions:  Function Name: &#39;__call__&#39;    Option #1      Callable with:        Argument #1          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;inputs&#39;)        Argument #2          DType: bool          Value: False        Argument #3          DType: NoneType          Value: None    Option #2      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)        Argument #2          DType: bool          Value: False        Argument #3          DType: NoneType          Value: None    Option #3      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)        Argument #2          DType: bool          Value: True        Argument #3          DType: NoneType          Value: None    Option #4      Callable with:        Argument #1          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;inputs&#39;)        Argument #2          DType: bool          Value: True        Argument #3          DType: NoneType          Value: None  Function Name: &#39;_default_save_signature&#39;    Option #1      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)  Function Name: &#39;call_and_return_all_conditional_losses&#39;    Option #1      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)        Argument #2          DType: bool          Value: False        Argument #3          DType: NoneType          Value: None    Option #2      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)        Argument #2          DType: bool          Value: True        Argument #3          DType: NoneType          Value: None    Option #3      Callable with:        Argument #1          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;inputs&#39;)        Argument #2          DType: bool          Value: True        Argument #3          DType: NoneType          Value: None    Option #4      Callable with:        Argument #1          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;inputs&#39;)        Argument #2          DType: bool          Value: False        Argument #3          DType: NoneType          Value: None</code></pre><h2 id="Add-TensorFlow-Serving-Distribution-URI-as-a-Package-Source"><a href="#Add-TensorFlow-Serving-Distribution-URI-as-a-Package-Source" class="headerlink" title="Add TensorFlow Serving Distribution URI as a Package Source"></a>Add TensorFlow Serving Distribution URI as a Package Source</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the same as you would do from your command line, but without the [arch=amd64], and no sudo</span></span><br><span class="line"><span class="comment"># You would instead do:</span></span><br><span class="line"><span class="comment"># echo "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \</span></span><br><span class="line"><span class="comment"># curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -</span></span><br><span class="line"></span><br><span class="line">!echo <span class="string">"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal"</span> | tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \</span><br><span class="line">curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -</span><br><span class="line">!apt update</span><br></pre></td></tr></table></figure><pre><code>tee: /etc/apt/sources.list.d/tensorflow-serving.list: No such file or directorydeb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universalUnable to locate an executable at &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_73.jdk/Contents/Home/bin/apt&quot; (-1)</code></pre><h2 id="Install-TensorFlow-Serving"><a href="#Install-TensorFlow-Serving" class="headerlink" title="Install TensorFlow Serving"></a>Install TensorFlow Serving</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!apt-get install tensorflow-model-server</span><br></pre></td></tr></table></figure><pre><code>/bin/sh: apt-get: command not found</code></pre><h2 id="Run-the-TensorFlow-Model-Server"><a href="#Run-the-TensorFlow-Model-Server" class="headerlink" title="Run the TensorFlow Model Server"></a>Run the TensorFlow Model Server</h2><p>You will now launch the TensorFlow model server with a bash script. In the cell below use the following parameters when running the TensorFlow model server:</p><ul><li><code>rest_api_port</code>: Use port <code>8501</code> for your requests.</li></ul><ul><li><code>model_name</code>: Use <code>digits_model</code> as your model name. </li></ul><ul><li><code>model_base_path</code>: Use the environment variable <code>MODEL_DIR</code> defined below as the base path to the saved model.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">"MODEL_DIR"</span>] = MODEL_DIR</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MODEL_DIR</span><br></pre></td></tr></table></figure><pre><code>&#39;digits_model&#39;</code></pre><ul><li>-p 8501:8501 : Publishing the containers port 8501 (where TF Serving responds to REST API requests) to the hosts port 8501</li><li>mount type=bind,source=/tmp/resnet,target=/models/resnet : Mounting the hosts local directory (/tmp/resnet) on the container (/models/resnet) so TF Serving can read the model from inside the container.</li><li>-e MODEL_NAME=digits_model : Telling TensorFlow Serving to load the model named digits_model</li><li>-t tensorflow/serving : Running a Docker container based on the serving image tensorflow/serving</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%%bash</span><br><span class="line"></span><br><span class="line">nohup docker run -p 8501:8501 \</span><br><span class="line">  --mount <span class="built_in">type</span>=<span class="built_in">bind</span>,<span class="built_in">source</span>=<span class="string">"/Users/ZRC/Desktop/TensorFlow Data and Deployment/week13/Exercises/digits_model,target=/models/digits_model"</span> \</span><br><span class="line">  -e MODEL_NAME=digits_model -t tensorflow/serving  &amp;</span><br></pre></td></tr></table></figure><pre><code>docker: Error response from daemon: driver failed programming external connectivity on endpoint wonderful_ganguly (32049ac8fc320931031b817d6269004dcac5878b1e8c8addceb79fb1cd5dca24): Bind for 0.0.0.0:8501 failed: port is already allocated.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # EXERCISE: Fill in the missing code below.</span></span><br><span class="line"><span class="comment"># %%bash --bg </span></span><br><span class="line"><span class="comment"># nohup tensorflow_model_server \</span></span><br><span class="line"><span class="comment">#   --rest_api_port=8501 \</span></span><br><span class="line"><span class="comment">#   --model_name=digits_model \</span></span><br><span class="line"><span class="comment">#   --model_base_path="$&#123;MODEL_DIR&#125;" &gt;server.log 2&gt;&amp;1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!tail server.log</span><br></pre></td></tr></table></figure><pre><code>docker: Error response from daemon: driver failed programming external connectivity on endpoint determined_antonelli (6a4d67f3751bc7f9fed7821f6df430cbf368992e00c710738cb6f1d9f1e647d2): Bind for 0.0.0.0:8501 failed: port is already allocated.</code></pre><h2 id="Create-JSON-Object-with-Test-Images"><a href="#Create-JSON-Object-with-Test-Images" class="headerlink" title="Create JSON Object with Test Images"></a>Create JSON Object with Test Images</h2><p>In the cell below construct a JSON object and use the first three images of the testing set (<code>test_images</code>) as your data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create JSON Object</span></span><br><span class="line">data = data = json.dumps(&#123;<span class="string">"signature_name"</span>: <span class="string">"serving_default"</span>, <span class="string">"instances"</span>: test_images[<span class="number">0</span>:<span class="number">3</span>].tolist()&#125;)</span><br></pre></td></tr></table></figure><h2 id="Make-Inference-Request"><a href="#Make-Inference-Request" class="headerlink" title="Make Inference Request"></a>Make Inference Request</h2><p>In the cell below, send a predict request as a POST to the servers REST endpoint, and pass it your test data. You should ask the server to give you the latest version of your model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Fill in the code below</span></span><br><span class="line">headers = &#123;<span class="string">"content-type"</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line">json_response = requests.post(<span class="string">'http://localhost:8501/v1/models/digits_model:predict'</span>, data=data, headers=headers)</span><br><span class="line">    </span><br><span class="line">predictions = json.loads(json_response.text)[<span class="string">'predictions'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions</span><br></pre></td></tr></table></figure><pre><code>[[1.68959902e-09,  3.5654768e-10,  4.89267848e-07,  6.09665512e-05,  5.58686e-10,  2.27727126e-09,  8.49646383e-15,  0.999936819,  1.21679037e-07,  1.63355e-06], [1.37625943e-06,  6.47248962e-05,  0.99961108,  4.23558049e-06,  5.8764843e-10,  7.63888067e-07,  0.000306190021,  2.43645703e-15,  1.15736648e-05,  8.24755108e-12], [2.70507389e-05,  0.996317267,  0.00121238839,  1.1719947e-05,  0.00065574795,  2.86702857e-06,  0.000181563752,  0.000955980679,  0.000630841067,  4.60029833e-06]]</code></pre><h2 id="Plot-Predictions"><a href="#Plot-Predictions" class="headerlink" title="Plot Predictions"></a>Plot Predictions</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(test_images[i].reshape(<span class="number">28</span>,<span class="number">28</span>), cmap = plt.cm.binary)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    color = <span class="string">'green'</span> <span class="keyword">if</span> np.argmax(predictions[i]) == test_labels[i] <span class="keyword">else</span> <span class="string">'red'</span></span><br><span class="line">    plt.title(<span class="string">'Prediction: &#123;&#125;\nTrue Label: &#123;&#125;'</span>.format(np.argmax(predictions[i]), test_labels[i]), color=color)</span><br><span class="line">    </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_42_0.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Train-Your-Own-Model-and-Serve-It-With-TensorFlow-Serving&quot;&gt;&lt;a href=&quot;#Train-Your-Own-Model-and-Serve-It-With-TensorFlow-Serving&quot; clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Deployment" scheme="https://zhangruochi.com/categories/AI-Workflow/Deployment/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
</feed>
