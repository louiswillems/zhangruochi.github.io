<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-08-11T11:08:03.053Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Create a Siamese Network with Triplet Loss in Keras</title>
    <link href="https://zhangruochi.com/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/"/>
    <id>https://zhangruochi.com/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/</id>
    <published>2020-08-11T11:04:16.000Z</published>
    <updated>2020-08-11T11:08:03.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Create-a-Siamese-Network-with-Triplet-Loss-in-Keras"><a href="#Create-a-Siamese-Network-with-Triplet-Loss-in-Keras" class="headerlink" title="Create a Siamese Network with Triplet Loss in Keras"></a>Create a Siamese Network with Triplet Loss in Keras</h1><h1 id="Task-1-Understanding-the-Approach"><a href="#Task-1-Understanding-the-Approach" class="headerlink" title="Task 1: Understanding the Approach"></a>Task 1: Understanding the Approach</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pca_plotter <span class="keyword">import</span> PCAPlotter</span><br><span class="line"></span><br><span class="line">print(<span class="string">'TensorFlow version:'</span>, tf.__version__)</span><br></pre></td></tr></table></figure><pre><code>TensorFlow version: 2.1.0</code></pre><h2 id="Understanding-the-Approach"><a href="#Understanding-the-Approach" class="headerlink" title="Understanding the Approach"></a>Understanding the Approach</h2><p>This appraoch is taken from the popular <a href="https://arxiv.org/abs/1503.03832" target="_blank" rel="noopener">FaceNet</a> paper.</p><p>We have a CNN model called <code>EmbeddingModel</code>:</p><p><img src="assets/CNN.png" alt="CNN"></p><p>We use three images for each training example:</p><ol><li><code>person1_image1.jpg</code> (Anchor Example, represented below in green)</li><li><code>person1_image2.jpg</code> (Positive Example, in blue)</li><li><code>person2_image1.jpg</code> (Negative Example, in red).</li></ol><p><img src="assets/embeddings.png" alt="Embeddings"></p><h2 id="Siamese-Network"><a href="#Siamese-Network" class="headerlink" title="Siamese Network"></a>Siamese Network</h2><p>All the three images of an example pass through the model, and we get the three Embeddings: One for the Anchor Example, one for the Positive Example, and one for the Negative Example.</p><p><img src="assets/siamese.png" alt="Siamese Network"></p><p>The three instances of the <code>EmbeddingModel</code> shown above are not different instances. Itâ€™s the same, shared model instance - i.e. the parameters are shared, and are updated for all the three paths simultaneously.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCAPlotter</span><span class="params">(tf.keras.callbacks.Callback)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, plt, embedding_model, x_test, y_test)</span>:</span></span><br><span class="line">        super(PCAPlotter, self).__init__()</span><br><span class="line">        self.embedding_model = embedding_model</span><br><span class="line">        self.x_test = x_test</span><br><span class="line">        self.y_test = y_test</span><br><span class="line">        self.fig = plt.figure(figsize=(<span class="number">9</span>, <span class="number">4</span>))</span><br><span class="line">        self.ax1 = plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.ax2 = plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        plt.ion()</span><br><span class="line">        </span><br><span class="line">        self.losses = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(self, epoch=None, plot_loss=False)</span>:</span></span><br><span class="line">        x_test_embeddings = self.embedding_model.predict(self.x_test)</span><br><span class="line">        pca_out = PCA(n_components=<span class="number">2</span>).fit_transform(x_test_embeddings)</span><br><span class="line">        self.ax1.clear()</span><br><span class="line">        self.ax1.scatter(pca_out[:, <span class="number">0</span>], pca_out[:, <span class="number">1</span>], c=self.y_test, cmap=<span class="string">'seismic'</span>)</span><br><span class="line">        <span class="keyword">if</span> plot_loss:</span><br><span class="line">            self.ax2.clear()</span><br><span class="line">            self.ax2.plot(range(epoch), self.losses)</span><br><span class="line">            self.ax2.set_xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">            self.ax2.set_ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">        self.fig.canvas.draw()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span><span class="params">(self, logs=None)</span>:</span></span><br><span class="line">        self.losses = []</span><br><span class="line">        self.fig.show()</span><br><span class="line">        self.fig.canvas.draw()</span><br><span class="line">        self.plot()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        self.losses.append(logs.get(<span class="string">'loss'</span>))</span><br><span class="line">        self.plot(epoch+<span class="number">1</span>, plot_loss=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h1 id="Task-2-Importing-the-Data"><a href="#Task-2-Importing-the-Data" class="headerlink" title="Task 2: Importing the Data"></a>Task 2: Importing the Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure><pre><code>(60000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train = np.reshape(x_train, (x_train.shape[<span class="number">0</span>], <span class="number">784</span>))/<span class="number">255.</span></span><br><span class="line">x_test = np.reshape(x_test, (x_test.shape[<span class="number">0</span>], <span class="number">784</span>))/<span class="number">255.</span></span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure><pre><code>(60000, 784)</code></pre><h1 id="Task-3-Plotting-Examples"><a href="#Task-3-Plotting-Examples" class="headerlink" title="Task 3: Plotting Examples"></a>Task 3: Plotting Examples</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_triplets</span><span class="params">(examples)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">6</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span> + i)</span><br><span class="line">        plt.imshow(np.reshape(examples[i], (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">'binary'</span>)</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_triplets([x_train[<span class="number">0</span>], x_train[<span class="number">1</span>], x_train[<span class="number">2</span>]])</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,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" width="600"></p><h1 id="Task-4-A-Batch-of-Triplets"><a href="#Task-4-A-Batch-of-Triplets" class="headerlink" title="Task 4: A Batch of Triplets"></a>Task 4: A Batch of Triplets</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_batch</span><span class="params">(batch_size=<span class="number">256</span>)</span>:</span></span><br><span class="line">    x_anchors = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    x_positives = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    x_negatives = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, batch_size):</span><br><span class="line">        <span class="comment"># We need to find an anchor, a positive example and a negative example</span></span><br><span class="line">        random_index = random.randint(<span class="number">0</span>, x_train.shape[<span class="number">0</span>] - <span class="number">1</span>)</span><br><span class="line">        x_anchor = x_train[random_index]</span><br><span class="line">        y = y_train[random_index]</span><br><span class="line">        </span><br><span class="line">        indices_for_pos = np.squeeze(np.where(y_train == y))</span><br><span class="line">        indices_for_neg = np.squeeze(np.where(y_train != y))</span><br><span class="line">        </span><br><span class="line">        x_positive = x_train[indices_for_pos[random.randint(<span class="number">0</span>, len(indices_for_pos) - <span class="number">1</span>)]]</span><br><span class="line">        x_negative = x_train[indices_for_neg[random.randint(<span class="number">0</span>, len(indices_for_neg) - <span class="number">1</span>)]]</span><br><span class="line">        </span><br><span class="line">        x_anchors[i] = x_anchor</span><br><span class="line">        x_positives[i] = x_positive</span><br><span class="line">        x_negatives[i] = x_negative</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> [x_anchors, x_positives, x_negatives]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">examples = create_batch(<span class="number">1</span>)</span><br><span class="line">plot_triplets(examples)</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,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" width="600"></p><h1 id="Task-5-Embedding-Model"><a href="#Task-5-Embedding-Model" class="headerlink" title="Task 5: Embedding Model"></a>Task 5: Embedding Model</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">emb_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">embedding_model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">784</span>,)),</span><br><span class="line">    tf.keras.layers.Dense(emb_size, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">embedding_model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense (Dense)                (None, 64)                50240     _________________________________________________________________dense_1 (Dense)              (None, 64)                4160      =================================================================Total params: 54,400Trainable params: 54,400Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">example = np.expand_dims(x_train[<span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">example_emb = embedding_model.predict(example)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">print(example_emb)</span><br></pre></td></tr></table></figure><pre><code>[0.42349347 0.43482512 0.5846526  0.5047948  0.4264534  0.48105526 0.37568194 0.5898737  0.61923265 0.38126072 0.51810735 0.6918024 0.42151055 0.31393877 0.550636   0.4718757  0.72107047 0.5304595 0.60560906 0.54731256 0.47088197 0.57321566 0.38795182 0.3528969 0.5260858  0.5058847  0.60069776 0.5351782  0.45879558 0.49318898 0.52481294 0.48127335 0.41399142 0.53644794 0.596148   0.35952103 0.4660656  0.51290053 0.34802675 0.28829136 0.49941048 0.41946915 0.5193161  0.59598917 0.42652634 0.7554737  0.51301926 0.3393702 0.61319596 0.3912717  0.58737236 0.5881264  0.5892425  0.62002826 0.47996673 0.44889334 0.47385594 0.4038328  0.60131633 0.57539546 0.47411144 0.5514124  0.6192302  0.60763264]</code></pre><h1 id="Task-6-Siamese-Network"><a href="#Task-6-Siamese-Network" class="headerlink" title="Task 6: Siamese Network"></a>Task 6: Siamese Network</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input_anchor = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">input_positive = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">input_negative = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line"></span><br><span class="line">embedding_anchor = embedding_model(input_anchor)</span><br><span class="line">embedding_positive = embedding_model(input_positive)</span><br><span class="line">embedding_negative = embedding_model(input_negative)</span><br><span class="line"></span><br><span class="line">output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)</span><br><span class="line">net.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            [(None, 784)]        0                                            __________________________________________________________________________________________________input_2 (InputLayer)            [(None, 784)]        0                                            __________________________________________________________________________________________________input_3 (InputLayer)            [(None, 784)]        0                                            __________________________________________________________________________________________________sequential (Sequential)         (None, 64)           54400       input_1[0][0]                                                                                     input_2[0][0]                                                                                     input_3[0][0]                    __________________________________________________________________________________________________concatenate (Concatenate)       (None, 192)          0           sequential[1][0]                                                                                  sequential[2][0]                                                                                  sequential[3][0]                 ==================================================================================================Total params: 54,400Trainable params: 54,400Non-trainable params: 0__________________________________________________________________________________________________</code></pre><h1 id="Task-7-Triplet-Loss"><a href="#Task-7-Triplet-Loss" class="headerlink" title="Task 7: Triplet Loss"></a>Task 7: Triplet Loss</h1><p>A loss function that tries to pull the Embeddings of Anchor and Positive Examples closer, and tries to push the Embeddings of Anchor and Negative Examples away from each other.</p><p>Root mean square difference between Anchor and Positive examples in a batch of N images is:<br>$<br>\begin{equation}<br>d_p = \sqrt{\frac{\sum_{i=0}^{N-1}(f(a_i) - f(p_i))^2}{N}}<br>\end{equation}<br>$</p><p>Root mean square difference between Anchor and Negative examples in a batch of N images is:<br>$<br>\begin{equation}<br>d_n = \sqrt{\frac{\sum_{i=0}^{N-1}(f(a_i) - f(n_i))^2}{N}}<br>\end{equation}<br>$</p><p>For each example, we want:<br>$<br>\begin{equation}<br>d_p \leq d_n<br>\end{equation}<br>$</p><p>Therefore,<br>$<br>\begin{equation}<br>d_p - d_n \leq 0<br>\end{equation}<br>$</p><p>This condition is quite easily satisfied during the training.</p><p>We will make it non-trivial by adding a margin (alpha):<br>$<br>\begin{equation}<br>d_p - d_n + \alpha \leq 0<br>\end{equation}<br>$</p><p>Given the condition above, the Triplet Loss L is defined as:<br>$<br>\begin{equation}<br>L = max(d_p - d_n + \alpha, 0)<br>\end{equation}<br>$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:<span class="number">2</span>*emb_size], y_pred[:,<span class="number">2</span>*emb_size:]</span><br><span class="line">    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=<span class="number">1</span>)</span><br><span class="line">    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.maximum(positive_dist - negative_dist + alpha, <span class="number">0.</span>)</span><br></pre></td></tr></table></figure><h1 id="Task-8-Data-Generator"><a href="#Task-8-Data-Generator" class="headerlink" title="Task 8: Data Generator"></a>Task 8: Data Generator</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span><span class="params">(batch_size=<span class="number">256</span>)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        x = create_batch(batch_size)</span><br><span class="line">        y = np.zeros((batch_size, <span class="number">3</span>*emb_size))</span><br><span class="line">        <span class="keyword">yield</span> x, y</span><br></pre></td></tr></table></figure><h1 id="Task-9-Model-Training"><a href="#Task-9-Model-Training" class="headerlink" title="Task 9: Model Training"></a>Task 9: Model Training</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">2048</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">steps_per_epoch = int(x_train.shape[<span class="number">0</span>]/batch_size)</span><br><span class="line"></span><br><span class="line">net.compile(loss=triplet_loss, optimizer=<span class="string">'adam'</span>)</span><br><span class="line"></span><br><span class="line">_ = net.fit(</span><br><span class="line">    data_generator(batch_size),</span><br><span class="line">    steps_per_epoch=steps_per_epoch,</span><br><span class="line">    epochs=epochs, verbose=<span class="keyword">False</span>,</span><br><span class="line">    callbacks=[</span><br><span class="line">        PCAPlotter(</span><br><span class="line">            plt, embedding_model,</span><br><span class="line">            x_test[:<span class="number">1000</span>], y_test[:<span class="number">1000</span>]</span><br><span class="line">        )]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABwgAAAMgCAYAAAA3IzbOAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAHCKADAAQAAAABAAADIAAAAAADj2ddAABAAElEQVR4AezdB3yV1f3H8S8hk4QQIIyQgDJlhCWCWq2zatXSOop11apYW6y19S9qa1vrv7X1X6tW616I27qKC3FWwVUQWWEIMrOYCdk7+Z9zw73cm31zb8a9z+e8Xg/3mec55/08Eszv/s7pUWeKKAgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggg4AiBCEf0kk4igAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggIBLgAAhLwICCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIEPIOIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAgAQKEDnrYdBUBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAoS8AwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4SIAAoYMeNl1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAAh7wACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACDhIgQOigh01XEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECBAyDuAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgIMECBA66GHTVQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQiIUAAga4RKC8v15o1a1w3HzBggCIj+c+xa54Ed0UAAQQQQAABBIIrUF1drT179rgqnThxomJjY4N7A2rrtgL8G7/bPhoahgACCCCAAAIIBCTAv/ED4uPibipARKKbPhiaFf4CNjg4Y8aM8O8oPUQAAQQQQAABBBwssHTpUk2fPt3BAs7qOv/Gd9bzprcIIIAAAggg4EwB/o3vzOcejr1miNFwfKr0CQEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIFmBMggbAaG3Qh0tIAdVtRd7LdOUlJS3Jt8IoAAAggggAACCISwQG5urmekCO9/84Vwl2h6GwW8nzf/xm8jGqchgAACCCCAAAIhIMC/8UPgIdFEvwUIEPpNxgUIBEfAe85BGxxMS0sLTsXUggACCCCAAAIIINBtBLz/zddtGkVDOkzA+3nzb/wOY6ZiBBBAAAEEEECgSwW8/83XpQ3h5ggEKMAQowECcjkCCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACoSRAgDCUnhZtRQABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQCBAAQKEAQJyOQIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAKhJECAMJSeFm1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAIEABAoQBAnI5AggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAqEkQIAwlJ4WbUUAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAgQAEChAECcjkCCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACoSRAgDCUnhZtRQABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQCBAAQKEAQJyOQIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAKhJECAMJSeFm1FAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAIEABAoQBAnI5AggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAqEkQIAwlJ4WbUUAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAgQAEChAECcjkCCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACoSRAgDCUnhZtRQABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQCBAAQKEAQJyOQIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAKhJECAMJSeFm1FAAEEEEAAAQQQQAABBLpAYMeOHZo7d67GjRun+Ph49evXTzNmzNAdd9yh0tLSgFpUW1urdevWaf78+brqqqs0ffp0xcTEqEePHq7lo48+8rv+qqoqV31nnnmmhg0b5qovOTlZEydO1BVXXKGXXnrJ7zq5AAEEEEAAAQQQQAABBBAIJ4HIcOoMfUEAAQQQQACB7ilQWSnt3i0VFEjV1fVtjDT/CunTRxo4UDK/G3YdLymRampkfiEsRUfL/AK6frHbFAQQQACBrhF46623dNFFF5m/w81f4geKDQouW7bMtTz22GNauHChRowY4T7s1+fTTz+tSy+91K9rWjp59erVrvZmZGT4nLZv3z7Zxe5/+eWXNWvWLJ/jbCCAAAIIIIAAAggggAACThIgQOikp01fEUAAAQQQ6GQBG/DLzpb5haxUV9f45vZ3zSYppdmyZ099oHDQICklRbJBRQoCCCCAQOcJrFq1Suedd54rSzAhIUG//e1vdeKJJ6qsrEwvvPCCHn30UX399deymXo2YGjP8bfUef2AiIqKUnp6uvkySbXWrFnjb1WywUHbvry8PPNFk2hddtllOv3005WWlqb9+/dr+/bt+uCDD7RkyRK/6+YCBBBAAAEEEEAAAQQQQCCcBPg1Wzg9TfqCAAIIIIBANxLYu1fatKnpwKA/zbTZh5mZ9RmG48dLcXH+XM25CCCAAAKBCPz61792BQcjzTc03n33XR199NGe6k466SSNHj1aN9xwgzZs2KC77rpLN998s+d4W1fGm7/c77nnHteQpVOmTFFsbKxuueUWvwOE5eXlrqxAGxxMMd8qse21wcaG5fLLL1el/eFCQQABBBBAAAEEEEAAAQQcLMAchA5++HQdAQQQQACBjhKwGYMbNwYeHPRuX0WFtHKllJMT3Hq978E6AggggMBBAZsR6J7/b/bs2T7BQfdZ1113nWteQrt99913y87952+xcxlec801Ouqoo1zBQX+vd59v50PcaH/4mPLcc881GRx0n2uzCykIIIAAAggggAACCCCAgJMFCBA6+enTdwQQQAABBDpAwIw658oc7ICqXcOUbtsmffNN/byFHXEP6kQAAQQQqBdYsGCBh8IO1dlUiYiI0CWXXOI6lJ+f7wkoNnVuR+6rMRPYPvTQQ65bnHDCCbILBQEEEEAAAQQQQAABBBBAoHkBAoTN23AEAQQQQAABBNohYDP8amvbcaEfl9i5CTdvJpPQDzJORQABBPwWcM/TFx8fr2nTpjV7/fHHH+859sknn3jWO3Pls88+M3PemklvTZk1a5bn1qWlpeZLJd8oNzfX/Gzq4B9OnruyggACCCCAAAIIIIAAAgh0fwEChN3/GdFCBBBAAAEEQkagulqywbvOKPY+5ve9FAQQQACBDhJYv369q+ZRo0bJzkHYXBk7dqznkPsaz45OWvniiy88d7LzJC5dulSnnXaaevfu7ZoncciQIRowYICuuOIKbd++3XNuuKzsK67Q4o17tGFnYbh0iX4ggAACCCCAAAIIIIBABws0/395HXxjqkcAAQQQQACB8BOwQbvOTNCw2YqDB0s9ekjFxVJlZf39e/aU4uLql/BTpkcIIIBAxwuUl5dr7969rhulpaW1eMO+ffvKZhmWlJQoMzOzxXM76uC6des8VdtgoZ3TsNp+a8Wr5OXl6fHHH9crr7yi1157Tccdd5zX0batZmVltXiizVTszHL/f77RM19sV25Bueu2lx8zXDfPHN+ZTeBeCCCAAAIIIIAAAgggEKICBAhD9MHRbAQQQAABBLqjgA3SdWaxAUE7H2FRkVRR0fjOCQn1AcTkZMlMk9Vppa5OMr9bdwUszbRYsgHL2FgpJqbTmsCNEEAAgYAEiuxfrAdKgv3LtJXiDhAWd/YPggPtssE/d7n22mtl5yS84YYb9LOf/Uw2wGkDl3aOwjvvvFP79+/XOeeco1WrVik1NdV9WZs+hw4d2qbzOuuk6po6T3DQ3jMjp6Czbs19EEAAAQQQQAABBBBAIMQFCBCG+AOk+QgggAACCHQngQbJGp3StAMJLk3ey/6e2gYQ7ZKSIvNLYikqqslTg7LTBgNte3bulMmkaVxlUlJ9wNIk27iyHhufwR4EEECgewjYDEJ3iY6Odq82+xlz4BsQZWVlzZ7TkQds9qK7VJhvjPz973/X3Llz3bs0cuRI175+/frppptu0r59+3Tbbbfpvvvu85wTiivpqYk+zV6XU2gy+evMl2JMaj0FAQQQQAABBBBAAAEEEGhBgABhCzgcQgABBBBAAIHwEbCjvtnFBunGjJGZTyu4fbOBwS1bZIa0a75ek7RiMlfqswltG9qQlNN8ZRxBAAEEOlAg1qY9HyiVNl27lWKDcrbE2fGdu6B4t9dmDNoswqbK9ddfr3vvvdf8PMjVCy+84FrvYcepbmNpbQhVW++MGTPaWFvgp6Wn9vGppLiiWtvzSjU8Od5nPxsIIIAAAggggAACCCCAQEOBIP9qrGH1bCOAAAIIIICAUwRs0ohXwkm37bYN0C1bVp9ROGSI1FpiTJkZtu6bRYtUvGuXqk0HY/v00cCJEzXs2GNNFmD9L5Vt4HHr1rZ32TplZEjjxkmmOgoCCCDQ7QR69+7taVNbhg11Z/C1ZThST8VBXPFu7ymnnGKGdjZjOzdRIs23Q0466SQ9++yzrizCreYv7xEjRjRxZtO7WpuPsemrOm7vwN4xSk6I0d7ig+NsrzXDjBIg7DhzakYAAQQQQAABBBBAIFwECBCGy5OkHwgggAACCHSRgJ1vzwbH7LCaoVJsm3Ny6ts8dmx9VmHDtucsX65lDzygjOeecwUGGx5PNtG9I+bMUdqZl2j7Tv+jfLW10oYNUnq6FE+iR0NethFAoIsFbEZespnAda9Jj87KymqxNfn5+WZY5fohPrtqjj7v+7YWxPM+d/fu3X4FCFuE6IKD9osqE4Yk6uONezx3z8gu1PcmmW/AUBBAAAEEEEAAAQQQQACBFgQiWjjGIQQQQAABBBBAoEUBd5ArlIKD3h2y7V+/XiaL5ODemqoqvXHllXr0iCO0ct68JoOD9uy95sJF11yjJw8fqfyViw9W4MeanbPQDktKQQABBLqjwDib5mzKN2Yi1+oWxk/eYL/tcKC4r3Fvd9bnhAkTPLeqsX+5tlC8j9uMwlAvDechtBmEFAQQQAABBBBAAAEEEECgNYHQ/7+h1nrIcQQQQAABBBDoMAEb3DKJIyFdbDbhpk1STIyZOyumSi+ee642vvFGk32KShqgQSfNUlzKoYrs1VvVJYUqy92m1X88XxN+87iSjz69yeta2llUJJN5QxZhS0YcQwCBrhE41gylvGTJEld24HKTVX3kkUc22ZCPP/7Ys/+YY47xrHfmynHHHee53ebNmz3rTa14H09NTW3qlJDalz7EN4s9I7tAdeaHm3sY7JDqDI1FAAEEEEAAAQQQQACBThMgQNhp1NwIAQQQQACB8BIoMAkKZmS2sCg2k3DHDpPN989fNhkc7DPhSA0992oNOnGWIqJNJLFBGfPLu7T741dUsmOj4oeNaXC09U2bgTlyZOvncQYCCCDQmQJnnXWWbrvtNtctn3jiiSYDhLXmL9CnnnrKdU5SUpJOPPHEzmyi517Dhw/X1KlTtWLFCr3zzjsqLS1Vr169PMfdK0XmWxnvvfeea3Ok+Ys3JSXFfShkP9NTfQOE+aVVyi0o15CkuJDtEw1HAAEEEEAAAQQQQACBjhdgiNGON+YOCCCAAAIIhKVAqA4r2tzDyPxytZY//LDvYTO30+hf3K4Zj3yhlNMubjI4aC/oGROrlFMvaldw0F5vpvgy2R52jYIAAgh0H4EZM2bo29/+tqtBjz/+uD7//PNGjbvzzjvNUM1mrGZTfvWrXykqKsrnnI8++siVyWaz2S699FKfY8He+M1vfuOqcv/+/bruuuuarP7aa6+VDRLa8vOf/7zJc0JtZ1rfOCXG+n7312YRUhBAAAEEEEAAAQQQQACBlgR8/y+ipTM5hgACCCCAAAIIHBCorJTy8sKLI+vfDzbq0PgbH1HqzCsa7Q/2Djtdll3CYCqsYNNQHwIIdLHAPffcIztsaFlZmU499VTddNNNrixBu/3CCy/okUcecbVwzJgxzQbl2tKF+fPn+5y2cuVKz/aiRYu0bds2z/aoUaNkhz9tWM477zw9+eSTWrhwoR566CFlZmbqSjOn7NChQ02W+A7XPluXLTbb8Oqrr25YRUhu2+DrBDPM6OdbDk6om5FTqFMnDA7J/tBoBBBAAAEEEEAAAQQQ6BwBAoSd48xdEEAAAQQQCCsBGxwMp4w3O5dg7jtP+zyjQy++sVOCg+6b2mFOKQgggEB3E7CBtH/961+6+OKLVVhY6AoQNmyjDQ6+9dZb6t27d8NDbd6+7LLLmj33b3/7m8+xn/zkJ00GCO1Jtq3nmrlk3333XVebbLsalunTp+v1119XbGxsw0Mhu52emugTIFyXQwZhyD5MGo4AAggggAACCCCAQCcJMMRoJ0FzGwQQQAABBMJJwGYQhlPJ+/ID1ZSVeLoUGZ+oEZf+wbPdGSsR/KusM5i5BwIItENg5syZWr16tezwnDYYaOf2s/MNHnHEEbLBOzvvn83q6w4lISHBNQfh888/r+9+97saPHiwa9jTAQMG6JRTTtG8efNcQ6Xa/eFUbAahd8nILvTeZB0BBBBAAAEEEEAAAQQQaCRABmEjEnYggAACCCCAQGsC4ZbtVpG3y6fLKadfop5x8T77OnrDjH6n4cNl5urq6DtRPwIIIOC/wCGHHKK77rrLtfhz9QknnGAyzlufZLUt5/hz3/PPP192cUqxGYTeZWdhufYUVWhA7xjv3awjgAACCCCAAAIIIIAAAh4BvqvuoWAFAQQQQAABBNoq0LNnW88MjfPqqip8Gpp29lU+252xsXOnZBcKAggggAAC/goMT05QXJTvD+e1DDPqLyPnI4AAAggggAACCCDgKAEChI563HQWAQQQQACB4AjExQWnnu5SS2RCkqcpsYMPUcKh4zzbnbmyfbtUU9OZd+ReCCCAAALhINAzoofGD/HNIlybwzCj4fBs6QMCCCCAAAIIIIAAAh0lwBCjHSVLvd1aoLCwUAsXLtSyZcv05ZdfKjs7W3v27FFZWZlrPpXx48frjDPO0OzZs9W/f/9u3RcahwACCHSFQL9+UqT5V0R1dVfcPfj37D1qsqfS6KRkz3pnr9ihW82PIzNnVmffmfshgAACCIS6wAQTIFy+Pd/TDTIIPRSsIIAAAggggAACCCCAQBMCBAibQGFX+AssXbpUF1xwQZMdtYHCjz/+2LX8/e9/1zPPPKPTTjutyXPZiQACCDhVIMKMQTBokMwXLMJDoPeYKUpKn6H9GUulCN8h2jq7h3aYUWvLXISdLc/9EEAAgdAWSB/Sx6cDGdlkEPqAsIEAAggggAACCCCAAAI+AgQIfTjYcJLA0KFDdeKJJ2ratGmy6ykpKao1qRtZWVl6+eWX9eqrr2rv3r36/ve/78o0nDRpkpN46CsCCCDQqoANYuXkSHV1rZ4aEidMvfIq/eeapaouOph90RUNLy2VKsyUiLGxXXF37okAAgggEKoCE1J9hxjdkVeqgrIq9YmLCtUu0W4EEEAAAQQQQAABBBDoQAEChB2IS9XdV8AGBnfs2NFsA8877zwtWLBAZ599tiorK/W///u/euWVV5o9nwMIIICAEwVsAGvUKGnTptDvvR3SM232eVp+++9VlLNVlfm7Fd13YJd1rKqKAGGX4XNjBBBAIEQFRg/sraiePVRVc/CbO+vMPIRHj2TKhBB9pDQbAQQQQAABBBBAAIEOFTADhFEQcJ5Az56tDx931llnaezYsS6cxYsXOw+JHiOAAAJtEIiLk9rwV2obauq6U2wm5PDhUnSvOJ3/2muKjI1R9huPdV2DzJ3DJSuzSxG5OQIIIOAwgejICB02uLdPr5mH0IeDDQQQQAABBBBAAAEEEPASIEDohcEqAg0F4uPjXbvKy8sbHmIbAQQQcLxAUZGUkSHV1IQ2RVnZwYBcyuGH6+JFi7TnPy+ozgw73VUlkjEeuoqe+yKAAAIhLdB4HsKCkO4PjUcAAQQQQAABBBBAAIGOE+DXTx1nS80hLrB+/XqtXLnS1Qt3JmGId4nmI4AAAkETsN+bMH9Nmrlbg1Zll1VUWCgtXVp/+wjz1anohGN1ymOvKmf9EsVPOL7T22UzMpl/sNPZuSECCCAQFgITUvtIyzI9fVlrhhilIIAAAggggAACCCCAAAJNCRAgbEqFfY4VKC0tVXZ2tt544w3dfvvtJiumPi3mV7/6lWNN6DgCCCDQlEBWllRd3dSR0NznDnTaT1e/eo4ywcFRiqirVG2P6E7t1EAz9aENVFIQQAABBBDwV2DCkESfSzbvKVZpZbV6RfO//j4wbCCAAAIIIIAAAggggID4vwReAscLzJ8/X5dddlmzDnPnztVFF13U7PHmDmTZ3563UHJzc1s4yiEEEECg+wrYANrevd23fcFsWUcHB+0wpnu/WKRdH76oyn25qqkoU7+URG1KH6fDZ89W8oG5cIPZJ+pCAAEEEAhfgXGDExXRw2T419X30X6uzy3StEP6hm+n6RkCCCCAAAIIIIAAAgi0S4AAYbvYuMgJAlOmTNFDDz2kI488sl3dHTp0aLuu4yIEEECguwvs3h0eQ4t2pXNNeakyX7lfWQseVFnOVp+m7F8lbVn0lj6/4w4NP+kkHXXttRrzve/5nMMGAggggAACTQnERffUqIEJ2rir2HN4bU4BAUKPBisIIIAAAggggAACCCDgFmAAK7cEn44VOOuss7RmzRrXstRMQvX888/r7LPPds0/aDMH33zzTcfa0HEEEECgKYH8/Kb2sq+tAhX7dmrZVcdp0wM3NAoONqxj64cf6vmZM/XOddfJZhtSEEAAAQQQaE0gfYiZh9CrrM1mHkIvDlYRQAABBBBAAAEEEEDggAAZhLwKjhdISkqSXdxl+vTpOv/88/X000/rJz/5iX7wgx/o8ccf16WXXuo+pU2fmZmZLZ5nhxidMWNGi+dwEAEEEOgqgaoqad8+qbxcZj7W+jnxYmKk/v0le4zSPoHKgn368uoTVLrja78q+OKuu1RdVqYz7r9fPXqYseMoCCCAAAIINCMw3sxD+OqKbM/RDJNBSEEAAQQQQAABBBBAAAEEGgoQIGwowjYCBwR+/OMfu7IHX3zxRV199dWuQGHfvm2fuyMtLQ1LBBBAIOQEis2IZDt31s8x2FTC2rZt9cHCkOtYN2hwXV2d1vzhPL+Dg+6mf/nggxo4caKmz5mjGhOl/ebtt7Vv40aVFxQoqlcvJaamaozJNozz42eVu+5gflaWlGjXqlUqtRFmU3qZqPKgyZMVHR8fzNtQFwIIIIBAMwLpqb4ZhBt3FamyulbRkQwg1AwZuxFAAAEEEEAAAQQQcKQAAUJHPnY63VYBmz1oA4Ql5pedb5tfxF544YVtvZTzEEAAgZASMLErZZtkgx07Wm92U4HD1q/ijII1nylv+YcBQSz+859VlJOjFSazvdhkojcskXFxmmh+Vk2/6iqlHH54w8Mdur1n/XrZIOaqJ59URaHvcHYxiYmabLLyjzDBzQHjxnVoO6gcAQQQcLqAzSD0LlU1dWZOwiI1DBx6n8M6AggggAACCCCAAAIIOE+ArxA675nTYz8EBgwY4Dl7+/btnnVWEEAAgXATsIHBtgQHw63fndWf6Gip+D8PBHw7GxRccuutTQYHbeV2GFIbPHxk2jS98z//o1o7PqwfxQb28jZv1u61a1WYldWm66tKS/WqmbP3gfHjtfTeexsFB+3tbb32mD3nFRPAtNdQEEAAAQQ6RiAxNkqH9O/lU/lahhn18WADAQQQQAABBBBAAAEEJDIIeQsQaEEg26bTHCgJCQnuVT4RQACBsBLYtas+ezCsOtVKZ+xwnwVrv1DWgodVtPErVRfvV4+eUYruO1ADjpmpITNnK6bfoFZqadthO2/jwPg9evvfL7XtgiCd9cU//qES83DPNnPq9oiI0K7Vq7V3wwZXsM5mGvYeMkTDjj3WNafhxrfe0jIzv+GW997zuXucafzU2bN1xM9/rr7Dh/scsxsVRUV65rTTlPX5542ONbcj4/nntfndd3WYydJPmTrVlfEY169fc6ezHwEEEECgHQLpQ/po+76DX8bIyC7Uj6a3oyIuQQABBBBAAAEEEEAAgbAVIEAYto+WjgVD4KWXDv4yd6KZ94mCAAIIhJuAHS7UaZmDO99/Qdue+ZuKNq1s9DjLcra4Aoeb592iQSfO0ojZtyh+6OhG57VlxyATXxw6VLLZgxvfWqpaM29gZ5c1zz2ncpO9V2Imlsz58stGt4818xXaLMPKBkOCuk8sM/MIfnb77frs73/XuHPO0cxHHpE7mFdbXa2XZs3yKzjoXe/KefNkn8B711+v9Asu0JHXXKPBU6a4T+ETAQQQQCAAgQmpiXprzcGhqDPIIAxAk0sRQAABBBBAAAEEEAhPAYYYDc/nSq9aEZg/f77Ky8tbPOsfJvNi4cKFrnMOPfRQHWuyLCgIIIBAuAnk5UldELfqEsY6Ew39+p5rteaPFzQZHPRuVF11lXa+95yW/nSG8lZ87H2ozevJyfXBQXtBeX5+m68L9omb3nyzyeCgu13NBQd92mEyLte/8ormHXOMCjIzXYdWP/OMNr/zjs9p7dmoNj+PVz7xhB42cyYuve++9lTBNQgggAACDQRsBqF3WZ9bqJpaM+EwBQEEEEAAAQQQQAABBBA4IEAGIa+CIwVuueUWXXfddTr33HNdgb+RI0fKDiFaZIZKW7NmjZ599ll9+umnLptok/rx6KOPKjKS/1wc+bLQaQTCXMAkljmmbH/+TsUOTNOkv76iyF69VVNarLKd25S76GkzzOiKJh2qi/ZrxXWn64h7/6M+E45UXU21CS6uUv6qT1RbXqLIxH7qM36GEg87vNH1PXp47fLZ8NofYqt2iNJnTz9dl3/yiWtI0qA23wQh3/7lL13zEx5zww1BrZrKEEAAAacJTBiS6NPl8qpabdlTrNGDevvsZwMBBBBAAAEEEEAAAQScK0DEw7nP3vE9zzNpMzbwZ5fmSlpamuaZIdC+853vNHcK+xFAAIGQFTAjS6qZkSVDtk/NNbwyf48Ovej6Jg8f8qNrtd/MR5j58n2urEGZQJV3qa0o08obv69vvbBRUQl9lDh2miJi4lyBw/JdO1ynJo49QmnnXKXB3zlfPc0xW6KiXB+uP3rZiQjDpOxZu1avXXFFs1mJgXbz/RtvVP8xYzT2rLMCrYrrEUAAAccK9E+IUUqfWOUWHBw1xQ4zSoDQsa8EHUcAAQQQQAABBBBAoJEAQ4w2ImGHEwQ++OADPfTQQ/rRj36kSZMmaZCZKMpmCNosQptNaDMLnzDDnX399dc65ZRTnEBCHxFAwEECNjC4a5e0erVzOh3dd0CLnU2acJQm/vEZTfrzi4qIjm10bmX+buW+/ZRnf8Lw8TrigSWKSR7i2le44Uut++vl+uyi8Sreslaxpgq7uEva0Ucrqlcv92bIf369YEGH9uHD3/3OxGl9A7UdekMqRwABBMJQYEKDYUYzsgvDsJd0CQEEEEAAAQQQQAABBNorQAZhe+W4LqQFbBDQLj/72c9Cuh80HgEEEPBHwMZb7PRxubmSDRJSGgsMOvGH6mmGH115/fdcw4l6n5H17wc09IdXq8eB4ULjBg/TlL+9rqVX2qFH60HLc7dp2Zxj9L2XPjDnTfNcHtunjyZedJG+aiFr3XNyCKy4+9tRTd2zbp22L16sQ48/vqNuQb0IIIBA2AvYYUbfX2++EXSgrDUZhBQEEEAAAQQQQAABBBBAwC1ABqFbgk8EEEAAAQTCWMDGr8z0ccrKIjjY2mNOPvI0jfnlnY1OK9m+QfkrF/vst8ONJh99ps++6uICvXfpmSq02F7liDlzvLZYbU3gywceaO0UjiOAAAIItCCQntrH5+hak0FYW0t2tg8KGwgggAACCCCAAAIIOFiADEIHP3y6jgACCCDgDAGbObhpk5Sf74z+BqOXaWfP0dan/qrKvIOZF7beoq+Xq99U36y28TfNc51nhyatLilQyda1yn79US2+9VZ9zwxn7S4pU6dq/A9/qHUvv+ze5ajPXkNHq9/0UxTdJ1k9evZUVVG+CjK+UIGZ/7GpsuX995vazT4EEEAAgTYKpKcm+pxZVFGtzPxSHdI/3mc/GwgggAACCCCAAAIIIOBMAQKEznzu9BoBBBBAwEECdr7BvDwHdTgIXY2IjFLq92a7goTe1VUVNo6yRvfpb4Je/T2nJY6ZqpTTLlbJtvXK3lamIYfEmeFG6w//YP58FezYoeylSz3nt3Vl8HcuUPIxZ8pmMuYsnK+K3b4Zig3rsUG4Acd+32Q4nqGopAEmKBepahuUW79MuYueMuv7G14S/O2ICA04ZqaGnnOV+s84tcn6izatVOarDyj33WdVW17qOafMRLRrTeprhOkHBQEEEEDAf4HBibHqHx+tfSWVnovX5hQSIPRosIIAAggggAACCCCAgLMFCBA6+/nTewQQQACBMBew2YN2zkGK/wJpZ/1cW5/5P5nx2DwXxwxI9ay3thJ/6Dhtz5GWv/iqdsz7vWKTEnWImVNvppmH8L0bbtDmd95prQrP8dFX36FDzv8fz/yHIy+/RXs+eV07XrpH+Ss+9pxnVyJ799UwM1di6vevVOzANJ9jdsMGL0f//DbtfO85bf/XP0zG47pG5wRjR5QJmk6+bYH6Tj62xep6j56i8Tc+ouGX3KQV15sAqLs99uW1CwUBBBBAoF0Cds7c8WYewiWb9nquz8gu0BkTUzzbrCCAAAIIIIAAAggggIBzBZiD0LnPnp4jgAACCDhAoLBQKivr2I4mJUkmUSzsSuygoYpLGe7pV1zqCA054yee7bauJB97jlLO/50ra/Cz22/XQ5MnK2f5cqUccYQGTpzYbDU9oqKV8t0fa8Zjy3ToBdd5goN1JmCZt/xD7f74VdXVVCu632BPHfGHjNVRT3ylkVf8qcngoPvEnrG9lDrzCh017ysNOvlH7t1B+7QZi9Mf/KTV4KD3DeNSDjXXfCobMLQlpk8fRUTyXTZvI9YRQAABfwUazkOYYTIIKQgggAACCCCAAAIIIICAFeC3LrwHCCCAAAIIhLGAHV60I0uvXtKYMdL27VJH36sj+9Fc3VGJ/VSWvdk1Z96Uv72unjFxzZ3a4v6UUy9S4Ybl2mEy9mwp27vXtdj1/gaw76hR6pU8QPt2lUuxfRQ/bKwr0y+67wB7iqvUVlcp698PKvOV+1SaaSaVbFDsHH9HPLBE0UnJDY40vxkRHaNJf3pBa8zwozvNEJ/BKHYo0yl/e002WOlvieqdpKl3LNR/Z0/TsBlT/b2c8xFAAAEEGgikD+njs2etySCsM9nZNruQggACCCCAAAIIIIAAAs4WIEDo7OdP7xFAAAEEwlygI7MHExKkcePMt43MvyYGmyS2cAwQul+P5G99TwnDJ7g32/V5iMkCzHz5XlfWn3cF+zZulF2+Ne9TDTrsW96HPOvVJYVaddO5yvvyfc8+75UIkxFoA2v+BAe9r59w0zwTdNyoQjM/YaBl8CkXKCn96HZXE5OcokMv/o0OP3lEu+vgQgQQQACBeoH01EQfCjsf4a7CCg3uE+uznw0EEEAAAQQQQAABBBBwnkAYDgjmvIdIjxFAAAEEEGhOoKamuSOB77fBx+xsqdwkvcXHSyPCMJ5TVVA/b9PQc64KGCzWzF844ffzFd13YKO6eo+ZqvhmgoM1FWVaMfeMZoODtrIhZijSXmmjGtXb1h0RZjjT4T/5XVtPb/G8tCBYDTnzUg0/9XSf+9RUVuobM2/jV48/rv/+859aOX++Mj/7zJUJ43MiGwgggAACHoGhfXupd4zv94LtPIQUBBBAAAEEEEAAAQQQQMD3/xTwQAABBBBAAIGwEujIuQFt8DEnp35JNqNajhwp2X12uNFwKGW522QXO/dg/xmnBqVLdqjRQSfO0tYnb9WWJ/7sqTPt7Dme9YYrG+76pfav/rThbp/ttLMDD2AOMFmSsYOGqXzXDp+6/dmwgc6kCUf5c0mT50b2SlRevjRokFSQmanljzyirx59VCVNpKn2jI3VsGOP1Wl33qlBkyY1WR87EUAAAacKRET00Pghifrv1jwPQUZOgb4z3vwFS0EAAQQQQAABBBBAAAFHC5BB6OjHT+cRQAABBMJdIDq6c3poptRTRkZ9QGesmXourn1T9XVOY9t4l6wFD8mkp2mgCegFq1SXFeu/V0xX7rvPuYJxtt6ImF4afPL5Td6ifHeWct+e3+Qx984+E7+l3qMCD4z16NlTqT+40l1tuz4HHDOzXdc1dVFeXp0+M0G/e4YP15Jbb20yOGivqzEprFvff18PTZ6sR6dPV/aywIdJbao97EMAAQRCVSA91XcewozswlDtCu1GAAEEEEAAAQQQQACBIAoQIAwiJlUhgAACCCDQ3QRsZl9nlZISad06af9+qaKis+7aMfepraxQ9pvz1P+o0zXy8luCdpPIuAQVb16jsuzNnky92opSfXH54cpZ+JSZn9B3TNjs1x9ttK9hY/pOPb7hrnZv953SfF19Djmk1XqbGj611YuaOWHv5ky9N3duq/33vjznyy/12FFHKeOFF7x3s44AAgg4WmCCySD0LutMBiEFAQQQQAABBBBAAAEEECBAyDuAAAIIIIBAGAvYAGFkJw4oXlws7dwp1daGNuqez99S+u+f1NS/v6meMbFB60xtdZUrK9FdYURMnIaccakm3vKc+bxENovPXepM9mKficeo/5GnST16uHc3+oxO7N9oX3t3RCX2a/bS2KSkZo+5D/SIjHKvBvxZsqd+/ke/KzIv3ysXXKC3r7lGVXaiTAoCCCDgcIGGGYQ5BeXaVxzi3+Rx+DOl+wgggAACCCCAAAIIBEOAAGEwFKkDAQQQQACBbipg5yAcOLCbNq6bNqumvFSDjj9HyUefrh5BnsSxav/BoNfAE87Vt/+dqQm/e0J9xk1vpNHDBAWTjzxVh9+1SN96dp0SRk5sdE5n7ti1alWrt6suNumjQSpVRWYSwgDK0nvv1b2jRmlnG9odwG24FAEEEOj2AiOS4xUb5fu//mtzGGa02z84GogAAggggAACCCCAQAcL+P5fQgffjOoRQAABBBBAoPMF0tLCY07AzpLrGdurw26112Qm2jL03F9o8l9eVnSftmX/xR8yVtMf/ERJk49t1LbKwn2N9rV3R1VhXnsvdV1X+PXygK73vrho41fem+1aL8rJ0RPf/rayvviiXddzEQIIIBAOApE9IzQuxXeYUQKE4fBk6QMCCCCAAAIIIIAAAoEJECAMzI+rEUAAAQQQ6PYCdojRceOkmJhu39Swb2Dmqw9o4Ik/1Nj/uc/vvkbGJ2rK/70uGyz0LvlffeS9GdB6/orA6tq9eIEq83cH1Ab3xdmvPeJeDeizsqhIz8+cqfwtWwKqh4sRQACBUBZoOA9hBvMQhvLjpO0IIIAAAggggAACCARFgABhUBipBAEEEEAAge4tEGum0ZtoRqjs3bt7tzOcW7c/43OVZm7U+BsfbXc3oxL7auzcB32uLzD1Fm1qffhPn4ua2KirqVH264EF5eqqKpX9xmNN1O7frr3/fUelWd/4d1ELZ5fu3av/3HxzC2dwCAEEEAhvgfQhfXw6uDa7wGebDQQQQAABBBBAAAEEEHCeAAFC5z1zeowAAggg4FCB6GgpPV0aP55sws5+BWqrq/TNg7/R4FMvUlTvpIBu3+/wExQ/fIJPHZmv3u+z3Z6NPZ+8rvJdme251OcamyVZXVrss8/fje3P3+HvJa2ev+6ll1SyOzjZja3ejBMQQACBbiaQnuobINy2r1SF5VXdrJU0BwEEEEAAAQQQQAABBDpTgABhZ2pzLwQQQAABBLpYoLZWysqSKiq6uCEOun2dQV9322zlr1ysoWdfFZSeDz17jk89ue88o5IdG332+bNRW1mhrU/91Z9Lmj23Yk+21tx8nmqrq5s9p6UDmx6+SXnL3m/plHYdq6ms1Ip589p1LRchgAACoS4welCCIiN6+HRjfU6hzzYbCCCAAAIIIIAAAggg4CwBAoTOet70FgEEEEDAwQJ1ddJGE0Mq5PeBnfYW1JSXavXvf6jcRU+brL/x6j16clDubTMRvUttRZlWzD3DzP+3x3t3m9fX/vUyFW74ss3nt3bi3s/f1uo/zFKNaZc/5ZtHfq9tT93mzyV+nbv+lVf8Op+TEUAAgXARiInsqTGDfMcZzyBAGC6Pl34ggAACCCCAAAIIINAuAQKE7WLjIgQQQAABBEJPwEzDpvz80Gt3KLa4Ym+utjzxJ316/hjt/vjfri7EDhwatK7YYUp79krwqa8se7OWXXWsSrO3+OxvaaOmotwE8s7Tzvee95wW71kLbGXP4gVa9vNjtHvJa7LzGzZXiret16rfz9KHpyRq65N/ae60oOwv3rUrKPVQCQIIIBCKAumpiT7NZh5CHw42EEAAAQQQQAABBBBwnECk43pMhxFAAAEEEHCoQG6uMzteW1WpiCgzAWMnlPLdWfr67mu059M3VWfmHfQuPeN8A3rex9qz3jM2XjUN5vorNcOM/nf2NNdQpqk/+JniBg9rsuqashLZYUl3vHi3SrZv8DlnhtnaZ5bVPnvbt1G0cYVW/eYsxQ4aptQfXKnkI7+rqD791aNnpPYufVfbn/mbSjPbPzSqv62qLi9v8pKSEskesqOi9uwp2fk6e5tEmx6+o/E1eS07EUAAgVARsPMQvvilGWf8QMnIKXCv8okAAggggAACCCCAAAIOFCBA6MCHTpcRQAABBJwnUFws2cVJxc79l/XaIyrL+kZjfnlHp3Q9ut8g7c/4olFw0N68uiS4v4itLml6rNjqov2u+QS3Pfs3JR99plnOUFTfAYroGaWqonwVrPuvdprgYFPXR5l2jjVLjFnMdJXKMEswSvmuHdpshg+1S1eW2KQkz+1tUuM+EwndubPp/zZskHDwYGngwPqAoedCVhBAAIEQFZgwxDeD8JvdxSqrrFFctPlmBAUBBBBAAAEEEEAAAQQcJ0CA0HGPnA4jgAACCDhRwA4vGm6ltqrCNXRlz9hePl2r2LdT2W886goOVpiMvkgzHOfIn/5JDc/zuSiAjbg4qezAVHsRkVFK+/5PXcOLNqzSDgEarFKxN0d23sGWih3Wc88nr7uWls7zPjbObNjgoC0nmWX3gcVuh0MZNGmSqxt2Hs6vv5aqfJM8fbpYWSnt2CFlZkqHHiqlpPgcZgMBBBAIOYFxKYmuzGg7J7EtteZzw85CTR3Wt34HfyKAAAIIIIAAAggggICjBAgQOupx01kEEEAAAacKVFSEbs9tJmCPiIPTJhdtWqWsBQ8q991nXZl68YeOV1RiP9nzqoryVLJ1nU8Gn82oy3z1fh164fVBR7BBo7S0+mCTDTrZYofS3PrUXxrNu1eWs1V5X32kfoef4DovkD+y35znujxh1BQljp2myF69ZX/fm/nSPZL7N79+3sDmj0zxusaKH26WRV77Qn11/KxZysurf15tZbLnbd1aH0wc1vSIraHOzaQSkgAAQABJREFUQvsRQMAhAr2iIzVyQIJs5qC7ZOQQIHRb8IkAAggggAACCCCAgNMECBA67YnTXwQQQAABRwqY2FlIlrJdmSrakqHMF+5Uybb1riEyG2bO2XnuWiv73p+nKXPman9B8CaVs8NP2swyO0/d+PHStm0m486k3MUOSNUh51+nbc/e3qhZNrAZaIDQZgZmv/6Iq+7ynVs0/cElJkBYP79hfNoobbjr6kb3bcuOU81J/RqcONpsLzZLaYP9obr5+T33KX3YeSaG6v97kJUlRZkxWMkkDNWnT7sRQMAKpJthRr0DhOuYh5AXAwEEEEAAAQQQQAABxwoc/Dq+YwnoOAIIIIAAAuEv0LODpxey87XZ4EmwS9ygoRp49Ok67Nf/VHVpUavDajZ1/8jYWJ31xDwdNraH+jWMgDV1QSv7THUaOVIaPrw+OGhPtwmOI0ZI06ZJQ4ZIo35+mwadNKtRTbs//rfKTdAzkLJ7yQJPHdXFharLeEnx8fU1Dj33FxpzzT/MRtsDYPYfgzY4eFh9FT5/2tfGxD7DpuR8/onyV3/e7v5s397ysKTtrpgLEUAAgU4SmDCkj8+dMrIPpL/77GUDAQQQQAABBBBAAAEEnCBAgNAJT5k+IoAAAgg4XsAGtTqqJCZKdmo3u/Tq1TF3SRg+XlPvWKiecfWZcm29iw0OznrpJQ09+mjZIOlhJgpmA3mttdOeaxebHWiDfzb4aYOL48wkfVOnSoMGHQwOerfFnmcS/FxDoqb/8TmlnXOV92HX0Ker/3i+airKffa3dcMOU7rhzl/4nF6Rvc5lv+4PZ2n7i3crJnmIjjvscKX6nNX0hh0x84dmaSkIaLoaVmW98avM39OuPtlMXJslSkEAAQRCVWBCqvmh7VW+3lmkyuoQHWbAqx+sIoAAAggggAACCCCAgP8CDDHqvxlXIIAAAgggEHICAwdKdojEYBZ3wC0p6WCtEydKe0zsJTdXKis7uD8Ya30nH2uG0/xEGX+6WMVm2NHWSr/Ro3X2U08p7aijPKfagJ8dGtQG+IqK6oM95SZWZ4N6NhAYEyMNGCDZPtlz/S3V1fX9t9dFREZq3HX3K+WUC80ciA9o139ecgUIC9Z8ptW/O1eTbn1JPWPbHlEty92mr/7nNFXm7fJpVnl+vqut1Xu3auM9r7mOnWH+tPmLe82y2iw5ZrEhSdsl00UNNYuJ56qvWVor9vzmSvyIdFUX5qlir71DaJTiTSv18VmpGnLaxa5sy6gE32ya1nqxy/DbLNH2vB+t1c1xBBBAoKMFGmYQVtbUatPuIjXc39HtoH4EEEAAAQQQQAABBBDoegEChF3/DGgBAggggAACHS5gMwht0Gv//uDcKsEk8tlsuobDitqgoTsAt2+f9M03UjDnP+w9erKOemq18ld8bIJu92vP4n/LzsnnLj1MlG/MzJmaftVVGvGd77gy+dzHvD9tcMdmPtolmCUvr3F/kyYdI7uMueYu7f74VVXsyVZNWYlyFz3tGoY0KrHlcU/rDODez97Uur9d2Sg4aNsedSAdsr9Jj9y12oYDTRtcf0rJ5vOkA+vt/XDX1fD6wadcoCFnXKqvrj2t4aHuv11dpZy3nlDhhi819c5FZt5IE/FrY7EB5ZISyf43QEEAAQRCTaBPXJSG9otTZt7Bb/GszSkkQBhqD5L2IoAAAggggAACCCAQBAEChEFApAoEEEAAAQRCQSAlJfAAoZ1rMNWMXWkz8GzGXXPFBuCSTXQqLk5av16qrGzuTP/2m6Q8k+1n5hI8/ATXUlVcoJ7FWUqMLdKA1AQlDUtVXN++qqurz2CsqpJr3QYubRzNfnZkqahovvaYfoM09Ow5zZ/Q4EhdTbW2v3CXsl57WGXZWxocPbiZYCOypkydPVvrzHCqthz8ta9rM6A/Spu4evCpF2riH5812Zw/buJo6Owq3rxGK+aerukPLFFkfNujxcF6n0NHipYigEA4CaSbeQh9AoTZBdIRNrecggACCCCAAAIIIIAAAk4SIEDopKdNXxFAAAEEHC1g4mayQUI7/Gd7ir3ezuHXUmCwYb3x8fVzE65dG5whR+0QnlOmSDZQaYOAkZF9zFCPB4eItAHB7GzJDgNpM728i223HT7UBjc7KvvLK5nR+9btWi/fnaVND9zY6rXjzj3Xdc7IU05Rv1GjlGfSNreaPVNbvbJtJ9i6vEtvM7/hhJuecO0qydzofSgk14u/Wa2N916n8b95tM3tD2ZWbJtvyokIIIBAkATSU/vo7YydntoyTAYhBQEEEEAAAQQQQAABBJwn0MJ3/52HQY8RQAABBBAId4FDD5XsfIT+FhtYGzvWv+Cg+x42mOdPUNF9XXOfBSbRwWYS2uFN3fPA2WChnWNx+XJp+/bGwUFblw3q2MChHYXTBixtMDHYJZgZitWlZpLEVsrwk09Wso3ammKHVz3yV79yre8wf+a71gL7o8RcvrlBFcMv/o0iosxDNaXaZHCGQ8l952lVFpgxcdtYgvmc23hLTkMAAQSCJjBhiG/G9PrcQtXUmh+kFAQQQAABBBBAAAEEEHCUAAFCRz1uOosAAggg4HQBG1AbOVI65JD6IFtrHjYQMmyYZBLTPMG41q5p6rjN/AtWaRjYs8HBLWYEzh0mKtbWzC4bZFyzRmppSND2tDcmpj1XNX1Nr9SRmvSXl9VvxinN4tu5Fr3LEXPmaOzZZ7t21c9G6H3U/3VD5JnP0F4dk5yiAcfV12+3e8aacVvDoNRWVijnzXlt7okdOrctpda8+Fs++EArnnhC/733Xq2cP1/blyyRnVeSggACCHSVwAQzxKh3Ka2s0da99ishFAQQQAABBBBAAAEEEHCSAEOMOulp01cEEEAAAQSMgA0S2nkE7dR1+0zS1E4zylhxsS+NHRrUHrfzCAYjW8oG8TqqZGbWZwb6W78dgnTdOmnixLYFS9tSf//+9cHKYMR/bPBt0AnnupbSrG+07dnblf36wWEwR595psaedZZPsyLMwzrn2Wf10qxZWvPWW0o3R02T2lVsbuDKBlemzrxCETZ980CJGZCqoo0r3Jsh/Zn9xmM69KLrW+1DUpIUG9vyaUVmHN+vHntMyx9+WEV2zNsGpa+J0ttg7pRLL1Uv+9JQEEAAgU4UGNA7RoMSY7Sr8ODEuWtzCjRqYEIntoJbIYAAAggggAACCCCAQFcLkEHY1U+A+yOAAAIIINBFAjbwZ4cbnTRJOvJIadq0+sWuT55cP1dfMIKDtnteMaWAe+tdV4lJeLBDi7a3lJUFdn3D+7pNG+4PdLtX2iiNv/ER1zx5PcxN0o4+Wj984QXXsKIN644y6W3nL1igo3//ey3s3VtFDU9ow3apOWeBWRpM46ikyd/2uXrwyef7bIfyRmnWpjZl9tnAeUtl+SOP6B4zlu9HN9/cZHDQXpu/ebPemztXd5tU3rUvvdRSdRxDAAEEOkQgvUEWYUa2/VoIBQEEEEAAAQQQQAABBJwkQIDQSU+bviKAAAIIINCMgA1s2eEx7RKsoKD3rUycKmjFuy6b/Rho2b1bqqkJtJaD17cWQDp4pv9rNoPv6PsX6cfvf6DohOYzPWyW30l//rOuNJls+26/XfvbOiamaZKdu/BfBz4btjCqdz+fXYNO/KGikgb47AvZDZPmWlPWIJW2QWds5mDfvg12em0uvvVWvfmzn6mmstJrb/OrVSbC/fJ55+lLk2lIQQABBDpToOE8hGtzCjvz9twLAQQQQAABBBBAAAEEuoEAAcJu8BBoAgIIIIAAAuEuEKygmR361B0Xs/Ma7tkTuJytZ+/ewOtx19DLTMtn523sqBI/8TvKL2rbJHjRBmzy9ddr2P79inv6afWYMaPZZu0yR941yzNmaT6PxHes2IjoGKV9/6fN1hlqByJimne1gfPDDmt2OkitfPJJ/ecPf2hXlxeauSQ3miFhKQgggEBnCUxI9Z2H0GYQ1nXkeOCd1THugwACCCCAAAIIIIAAAm0WODiJTJsv4UQEEEAAAQQQ6GoBm6Bkh9e0mW92TsGoqPrAWUQ3/eqPDezZzL+i9ox36YVtA422v7bk5UnBmOvP1mUDjYMG2bXgFDvHow085uQEp76Gtdh67fCwbgv38TozZmrVyy+r5ssvVZu3X+U1USqLH6DiY2eqfNJFinz8YiXuWKV+GR8qsqgecOMnn+izxYtlA4Stlcr9jSOph1x0g3YveU0lW9e2dnm3Pm4zISMizX9ITRQ7rO3YsZJ9j5sqVaWleufXv27qUJv21ZkXeeEvfqFR3/2u7DySFAQQQKCjBdIbBAgLy6uVlV+mof3Mt1woCCCAAAIIIIAAAggg4AgBAoSOeMx0EgEEEEAgHATsF/sLTGqXHVbTBscaFhsktEEuu9ihQrtbSUuT1q9vf6tsn5KTD17fxlEcD17Qwlow67K3sYE7Mw2d7JCUO3bUBwtbuL3fh+zciYVmNLg+BxJAardtU8V996n4sce037wkdoBLG2ay8axEs/R+7P9UNmqy8s+5StnfvURZx052XWsDmaOu3auP7MSTW7aYM1suecs/VPJR3/U5KSqhj6besVDLf3mCynK2+hwLpY3B32l6PsXevSo1YmSE4ns3/8/mNc8/r3KTpRlIKdi+XW/89KdKNVmefczchCNOPlk9o6MDqZJrEUAAgWYFhvSJVd9eUcovrfKcY7MICRB6OFhBAAEEEEAAAQQQQCDsBbppnkHYu9NBBBBAAAEE/BIoL5dWr5bWrWs6OGgrqzK/48vKkpYvrw9KdbeRwuzcbe0detMmVY0b5zs/YjDnDQxmXd4P1mY8TptmgnCjms8+8z7fn/VdB1L+Kl97TRvN2Jdv3nmnHjbBwedMJS+b5V9mmWeW581ic/siv1mlIbf/TMN//i1F7s11BZtd71N5si5c+LYSUlLMWS2XnLfmqabCvIwNStzgYZr+8OdKmnJcgyPNbDZMfWzmtM7cbed3tMVm81Xsy9G25+7QJ+eN0qtTY/SPAQl65cILtcNkWzYcgs9uL7v//qA0deUTT+itOXP03Bln6G4TJPzQDFlakJkZlLqpBAEEEPAW6GH+Hp4wxHeYUeYh9BZiHQEEEEAAAQQQQACB8BcgQBj+z5geIoAAAgiEuIAZvVBr1tQPKdrWrthA4TffmGCH75Rxbb28xfMqKuqH5MzNlexih+e0Acy2FJuxZuIefhWbRJWeLtm5/bxLMEditENIdlSx7bTDgU6eLNksymAVO8Tsjj//WU+edZb+ZVIgTexYZsTZRsXGEd8zy2NmsQmccRtXaPicYxS5z6SimmIzHMvjx2j2559r4KRJrn3N/VFVsE+7PnyxycMx/QbpiPs+MoHCz5Ry2sXqEdU4+y0qKVmHXvwb1/EmK+nCnV/8ZLI+OKmX3j8uUou/n6pN91+vsuzNrhbVmJc+w2QJPvHtb+vhKVO049NPPS21mX87V6zwbAdrpdikCi+59Vb9c8QILQ1SADJYbaMeBBAID4EJqTbH/GDJyGl+BtqDZ7GGAAIIIIAAAggggAAC4SJAgDBcniT9QAABBBAISwE79KXN8rLZgf4WG7gzsYugFBtozM+vHyLUZihu2iRtNaNJ2sWuf/VV/TF7TktBSZs4ZoOEEyZISUktN80G1mxSm41ZNTX3mx2+M1ils4ZkbcnG377kZazQizffrJw2XmjiunrHLEvNEm2GAh12w0wz9mn9i2WDhHWJh+jkv/yl1dq2PX2bqkuLmzzPZqQkpR+t9Juf1nGv5WjaPz/QpFtf1uT/W6DpD36ib7+aqdSZs7X741ebvL6rd9ZWmLFbW3lIu0wq71Nm+M91r7ziaq4N5HVkqTWTWb599dX66H//tyNvQ90ItCqww/xFMXfuXJPNPc78nRyvfv36aYYZEveOO+5Qqf0mSwCl1mTurjM/7ObPn6+rrrpK06dPN0Nlx5jhmnu4lo8++iiA2qW3337bU5et85ZbbgmovnC5OL1BBmFGthm7moIAAggggAACCCCAAAKOEejA78s7xpCOIoAAAggg0GECNnATyPx4OSZ6NGBA0wG2tjbaBic3bJCKilq+wgYH7dK7t2RGvFRL06fZufPsYjMP7VCZxSbeZOIgijBfXbJzKdrhSO18gy1lCZrfTctm/tnrAi02w68ziu1fMEqFyf778rqZamPips8tPzNbcWaZuOFLJS5eoMKTZrmOZ2ebTNVHHvE5t6mNku0btObmH7mCfhGR5mE1U6L79Fe/aSf5HK3M36MVc89QTZlJfwzhYjMKXzn/fEW9/rpr2NHO6MrHJqDRZ+hQTb388s64HfdAwEfgrbfe0kUXXWSGJj6YYWaDgsuWLXMtj5n5TxcuXKgRJuO1PeXpp5/WpZde2p5LW72mxKRbzzFD91IaC6Snmh/EXmVvcYV2F5ZrYGIQv4HjVT+rCCCAAAIIIIAAAggg0L0EgvRrqu7VKVqDAAIIIIBAOAjYwNfevYH3JJAEJxuctMObthYc9G6lPTcjo22BTZsFaIcctRmFdgjOiROlsWOlQYNaDg7a+9lgWzACezYgaYONnVHsvYJRNj1wg8r3mIheO8tH5jobouv36gOeGgpN4si2jz/2bLe0svfzhVp5/fdMJmErUWOvSkrNcJ3LzNCmpZkm5TQMis3se+7MM/XJX//aab1597rrVBVgplanNZYbhY3AqlWrdN5557mCgwkJCfqLyTT+7LPP9MEHH+inP/2pq59ff/21zjT/PRTbb3u0o3jP7Rll/qKcOnWq+XlgfiAEofzBzOW53aTTDwzGD4wgtKc7VXFIv15KiPH9zjDDjHanJ0RbEEAAAQQQQAABBBDoWAEChB3rS+0IIIAAAgi0W2D3bsmMuhZwsUONtifLrsZMaLfeTFrX1vkFvRtqr7HX2jo6sgweLDNsXGB3sHUEK7OvtZbYzMhAS2X+bu384F8BVWMfy1qzxK/4SDFbzRi2ptSa4UYrbZSwjWXf0nf1+cXp2vbc31Vp5iZsrpRmb9FGM5/ffy87PGyCg56+tjIcqee8IK2U79+vNWYuRAoCnSnw61//2jWEaKRJ2X733Xd100036eijj9ZJJ52kR0zW8e233+5qzgaTan7XXXe1q2njx4/XPffco8/NXKiF5u+hr8y41eecc0676vK+yNbzz3/+0zVc6a1mTk+Kr0BERA+NT0n02ckwoz4cbCCAAAIIIIAAAgggENYCvl8XDOuu0jkEEEAAAQRCSyAY2YO2xzbIaIf+tEON+lNsgNKMzNbuYq+1w4cOGdLuKlq90GYgDh8ubdnS6qlNnpBofi9q50TsrGLnOrTZinl57b9j9huPq67KpHYGWExiqI4wS/yy91QxfLxZ8z/SWr5rhzbdf4M2P3qzBpmhSvuY+QejevdVXU21qkzQcN/Sd7T3i0WtzusXYFccdfmy++93DTNq51GjINDRAnYIUff8f7Nnz3YFBhve8zqT2frEE0+YL4Ws1913363f/va3Zqho/9Kl7VyGdglmqTHfULEZjvbzZjNf6+jRo4NZfdjUNSE1UUu3HfyhlJF9cBjZsOkkHUEAAQQQQAABBBBAAIEmBQgQNsnCTgQQQAABBLpeIJC5Bxu23t+6bGJUIEOTuu9v60hJCTzLz11fU582A9BmSNr5Gv0pdq5EO5xpZ2UPuttm2xtIgDD3nWfcVQX0aQcHtYOUxhzI/osw2UGRCX1UXez/L4drK8uVu+hp1xJQo9p4sW2rHeLTiWXnihUqMC97kh2bl4JABwssWLDAc4fLLrvMs+69EmH+Er3kkktcgcF8820UG1A85ZRTvE/pkvV//OMfrkzEMWPG6MYbb3RlJ3ZJQ7r5TScM8Z2HcG1O2zPJu3nXaB4CCCCAAAIIIIAAAgi0IsAQo60AcRgBBBBAAIGuEgjG8KLutvtblx1psqzMfXX7P+1QowX+x5v8vmFammR+B6zo6NYvtYlXdo5DM6KdTJyp00sf87vYQOY8LN+1PWhttr8G7lF3cBzb5CNOCFrdHVlR2re+pcvNHGgXLlyo75phCZ1WSmxqLgWBThBYsmSJ6y7x8fGaNm1as3c8/vjjPcc++eQTz3pXrWzbtk1//OMfXbd/4IEHXEOMdlVbuvt9000GoXfJ3l+m/JLAs9S962QdAQQQQAABBBBAAAEEuqcAAcLu+VxoFQIIIIAAAurZM3gI/gbC9jU/pZzfjQpmXS3dPDlZ5hfY0mGHSTYI17DY4T2HDas/Z+RIBdW34b1a2rYBSjvSXVyU/xHYmvIS1ZQFMO5rg4ZVme0aMySou4z80Rz3arf+3LF4sWKTkjT69NM1ohtkKnU2VnVFRWffkvs5VMAOG2rLqFGjzBcqmv9GxVibjn2guK9xb3fF55w5c1zzJl500UU6+eSTu6IJIXPPUQMSFBPp+2sBsghD5vHRUAQQQAABBBBAAAEEAhJo/v/yAqqWixFAAAEEEEAgUIG4OClYcQA7V58/pcpGjoJUgllXa02ywbf+/esXM+2Ua+hRmz1pf69tl+4ybVvJzmx9fP4JGnbpXzT45PNa65brePmuTK24cWabzm3rSTbhsnTiMZ7Tx5x+itaZ6Gn+5s2efd11Zcv772vAuHHqZwIXCWbc1uJgjInbXTvboF02OEpBoKMFyk0K+N4Dk+Gm2TTtFkrfvn1lswxLzOSzmZmZLZzZ8Yeee+45LVq0SEnmv5M777wz6DfMyspqsc7c3NwWj3e3g5E9IzQ2JVGrMvd7mpaRU6BjR5tv3VAQQAABBBBAAAEEEEAgrAUIEIb146VzCCCAAAKhLDBwoLT/4O/r2t0VO+ymv/EEf4ckbalxwayrpfs0PGYzMIOZheldvw3c2lEei8xEfu6p8KKi6jMX7XOz682VOjPB40uzZqlgyzdac/OPlPnyP5V29hwNOuGHiog2aY4NSuGG5cr89wPa+d7zqq3wP+uwQXU+m7Fpo1Q24UjXPhs8HZwSoRP/9Ce9arJuunsp3bPH1cSeBvvwn/5Ui//85+7e5KC0L6pXL/UdMSIodVEJAi0JFNm/4A6UhIQE92qzn+4AYXFxcbPndPSBPDPB67XXXuu6zW233WaGkx4U9FsOHTo06HV2dYUThvgGCMkg7Oonwv0RQAABBBBAAAEEEOgcAQKEnePMXRBAAAEEEPBbwM5TZwNNgWbgmeQqvzPnghlYa2FUOr9NuvoC+/vy7GzJ/A66yWIDujZ5xmYx2oQbmwXasGR++qmyPv/cs3v/6k9ll6/vuVbJR56mqKQBioiKVlVRvoq+Xi4bIOyIYnPQYi6cq7IDaZW2zTaYPPHCC5W7fLk+v+uujrht0Oqs84o8T7vySi35619V9//snQd4E9fShj+52xibXmx6BxsIJRBSgSSkElJJI6QQUrgpN396uzc3vdyQ3ntIJb33XNIgoQRIbHoHm15s3Jv+GRkZS1ZZSStZkr95nkXa3XPmnPOuZJv9dmY0bDTKbaCItwkSqUUjgWAT0AhCuyUYKPCaqHmcxUrNKGBrH9jH1+uuuw7btm3DyJEjcYn8XKAZI5Cd4ZiXOzcvBMWDjU2NrUiABEiABEiABEiABEiABIJIgAJhEOHSNQmQAAmQAAkEQiBGSgKpuBdItjb1oRFtvprqD/syy/natUF7CXiKCtOIwTVrAAkA9GiqW2lwm4qIWg/ROXpz3tNPu+xfuWc7Nn/zhstzwTg4QGoPFoybZHOtn5P6GQSP/u9/kTd/PrTWX7hasiro+yxNJq9RhAuefdZ+KGpfD5w2LWrXxoWFF4GkermpKyoqvE6ufF9O7GRXT0Z47R14g1mzZuGVV16RyPFYPCs/C2L0B1sQzFsKVU0xOmLEiCCMHDyX2ZlpDs7X7ChGUXkVUhN5u8ABDHdIgARIgARIgARIgARIIMoI8C/+KLugXA4JkAAJkEB0EcjMBAoLgQI/H+bv3bs2KsxXKm3bAhs2eBfDvPnV4DR/BEpvfkN9XgJS4GtZPg1mW7oUGDCgNvWozrlEVNcl778f6uk3GE8zoKY++Dmsyc1s0aUqZNYXci1y4c7/8Ue8esQR0IjHcLROo0Y5TOu4xx7DzuXLse5//3M4Hk073ceORYcDDoimJXEtYUygefPmdbMzkjZU6w+qGUlHWufYpDcqTl566aU2b1dddRUOCOL3xFs9RpOWFFI3fdo3R2yMBdU1+5+AWbq5EAd22/8gRkgnxMFIgARIgARIgARIgARIgARCQoACYUgwcxASIAESIAES8I+ABkCoeLNsWa1Q6IuXXr1qU1267KMp4FR51AJ6GhKnuUz1ZvA+lUizyWnKyUCjCDXIy0BmOpdTDJeDWk7LV3HQPndFq9duyJBaDlv//hs1geaMtTsP4HXgPx6CddDBthqN/frtFzDru4yRKJzJP/yAT6dMwd9vvln/VKO/bz94MDoddJDDPGLlg3b2Z5/hQ0mRuvzTTx3ORcNOeteuODXMrkM0cOUa3BPQCMI2bdrI74Ed2LRpk/uGcmb37t2wC4SNUaPvww8/xIoVKxAnOa0HyFMZ77zzToP5LlmypO5YTk5OXRtNR9q9e/e6c03xTVJ8LHq3S8WyLZJHe5/lSJpRCoR2GnwlARIgARIgARIgARIggegkQIEwOq8rV0UCJEACJBBFBLSGn0ahaUTfli1AvdJrLlepGl+3bg1TW9qEQLmJa3OixfJcWZqkGdO8pqLsdewYE7BA2LGjq0Ei65jWHPSWVtTTijSSUK9bly5AmTvunhx4OKeiWLWB1H92FxoZ2P/yB9DxguvQvj1smycBN05qip0yYwYsolT/Ja/hYppmU9fibFqbb6IIBSpozn3ySeTPm+fcpNH3Ne1hK1H9d2h4qUFrJWr/uV99hVT9btJIIIQE+vfvj19++QWrVq2S50mqbAKcq+GX6ZMQ+0z7hNrs6U11jlMl3bA3++CDD6CbmqYlbeoCoXLIzkx3EgjlISIaCZAACZAACZAACZAACZBAVBMITmGGqEbGxZEACZAACZBA6AloJKGKfsOHQ25k1gX61U1Ez0ugB7KzAQmualD3DioiSfSaLZzNk0ilUYUShaFtmyeU28asG8THNxLwBNUbI9kUm9YSDNS0fqEKu7EaqWmiXSwCmEaWGbGE9NYYed+LGHvn9Rg6FOjc2Vh0pwpxWRMnGhkiJG10vQPPPdftWBr5OHjyZEydOxcXyzZC0g32O/lk9DjqKGQceCAscr4xLfuMM3Dx779jxJVXIiE11eNU4qSWm9ZWnDJnDlQkpJFAqAkceuihtiE1OnDBggVuh//pp5/qzh1yyCF17/kmcghkZzj+ws7N9zO3eeQsmTMlARIgARIgARIgARIggSZPgBGETf4jQAAkQAIkQAKRRECjCTUqTzeNatMMoSoO6uYioKp2aXZxUGo0GTatJSUiYYYojjU1SbboRcN9paFGy2Vk+NIjPNuqsBdI9KB9VZpVVIXGZiYWZIwRsbG9XJ9/rluHPBHCvr3uOlu9QKtTiGkziTobKWLUwTfcgFj9APlhKao+h4GpYHb47bcjXl6NWKYIgrrVtz3r12P+M89g4Usv2WpC1j8X7PctJI3vMRLZmCjK+XGPP46x99yDv954A4tffRW7165Fxd69NtEwXb5Ag847D4PPPx/JLVsGe1r0TwJuCZws4vp9991nO6+RdpqO09lq5GfO66+/bjvcokULjBkzxrlJ0PcvuOAC6ObJZs2aVTe3f//737jjjjs8NW9y57IkgrC+rdxWhLLKamj6URoJkAAJkAAJkAAJkAAJkEB0EvDvLlF0suCqSIAESIAESCCiCKgg6DUgTfNbaipDX8RBOwUVFqVvp4EDpTRhnE0kLCmxn3T9qulNNTJN6xdGg4leY5qpr/YDhqBZxwwUb84P2G+fE06wpf5UR5kjRuDCn39GtSiRe0RoKti4EUnp6WjZs6cpAlMHKaKoImFJoEUpA1x1ldTO/Ozii/GrCBbDL78cQy68EMla6NIHayERiEfdfz9Giziw7JNPkCu1ypZ9/LEPHvxrqrE554iIUl9sTRTB8EBZh240EghHAiPkZ8thhx1mSzP6kojq54toPWrUKIepPvzww/KrojZl7tVXXy2/lxwjpesLc9r/VRHEaeFHoH/HNNuDRvaHYqprrFguNQkHd24RfpPljEiABEiABEiABEiABEiABEwhQIHQFIx0QgIkQAIkQAJhSmDbNkCjAf01EWS0gF6rTp2ggUxFRbX19Aok85hGL6ppUJq9dKHoHe4jGWubR9S/qq+aZaqtbd4sUX/HX4I1L90RsNvhUofP2TSFaes+fWyb87lA9rUW4RAR5n4TYS0cbPfq1fhOIiZVKDzzo4/QVQQMXy0uKQnZZ54JFR2DLRB2k8mdePPNaC3RWDQSiDQCjz32GDRtaKl8V8aNG4dbbrnFFomn+++IwP7888/bltRHfvZce+21fi/PWThctGhRna+vv/4a6yRa2m69JOWuPf2p/RhfAyOQmhiH7m2aYc32/X8z5EiaUQqEgXFlbxIgARIgARIgARIgARIIZwIUCMP56nBuJEACJEACJBAIAQ0DEHEvYFMfmZki/FmgAqBuNN8JaJpRtczxU7H21btgDUB91Hp0PY48stZhiP4dfuml+O2BB8zJuWrSnEt37sQMqS149mefoacIF/5YkqREDIYlitMBsg2UreONNyJR0onSSCASCQyRCOJ3330XkyZNQqHUqVWB0NlUHPziiy/k94P/vyAulIhgd/aA/uypZxqJSIGwHhCT3mZlpDsKhHlSl5hGAiRAAiRAAiRAAiRAAiQQtQSkYhGNBEiABEiABEggKgnIjVwJ+Qh8aZpqdPfuwP1EoAcJyDPdktpmoNu5N/rvV4TacZLSz6KFJ0NoLbp1s6X1DOGQhoaqls/nzNNPx7bcXEPtnRtlDB9uGssO4vxc2abIdolso6VYaKakZUySyEsV2GkkEKkExo8fj7/++gvXXHMNVAxMkXzSWm9wuHx/VLxbuHAhNKqPFtkEsjM0GfJ+WyIRhDQSIAESIAESIAESIAESIIHoJcAIwui9tlwZCZAACZBAUyewa5d5BNSXj7XezBu8cTxpgF+zZoCZGO0r6Tn1LpRuWY8t375pP2T49VhJ99f3pJMMt6/fcMvixVgg6QC3Suq+sj17ECupQ5u1a4d+kvpy4LnnQmviebJjH33UVuNw1VdfeWoW8nMVUuBx1r/+hYkffODz2GmSPrfPiSdi+aef+tzXucNQOdBWhMDYI45A4j/+gbgJE2AJhsrsPDD3SSAEBLpK/c7p06fbNl+GGz16NKz2wnYeOhpp46G711NG5+HVURQ3yM5Md1jdUqlBWFldg/jY0D6Q4jAJ7pAACZAACZAACZAACZAACQSNAAXCoKGlYxIgARIgARJoZAL2nJZmTMNMX2bMJ0g+9B626Ga2zKzBDJrU6L/s215DQst22PDuI4ZWE5uQgPEvvojB551nqH39Rlpjb/ZDD2Hj7Nn1D9e9X/3NN/juhhswePJkHCbpA5tnZNSdq/9Gaxxqzb9Pp0zB328aFDdFMEtu3RqlWoQxiLbsk09QuGkTVPDz1bSeY6ACYaoIrQfMm4e49u1hEeGVRgIkQAKRRiDLKYKwoqoGq7YVoX9Hx8jCSFsX50sCJEACJEACJEACJEACJOCaAB8FdM2FR0mABEiABEgg8gnU1Ji3BjN9mTcrUz0VSCY1yZKHpUtDk1HVEhuLvldNx8iX5iPjxIsQk5Dkcj0pbdviUBHtrly50mdx0CrX7Tupf/fuKae4FQftg2oU3rynnsILBx4IjTR0Z3Eifp0yYwbO+fJLW+Sd5M502TQuORlDREi89M8/cdWqVRhz111+iXcunbs4qDUdF7zwAoq3bcOvktLzLYkKfOngg/HiqFGYcfTR+P7mm7EtJweaktTZesr5jkM1/s9/G3X99Yjv0oXioP8I2ZMESKCRCbRISUBmi2SHWeTmS7pyGgmQAAmQAAmQAAmQAAmQQFQSsEgqF3lWnkYCJBBqApskyqFz5862YTdu3IhOfkQ8hHrOHI8ESCDCCIgoAxFLTDGJAEPfvqa4Ckcn27cDiqsx/yqqLNyFbb98gvId+aguLUZiWhoGju6DvuNPgIpy/ti3112HOVKv0FdLTE/H2HvuQUJqKjRqUNOQdhaxLV7qjjnbnnXrkPPuu9ibl4eK4tp5t5YaZdlnnYXkli0dmtdUVWHF559juUT7FQv06vJy2+tWD4KkgwMvO0kyXkVREWo8RLzGyHqyzjgDh4pg2C47u85jgfwufknERF2Hr5Z99tk49Y03TKtl6Ov4bB9+BPh3Xvhdk1DNKNKv/aUz5uOb3K11uC44uBvuOCmrbp9vSIAESIAESIAESKCpEoj0v/Oa6nXjuj0TYIpRz3x4lgRIgARIgAQil4AW0DPLfPSlQpsGHUomTXcBZmbNLGA/Kg5KcF6jW3xaK2SecKHDPDIHQMRBh0OGd/6SFKD+iIM6QLmEU351xRUOY6n4dsCFF2L4ZZehde/ededadOuGQyVK0YjFxMXZ6h32ldp8G375xRaxuPbHH410NdSmzEBeWBUP/37rLdvWfexYTHjlFaRL5F+6PLRzwaxZePO447BL1WKDNvj88zFe6jpq2lgaCZAACUQ6geyMdAeBMDdfwutpJEACJEACJEACJEACJEACUUmAAmFUXlYuigRIgARIgASEQJs2wPr1tUpdIEA0haREkHkzzdy4VYIOVHArK9vfWkrnQbJkQkqzIcl1Fs39jUP4TjJqIj8f2LkzhIP6OJSLbJiGPGiCiF/vu89QW6ONVHz7ffp02zbiyitxjLxXwc9X27V6Nd6fOBGbJfVoY5uKky+OHIlJUoOx/aBBaNWrFy7+4w/8IuwWvfwySnftcjtFbX/QNddABUKLmzSrbjsH4USNpFjdIflxS6TWo77X6M02/fsjXlK90kiABEjAKIHszHSHpkskxWhNjVUe+JG/BWgkQAIkQAIkQAIkQAIkQAJRRcD3uzpRtXwuhgRIgARIgASimICkUrSJhIGmGW3VClCVz41pNse1a2uFNlcpOlXk0qyNuqkrCThrVKFQ5yhZMbF5s5sFhdFh0Xn8snU//YTtubl+9TXSae4TT6Bgwwac8d57thSkRvpomy2LFtnqAaqIFS5WtGUL3jj2WFz8+++2SMJk+ZCOe+ghjLnzTuRK6tTcmTNRJB+WytJSJEnqVRXdhk6daku5Gg7CYJGo8n+++CL+lChGvSb1TVPFqoB54OWXo3XHjqgW/laNspRoR4s8QBArdRct4aTa158835MACTQKgayMNIdxiyuqsW5nMXq0TXU4zh0SIAESIAESIAESIAESIIHIJ0CBMPKvIVdAAiRAAiRAAu4JiCgQcB3CjAy3/jVScMkSx4hBt43lhAZkaeSeaCyQ8nYhNxUHNXukRjmaaZqBVcvttWgBSBAX/BX2nOfka4BeZUkJFs+YgR9uusnZlen7WkfwS0lDOv655wz53iPRrJq+M5zEQfvEVQD8/NJLce5XX9kP2SLvDrjgAugWjmaVHL4/3nYbZv/3v25rLmqq2LmPP27beokoeLT0qZ+x1iK1ReOnTEGirD2mR49wXCbnRAIkEGIC7dKS0LZ5IrbvLa8bOUeiCCkQ1uHgGxIgARIgARIgARIgARKIGgIslhI1l5IL8ZXAn5La7N5778VxcrOys9QdSkxMlJvVqejTpw8ukJuBv0htJBoJkAAJRDwBVa569vR/Gd27A82bu+yvkYEapFY/najLhk4HNeLQF1HRqXtAu5s2mS8O6oR0TVLGDmkSeGFmQJYvvnZLGOfzw4bhC6kRWLZnT0CcjHbWqLVNEnlnxL6RdJwarReuturrr7EzHIpRGgBUU1WFD84+25ZGVmsqGrFVIg6+Jw1L6zW2Sn7digcfxF5JrVoqUYZWg77queBbEiCBKCSQ7RRFmJvHOoRReJm5JBIgARIgARIgARIgARIABUJ+CJokgSOOOALD5Cbqrbfeiq/lhuAmuWNcIXe6i4uLsVJuDr722ms4/PDDMXnyZNvxJgmJiyYBEogeAlr8T4U+X61rV0AjEN2YlJJD+f4AAzetXB8WfQMrVgCuUpK67hH4UdU+NM1pMEzFUtFfbKa4zbCUFONRlrvXrMHLBx+MHcuWmTG0Tz7mPf201/aa+lIjDsPdXh871pZStFovaBjbV1ddZZunr1PUxK6fyiZfP0eTL2LFs8+iZPx4WP39Ujt65B4JkEAEE8jKcKxDmJNPgTCCLyenTgIkQAIkQAIkQAIkQAJuCVAgdIuGJ6KZQN6+O8QZkjbv6quvxvvvv4+5c+dizpw5mD59OjIzM23LnyFp2jSakEYCJEACEU9Ahb4BA9xGAzqsT3N/9usH+WHocLj+jpRjg5YyC8SKimrTjQbiw5e+WorRLuL50s9oW3ta0bZtgdhYo73ct+vQAbBY3J+3n6kQkG8ef3yjRedpnT5vaUMXSKShpsQMdyuUB4beP/NMPCri+B+SmtMaSgXbIJx1s2Zh/jPPGGzdsJmW3lzY8LDtSNU336D0wgvDct1upszDJEACQSCQnelYhzBXUoyG48/DICydLkmABEiABEiABEiABEigSRFgDcImdbm5WDuBfnLjW9OLnnbaaXIT1/Eu7kEHHYTzzjsPhxxyiES3rMDbb7+NyyXt1mGHHWbvzlcSIAESiEwCWiBPN4mWhqZ6lPpk0FA+NS12p/kxVZUyUBzQrEyR6keHDbapzrN1q/+j1GzdiMpPnkfVnz8Kt522G6WW9NaIG3wY4idI/bbMHnWioP5aadcOkLJ2fpteDhUajdifL72EncuXG2kalDYabbdMogOHSi07d/b3m2+6OxWWxzUV6tfyANHWv//GiRJZF+P0t4J90nvz87FIsg5sl1y7Wu8vTnLCpooYP+CMM9Dl0ENF4DWg8NqdGXyd++STBlu6b/aXnBomm6snBSvl7574SZMQL6IzjQRIoGkScI4g3FNSibw9pejUUkLbaSRAAiRAAiRAAiRAAiRAAlFDgAJh1FxKLsQXAp9//rnH5m3atMHDDz+M8ZJqS00jDCkQekTGkyRAApFEIMC6hCq2bd9uzoKlBJpNo1RBLJimdRJ9rZWo86letgAVr92Nql8lMaNTBJx1I1CRMwcVbz2E+FHHouDBW9FSHi5R03qEe/cCGiXpq6mmJOVw6wRHT/01Km++gRSfnnyYcW7Nd9+5FQg16qRgo8CKQFv44os20e84iSasL/ZtnD0bvz/6KJZ99BG0HqCzzX3iCbTLzsbwadMw5KKLECd1js2wQsmAsOzjjwN2JR9NrJOthxtPFU89RYHQDRseJoGmQKBTy2SkJ8ejoFRyc++znLxCCoR2GHwlARIgARIgARIgARIggSgh4OrB4ShZGpdBAoERGD16dJ2D1Vpoi0YCJEACJGAjoOXZXGgiftFRsdEf4c7XwbT+oK9W+cNMlFx2MKp+FkHGSRx08CWLqJz9FeZJfdtNEs2npgFn/fsbCsZ0cGUXBzXQ04it/fFH7NRijo1syz/7DFVuatdVy3GrPf+qifNMTHeskWWiawdX8yRizy7Kqdj5y3334WURgpe8955LcdDeeVtODr4UgfC1MWO8pmC19/H2mjtzpmksl3oYrOqrr1AjdS1pJEACTZOAPhCRleEY3r+EdQib5oeBqyYBEiABEiABEiABEohqAhQIo/rycnGBEKjQO+D7LCaGXxU7C76SAAmQgNlaj9n+XF0hFSJ9scpZH6Ls32cBlft/F3jrryJY7sUXI09STqrFxwNZWUD79pLK0cCvEc3sKkFnaN3a20j7z6/6+uv9O434rqqkBEs/+MDlDGIlei5GYZhkXQ8/HBdJBN9RDzxgkkfvbv547DFbo1l33IEfb7nFe4d6LTZJfeNX5aGjsj176h31723Bhg3+dXTRy2Nwq3xhKqS2JI0ESKDpEsjOdHwII0fqENJIgARIgARIgARIgARIgASii4CB21XRtWCuhgSMEvjpp5/qmmrNQhoJkAAJkEAtASNily+szPbnamxfUpjWbFyJsv+cCyk06MqV12MqEhYuXGhrp5GEPXtKvTcp+NatG5Cc7Nhd1671CgcOBAYNApo3dzzvba9kxw5vTUJ2fp6bVKcaidJKIZhk46ZPR+dRozDo3HORGIoCljLv9fI3wW8PPYSf77zTr1VojcL3zzzTVrvSLwf7OlVq/VCTzFtQrVXqK9JIgASaLgHnCMKcPKlbTCMBEiABEiABEiABEiABEogqAkGu+BNVrLiYJkSgRlLJ3X///XUrnjhxYt17o282bdrksenmzZs9nudJEiABEghXAiYGg9mWaLY/V9xUmFOxzki0YsVMiRarkKKFfppV8q+uExFr0IwZdR50jRkZtZvqjpqiVcVB3TStqL8WjNSd/s5l42+/Ya+ISs11oU42+IIL8MNNNzkd9X23vaioHYcOtXVMkJBL9TtX6gOGwn69996Ahln97bfQ2oVd9tWp9MdZgq8KsodBEjyc01NWE8VIL0PxNAmQQBgSyMpwjCDctrcc2/aWoV3zpDCcLadEAiRAAiRAAiRAAiRAAiTgDwG5LUUjARJwJvDII49g7ty5tsOnnHIKhg8f7tzE637nzp3haRsxYoRXH2xAAiRAAuFIQIW2li3NmZmm1UwKwb1Ge6Set1lbS4pQ+fXr3pp5Pb9FasVVbN/usp0KgioYKsdAxEF1nmTWhXA5U98PFm3Z4rLTkIsuQmyCN0nKZVeHg8Olpp9GJNrt4GuvRbIvOVntHf14NSNF6Hw3UZZGp2NmJKbjrf+GM7CEKDqz4cg8QgIkEA4EurdphpQE+UVVz3KZZrQeDb4lARIgARIgARIgARIggcgnQIEw8q8hV2AyAU0tetO+KId2kvftmWeeMXkEuiMBEiCByCfQoYM5azDLj5HZGBmr8vu3gZK9Rtx5bGOVOrZ5r77qsY0ZJzNHjjTDjWk+KktLXfpq1rYtBp5zjstzRg+qGKppRetbepcuOOuTTxDnnLu1fqMwep/73nso3rbN7xllSZrSOJMU9QFeZhHTp4+XFjxNAiQQzQRiYywY0DHNYYm5TDPqwIM7JEACJEACJEACJEACJBDpBCgQRvoV5PxNJZArNYI0YrBKcr8lJiZipkSAtG/f3q8xNm7cCE+bPULRL+fsRAIkQAKNTKBFi8Aj/7QuYJs2oVuIakitWnker2ZJbfS451bGzhbsi0Q31tq/VgNOOy1kEXRGZpikHww3Nu7hh9G6b183Zz0ftki45alvvglNK+psmrLz/B9/REooP0zOkzC4X1NZaUszarB5g2YpEi2ZfdZZDY77ekC/Bp08dZIvS8LZZ3tqwXMkQAJNgEB2pmOscU5eYRNYNZdIAiRAAiRAAiRAAiRAAk2HAAXCpnOtuVIvBNauXYtx48Zh9+7dkvYtFm+//TaOOOIIL73cn+7UqRM8bR07dnTfmWdIgARIIMwJaJbHnj0DS5HZo0dtDb5QLrVXLyAlxf2I1r273Z/08Uyl/D4x2woWLEDOlCn4WRbyg6idP0m9v7ZGCiuaPREX/jSKr0W3bi7O1B5KlvlO+vprtOrd220bVydiJB/rKa+/jt7HHefqtO1Yp4MOwjR5yGf0f/4DHSecrXTXroCmN+LKKwPqr50PkG1/otaG7uJFHLSEWfrahrPkERIggWATGJDhFEG4uSDYQ9I/CZAACZAACZAACZAACZBACAlQIAwhbA4VvgTy8/Nx1FFHQV+1ttHLL79siyQM3xlzZiRAAiTQ+ATSJbBABTd/THWkxgj40qjFrCzARSBa7TJipYFJpsKWWbb9yy/xu6QT/V1q4ubJ76jS1atRJQKkbq337IFjlSgfR7WY8+fgQEn/mdCsmcfBVUCcMns2+p96qiF1WSMOJ33zjaH0pM0kLfgR//oXrpU6iKdLBoDeJ54YVtGVHsH4cLLj0KEYc9ddPvRwbCq6PLIdDzXYS7ziigbHeIAESKDpEcjOcIwg3LirFAUllU0PBFdMAiRAAiRAAiRAAiRAAlFKwJw7QlEKh8tqGgR27NiBo48+GmvWrLEt+IknnsDkyZObxuK5ShIgARIIkICUlkP//oAKb0YsRv7y0AAyCXxrNFPdTkVCKV+HhATHaVhayIJMsgSFY4Ktk9Scf55wAtylLE2UMTQmz1NEmKdp9Jzqv9hU3++Bl19ef9fte00FOvGDD3CViJyHSM3fFCdOmk5UBcTzvv8e/1i6FN3HjHHry9WJWLnAWWecgXM++ww3yO94M+o0WvSDa5KZEeF42K23YtS11/o8I/nIQ+MwPa0m6cEHETtkiM++2YEESCD6CPRun4qEWMefGLn5jCKMvivNFZEACZAACZAACZAACTRVAo5/7TdVClx3kyVQUFCAY445BkuWLLExuP/++/GPf/yjyfLgwkmABEjAHwKaiXDYMEBThrpL35mUBGjUoATAwUkP8mfIgPuIBiVpoGvnrWXxNCulBr+ljD4hYN92B22OP97+1u/X9Y8/juXXXee1v1b+6yObL3/YqRA34OaX0OP8W9D6oGO9juGpQdfDD4dGtvliLbt3x1H33YfrJOLvum3bcMWKFbhG6vfeUlRkExB7HHmkLarfF5+u2h50zTWuDvt0rE2/fj61d9dYo0o7jRrl7rTh45rtYNx//4sTnn22gcDqyolGmKrkN0E2T3GtibffjgQDnzdXY/AYCZBA9BGIF3GwX8fmDgvLoUDowIM7JEACJEACJEACJEACJBDJBHy5jxTJ6+TcSaABgZKSEpwgERl//vmn7dyt8jT+jTfe2KAdD5AACZAACXgnoIJbhw7A4MG1m4pumn60j6hWAweKOCHqhEYNGo009D6iOS20lmLr1oDqPzr3kdPGIVmVzgAtoX17tNc0mgHYnrlzseyf/zTsQXRaW+rINvLqMZpQouHaHjYBw5/+BZknXmTzn3XLK0jq2M323td/mmdm4tQ33/S1W117jc5rJqpxawktTRPVNk7VZBNtwOmno7f8vvfXOor6faQImWbYgNNOQ6p8Nsyy4ZdeahNVlX/ngw9u4LalfLgPF55T5YxWVVah0JVZOndGstR5TLrzTlNEWVdj8BgJkEBkEshyrkOYXxiZC+GsSYAESIAESIAESIAESIAEGhAwmBCsQT8eIIGIJlBRUWGrMfjbb7/Z1nH11Vfj7rvvjug1cfIkQAIkEA4EVHDTSDwvpejCYaou56BiVWdJlbni+utdnjd6sNPUqYhxzl9qtPO+duunTwesVp96pUhr0WXRVbZtsu1t3RHVrdqLmxrEN2+FFoMOQeZJlyC5gyab3G+JrTtg2KPf468bjsXe9av2n/DyLr1rV5z71Vc2Yc9L00Y7HSPq9envvIM3JGPARql/6Iup6HpyixZoKVsrUbx3rTLOxtU4w6dNc3U4oGNxiYm2Go0DzzkH5Xv3onTnTtRUVUFTmSZpeG9pKSpl/RXPPIPq+fNtY5XKv0vky7o6LQ0lIiBWVVYi6d570e7TTzFUPrs9pC6zmWlVA1ogO5MACTQqgSxbHcKNdXPIySuoe883JEACJEACJEACJEACJEACkU3AYhWL7CVw9iTgO4HT5An+Dz/80NZx7NixePTRRz0+MZ8gN3n7aBiMibZp0yZ0lif21TZKSrVOmuuORgIkQAIk0OgEqkRkmSO5UEsk5aU/liTFDUctWIAEqbXnr5VL2s2f5HeEVYSegCwhEc0+2oSYFsbm0ipxO5Y9cT1y3n4b1fIwjTvTVJla52+c1EdM1dDRCLBKEco+nTLFtjYj01UJVZPE2uMZV519Nj4XLv5ad0mZet5333n8e8Nf30b77ZG6j7Nuuw25H3+MqrIyt91UDNXUrMMvu3WO0A0AAEAASURBVIxCoVtKnk/w7zzPfKL5bLRd+0Ub9+Dkp2ofqtTrpg8C5dxxDJol8lnjaP4cc20kQAIkQAIkQAINCUTb33kNV8gjTZEA/6pvileda64TBxXFjz/+iEGDBnmk0lUiJNatW+exDU+SAAmQAAlEB4G45s0xTKLi/jjkEFSIUOeLxUnE1rAvvwxIHNTx8l59NXBxUB1VlKPq69eRcNb/6Z5XS89oi5Nl7KMfeggLX34Zi+X9Hvn9p2JSrESqpYv4OXjyZAy9+OKIEQbti45PTsZpb72Fg888E79L2tGlIr5W2k/ue9UUnPo4kP5VoLJn/VStvUQcPOiII/D7Tz/JGd+sTf/+OOO99xpVHMwX0fotSbVavHWr18lrpOSXUpN5w6+/YsIrr0CjFOubVdK0V37+Oaxr18IqgjpSUxHTrRvix4+HJVLDh+svkO9JgATqCPTr0ByxMRZU19Q+V6yPFy/dXIjh3VrVteEbEiABEiABEiABEiABEiCByCRAgTAyrxtnTQIkQAIkQAIkEEQCKVKH8KA5c7Dg+ONRvHSpoZGSu3fHUBEHU7WgYYC2d9GiAD3s7169cvH+HQ/v7PUYtYnWBDxU6vLqplYtKShjJWow0s1aXIxmEhU3VsTBQ2UxKv9quk297a2Rgu1lS5bNnY0UcTDh3HPxsw81FzNHjMDZn32GZE332Ui2XT7DMyRtaNmePT7NQCNJrdXVOE1eNeVotQiHmqq0UkRD6+7dDXyVpqcj4fzzkSBpemNN+B40GIAHSIAEQk4gKT4WvdqmYvlWeRhgn+VKHUIKhHYafCUBEiABEiABEiABEiCByCVAgTByrx1nHgABZtYNAB67kgAJkECEE7DW1GCjiH8F69ejoqgICRIx2FLEvcyRIx0ivJIlIkpThW6R+m0bnnoKhfLelaUOHIguUluu46RJiJNIKjOs0kchx+OYRcZEodatAXdlE6NBHFRGFS++COu+qNAE2XesxOiRou2kRhQOk6i6HiIU/i7pyZd/8gn08+TKNGrwQPlcaLRlnNT5ayzTVLHvnHSSz+Kgfb65M2ci48ADMSwuDmXXXgu4Wa+tfUEBKh5/HBVPPIGk++5Dwg03OHyn7D75SgIkEFkEsjLTHARC1iGMrOvH2ZIACZAACZAACZAACZCAOwIUCN2R4XESIAESIAESIIGoIlCycycWSeTTKqm/1j0rC+kdOyI1JQUV+flYK4LPT7fcgp4TJuAAiYBKatHCtvZYSUuZeeGFtq1g3jzs/OEHVIofSI61+Fat0PLww9FCUpFaNPzORItxSukYkGupQ2jEBEdUmz4cVC6pUwM1qwjLmZJG80ypZbxnwwYseuQRbM/JQZmIY/EiEDeX6NMBEmXYbfRo0z8X/sx96UcfQVOGBmIV//kPykRMN2zCuuymm1CzaxeSH3jAcDc2JAESCE8C2Rnp+PDPvLrJ5UgEIY0ESIAESIAESIAESIAESCDyCVAgjPxryBWQAAmQAAmQAAl4IfDXG29gudTTGyY10g5+8EG3rddKlOAXZ5+NAVOnov+ppzq0S5coKt1CYYntNdmlOWZp0c6ro4wMQAIpo9qqZs+GNW//De5AFlt23XWIl1qGsU8/jSF//+3o6vffoclYa9LSEDtsmOO5RtibL3MMxAZI5yG+iIP1BquQ71qMROcmSlpXGgmQQOQSyMpIc5j8Skk3Wl5VjcQ4rdxKIwESIAESIAESIAESIAESiFQCMZE6cc6bBEiABEiABEiABIwQ+OOxx1AtNdjOuPNO9PAi8HUXQee0229H5eLF+FPSUTaWtZNIRrMs7vCTPbpqJ/ph164em0TFySpJFWuWVc+ahTKps1fjLA7qAKWlqHzpJRQNH46iMWNQs3mzWcP67EdrD67/+Wef+9k76K3/0fYdP181klBrP9JIgAQil8AAJ4GwqsaKFVt8iCqO3KVz5iRAAiRAAiRAAiRAAiQQ1QQoEEb15eXiSIAESIAESKBpE9D6aal792LICSf4BGLQMcegmaRHXPHFFz71M6txm2OPRbJEXgVqMV36Inb4kS7dxMhfgV2kCF/PnpBUmC6bRNXBmjVrQr4eFRKLDjoI1QGm+PR34lsWLvS3q63fEfKv1moMyCT1auXbbwfkgp1JgAQal0DzpHh0b9PMYRI5+QUO+9whARIgARIgARIgARIgARKIPAIUCCPvmnHGJEACJEACJEACBghUlZejQNJKZo0da6B1wyZ9Dz0Uhb/+iprq6oYng3zEEhuLziakZUw89bIGdfCkrCJUe5QAN3Tq1DTEQb1c1srKIF811+6tUqew5LjjULNjh+sGQTxatmeP3941Me1Av3s7dix/6ikp22l1PMg9EiCBiCLgHEWYk0eBMKIuICdLAiRAAiRAAiRAAiRAAi4IUCB0AYWHSIAESIAESIAE3BPQG/27fvoJi886C7OkeN13SUn4TlSnWaI2/T15MvZIDbZwEAOWf/ghDjzpJPcLMXDmAIkkXPPllwZamt+k87RpSM3K8ttx88GDMequSzBkiAg9ovQMGgRoSbwDDgA6dgTimlglakuLFn6zDLRjjUQQlt1yS6BufO4fE6/VEP2zkdLNrMDSmkWLYN20yb+JsBcJkEBYEMjOSHeYR25+ocM+d0iABEiABEiABEiABEiABCKPAAXCyLtmnDEJkAAJkAAJNBqBze++i9+yszFv9GhskfflUl+tRiL1asrKUJ6Xh/wZM/DHqFGYI0rUts8/b7R56sB7RZSIF/EyEItLSEBhgGka/R0/LjUVQ0WcTOrc2WcXyd26YaikR41PTYFGDDZvDog7JCY2nYhBZ2hxhx3mfCik+5VvvAFrABF9/ky2mRaY9MM0kWA3P/p56mLdvt3TaZ4jARIIcwLZmWkOM1y6uRBV1TUOx7hDAiRAAiRAAiRAAiRAAiQQWQQoEEbW9eJsSYAESIAESKBRCGhE4Mrbb8dfEjVYvGSJ1znsFVFt4fjxWPfww17bBqPBXhEre2nonAnWQ6L4AknVGMgUkqVI4EiJyEwbOtSwm/QRIzByzhwkZWYa7tMUGiZMmdK4YZOlpah47bWQou4mQn58M8e6YUYmoHGrsUYa+tDGWlHhQ2s2JQESCDcCWU4RhOVVNVi9vTjcpsn5kAAJkAAJkAAJkAAJkAAJ+ECAAqEPsNiUBEiABEiABJoqgTX33os1d9/t8/KXX3cdNjzzjM/9Au1QLJGNbURcM8NaSD7OUvHXWJYkaVxH/vEHDvjoI7Q++mi302gt6VCHfPIJRkrdxcQOHdy2a6onLCkpiB03rlGXXykRtqG0pPR0DJo0yechJQOtYdPKgpo89FfZvpPtW9l+km2FbPWrd1patpQjNBIggUgl0KpZAjLSHaPyWYcwUq8m500CJEACJEACJEACJEACtQTiCIIESIAESIAESIAEPBHYNWsWVt12m6cmHs8tveIKtJC0o2la/C5EZpW0p7BYTButWqK/GtNipGBg+5NPtm3FK1Zg9y+/oHLXLtuU4lu1QssjjkCzXr0ac4oRMXbSQw+huJFqSiqgmo0bQ87pQKllueC553wa11ECcN21Ug5rLPFi2Wo/iY7tFspuimwajTg4LQ1p3brJOxoJkEAkE8jKTEd+QVndErQO4WlS25ZGAiRAAiRAAiRAAiRAAiQQmQQYQRiZ142zJgESIAESIIGQEVg3fXpgY9XUYP1jjwXmw8feCVpwz0RL1CJ+YWLN+vRBJ0mX2f36622bvqc4aOzixA0YgISrrzbWOAitrCLqVi9WSS101n7QIAzR9Ko+mEYFerJCOfm2bP+TzZU4aO9bIm/myfa61Chd+6vGGNJIgAQimUC2U5rRnPyCSF4O504CJEACJEACJEACJEACTZ4ABcIm/xEgABIgARIgARJwT6B03Tps//xz9w08nInpfyDiJ1yChHNvwI6KNti8di8qNewoBNa8c2dTR0lp395Uf3TWeASSHnkEcVJLs1GsqgpFEklbdNhhqM7NDdkUTnj6afSUFLRGzVO8rIqD78rmSRh0HqdC6g++edxxWPXNN86nuE8CJBBBBLIz0xxmu0QiCGtqvD1S4NCFOyRAAiRAAiRAAiRAAiRAAmFEgAJhGF0MToUESIAESIAEwo3AppdfBqw+3PxLSkH8+IuR8sqfaPbiXCTd8BwSpz2AxH88hLWbm2P+fGDlSmDv3uCuNEHqnRUXqpQRuJUWFyM2jCIIA19R0/ZgkdSzKW+9hYRbbzU1Da0vVKslmq7okENQ9ZNW6wu+xSYk4OxPP0W2QWF0g5spVcnxT2QrdnPe0+EaeTrgvdNPx/alSz014zkSIIEwJpDlFEFYVF6FDbs0VphGAiRAAiRAAiRAAiRAAiQQiQQoEEbiVeOcSYAESIAESCBEBIr+/tvwSDH9hqHZuyuRdNMLiO0zxGU/1Rq3bwfUrQqFkn00OCYiUJWIhGZYTZs2jSYkmTH/aPFh3b0b5RL9VzxuHPYOGYK9kjqzSGovlt5wA6pXrfJpmSoSJt99N5pv2IAEra/p5rMSO3Yskt9/H8jM9Mm/ocYFBSgePz4kKUfLRSz/9rrrsMJANHB8SgqSY2Lg6rGAZbKwnYYW57pRRVERfr3vPtcneZQESCDsCbRPS0Sb1ASHeTLNqAMO7pAACZAACZAACZAACZBARBGIi6jZcrIkQAIkQAIkQAIhJVAlIoYRix18GJIf/gqW5GZGmtvaqFAomQfRvz8geoTplj50KKpmz0ZcfLzfvqurq9EsK8vv/pHSsUZSX+5YvhylO3fCKqptcqtWaNOvHzTyrLGtevVqlN97Lyrflqp3pQ2TX1b//DMqHnoIcZJCM/HGGxE3ZozhKcd06oTku+5C0u23o3rOHFi3bYNVxrC0aIHY7GzE9Ohh81UjUW/l0sZ0k1DakjPPROqSJbAE40sgE96bn483jz8eWw3WPqwsKcES6XeAbEn1FqyC4eJ6+/6+zX33XRwjdU1TVHinkQAJRBQBfbhCowh/WiG/wPdZTl4hThyUYd/lKwmQAAmQAAmQAAmQAAmQQAQRoEAYQReLUyUBEiABEiCBUBOISaovEbge3dK5N5Lv/8QncdDuSfVHDf7q3TsIQXoiDFaL0BUXQD5T7R8OIpmdl9mvezdvxp8vvog/n38ehZs2ObhXAWfIlCkYdumlaNm9u8O5UO1oCs7iCRMAA0J1ldS3q/r2WySJ+JT4z3/6NEWLCKFxEo3ozhIuvhjl99wDlJW5a+L38RoRZnXe8cce67cPdx3L9uzBDIm43O5jvUNNzvuxbGfKZtnnfIu87pcE9h3046VangpYKKmLD5HITxoJkEDkEcjKSHMQCHPzjT1IFHkr5YxJgARIgARIgARIgARIIPoJBOF5/eiHxhWSAAmQAAmQQFMhkNC+vdelJk69G5a0ll7buWuwY0fwahImShRYeZx/z0NViGiUoOGNUWgaMfjNtdfi0S5dMOtf/2ogDuqSS+TC/PbAA3i8Z098fP75qHQRvRdMNBr9WayimQFxsG4eksO27JprUP7oo3WHzHgT06EDkkVIDZZVPP20Ka6tUudPI0Dt9sXll/ssDtr7qiA4y74jrxvqvQ/07WoRc2kkQAKRSSA7M91h4rn5hVKq2FVSYodm3CEBEiABEiABEiABEiABEghDAhQIw/CicEokQAIkQAIkEC4E2p96qsepWNp0lMirUzy2MXJyi6oRwTBJh5Y4fDgqmzXzyXtVaioSJEVpUHKf+jQT8xtrBNfM007D7xJpp0KhV5Mbv4tffx2vH3kktJZdKKxGxMkSjRz0M2Kv7P/+D1WzZpk61YRzz0XS448HIdQVqJLagDVbt/o8X6tcy4p33kHR6NEokM9soYjahSKIF7Ztiy1yjXNnzvTZZ/0Oi2XHXnOwYXLX+i19e18iqWxpJEACkUkgW1KM1rddxRXYXGB+dHX9MfieBEiABEiABEiABEiABEggOAQoEAaHK72SAAmQAAmQQFQQaCu1y5K6dnW7lvjxF8MS53+NP7tj1Qu0HmFQTGq7xQ8aBPTpg6rERI9DVGtKVcl3GjdwYFSKgxrl8dnUqVj+6aceObg6uUlq9M08/XRUS5RasK1CovWsGlrqr8k6y++/39/ebvslXnklUj78EJbOnd228euEzLdmzRrDXTVKsPzBB7FX5lF69tmollSsKC6u7S++lN1CmWf9aELDzp0azt+3b2Z8kNWIMO00D+6SAAmEB4HOrZLRPMkxMj8nj2lGw+PqcBYkQAIkQAIkQAIkQAIk4BsBCoS+8WJrEiABEiABEmhSBCyxseh82WVu1xx3eODRg+pcNA1IubTgmUQSQmrqxQ0bBgweDGRkwCr1Ba1paYC8omNHQETEWI0alOgraPsotJVffGGLBvR3aWu++85Ws9Df/kb6WaurUfHss0aaemyjNQmrtcClyRZ/8sloLmJe0jPPmOrZajA6U6MGS885B2U33gjrtm0u56BiXo7LM74fXCFdymUT6dw0i5eaiMVHH43Kjz4CxULTsNIRCYSEgEV+P2odwvqWI2lGaSRAAiRAAiRAAiRAAiRAApFHgAJh5F0zzpgESIAESIAEQkqgyxVXIFVq+bkySyvvNQpd9XN1LASBabXDarrRbt1g6dcPFl2XvKJ7d0BSNEa7zX3yyYCXOO+pp4Jab0qFPev69QHPUx1UPPecKX6cnVgkjWe8pkA10Swe0uBWlZcjR1KJfit1I1cOGIDKd9/1OHKJnNXNDKsWJ6rdi4RummWIp6rvv0eJpDDeK99FTZNKIwESiBwCzmlGl+QzgjByrh5nSgIkQAIkQAIkQAIkQAL7CTjmBtl/nO9IgARIgARIgARIwEYgToSzoV9+ibmHHYYyJ+HGEu85ZacvCCVrIi2IBHZJNN1qEd8Cte0S/bXhl1/Q9fDDA3Xlsn/177+7PO7Pweo//vCnm6E+ltatAU1ZK+KdGRaTmdnATcHGjZj39NNYKClXSyRtqMr0RiR5c2a0fzrqTxMNa+UxM2SA+o8bWPPybGlSa9auRdLNN+8flO9IgATClkB2pmMdwpw8RhCG7cXixEiABEiABEiABEiABEjAAwFGEHqAw1MkQAIkQAIkQAK1BJKl1tlBItykjxjhgMRaZF5eUAnKogWRwN9vvWWa98UzZpjmy9mRdfdu50N+75vpy3kSloQExEtNRjMs9qCDEKNRrPVspYjyT/Xvj9+klqKKg2pD6p339Nbsr5L6s8gmlTwDNl2lo7RQ67L8lltQbnLa1oAnSwckQAIuCTinGN1SWIYdRWY/muByaB4kARIgARIgARIgARIgARIwkQAFQhNh0hUJkAAJkAAJRDOBxA4dMHL2bAyVOnZtTzjBVqeveuUi05bsIcOiaWM0ZUcFGzaYtvxCiWwLmkndS9MsyKpzwrRppkzV2c+yjz/G2yedhMri4jr/neSdxCwasmRDrYw3StnXNEte7e+N997fUkXGA/fvNnhXduWVQakb2WAgHiABEgiIQI+2qUiKd7yVkMs6hAExZWcSIAESIAESIAESIAESaAwCjn/VN8YMOCYJkAAJkAAJkEDEELCIeNP2+OMx9PPPcfiaNejUr5Upc08WRaN5c1Nc0YkbAhVFRW7O+H7YTF/Oo1vatnU+5Pd+TJs2fvc10jF21CjEDDEa1+fao643/owz6k5uWbQIH5x9NqzVWv1vv6k4Z9TipWE3o429tFOC9oi/JHk/QTb174+NkU5af9CtyZornn3W7WmeIAESCA8CsTEWDOiY5jCZnDwzEhA7uOQOCZAACZAACZAACZAACZBAkAlQIAwyYLonARIgARIgAaMEtAbf9u3AkiXAwoXAggWAaAVYtgzYtQuwWo16Ck275G7d0OOUMUhS1SBAk+BEWDS8iBY0AolpjjdzAxkoMYhqbvyxxwYyNYe+cccc47Bv9o5FPrTJzz8vYXV+xtVp/xdegKXel+in//wHVWVlDaZqNHrQ3nGw/U2Ar5pWtP5XU2sgniqbL1GK2v9I2dSXN6t8+WVYS0u9NeN5EiCBRibgXIcwN58CYSNfEg5PAiRAAiRAAiRAAiRAAj4ToEDoMzJ2IAESIAESIAFzCWig0Pr1wPz5wMqVwB4p66f3x8ulnE9JSa04qCKhCoZ5eYAKieFiKupleAwJ8j5TzQJpYtCY9wGbaIuWPXuatnIzfTlPKnbYMMQ61bp0bmNoX0S3+IsuMtQ0kEZxw4cj5b334LNSLl+eJKm5Fz9BY/JqrUBSty7/9FP7rsNrgsOe952u0iRQSViF4IGTJzdYW0fxfY5sKkJ6mpcKg71kmyjbQNmMmNaNrHTDwEh/tiEBEggNAec6hDl5haEZmKOQAAmQAAmQAAmQAAmQAAmYRoACoWko6YgESIAESIAEfCdQUQHk5NQKf1VVnvtrWxUSly4FvLX17Mncs+0lpMhfgU8Fxn79gCCXijN3wRHqbdC550JTxJphB1x4oRlu3PpwrsnntqGHE/GSpjOmlTkpcD0MYzsVL2l3m/34IyxdunhrajtvkXmlfPABEi+91KH9AolGtLp5AsAx4ahDN5c7+kf+GNnkK+a3Hf3ww2jx2mtoLk8mJE2fDhVvLfqFT01FmjwZcNRhh+FKETlPePxx9BQxUZ8VkGBgdJPtINmmyHaibCoo+mI1q1f70pxtSYAEGoFAVka6w6gbdpWgoLTS4Rh3SIAESIAESIAESIAESIAEwpuAPLNPIwESIAESIAESaAwClXIfLTe3NlrQl/ELJIuXRhQOGADEhMGjPiry2YPTNEWqUdO5qzhoYuZLo0NHTDtNNbnk/fex9ocfULJjB2ok3DSldWt0PuQQDBTBz5dUn2mdOqGfRKst/fDDgNafKdF9GSIUBdPizzoL5Y89hhrNteuPpacj8ZZb/Onpd584qUfYfNUqVEn0W7mIZtVyzZxNBbaEf/wD8WeeCYuLtKRrvv3WuUvdfrG88zXNaHfpM0a2H+u8GH9z+O23Y9jUqbYOKrQmXnONbXPlYbgc7PPgg7Du3evqtM/HrPpDjkYCJBDWBPq0b474WAsqq/fnP1+SX4hRPX39SRXWy+TkSIAESIAESIAESIAESCCqCVAgjOrLy8WRAAmQAAmEMwFNJ+pvqa1CyeS1bh3Qo4fvKyyQXKZ5UuereMUKVImjWBEqkkQ86igRV22k/ps/UWYq9vWSXIISWIT8/Nr0qJ5m1rIloMFWzZp5atV0z+3dvBl/iEC28KWXbMKgM4m/3ngD311/PQaddx5G/d//oZXCN2AjrrwyYIHwwCuuMDBSYE0siYlo9vnnKBIh1KofdF9M+370EWINMvHFtbe2lvh4xJ92mm2rkXDfmjVrYJXvmEU+6JbOnRHbt69HFyoCuzP5cQFj8YmOHrTun5YJVenRS5CyrWOMrGGcRA6OlM+KL6Zr3C8T+NKzYVuL/iChkQAJhDWBhLgY9O3QHPVTi2odQgqEYX3ZODkSIAESIAESIAESIAEScCBAgdABB3dIgARIgARIIDQEiiUcSGsNBmJbtwKiOUDu53s1q9WKzW+9hfUiOhXOm+ey/eY330RS167ofPnl6CrigAqHvphGEnaUXIIdJMegrm3LFkDXqTUW9ZzOUzM+aoZCKQ9Hc0MgT67P2yeeiOJt29y0qD1cUVSE+RKptvj113HGzJnoLWkuvVm30aMx6tprMUcEIH8sWyL7NFVpKCxGUlim/vYbioWF0UhCi0RXpnzyCeJEWGxsi5Hvkm6+mEaIujMJGsahsiW6a+DheB85Jz8qIAHL+FuEvAL9YjpZqnxxh15yiS1qUKNNfbUYUfxrli/3tZvL9jF+jO/SEQ+SAAkElUBWx3QngZB1CIMKnM5JgARIgARIgARIgARIwGQCFAhNBkp3JEACJEACJGCEgIpngZpoflANKTPTs6caKV6YK6kC80VI8mZlEvW08qabsE3SUA757DMktmvnrUuD8yoGaoSgbjTfCOQvWIDXxoxBpQsBx50nbfv2SSfhrI8/Rh8R07zZ0ZIKskxSOC588UVvTR3O9xk/HhNefRWWEOa1tYmEc+ag8p13UPH006ieO9dhTvYdjc5LuOwyJMjnPMbfgph2Z434mixfmgItNOrCKuXYEtmGuDhn5FCyNNJUoIfJ5yRPalHulujGcolu1DS1Kgh2P/JIxBp52sDNYPEiHFd9952bsz4cTk5G/Cmn+NCBTUmABBqLQHZmGt6dv3/0nLyC/Tt8RwIkQAIkQAIkQAIkQAIkEPYEKBCG/SXiBEmABEiABKKNgAYJ+VKrz9P6VWiUQCtbhJ6rdtaaGvwtaSi3SISZL1YgQsz8sWMx4tdfEd+ihS9d2dZPAppeUiMHfREH7UNZ5UP1vtS1u0QExjZa2NGDqcA3/vnn0UbSXf58990o91LvLU4Em5FXX42x0jZGhKVQm6YbTTj/fNtWJelxq77+GlZNxVlVBYsIarEjRyLuuOP8So0b6rV4G6+T1DHcsmiR22Yqj2pW4XS3LTyfUBEvXoTA7qLidxch2kyLnzgRZZLu1rprV0Bu40XstvBnTkAM2ZkEQkUgK9Pxp9Hq7UUoqahCSgJvM4TqGnAcEiABEiABEiABEiABEgiEgFQMopEACZAACZAACYSSgNYdFN3OFCsvt+kkbn2tue8+n8VBu7Oi3Fz8PWmSfZevQSYw/9lnURRAaGllSQl+e+ghQ7O0iEB08HXX4f/y8jBeIgk7Dh3aoF9rERCPlZS010pRyaPkc9QY4qDzpOKGD0fSbbch+dFHkfzkk0i66y7Ei6jqT91MZ9/hsD9coiA9mfzowMeyNUwQ6qlX7bm4ceOQLDUt9doHwywa+XfBBQG71tqNNBIggcgg0L9DGmLq/UipkcwGSzfvjYzJc5YkQAIkQAIkQAIkQAIkQAKgQMgPAQmQAAmQAAmEmIAEPplq7vxVi2C0zs9ac/YJbv/iCxQuXGjf5WuQCNTIRVzw3HMBe895+22U7t5t2E+C1KMbOmWKLfLwBon8unLlSlwhdeSulxDXfyxdipFXXYUkRnMZ5hlow/aDBqHLoYd6dKNX913Ztnts5Xgy/uKLkSIpgzUaM5gWK/MP1Kp//x3Vy7TiIo0ESCDcCSQnxKJn21SHaS7JZ5pRByDcIQESIAESIAESIAESIIEwJkCBMIwvDqdGAiRAAiQQnQTMDuBxVxJus9Rtq/JBLHJHe4PUfqMFl8CKzz9H4aZNAQ9SJeGpi197zS8/Wv+uVa9eaN2nD1LatAlapJlfk2tCnQ695Ravqy2UFm/JptGEa2WToJ2GJrUFE664AqkSCZzywguwJCQ0bGPykQqpUWmGVUg0LY0ESCAyCGQ7pRnNydOfUDQSIAESIAESIAESIAESIIFIIECBMBKuEudIAiRAAiQQVQTi481dTpybUj8bn3nGlIE2v/kmKr3UqTNloCbsZN1PP5m2+nWzZpnmi45CT6C31FM88v77vQ6souA62T6R7WXZPpPtf0lJqLz3XqR8+SXSJDVs8hNPIHbAADkTfNPUoNUmffYqROTW+qm08CKwYcMGXCepifv3749mEn3cqlUrjBgxAv/9739RIhHrgViNXO8lS5bgVRGZp02bhgMPPBCJEvGqKXF1m2Xws6VzfEZ+950pNVn7SppknWeSfC86deqECRMm4G2Jsq5yF3YfyAKacN+sjDSH1ecwgtCBB3dIgARIgARIgARIgARIIJwJuLmlGM5T5txIgARIgARIILIJSKkuufEJaP3AQC1N7svFxjb0UlNZicL58xue8ONIjUSl7V28GK0OP9yP3uxihECZpPc0y0pN9GXWnOjHNwKH3HADLBIa/L28GjGt+GXt0AFHSkrgNi7qSRrxEWibmrUay2iS7dkDq2wWEaBo4UHgC/lsnXvuuSio97CIioLz5s2zbS9KLdMvRZju0aOHXxOeMWMGLgiwhuW//vUv3H333bBaG8bU5km9Vd0+/fRTTJ8+HR988AG6dOni11zZyZFAVka6w4EVW/eioqoGCXF8FtkBDHdIgARIgARIgARIgARIIAwJ8K/2MLwonBIJkAAJkEB0E9AUo3Iv3xRz56fShNSi9SdoRqrS+v74ngRIwD0BjZg65PrrMfnHH9FLIgolhMpt48T0dIy8+mpbHcmOjSQO6uSse1WmNM/2du+O0ptugtXkn2XmzbDpeFosD4hMnDjRJg6mpqbinnvuwezZs/HDDz9g6tSpNhDLpXbpCSecgKKiIr/A1Bf14iXMfsiQIRg4cKBPvvIlalb9aNTgpEmT8Morr+DXX3/FfHlYRgVIjUpU0/2jjjrK77n6NKkm0HiAUwRhZbUVKhLSSIAESIAESIAESIAESIAEwp8AIwjD/xpxhiRAAiRAAlFIoF07YMPaalhjXIT/GVyvpip1F2Cj0Uemmtn+TJ1c5DtLbt3atEUku/tQmDYCHYWKQPcxY6DbrtWr8afUEdy8YAFKRTCLlXqCzdq2Re8TT8TAc85BgggijW0WEY5MtcJCVDzwACoefBAxgwcjpl8/oLgYFqmtGNOtG+InT0aspJCkBZ/AP//5T1sK0TjJZ/3tt99i1KhRdYOOHTsWvXv3xg0S7bps2TJbdJ5G8vlqAyQV7mOPPWZLWXrAAQfY0oLecccd+Pvvvw27ai0/Rx+Qz8zll1+O5vI5qW/Dhg3D2WefjXPk+zJz5kysXLkSjzzyCG6//fb6zfjeDwLpyfHo2joF63fuTzObK2lGnWsT+uGaXUiABEiABEiABEiABEiABIJMgAJhkAHTPQmQAAmQAAm4IhD/v2/R6a9d2HjwWa5OGzom98jhTreLa9Gi9qRJdbziTRSwDC2uiTXqNno0fpeb1WZYd7lhT4suAq169sRRBuoSNuaqY7p2Dc7wEhFWs2iRbas/QLnUWow98kgkXnkl4k46yVanrv55vjeHgKYQtdf/mzJlioM4aB/h2muvtUXrLV26FI8++ihuvvlmaBSgL6a1DHULxFQc9GSxko/76aefxscff4yKigq8//77FAg9AfPhnNYhrC8Q5uQV4szagE0fvLApCZAACZAACZAACZAACZBAqAmYHF4Q6ulzPBIgARIgARKIQAJS0w9vvolOcz9Eu5wf/VpA54Wfom3LKrd9YyTSo7VJQlG8RKSlBTl1odZj3LgRWLIE+OsvSNQIIPeasXkzUOV+mW7XH2knektqvnQT6mHFp6RgsERW0Ugg1ARipPZc7MEHh3TYaklxWXLyySi96CJYpe4qzXwCKqbZ7cILL7S/dXiNkSdVJu/7ubNbIlztgqJDozDZ0SjDQYMG2WazWiJzaeYQcK5DqBGENBIgARIgARIgARIgARIggfAnwAjC8L9GnCEJkAAJkEC0EfjlF0BEQq0q1vP75xBXthf5wycYW6VEBHb/+TV0XPQVMEzylB50kNt+nadNw87vv3d73uiJTIkaiU1KMtrcp3ZatiwvD9i1y3U3LT+2fj0g2RSRmQlJO+e6naGjEolkG2jbNki+PKC6ura2W0ICoBGS7dtDwl4MuTK7UYxEtgy79FL8eOutAbnOlvR5SRo9SiOBIBGwSprPalHxbbUBpTaiRb47sSK4WOTLGTdhAqqlNl2orfLVV201EFPefRcW+S7RzCPwi/6+EtO6fpqm050dccQRdae07t/RRx9dtx9ub8r1iRQxFTZp5hBwTie6ZHMhqmusiI3Rv3RoJEACJEACJEACJEACJEAC4UqAAmG4XhnOiwRIgARIIDoJqEglNZzsZoEV3X59E21WzMaWQcdgR79DURMngpWTxZYXo92SWejw17dI3i1hdWrffONRIGw7fjySOnVC2aZNte39+VcEgM6XXeZPT699tm4FjARwaJZUbbtzJ6Alx9LTvbp2bKDM8/NrwxElrVwD08gjET1sIYxt2gAayZeY2KBZsA+oQDjvqaewV+fqhyVIDbhDrr/ej57sQgLeCVRLeG/FM8+g4rXXAFX265mKhPHyIIFV1f5GsqoPPkD5v/+NpLvvbqQZROewmjZUrVevXtAahO6sn9aI3Gf2Pvb9cHrdJg+I2OdXf87hNMdInIumGK1vZZU1WLO9CL3bO9aCrN+G70mABEiABEiABEiABEiABBqfgPv/5TX+3DgDEiABEiCBKCBQI+rOH3+skCixnaLBlCEtLQV9+mQiK0tEmKZo27cDGzY0WHnqtrXo9f2z6PrLDOzqNRLlzVujOl4icipKkFSwDa1WzUVsVW3UQ13n3NzaSDhJK+nKNM1or3vuQc7557s6behYp0suQYqkDjTbjIqD9cfVVKN6rzorC2hu9J6jRgmuWAFoKKI3UyFRr8+ePUD//oAIbqG0FBFZzvniC7xy+OGocBJgvM1Dr/UZ772H1n36eGvK8yTgEwFrUZEthWelfL7cmVXU+4oHH3R3OmTHy6dPR6LUw7O0bBmyMaN5oLKyMuzYscO2xE7ysIknaynMNcqwWB622Kj5osPUHnroIUlbXZu3euLEiX7NcpOXh242a27sJmZtUhPRIS0JWwrL6laeI2lGKRDW4eAbEiABEiABEiABEiABEghLAhQIw/KycFIkQAIkEPkEduwoxCuvfI9nnvkKa9dK+JeTjRjRB9OmHYeJEw9FcnLoo7WcphO6XRWfPFi8RAq2z/3RQwunU4WFgBuBUFtmSl2okuXLsebee506et9tPW4c+j/xhPeGPrZQ7ctI5KArtxpNqCLhAQcAmhnUo6ngJ2u3CX4eGzqd1IhCLYaYne2RrVMvU3Y7yMIumDULb0lNwqItWwz5TBC1dOL776OnXC8aCZhJoEZy/5bI56p6wQIz3QbPl6RuLn/lFST93/8Fb4wm5HlvvQcVUg08MGEXCItEVA5H++OPP/Doo4/apqaC5zRJw+2Pde7c2Z9uUd8nOzPNQSDMzSvEKUOiftlcIAmQAAmQAAmQAAmQAAlENAEKhBF9+Th5EiCBYBDQSLeXXvoOs2cvlaCjYqlRY0GbNmkYN24IJk8eI+kNmwVj2Kjy+cIL3+Cqq15AWZmLdI77Vjp37groduONr+Hdd2/AEUeIGNMUbF/kgmlLVTHLi/WSlHuxaWlYedNNXlruP91BIiuyJZVgTBBq8gWahVARagSi13u0GuXhRZDdv2KndzrIsmXAELm7KWlWQ2kdhw7FZYsXY+6TT2LB88+jWBfrwhIl1+oBF1yAkVdfjZbdu7towUMk4D8Bq6TjLTnllMgRB/cttfzGGxEjPxwSzjjD/8Wzp42ARhDaLcHrExmambn2YZ9SEWrDzbbKz9HTTz/dFj1okZ/pr8nvtxQPD9eE2/wjYT5ZGen4fqnU+N1nGkFIIwESIAESIAESIAESIAESCG8CFAjD+/pwdiRAAiEkMG/eSjzwwAf4+OPfUV0tYUpO9vnn83DTTa9h0qTRImqdhh49Oji14K4SuOuud/Cvf71lGMbWrXtEfP0X3nvvRpx00kjD/SK2odk3JCWlmzfTm6E95KZ566OOwgaJCNzyzjuoKXdKV7rPSetjjkEXiarQ+oXaz2zTYSUoKWBTzUwz3rmdoqYWDTTNm94c18lK6s9QW7N27TDmzjtx+G23YdnHH2P1d9+hVNI4WiWEMrlVK3Q+5BBkn3UWEgxc/1DPneNFB4GK555D9c8/R95iRNwvlQccrJLyNJE1OQO6fklJSXX9K1zVb607W/umfN/vleTkZKczjburkZAnSFS2PTXovRJRP3bsWL8n5S2FqqYYHTFihN/+I7VjdqZjgWCNIKypsdoetIvUNXHeJEACJEACJEACJEACJBDtBCgQRvsV5vpIgAQMEXj11R8wdeqT8mS5iAoerKSkHM8//w1mzvxVhMRbm07Umwcm9U+99NK3PomD9r4VFVU488yHMGvWPRg5sq/9cHS+dhBhWaMs3Ah0Pi1aIsjQooXhLunDhmHgq6+i78MPY/Nbb6FEavNVSYrSWBGZEjMz0eHMM9GsVy/D/vxp6CYYzmdXeq9aywqKVubaREyTL7Trc74c1TSfjSAQ2qcYK1E7WSJ26EYjgVARsEp63oqnngrVcEEZp+yGG2CRn48JU6cGxX9TcNq8XrFXI2lDtf6gmpF0pKHip1GQEyZMwIJ9aXL/T9LP3uRDNL2reXqrx+iqT1M4lpWR5rDMveVV2Li7BF1be3+QyaEjd0iABEiABEiABEiABEiABEJGgAJhyFBzIBIggXAloHXyLrrocZ+mt2dPMY455t/44Ye7cMghA3zqG62NCwqKcfXVL/q9PE1HOm3as5g/f3pQItf8npjZHTUi47DDgO+/D9zzkUdCHs332U+CCF5dr7zS535mdKhX0ipgd+rLrUBolhJZICnSNF1emEXEBAyPDkjAA4Hq//0PNVq/M8KtVKKh4+TnZEyPHhG+ksaZvkYQtmnTBjt27KiLvnM3k93yxIZdIAyXGn1V8pDIRHm44n/yeVa7+OKL8bA8IEMLDoGO6Ulo1SwBu4r3p5fPzS+kQBgc3PRKAiRAAiRAAiRAAiRAAqYQ8P2uoinD0gkJkAAJhAcBTSt6ySX+RUmUl1fKU+n3YNu2PeGxGOdZSAQIJDoMa9cCEilmq6e2ciWgBeAMpApzdudt//XX/yc3B/fXK/LW3tX5P/9cbatL6OpcVB0bNy7w5WhuTUkZGmlmRlCffc0efe2LZLG3Dei1pCSg7uxMApFGoOLttyNtyq7nKz8kyp991vU5HjVEoH///rZ2q1atstXvc9dpmdZs3Wf2Pvb9xnitkXTM5513Hj777DPb8GdKhPxzkjaXFjwCmpbcOYowJ491CINHnJ5JgARIgARIgARIgARIIHACFAgDZ0gPJEACEUxAaw56SyvqaXk7d+6VG05fe2oS+nNyUwyaFnHxYiAnp7YOmzz9b6ultn07sH49JNdWrWhYVGTK/DQd3dNPf2mKL7P8mDKZYDnp1g0YPDgw7wcfDAntCMxHI/R2WzPQzLnod0A3s8yjEmnWIPRDAv4RsErkVsULL6DslltQKpHBZTffjHIRQmoCKPZp3bTJv8mEYa/Kl16CVaOAaX4ROPTQQ239NDrQnqbTlaOffvqp7vAhUiO1se3SSy/FO1JvV+3EE0/EjBkzJOCe//UN9nXJynCsQ5gjEYQ0EiABEiABEiABEiABEiCB8CXA/yWF77XhzEiABIJMIC9vp9QR/D3gUZ577puARMaAJ1DfQWUlkJsLrFkDeIp60uhCFQ3/+gvIz6/vwa/3OTnrJUDRnBvK7733m2g7Joo7fq0oBJ2uuAJo396/gbp2hRTN9K9vI/eKMzG5eXy8m8WYrULyprIb0DzcmASqFy5EyZQpKJT6oaWXXILy++5DxZNPovz++1F22WXYK8dLLrwQ1ftqr/kyV6tJD4/4Mmaw2lpFKK3cF0UWrDGi2e/JJ59ct7xXXnml7n39N/o7+/XXX7cdaiF1H8eMGVP/dMjfa53BF1+sTXl+pKSYff/99xHv9hdGyKcX1QNmZzrWIcyVCEJ9iIxGAiRAAiRAAiRAAiRAAiQQngQoEIbndeGsSIAEQkDgpZe+Q3V14EKUCo2ffTY3BDP2MoRGOak46GuRt3XrIMWFvDj3fDo/f5fnBj6cLS2tQEFBE0jpmC5P2d9+OyA38X0yraV1661ASopP3cKlsdw7Ns0UoUtTgdDMm8FmqpouJ8yDJGCcgN5sL7vjDhQNHYrKl1+urZHpqntZGSpffRVFw4ej7LbbfLpJb3H75XI1UPgfq5H0mDT/CIwYMULK5krdXLGXJBpzzpw5DRxpXb+lS5fajl999dUNxLhZs2bZagtrCsoLLrigQX8zD9wh341HHnnE5vJgibT/5JNPkJiYaOYQ9OWBQLZTBOFOqUe4tbDcQw+eIgESIAESIAESIAESIAESaEwCJj7H35jL4NgkQAIk4DuB336rvZnle8+GPebMWYZTThnV8ESojujT2cuXe44a9DSXDRuA5GSgdWtPrVyfk7FLt+10fc7PoyUl5WjZMtXP3hHUrV074K67gJkzAU3P5ikNXrNmgERC4PTTgaSkCFqk41R1yfpxCzRIVD+uaY6BCo4DtWoFbN3qeMyfPRUHPQ7kj1P2IQH/CNjEwX/+ExWPP+6Tg/J77oFG0iU99ZRNqPHWOaZ3b29NIuu81uOl+U3gscceg6YNLZXfUeOkhu4tks5WowR1X9N4Pv/88zbfffr0wbXXXuv3OK+KoF3fFi1aVLf79ddfY50+0LTPevXqBXv6U/uxJ554Av/5z39su5ny8M2DDz4oZZjX2k+7fO3bt28DQdNlQx40RKBLqxQ0T4zD3nJ5aG2faR3CDumR+3eLfR18JQESIAESIAESIAESIIFoJECBMBqvKtdEAiRgiMDu3ebU39PBdu0yz5ehyTs30qjBggLno77tq2qjooov6RklQkXCBpC2R2obmmgtWogY1lQsVYTQiy4Czj4b+OUX4OefgZ0iuCpbFQJVUdN0bVpzMCEh4qmo3qalE7dtC2wpHTp4+ahqAzMEwrZtgdjYwCbL3iRgEgFNIeqrOGgfuuKZZxDTsycSDQg4CZKatOLRR+1d/X/V3yeaore62n8fJvRkgsPAIA4ZMgTvvvsuJk2ahEIRW1UgdDYVB7/44gs0b97c+ZTh/Qvlc+fOHnjgAYdT559/fgOB8IMPPqhrk5eX1+B83cl6b1RA7NatW70jfBsIgZgYC/pnpGHu2v2ZJXLyC3DUAD9TqgcyGfYlARIgARIgARIgARIgARLwSoACoVdEbEACJBCtBPQmhlkWGys3QBvTtmwJfHSNXtMoC6Op5bTGYU4OpAAj+nZpbdMVzSgz06lTG8me2QTTgWlInERm2LbAr2ZYe9CsqqqB+qsZKCrVTT2aRlzqjWpfU+46O1WhkUYCYUDAWlyMMk1LHIBpatIEqVdo8SLiWORLahHRxFovYsufYeMl4jlJ0j1WSD24yvfeg1VEe10HyiXlYKBhxD5MqPrPP31ozaauCIwfP17KFv8FjSZUIXCTpCZPkIdWNJLvjDPOwBVSVzclQlNfu1ovj/lPQNOM1hcIc/MZwes/TfYkARIgARIgARIgARIggeASoEAYXL70TgIkEMYE2rTxlJ/Qt4mb6cu3kaV1RUWt2uJzRxcdVGg0IhDqmEuW2MRB9ZLZtjmOH9UTX8xe7cKpb4emTh1nKAWeb17ZOpwIqMDXr1/tR8hXUVlLC/bvbzCoT6NCVMT2dRA7rI4da1Pv2vf5SgKNSKDy7bcDjxQvKkLFG28g8fLLXa6kWgSg8unTUSlpI20instWxg8mXHUVYkRsTPr3v22bvWfN+vUof+ghVLz2GiBzCrZV//EHrPJQi4UCVkCou3btiuny+dDNFxs9erShGpiaQjcQ01qHtMYnkJ3p+Pd1rqQYpZEACZAACZAACZAACZAACYQngUYOeQlPKJwVCZBA0yBwzDFDTVvo0UcfYJovnx1phFSAN9XqxjSaplQiB2zCZF1HYNopgfOMi4vFxRcfXc8r30YrAdWhs7IATTlq1FRYHDjQhxKMGiXlby01TberAiONBMKAgAon5VI/0AyrePppl2JNxeuvo2jYMFSqaKcRfgFa4m23Ie7QQ116iRGhKVnSpTaX1Nbxl14KS/fusEg6X4vWwdUcxGabRMfbRE+z/dIfCZBAAwJZEkFY3/ILyrCzKPCfKfV98j0JkAAJkAAJkAAJkAAJkIA5BCgQmsORXkiABCKQwOTJY0xJZTlgQGcccUR24xGQFJ+mmfryJjZqXsjtDWsOHjOyu6QaFVElAJs48VBkZMgNYlqTIJAmQQYHiLbeqROgkYHuTIVB0Q8waJAP4qDdmYoNhkMO93XStKJ9+3opcmgfgK8kEHwCmpqzZtEiUwaqkaha68aNDr4qXnkFpVLTTVNGm2EaOZh4551uXdVIjuGyu+9GkXypK597DlapA2eV3ytWzT28Y4fbfoGc0BSnNBIggeAT6Nm2GRLjHG8zMM1o8LlzBBIgARIgARIgARIgARLwh4DjX+7+eGAfEiABEohQAunpzTBp0uiAZz9t2vGNmxLTm6AX8AqdHKg46KJ4nNZhfP+eU5DWzL/6gSq0PvnkpU6DcTfaCUgJK3TpAkjgkk2Ty8iorS/Yvr2krpVahRplqCKiZvuMjfWTRsuWwFCJcJWoJSS6+XzGyJ9EWthQVcgePSgO+oma3YJDQMUzM81aT4Srmj0bpVOnmuLeIt+hJIlQTJY6dRaLxaXPahE6VRgsl3qKVo1GD5FViwhJIwESCD6BOPl7sH9HpzSjrEMYfPAcgQRIgARIgARIgARIgAT8IOBDYi8/vLMLCZAACYQ5gRtvPA0zZ/6KPXuK/Zpp376ZuOCCI/3qa1onX3I0ehtUfbm5qVvXVSM83Fh2j7b46uEzMP6G97GrsMxNq4aHVRz8+us70LJlasOTPNIkCKg+p9kFdQuKaYiiKo6qQGoq3dLS2mgpHVhVShURzfwuBWURdNpkCbh4KCMQFtZ6kYLlEsnn6qEPX/xbJJVv0l13If6UU2DR75Mbs4mDhx8OaGrsEJt11SrUrFuHGKYODjF5DtcUCWgdwkUb99QtPSefdQjrYPANCZAACZAACZAACZAACYQRAbkrRiMBEiCBpkugR48O+PjjWyWoyEN+Qzd42rdvgS+//DeaNUty0yJEh7XOmlmmOR+9WWWlxxYHD+yEOc9NxrEHSRSWF0uIteDC7nH47YUL0LlzWy+teZoETCCgAniLFrUhiZ0714qGUvuM4qDvbCsqgNxcYM4c4NdfgT//BHbt8t0Pe3gnYNHPrIlmUUFcrHr1alR9/XXAni2pqYifONGjOFgjUYvFJ5zQKOJg7WKrUXLGGS7rLwYMgA5IgAQcCDjXIczNo0DoAIg7JEACJEACJEACJEACJBAmBBhBGCYXgtMgARJoPAJaP/CHH+7CSSfdIze3jUU1aOTg/7N3HvBNld8bfzKaLsqeLXuPIrIdKAiKijhRVMS9cf7de29/bsE9UXHhFgUHKnuDtIDMMlo2hdLdJvmfkzRtkmbcm3uz2nM+n9Dk3vuO+016U97nPuewOMgCY9SDUyY2pdp/eqzMc+21YGGzBTsC3akW4S8vjMfGHfl487sV+GzWGuzcX+hoZzQa0K1tE1w5th8ub2dH8+ULgbcnAz1fdJ5H0N7lACEgBKJJgMrh4bffgNmza2s9rL8OGgScfDKQSaVZ2aApoZ2AgcRsA12f7bt2ae7MQIK4y0VXTvX/gtadVTCibcUKWBcvhnno0Oqj2aVY+eOPqPjiC9jy8mAjMdJOP6MZ1qVLYZ0zB2Z2MUoIASEQNgKZ6Y08+s7ZX4zDpRVIS1J/Q55HR/JCCAgBISAEhIAQEAJCQAgIAV0JiECoK07pTAgIgXglcOyxvbFmzet4662Z9PgVeXm+bTCcCpNrDl528QiklhUD27Z5pink/Ij+apyFEw4Le1oFwiRyQjbyXNDxOWUVheC6khD4vxtHOh5Wqw0lZZVIocUhFgkdwdYjjmJiOWsWcMEFztfyrxAQAjFHgO8NmDoVdHOEf02JS6IuWeJ8cCnHO+4AmjePuVOJuwkZ6LproTqBZZTGU2tYrroKBk65S2H9+2+t3VW3575YILQXFqLspZfA4qM9N7d6f6w8Kb74YqT+8QdMXbvGypRkHkKgzhHo3roBzPS3XqWNvhSqYg3VIRzaOVx5xF2jyE8hIASEgBAQAkJACAgBISAE1BAQgVANLTlWCAiBOk2gVasmeOihC3Dvvefip5+WYP78dQ5HoclkpAXuhhg9uj+GD+kCA9tn1mb5rtmUk+N0wbFgx2JbsHp+ehHlsTjVqJa6TpxuUcl8U1KAoiLVM2eODVK8alO51zOkBVuce66kelRNVhoIgfAT4BJbipVxAABAAElEQVR4pPmATGKKY/Nm4P77gYcfdpZ+VNxQDvRJwHLNNSh76inf3z0+W/jYSNd4y7XXVu+wa72xpLonEo2pLxsJgkVjxsD2779ue2LrqZ1u7CkcPBip5G40DxsWW5OT2QiBOkIg0WxC91ZpWLOzoPqMskQgrGYhT4SAEBACQkAICAEhIASEQKwQEIEwVt4JmYcQEAIxQyAhwYyzzz7a8fCY1J49wKpV/q0zroN5wZUfXNesS5fI5NhjYa9nT2D1aqC01DUT5T8zMpzzVdKiVStg714lRwY+hkVGVhBccYjq07D16OijXVvkpxAQAjFC4L331ImDrmnn5wOsafFDSYlTVzv5WZuAsW1bJJD7reLDD2vvVLglYcIEGDt0UHi0usPs5AQvGj7ckUpUXcsoHH3wIIpOOQUNyPVoGjgwChOQIYVA3SfQJ72hh0AodQjr/nsuZygEhIAQEAJCQAgIASEQfwREIIy/90xmLATqJIHDh4vx9dfzsX59LgoKSpCamoi2bZuToewYpKfHQDqinTuBLVvUsWcRraLCKdxFohAXp4zjol9r16pz+LFzkBaeFQc7FdlFyGlBtUQWuTC96xlu2CACoRam0lYIhIHA+vXA77+H3jHfW/H118AVV4Teh7R0EkiePBk2usZbFy1SjYSFsGSuOegWBk6LvXGj25bQn1b8/DPs7jd9hN5VZFrSTSpFZ5yBNPqAG1JTIzOmjCIE6hGBzIxG+GrZjuozziYHoYQQEAJCQAgIASEgBISAEBACsUVABMLYej9kNkKg3hFYs2YbJk+eQXWtZlN2zJJa53/bbe+Rk+8o3HDDaRgxom+t/RHZwG5AteKga2LkUnC0ZSdhJMJCKTxZJNy1C+BUqIHchE2aOPP+Kak76D53diump2tbVGbhlN2O3kG1qySEgBCILQK//qp9Plzqjsxr4FKnEqETMNDNGam//IKis89WVT/QRKk0U777rpYQZh4xIiSx0dcZxJU4WHUC9rw8VEybBq7LKCEEhIC+BDIzGnp0uGHPYZSUW5FsMXlslxdCQAgIASEgBISAEBACQkAIRI+AMXpDy8hCQAjUdwKvvfYT+va9GVOmzPApDjIfq9XmcBaecML9uPrq18mQVxlZbHY7sHWrtjFZqCupLX5q6zRAaxMtvHDK0P79gV69gJYtARYDWQhs2tTpFhwwwLlPrTjoGpbTp3K/oQQzZcXBlxjIc5cQAkIgZggUkOFj4ULt0+FL4Jw52vuRHqhULF3PU2fORNIzz8DQvn1AJAZyiCdSftfU336Dkd2CXuGoR6ik9qxXO++XBv5uidMomzIFdv5ekhACQkBXAj1bN/QobW2jX7N1u+hLRUIICAEhIASEgBAQAkJACAiBmCEgDsKYeStkIpEmsIdyni1evNjxWEJ1z/ixf/9+xzQuvfRSfKihxk+kzyUex3vyyS/xwAOfqJr6u+/OoveoAF99dTdMkRKSeHVcD3GPHX2dOqk6X80H86IvC4P80Du4786dnfUY1dQjtFoBWqjGpk2+Z8TpSyU0EbAT472U6i9v6lSUkLhtJSHWTFyT6fOXTte25iefDEMkUt5qOgtpHCsE6GsSlTrdlzF/PnDSSbFyZvE9D0NiIhLvvhuWO+5A5YwZKP/4Y9hzcmCn7ywD/b4bqM6g5ZJLYD7tNBjM/v/cN9J1wTxmDCrpmqEl7FxDNk7DtmIFrEuXwjx4cJyegUxbCMQmgdREMzo3T8WmvVRzuiqyKM1o//Zh+LvUNYD8FAJCQAgIASEgBISAEBACQkAVAf8rBqq6kYOFQPwRaNWqVfxNuo7M+PPP/1EtDrpO/dtvF+Luuz/C//4XoWJWLOzpEVyEi50e3sImpwBlAZKFMxZtaNHXUd9PB0eHHtMO2AfPt2tXYMcOoKwMaOiZSqpWW0rlhnnzgNzcWruqN7DrUSIkAlb6LG19+WVsf+MNlG7bVquPQ6T07PriC4dQ2P6GG9D+pptg5JS0EkIgAIH8/AA7Ve7Ssy+VQ9fZww30nZJw+umOR6gnmfjgg6icNctZMzeETgxUw9bO3wNxHNbly0UgjOP3T6YeuwS4DqG7QLgmL35vJohdyjIzISAEhIAQEAJCQAgIASEQOgFa3ZUQAkKgHaXgGj16tICIAAErCWEs8GmJl176gTSpfVq6UN5WrxVtFgBdKTVtNmAfzT8rC6BFSaxdC6xfT3mX1gGrVgErVwI7d+pn21F+tuqPZCHzqKMAquGEH34AyMHiEDz5HNl2dPiws9bgp58CX34ZWByk30P07Kl+DtIC5VQncylZszbce69PcdAdUQnV0/yPXEfLyDVUEceuH/dzkufhI1Berl/ffB+BROwRMA8diuT33gtpYga62SrxtttCahtTjbhesIQQEAK6E8hMp/T2bpGVKylG3XDIUyEgBISAEBACQkAICAEhEHUC4iCM+lsgE4gWgYceegiDKZ0UP9hNmEPCRqdIp4CM1slHcdwZM5Zh27a9mmZgI/Hp7bdn4rHHLtLUT9DGLOqx0KVXVFQ4BbP//gMCrbqzo5BEHEftQ07jGWqtP73mHayf5GTguOMAdqBs3hzsaP/7KfWlR7Ea/0fKHjcClSQ8L6MbHAqWLXPbGvzpgT/+wHJKPzjo999hSkoK3kCOqFME8ukak0XC/qHt21FBnyELpaVs0qUL+k6YgLQ2barPlX+99YqUFL16kn70JmC5+GIgIQEllIY44PeT28DGbt2QQulNrZSiPe5D3NRx/xbKCcQmgT7pntkl/tt1GBVUXzzBJPcpx+Y7JrMSAkJACAgBISAEhIAQqG8ERCCsb++4nG81gUcffbT6uTyJHIEpU2boMhgLhA88MB4WS4Iu/UWkE3YQbtyoXHRkcZKPZ2ExIyMiUwx5kLPOAhYscAqgoXTC7sHhw0NpWe/brLnuOtXioAvaQUr5uu7mm9Hn7bddm+RnHSZgt9uxaeZMLH79dWwgYQf02jv+uOce9Bo3DkMoBW37Y48FZY/ULdLTdetKOgoDAcsFF8B0xBEof+UVlH/yCVBc7HMUQ8eOSKTrjoUehkaNYOMbWuI8DLF+I06c85Xp118CfbwchOUkDm7YXYjeXsJh/SUkZy4EhIAQEAJCQAgIASEgBKJLQG7diy5/GV0I1CsClZVW/PYbpc/UIXbvPkiZOMO8KMk19vSsBchpQ0NxJG7dCnANw1iO5s1BuWOdNRTVzrNpU4BSYzrqL6ptW8+PL6aF+Z2ffaaJQu4HH6CMP5sSdZqAlW40+OHKK/Hpqadiw88/+xQHGYCNUgNnU63KD4YNwx/33Yf+R9pAGpAuMWqULt1IJ2EkYOrdG8lvvYWGVCs26bXXkECOQvMZZ8B87rkOQTDlxx+RRjeuJNL1nsVBDjOnmW7QIIyzCnPXVPvXzA52CSEgBHQn0CglAe2aelrRs6QOoe6cpUMhIASEgBAQAkJACAgBIRAqAXEQhkpO2gkBIaCaQH5+Iax057BesW9fmOuYsDjIi55cR0+P8OHWUdwtOzRYSDPH6GWbhU9OU3neecA33/h1ntQ6X3YOsjjIAqOEagLb33zTr9CjtDM7CULb33kHXSntskTdJGCjdMlfn38+1n37raoTnPv00yilOpUjR76Ob7+l66GGoEzeIHOaRJwQMDRujMQbb1Q0WwOlp7VccgnKp0xRdHysHZQwfjyM8h0Ua2+LzKcOEejTphG2H6D0+VWRnXsIGER//0kIASEgBISAEBACQkAICAEhEHUC4iCM+lsgExAC9YeAnuIgU2NHYqAoKipFdvY2zJ+/FsuXb8LOnQcCHe57H69qx0JwPcS92mo3hu00eG7r1gE7djiFPlooBrmPAtqOOJ0b17wiAULEwdDeGRvVscx9773QGnu12kGOIXso7lavfuRlbBL4ndKGqhUHXWeylESf5js+4PJ0moKMi2BTtkTdJGC5/vq4PbF4nnvcQpeJ1ysCmRmedQiz8sJ8g1+9oisnKwSEgBAQAkJACAgBISAEtBGIUSuKtpOS1kIgFgjsYLEkQOyshyn9GjdODUBE/a4mTXynNFu2bCO41uG0af+gpKTco+Nhw3pj0qRTcc45x1BGSwUr3uwqyMlhNdKjn6i82LULaN1a37SnWk+EXZH//QccPFjTEzsJBw0CBg4EOD0qf9ZLS53zTqY0U+3bO8+D1QLebrHUtJVnigmUkKu0Yv9+xccHOrAsLw+llFIwmR2dEnWKQCFdNxa9/LKmc1rxzJ249uuJeP2N0H5XhwwBTjlF0xSkcYwTMFKdXOPQobAtWhTjM/WcnqFDB5g4RaqEEBACYSPQJ8MzT/XanQWw2uwwGbU508M2YelYCAgBISAEhIAQEAJCQAjUIwIiENajN1tONbIE2slCey3gSUkWSjHXEf/+m1Nrn9oN3Fffvh08mm3fvhcXXfQC5sxZ47Hd/cXcuWvAj5Yt3yUR8XqMG3eM++7az1nESk93Cl0s0BXQXc/k3HLYaRrSHdFt2kROsCuh9EzFxUAqCa0szLFzz2SK3Pi16Thdg+7ioPsxnKK1Y0fnw3276zmfAzsPWUjk85BQRaAiP1/V8cEOruT+5LoVDFPc7V/+7ruOuoJaJl5y4AAabpuGa665FJSN1nH5UdrfgAHAzTeLe1Apr3g7zrpqFcqoVmEF10Ll76g4C/u2bbDTjSwG/q6SEAJCICwEMtM9BcLiciu27CtC15a+b/QLyySkUyEgBISAEBACQkAICAEhIAR8EhCB0CcW2SgEhEC4CFx33Snk4HtTc/cXXngcGjeuWVhYs2YbTjrpIeTlKUsjumfPIZx77jN49dVrcNNNY/3Pp6gIoAVQ/PwzsG9f7eOoTpOjsFbv3s4afN5HcF6+igrvraG/Zmcq10RkkdIViYlAixYAp0Pl55EKTknJoqmWYGcmp05lZ6SEKgIGnUVVQ6zWt1RFRQ52J2Cj369llD5Wj1gyeTKuXnyp41IzdSpAukrA4PsYOK3ouHGi/wcEFac77XSDR9kzz6Dsvvvi9Ayqpk3nUU6/I0mc7lpCCAiBsBBokZaIlvTYc7isuv/svEMiEFbTkCdCQAgIASEgBISAEBACQiB6BEQgjB57GbmOE9i+fXvAM+QUo0M471o9i4kTR+Cuuz5CYaE2p8GkSWOqyeXl7af0dY8oFgerG9KTm29+m0rgpeHCC4e7b3Y+X7gQeOONwK4Ids/98w+wYAEwahTQs2dNP5xOs2tXYPXqmm1an/lKKVlGCy4sHPKjWTOgU6fIpO0kV5Eu4ieLjCxusuNQQjGBBE5/q2Po3Z+OU5OuQiSwj9L/FgRJd62067wlS1B66BD69WvkuCeCMwvPnAksW+bMFMz9sOG6c2fQzRrAMcdE9n4Fpechx+lDoJTqWpY/95w+nUW5l3Kq5Zr42GMwaC20GeXzkOGFQCwTyKQ0o3+u21M9xWyqQ3jmkRnVr+WJEBACQkAICAEhIASEgBAQAtEhIAJhdLjLqPWAQNu2bevBWao/xbS0FBLlxuKpp75S37iqxYkn9qMSd92q2//f/72H7dt9uPuqjwj85OqrJ5PAOBAeNQ1/+w2g1HyKc+mxS/DXX50r5Vx/r2VLgD8DkRa9WEAsLATY0cgCZThj9259eue0qTzntDR9+qsnvSRTSrxUEqSLOE2rxkjr3x+J/JmVqFMEin25njWcYQldX5IaNXJc1vheCNf9EGxoZjMwX3IifcnzdTqcvZi0TMdlhefFc2KDbJMmQEqKrxayTQ2BcsozW1fEQT5vO7nYbVTT1dS9uxoMcqwQEAIqCGSmN/QQCLNy6SItIQSEgBAQAkJACAgBISAEhEDUCdC93hJCQAgIgcgSePTRCTjtNBLRQohu3dLx+ed3Vrdk9+D06fOrX4fypKioFB999EdNU7bEqBEHa1oCf/0FcOpNEm8cK9JsqbFY3I8I/3N2FGZne6YhDceoLOzpFXr2pdecYrwfA6ke7a6/XpdZtr/hBl36kU5ii4Cdr0U6hr/++BLHwlu0xUEWA/PygBUrgDVrnGlQ+XVurrOM7MqVQFaWM1u0zmh0pBzbXdnp+6X0/vtje5IhzM7OjngJISAEwkagDzkI3YMFQk5VLCEEhIAQEAJCQAgIASEgBIRAdAmIQBhd/jK6EKiXBMxmE7766m6cc87Rqs7/iCM6YvbsJymLZsPqdm+/PRNWq/ZF8ClTfiFdj/qxWkMXB12zonRlDjsNv+YV82g4s9jSs3Gja0bqf7Jgt3mzM38gp1qdPx9YvNi56s4uRV7UYVZ6Ba/sS6gmkH7JJTBptESZqY5mmwsvVD22NIh9AslNm+o6ySS24MVosAmZhcGcnJqUp76mWlAArF/vFArdS7n6Ola21SZQMX26w3FXe4+6LUZ2ucdQSHrRGHozZCp1kkAfchC6R0FpJXbkays34N6fPBcCQkAICAEhIASEgBAQAkIgNAIiEIbGTVoJASGgkUByciK+/PIuTJlyHaWpaxuwt1atGuOBB8Zj7txnkJFBNfbcYurUv9xehf50w4Y8LFpEq8bsHvRV509N15zbbtGimhZcXy8awfUR1TrzDh92rpyz1YZrA7Ib0WW1YRGP++TiY8xJzzu/2WkpoZpAAol7nR98UHU79wZdqfaWVpHRvT95HjsEmvfoAb1EvWbUl96Co16kWPRjZyBnelYaLChyGxEJncTYyWMlIBWUXrvixx9RSbV1bXv21MJZPmVKrW2hbLAXFcE4enQoTcPSxqBzTdewTFI6FQJxTCCjcTIapyR4nEF2nqQZ9QAiL4SAEBACQkAICAEhIASEQBQIyIpsFKDLkEJACDgJmEwmXH/9GEoFN9nhDJw4cQSGDu2OXr3aUY3BrjjjjCGOdKLbtr2Hxx+fSCXqKIedW/CC5tattRcw3Q5R9XTbtr3AzJmq2vg9eNasml2JiSDbY83rSD5TUyeQ65XxijmvtgcLXlXXUyCMdBpWf+fH58TnVkJ3tet9jv7G1Li90913I+Oqq0Lqpf3NN6P9jTeG1FYaxT4Bc1IS+l9xhS4THUTpbDmtbaxFaSnAZThd9zGomZ+WtmrGieVj7XTTR9mrr6KQHH2FffuimES74jPOQNHw4Ticno6iceNQ+fvvjlSAdrqBxDpvni6nY9+61VHzz9ivny79aenE2KsXDO3ba+lC2goBIRCEAH9/ZKZ7pxlV8PdmkH5ltxAQAkJACAgBISAEhIAQEALaCJi1NZfWQkAICAHtBHjRYMSIvo6Hmt7Kyip0SS/qGrNwH7njVq92vdT2c+1ap9uOHF6O6NwZIMcEeEU6ksEuQJetxkyX/AYNnGIlibMekZ/vzLvnsTFCL3guLk4RGrLWMPzeMKu9JBK7Kw3sbGzRAmjdGkhNrdUsFjbw70+ft95CAqV/zHn+ecVT6vLQQ+jyyCMxKfooPgk5MCiBQdddhwUvvBD0uEAHmJOTceSllwY6JGr7duyoyegcyiTYScimcf41r29R9uabKL39dv9Oc0ojXfnNN46H8YgjkPzSS7oiqnj99Zr+WHzW86aTmp6DPrPEqPgddOJygBCIMwKcZnTuRroZrSqyxEHoQiE/hYAQEAJCQAgIASEgBIRA1AiIQBg19DJwtAnMnTuXSrTV1Gjbx+6pquDtH374oeul4+dll13m8VpeRJ9AYmICuJ5hZaU+tfDSjFSDUM/gdJwu4YvFOXLzRFwg5AVXt8+24/S2bHHWRWzTxjkndspxUa5oBa/MewuWkZoLOwU3bfLvmmSxkF2Y/EhLA7p2BUgsibUwkJDZ47nn0IrcPtsnT8bOL76And9XrzDSZ5DrDba74QY0GjjQa6+8rIsEmtJntu+ECVj92Wchn96Qm25CkutaFnIv+jfkrMfel7dQRuF7A+qbQFj6wAMoe/JJxbhs//6LonPPVXy86gPdxUGudUnXWa4LaHelulbdocIGVMPVQrVcJYSAEAg/gT4Z4iAMP2UZQQgIASEgBISAEBACQkAIqCMgAqE6XnJ0HSLw7rvv4qOPPvJ5RvMohRY/3EMEQncasfGcnVOdWqRiw059UhR1bOO5cKH5LN0FGrapsGAYC0GuEOzc6XTMUV0xR51C3hatYC6c2pQWasH1GiPl1ON6i+z0ZJVBSfDx7DCldHQOsVBJmwgf03joUPCjBznGdn31FUoojZ+VPntmEjeTO3VCq/POg6Vp0wjPSoaLNoGxb7+N/Rs2IG/JEtVT6XbaaRilQkhSPYCGBlwiz93wG2pX/KvNJuJIXXpCnade7crIuadGHKwel53mkQgaJ4FSnSaTqM0Ox/KXXw7bqIlUw9XQSOfv/rDNVjoWAvFNIJMchO6xr7AMewpK0bIh3UAnIQSEgBAQAkJACAgBISAEhEBUCIhAGBXsMqgQEAK6EFiwAJcOTccD32kXCLnu4eCjegKhm2xqnxILXq5gF0SsBTs2uHgXp3aLZnDaVX5w7UPmxE49qn0V1rqN7BxUIw66+LCYuGYNQLW6HIKma3uM/bSQHar9pEnRmxV/tli54Z/sDo32Zyx6JGJiZAspXxdTXdQvzj4bOX/9pXhOvUlQPvvjj2FkB3QMhp73XLD2VR8EQhvdHFJ6220x+G56TqmCXNCOuoDsJgxTJFDt1kSq4SohBIRAZAh0bJaKVIsJReU1N6VxmtGRIhBG5g2QUYSAEBACQkAICAEhIASEgA8CVFxJQgjUTwKcQtROi9dKH/WTUgyf9aFDAIk8V15xIhLM2i9lV189GoZmzfRLH2mxAM2bOwFyDUA98uCF6+1gESeWgu08//0H5OSEpyYVny+5qRQ7B73ZsNuS28caN+95Rvo182CVhYXXhQuBRYuAxYsBEvIdzkvv+o6Rnl89H49ThE6cOROnvPIKmnXvHpBG6/79ceYHH+Dczz+HmVMjx2i4yqvqMT09+9JjPuHqo/ydd2rq0oZrEJ36Lf/f/1D+/vs69ebZjWnYMCRT7VbORCAhBIRAZAgYjQb09nIRZuVqv8kvMrOXUYSAEBACQkAICAEhIASEQN0kEJu3hNdN1nJWQkAI6EWAc8FlZzvq57WmtevxI3vh01n0WkM899x0DBrUFccdfzxAi+iagxYfHTUHuSN2xomYpB5pXp6Dm61DJ7BmyBlbGSMb0tjpE1S34IPZmcgr/+xmYxcU1w8sLgY45auW4M8gT6qhZ7osLV3GdVsWwCmdKcrKfJ8Gs+IH17/MyHA6RGVh3jerMG410Y0LQ2++GVxTcMuff2L1J5/g0PbtKKf3JpE+y407d0b/yy9HBqWpjQfhRM/Lqp59hfEt1NS1na6F5SSKxU3wm8LXlTCElW5i4PqGBnarSwgBIRAxAn3SG2FJTk264mxyEEoIASEgBISAEBACQkAICAEhED0CIhBGj72MLASEQCgEWOjhtJhu8eLNI7EgKxeb80Kv8bdr10GceOKD+PqNq3C6W98hP6X6SdVRX6wp1Sesz5NSJGL3zgTs3mNDpbW2S5TLRrVuDXBJPQ+tidOAsluN05VyKlH3MFI/7O7UI7h/EQgBEpgcDyVM+b3hBX8WWLt2Bfj9kIg4ARb/Oo8a5XhEfHAdB9Qz86mefel4irp2ZZ0zB3a+8ULC4SAvp1rUSQ89JDSEgBCIIIHMDPrjzS3EQegGQ54KASEgBISAEBACQkAICIEoEJCVuShAlyGFgBDQQICdSl4upZZNUvHri+PRobU2N1d5eSXGX/c2FlWQPU1L9O4NkBOnXkdCAtCtG9CypWoM5BnBNrTDcgxALtr6FAe5U84yy5lIV6xwmgId9kJe/F661OlU8xYHuRELzOwq1CP27wc43Wh9DubNAqHa4N/jzZvFWauWmxzvQUDPmoENGnh0XSdf2Hbs0O+86oC4X/baa2BXpYQQEAKRI9DHK8Vo7sES5BdRiggJISAEhIAQEAJCQAgIASEgBKJCQATCqGCXQYWAEAiZALu2fES3dk2x8O1LMGpQBx97lW8qrbDhur8OkNbEMlUIwXY2St/nEfXBmuJxwvSCF115AdlLzPU+zPs1U9+IrthBAiH5Ar13+3zNet/q1XYcXk9iFdctZBEwEsGfEZXnF4lpRWwMdgEy71Bjz57Yrs0Z6nlJu4gRYAexHsGm4iZN9Ogptvuw8++sXkHXWeNJJ+nVW3T6oRsVyt98Mzpjy6hCoJ4S6NqyASxetcOz86QOYT39OMhpCwEhIASEgBAQAkJACMQAAREIY+BNkCkIgXgjYCXX1P79BZTBMZ/MWLXv+uX9Bw4cRl7efsokWBq62OYNhhc3A9SOa92sAS4YRe49jbHyoB0LD4QgEDZvDjzwANCsmecM6oM1xfOMna8OHHDa/Hzt87NtG9pjL9S7Dq1WA9bubwlOSxrRqM8Owp07taPWow/ts5Ae4pQAlxTlVMNao1UrrzTFWjuMQHvb7t0oe/11lNx9N0puuAGl99yD8nfegf2g/1TbBj1TIrN9k+u5xnkwN+uGDXF+FjJ9IRA/BBJMRvRqneYxYalD6IFDXggBISAEhIAQEAJCQAgIgYgSkBqEEcUtgwmB+CVgI7fAb7+txJQpMzBjxjJUVtakVszM7IBrrhmNLl3a4OOPZ+O77xaSsaombVenTq1o/8m48sqT0KKFhtXcIO4Hdv1N/maZLpCnbKzE0c3IVqIkTCZg6FDg0kudq8zffOOsk8hiJhfHS6OFkBYtgE6dgKQkJT3WjWNUpvIsRrIjpWioJ1+JBGxFB/TA+lC7UN+uDqTZU3/S1IJrCXKaUK3BvyP8qK8iulZ+0h4ZGarvQ/CgxpdvFgjjJSrnz0c5CYMVX3/tdGp7Tbzk1luRcNFFSCTR0NSvn8deI6d91ikMdEOMbd48nXqLYjckcpbedx9Sv/oqipOQoYVA/SLQh+oQrtpBeeKrIkschC4U8lMICAEhIASEgBAQAkJACEScgAiEEUcuAwqB+CPw/fcLcfvt72PTJt/pPbOytuLmm9/xe2JbtuzGvfd+jIcf/gwXX3wCXn75KtIDyPqhNliUCBAbtudj5QZKW6hDfLXDhvcH25Fg9JPmkoUhrq93/PHAqFEAuzY++ghYtMh/XTpONdqjBzBoUD3JZ6fOhbkb2lfpD6ApykkotKBGoNbh4+C/C85NWB+DxUG9UrmSE0oEwvr4IdLnnBs3Btq3p7ql29T3x/dv9OwJxMOvsZ3cyqW33YbyV18NfKIkeFWQk7Di3XeR+NRTSCSHoYFPlMI0eDCMffrAlp0duA8le+tQ6uzK6dNRuXgxzEOGKDlzOUYICAGNBLzrEGbn1oiFGruW5kJACAgBISAEhIAQEAJCQAioJCACoUpgcrgQqG8EXn75e/zf/72ny2mXl1fivfd+w4oVm/HLLw+TvkYru2qiapHTX5PcvYf97VK9vYzK2OVT9tSW/gx/p50GUjud/c6dC7JWOl1VgUZigZMXZjmd2YkngnKvgnK0OoUWdha2aQPwanddCRXuOiuM2BNCalFvVHbqh4XGdlTFMOzBRcsSEsI+TEwOUFKi37RUOk31G1h6qisE2EXIoUYk5MsTi4N6pCh1jh6+f+0kxpeQQ73i00+VD0LfL2X33uu4eSXpmWcc7VgotEyahFJyF2oKyu1q37RJUxcx1ZhYFZ96KhqsWwcju/0lhIAQCCuBzHTPbCKb9xWhsKwSDRJlaSKs4KVzISAEhIAQEAJCQAgIASHgg4D8Fe4DimwSAvWZwNq12/HWW79i6dKNYOdfXh7VkQsQXVINuKazCUObGdGYtBIraV57y+z4ZZcNH+VYcdCHkWv58k0YO/ZxzJ79JFJT/SlwPgYNIsYUu6U19dFa9aZiPhn4cRDOng2cfz5ArgMEc3R4j1xOyuOMGd5bna9HjgSOOML3vnjbqsJhchCNYYU+X0n70DwyAmHr1vH2jug33yBuXlUD6dmXqoHl4LpCgO8dadsW9H0C5OYCBQX+z4yP5TKxfHxKiv/jYmlP2aOPqhMH3SZf9uyzMHbtCstVVzm2WiZOROn99ztd727HqXlqOvJIWBcsUNMk5o+1U83ckmuuQeq338b8XGWCQiDeCfSgGoQmytBhtfHf2c5Yu7MAgzs2db2Un0JACAgBISAEhIAQEAJCQAhEiIA+q7ERmqwMIwSEQPgIzJy5HM8+O51Eu9WKBhnRwoi7e5pwSmsq4OQjTqbtT2Wa8dk2K55aZ8WWoppFAD58yZINePDBT/Hii1f6aO1nE7vr2PbhJ7Vho9REPw1D29wowY84yN1x3bQffwQoNZmvOFBux9x9NuwjsZTXP5paDBjS1Ii2KQH65DpslP4t7MHOxR07gH//BfZQSlZ2cHEhLnKFOOok9u2rj62GUuI5ajDyeEGinJKC6hUVlGI07JFIn7W65PZUC4w/L3qFnn3pNSfpJy4JsKmXH5RlE7t2OS/TrD+zKMj3K/A+zgwdDylFXW+Abe9elFU5AF3b1P7kGnsJ5Hg30HXL0LAhUqZNQ/HYsf7TYQcYwEh1DY2UirOuCYR8ypXffw/bli0wcr1gCSEgBMJGICnBhG4tG2DdrprMH1mUZlQEwrAhl46FgBAQAkJACAgBISAEhIBfAiIQ+kUjO4RA/SBgJ/Hmqae+wgMPfKL4hJNIo3u8jwnDWgQWCVLMBlzV2Yxz2ppw1rxyzNnnKRRxutHHHpugvB4hr/A2b+4UtXzMtmvbJkr1KB+tPTe1IP2nUTCd6c8/a6UVXXLAhimbrPichNFSSlPqHoQNZ6QbMamrCaNaGmHkVWv3yMx0CnXu2/R8zkLdmjUgeyjlT82v3TOvqu/f79zPC6THHANoSbfGAiFbdbheXZCwUWpQvULPvnzOid+3bt2cqoPPA+rBRj0VliDO4HpAU05RZwLsDOzcWedOo9RdxfvvA+w61xB2Ehkr6GYWy4QJjl4STjkFyVOnOtKWosKHzd/PWEa6eSSV3O9lzz/v54g430zfkeVvvQVXStY4PxuZvhCIaQJ9KM2op0AYwPod02cikxMCQkAICAEhIASEgBAQAvFNQL8V2fjmILMXAvWWwBNPfFFLHBzUxOBw/703yIxPhiTg9f5mXN3JhLSqWwpY+Br9TwWWkhimJNg9N/N4C4Y29RTECgqKMW3aP0q6qDkmQFrH1s0aYOwxXWuO1fDsSjrfWgKed39uwldRpR3j5pdjyB/l+JBSq3qLg9yUaX2XZ3OwO/bPcuwqdRNM2RnJzr1wBbsuOS3qb7/5Fge9xyUXBb74ApRn1nuP8tcsEAZ4v9w7MoNsPjqFiZKVhi1YHOzeHSAXTr0OFn71Chb9JYSAEKhFwE7X0LI336y1PZQN5ZMnezSzXHghUun7wMg3pgQLujmHHYgN5syBMT0dhjrsni4nQZZrPkoIASEQXgKZGZ5/R2XnHQrvgNK7EBACQkAICAEhIASEgBAQAj4JiEDoE4tsFAL1g8D06fPw0EOfOU6Ws2le3tGEJaMsWHJiIu7tZcYVncy4qIMJN3Q14+1BCcgdm4jJJBb2SDOghNbPjv+rHLP3KBNjkk0G/HCshVJterJlF6Gq4DScAVxtk84ZoKo7XwezjHkt1VUMGlWpMw9X2DHy73J8k6t8UXHhATuOJjFxW3GVSOgqoBV00BAO4Hn+9Zczpaia5pybj9Oobt+uplXNsZw6Mi3NmdevZqvPZ8ko8bk9lI169uUxPrvmevd2uiI9dtTDF5yOVg+RgJk2lZpD9fATJKesgIBt/XrYc3IUHBn8EOv8+bB7FWc0Dx+OBpRqOvWff5BwwQW1cq8a2rVD4mOPIW3bNqR8/DEMjRo5BjINHhx8wDg9gt2WXI9QQggIgfASyMxwXk9co2zYU4jSCmX/p3C1kZ9CQAgIASEgBISAEBACQkAIaCcgAqF2htKDEIgLAhs35uHUUx+hOkxnURrOMxyPc899tnruxzU34DISCAeSe9BfpJGKOInEwn9HWzCxvREl9P/4kX9X4PplFajkQntBomWSAezMc4/16/PcXyp6bqfccQftVXZGrxajh3RCzw7a3E1nZRjRMVXZ5dFK4tt5CyqwmAQ/tZFD4uCYOeUoIIGR8qyqba78+HXr1IuDrt7ZSfHTT0BJCAJeUpIzDSc77lJTXT36/JmGw0gGpTjVIVphd2i9sOjlK3hRvEcPYACJz1UL5L4Oq3fbFLpDA3Jp1ap+p2oNCEd21ncCNq4Rq2PYOYW0VxjIFW0+7jhHXcKGRUVII4GMBcGGJCambd2KpAcfhLFNG49W5pNPhqF9e49tdemF/ZA4merS+ynnEpsEerVp6CgL4Jqdlf4f8Z9bTULXdvkpBISAEBACQkAICAEhIASEQHgJKFsBD+8cpHchIATCSOCPP1ZSlsdLqGTadfj11+WwWn273P7ca8dwcgRmzizDzzsD38FrMRowdagFV1WJfW9utmL8wgqwWBYsriNnnvuFh9OMqom9ew9h5IkPodUJT2Pqr1m1mhppbl8+fiYaplIRwRCic6oBbw4IVnywpuOvd9gwc7dvpjVH+X+WXWDHyxuIN9dXDEfwe7Jsmbaey8qA7Gz1fbRs6WzDTsI+fQKKayxLt8Yu9WN4tTCjAs1QexHc67DaL5l/v37AwIEAp9zr1cv5k1/z3DmlJqeBlagh0KSJNvcfi8aUrlBJVFIK32++KcX48YeoNOYBHHnkAQwfno9rrinAokUV4FqqEkKgrhGwLVmi6ynZ2RUeIAx0HTRSyl8jOQcN5P5m8dBXGOiabrnuOl+76sQ2AxexlBACQiCsBBokmtGpGf0d4BZZkmbUjYY8FQJCQAgIASEgBISAEBACkSEgq52R4SyjCIGoEHj00Wk4kcS03bsPKh5/zWFg7NwKvLoh8EIid/jGADNOaOG8jHxL6TVPIIFxX1nghfrODYwY3brm0pOaSi4zhZGXt5/EgbsoW+ZqlFMaokse/wnDrv8En83Kdrx2ddO3S0vM+N95aJKmvG9u24XEwVnHJ4CdjkGjSiyavDE4p2B9vbWpEpVl5cEOC23/zp2AW63E0DqhVpSGDmrqMnHqSHe3HQtwnJ4zQP2+FtgLFvi0BIuMRgT+DPrsn91w/J4mkrDM9QVZvFq5EnjlFeC++4A77wQefhh4911tdRl9Dh6nG1k86NYttHqM7C5lEZbF4wBRWGjD448XoWPH/Rg3rgBffVWGBQsqsWpVJf6hOqjvvFOKo47KJ103Hx99VCJCYQCWsiv+CFT8/LOukzawqK9TWK6+Goa6mB44IQEGPWus6sRbuhECdZFAH680o9l5BXXxNOWchIAQEAJCQAgIASEgBIRATBMIk2Ulps9ZJicE6gWBJ574Eo88Mi3kc71lZSUa0BWC6xD6CzO59R7tY8ZsEgY55uyzI/3HMlxA6Udv7WbGgCY1QqB7H0ObGvHrLqfrLiODnFkKorCwBGPGPIaNG0nwcot5/+4AP/7v1T8w9tiuaNU0FUkWMw4eLqVUo02xICt4ClMqj4hz2xrxWv8EtEhUIA7y+OSyWL1hl+Oc3aYT0tO8UmK3YT9OODWk5oEbrV4deL/SvVy7itLOkVKjrAULgt7uE35N3BwP7o9diW7OLzOs6I71WIteJPH5/uwEGrwRDqItdgQ6xPc+Tu+akeHcV1gIfP898OefwGFSy71j7Vpg1iynMHbaacDRR9c+T+82dfk1C3ws/G7eDChNh8jCMQvFtBAfKPLyrDjttEOk0wYX4VesqMRllx3GH39UkIabBotF4e9xoAnIPiEQRQI2qv1q/ftv3WZgpNTcegpf7DRM+e47FI0aBVRou7FDt5PUoaOEc86BgW9wkRACQiDsBDLTG+LHVTV/p2fnHgr7mDKAEBACQkAICAEhIASEgBAQAp4E/K/8ex4nr4SAEIgjAn/8sQoPPviJ5hlfu6wSw5ob0T3Nv1hzHDkI+zYyYPUhp2uLy+lN3WrDp1vL8Wp/M27oWvsy08Rt7W3ChOMVzfPll38g19AWv8fuyS/G+z+Ryy1ApKUlk+ZTU0uvHZWcu4LSpF7d2YyMZBWCArvMRozA9NmfBhhN+S4WNIs6k2DCDj2901hSPSndgvvq2FFZd8WUOpZTkzIrX8Him5s46DqkMQ45RMIN6AYbArvLXG34J5v+ejQqhnG783Povi/gc3YKupxsu3YBTz0F8M9gsWED8PLLAAuGl1+u//sWbPxY2s+f2a5dnelCd+92CoVWrzTFLnGYnZosyHqLx17ns38/OZJPOIj167368TrO++XUqaVULtOOL75oSL9KKn6nvTuS10IgygQqvv7a5zUy1GlxSlB/KUND7dNRu5BcjsWnn+683ofaUQy1s0yaFEOzkakIgbpNoE863TDkFmupBmEFlUJIMPn/f4fb4fJUCAgBISAEhIAQEAJCQAgIAR0I1F6516FT6UIICIHoErj22td1mQCV/cLkjVa80j/wf9QndTHh+uWeLh+SunAjuXrK6Mlt3T0vNUXccVVMmTIDzz//LWkGSejQoSUuueQEXHTRcKSl1dQAqqy04q23ZrqahPzzqKN64IcfHkBRUSnStm6C5fnnQEUZ1fXHwsattzrq0u2873Nqq7K912hNGybhx+fOxTF923rt0ekli3R6hdq+DhwA2rTxPfpB/2lvm+EALMhGDjriMEj5CxAmVKJVWgna904jQSgdSCb1OScHKHe6Wv02dQlW5KpxpLnkNKycQjQ/328Tnztm0ueSP0OUbi+Y6OWzfV3ayHW7OnUC2rcH2InpchWxm5SF2CCOQRcKrid4wQWHVIuDrvZff11GOm8xHniAxpQQAgEIHM7KQt7UqSjduhVW+syaqO5eMn2G0y+5BA169vTb0kopl8vffBOV//wDO18z6HrC7jwzuelYiDOxQ1Zj2HJzNfbg2dxOv492uu4aGjf23KHxVcJJJyGVaiUWn3wy7JzSOo7DSPVmTccdF8dnIFMXAvFFoA85CN2jvNKGjXsK0auN53b3Y+S5EBACQkAICAEhIASEgBAQAvoS8Fy117dv6U0ICIEoENi6dTc2bSIXj07x0VYrnuprRqrZvxvnxFb+BcTbqVZYjzQDTmtjwmGyFz6yptIhOrqmt3OnU5ApKChGXt4Bqi+2Dnfd9SGlCxyFxx6bgMaNG+DHHxdjxw4ScDTGb7+tpCyZeym7IaWTbNYfuOce4IUXgFLK8akkOO0Yi4MDBzqOLm3RCtiwXUlLn8ewc/Cn58/D0ZlV6S19HqVu48FiMxqnuIm1nAJSr1Dbl0sg8jV+oH10fBoK0RdZKEIKVRVsjf1oRlKgMyWlgbyFKShGK+wG1y00pZErzZjmHIVTmHL9KBYn2Ql4yCtdFb+Hreh944crjRw7N59/Xr046Dqv3393OitHj3ZtUfeTP3/szmRRk8VG5szOyxYt/DswvUfgdtzH/v3OfvicWJhj0Y7PlYSPiAmYPH/3+pPecw3yesmSSvz+u7aUhS++WIzbb09BshpncJB5ye66QYAF6N3kztv2+uvIJ4HPV2x5+mk0HTkS7W+6CS3PPJN+dZzffxV0Q0DZ44/DOm9erWZ2EvTKWTh86SWYSTRLfOQRmI85ptZxijewC1vHKLv/fpQ9+SSSHn0Ulttvrz4nPYYw9+2LtPXrUUTnbV24UI8uI98HXXOT33pLVy6RPwkZUQjEF4EmqRZkNE5G7sGS6olzHUIRCKtxyBMhIASEgBAQAkJACAgBIRB2Av5X9cM+tAwgBIRAOAiwuKZnHKJ1+q93kNgQIJoFqff1UFYlcottOG52OV6klIHsKgwUnAb0tdd+wrBh9zgEvS++mBvocFX7vvzSra9+/YBnnwW4hpK/VJjcOzufRowAnnkGGDSoerxGvbW5RJ6+friu4uD2fcm4+f0B2JlP4pIrkimPql7BYpOaqHQTKr3b+Ugv6n0Iv04lIbALNmMIluBozMdQLMRR9OiHf0k23E1JSOnD5N0XL+azSEhuEAwdCgwYAPQnQZjfOxZ327WrEQd5kJUrgS1b+FnowXULWZRTGjxndh6tWQMsXw5QvTFwek52MvJPrve4bBmwbl1tkdN9DBYVuf7f0qXOnyyIltBCG7s9i4qcoiG5pBznyP16s3LvK0aeT5lSs1AY6pTy8znNqELhP9RBpF3cEbDR78W/EyZg1fjxfsVB10kdoDqkK88+G1lXXAEb3dBQ9uKLKD7lFJ/ioKuN62flb7+haPhwlH/0kWuT6p8GDSK738FIdCy9806U3nADXQpqnPx+j1exw0Cpg1Nnz0YC8YrYzQgq5hfwUPqOT/n0U5iPPTbgYbJTCAgB/Ql4uwizpA6h/pClRyEgBISAEBACQkAICAEhEIAA2QskhIAQqEsEfvmFxAadY00BCx/kCvITwUp9LT9oxzASB3NUGiKys7fhxBMfJK2ngZ+R1W92ORarW3IKzGuvBSZOBNhNwmIKp0fk4LSIvXs7xUGum+YVvQeSQPjuH15blb1MS7HgqrEkUOoUm/ek4unveqGs0oTCUnbakUDE0aULoEfaNxbdOH2kmgjkEmR3m8qgGTgFQe92gZyNvC/Qfu6L04RqDXbvsdDnJiD77ZKFRBb19uzxe0j1DnZCulK1duzoufDODiMWGIOlU+XOWDTctAkoKHB+JvSudVk9YW1P8vNt+PxzfYS9N94owWWX6SiQazs1aR1lAja6YWHFOedg34wZqmaS9+GHSF2xAs1WrVLVDjReyWWXAUlJsJx/vrq2dLQxQIpT1Z15NSh/4w0YyG2d9NhjXnu0vTTQuaa89x5sDz6Iw5mZzpsUtHUZ/tb0XZT6668wk2NUQggIgcgTyMxohFlr6AamqsjOoxudJISAEBACQkAICAEhIASEgBCIGAH1K7QRm5oMJASEQCgE2H2nd7CLMFDkByn3xm3VioOu8TZsyCMdRaVzzdXYx8/i4irhzHsfi4Gnnup8eO/z8/rCC4+nNIbvw2+fftrx5otPyUQDEgm1ho1MILOzW+KjvzuhtMKEi4bloFubKoGTO2cX3YIF6mstek+MxcGGDb23Bn4dKHUrC66HDwdur3SvD/FWaVOHSMcOQj2CXENBBUJ27VAqPofop2ZMFnk5jSgLvizWMtvs7Jo6f0r7YiGT59Ctm6fYqLS923GsS3IpSZcOzDosZzLlX6VQY+VKqlvq51dUbZ+LF1eSRmOnTKssLUvUdwLr77pLtTjIzPjWkKZqxUE32CVUz9B05JEw9ejhtjX404Rx41By881OUT/44aqP4FSpleT4S6QxzGedBYPCGqFKBjJwHVKdU6QqGTekY0jINSm5sSOkzqWREBACwQhkZnj+bbmGUoza6I9bY7C7D4N1LPuFgBAQAkJACAgBISAEhIAQUETAqOgoOUgICIF6TSCFFv4Dxbx97DAMXxw6pNJ6GGAqjRrpJzY2apRKxsMRAUbzv+vqM/RxD/L6yY/L0h3iYKcWhThzUJ7noJxitLu2VKiODo84wrNfJa84xaU/dxvXxdMjuI5gkyah97Rxo35pN1n4CxacOpQdgaEEOw6pzplD4OPUoy5lTm1fnMY0RFcpa4ucxfS//5xZTRnf1q3OB5siWUdZvdppjlSTcdV1CgcO6HstOUjuZQkhUEbpdbnmYCjRhhppkpjpGlj+6quqh+aUnZZLL1XdTk0D69y5KKZ0q4c7dEApuQntgW7qUNMxX/d1TmGqZni1x9o53bOEEBACUSGQmd7IY9yicity9tPfjxJCQAgIASEgBISAEBACQkAIRISACIQRwSyDCIH4JtA6KfDy6JRNAWrNxdip9+iRoeuMbrppLGWwVH8p7d2xuW7zyGjqdI2e3G+X7z6POsqR5s73TgVbO3cGaAE5pPC38Mr1DNU6En1NoHVrbU44FjH1CnbMBFLFeNE8z0vAVTs2C4T792t35/A8As3Vx7zYwMi6JBsXeQr+go2hLByyMVOtichkCnyt8Temv+0hZLL115Vsj2MCO959F/YQBPUEOuemOpx3+dSpsIfgmLZQrcCgKZJ1mJ+dbhgoe/hhFJ10Emyh3sDgPg+u6cs3b8RLxGjK5XjBJ/MUAloItGyYhOYN6JrhFlnkIpQQAkJACAgBISAEhIAQEAJCIDIE1K9qR2ZeMooQEAIxRODctv4vFasO2jB/f3y4dJKTLeC0oHpGZmYHTJ58naoukyxmWBKC2DJV9JiaWInUxAoM60nOMF/RiO7OPuMMIJQUcizAcepVTmsZSgRalM/QKNay+qPViajnwjD3FYgTOwC1umpYpdu+PZR3wrMNi5X+xFvPIx2vKAufozyniiaOLKhc0lONBtu8eYifMx9z5o9Hw4b69edjCNkUBwTs9Duz4623QpppS2qlyyeIxMGKadNUz4HTkiZNnqy6XagNHI7CMWNgV6vsew1ooOugkev3xkkYmuohA8fJyco0hUAMEvBOM5qdK3UIY/BtkikJASEgBISAEBACQkAI1FEC/lf96+gJy2kJASGgjsCprQzo3MD/peLxNfHjHrzoohFo3JgrSukb1157Cl544QrFnZZXksijY5RXmjCk6wFYzAHSM6anA+edRwW1VJw/OwepDlZIwqLr/AK51Dg1aLt2riPV/WQhrmdPbXPjEblonl7BbP0JhCwMUppDXaJEpzqju/w4Tr0myVPn7KlqhD5XFywsrl3rP9Os6zjXz0GDEtC0qS6SDEaPtkgNIxfYevyzeMsWlIYoqmsop1mLuHX58lrblGxIvPZaJD33nJJDdTnGumiRs/ahxt4sV1+tsYfINDcdfTQMWgqnRmaaMooQqNMEvNOMZouDsE6/33JyQkAICAEhIASEgBAQArFFwP+qf2zNU2YjBISAQgKNG+u5pAkc18L/ZeIxEgen5wYQpRTOORKHsaPhpptOC89QVJTttlO644dnx6Fv5xZBx7DZ7Ni1vzDocUoPOFicgOZpZcEPb0l+GK5pNWoU0MLPPNkF160bcO65wOmnaxfggjn02rZVLxKayH3Zq5c+KUrZ5RKKs9IX7X4B6kpyba8yBe+Rr37Dta1Q2WeQMw4ePBj6JNisuGOHsvZJlM74iiuobqYOMWmSPv3oMBXpIooEKgLlww0yL/183mQeVmO/9ZpX4p13ImX6dBi7dvXaE56XFR9/DJvCGwj8zcAycaK6G1L8dRTm7ZZJk8I8gnQvBIRAMALeDsKsvEOUcCE+spMEOzfZLwSEgBAQAkJACAgBISAEYp2A/5X/WJ+5zE8ICAGfBHr1CtGR5bM3Sit4yPd/0B/IqsDD2fHjHnzllatxxBGd/JxliJt58YKVDy7KRgrK6cO64clrlaUw/Wr2fyEO6tmssrAUV+U9ipHmfzx3+HvFYljfvsCECcAFFwAnnAAccwwwbBhA9adw5ZXAaSSksnDnzw3nr29f27lmXk6Ow0LGiz1FZEXbP3s29v78M/LnzEEZL0Kzi1CJ4Mfzad7cOf/GjX2Npn4bu/6OPVZ9O18tTjnF11bnNrbSxVpwulIFC3AadQLHWe/dC/BwSuK665KUHBbwmI4djTjllDiqgRbwbGRntAj4/vYLcTYaa/IlnHMOUsmFaOQbOMIdlBq6nOo2agkD1Zi1XHWVli7C35bSb5vZJS8hBIRAVAn0SadU+G5xsLgCuQd1ypbg1q88FQJCQAgIASEgBISAEBACQqA2AREIazORLUIgrglcfPEIXeefXVCzRGojMeHnnVaM+rscT65VuNqv62xC6+zJJyeSe3BsaI0DtWJxcNs2jyN+X5rj8drfize+DS3dnHd/5qyVaGvbgWblytJFVrdnsY3rC7LrbcgQYNAgoE8fQO9Ua6QKWTdvxo7HH8fCAQMwl2pqLR05EsvHjsXi44/HX1SHcDk5FfcuXAg7u/l4PjwvFu6SSChKJhcYpwFt3x4YOBDo3h1ISak+DV2ejB6tvRtOx9qli/9+FAhxUq2pnAAAQABJREFU/htHbw9nMyWDrOZgcZBFQiXRpYsZ112nzf331FMNYDLRZ1yi3hNIaNYsZAYVIbes3dDADm6NUfbYY7Bt2KCxF2XNy6luo1YHT9JTT8HEN6DEatDFrZhujNHqlozV05N5CYF4IdC2STIaJpk9ppuVW+DxWl4IASEgBISAEBACQkAICAEhEB4CIhCGh6v0KgSiRmDixBG61t3aXWbHT3lWPLW2El1/KcfYuRX4c098pBXt2rUNvvjiLtx333j93w/Ou+ijrtU+hXc8r83Zj9nLt2qbF9f3W73a2YceKo622fhsveuPP/DXWWch+4knULByZe1jSDjb+9NPWD5mDOaRQHh40yaAxbYjjgBIUET//k7HIDsaNTpwag9etYXT9h11lN/dQXew2MqOzECOS7PnwlfQPiNxALtJA82Z5sAfc71CTV+vvtoAJ58cmgPw0UdTceGF2l2Iep239BNdAimdOiGJrx8hRH4Ibfw1STjjDH+7FG23UxHQ8nfeUXSsHgfZ6QYYu1JV38+ABrrBI+XHH2NaJLTOm4dCqkNo26rx+9gPg7q2eRvdFHXHHXeQ6b8X3U+USjVjm9I9RkPwv//9D8XFxZpO10Z/06xZswYffvghJlHq18GDByMxMZG+pgyOx19//aWqf57P888/75gfz7MB3XjE8+b583lIxA4Bfo8zMzxdhGsozaiEEBACQkAICAEhIASEgBAQAuEnEIMrluE/aRlBCNRlAmlpKWjVqjF27tRnaXNXKZWim6enjyK89BMTzSQsDMANN5yGE0/sR2JpmO6DYPegj2CXpdK49ZU/MO+NiWiQEpoQggULAFo0dgQLa1zjjhbTYiW2fvUV1r38suLpFP33HxZRqtMBJBg2JXdhROOGG5xqGKVAVR2clpUFzUDBbkgW5Ch1X8xEkyZBp6LndNX0lZBgwPffN8Lllxdg2jRltRtZ63zxxQa49VadHaZBKckBsUzAQDVL2157LTY++KDqafK3aDk9QrxCV49npJsfTBqvaRWff66Pnbd6VsGfOOomanQ+GkmYSfntNxzmVNJq7hIIPj3djrBTGuyiU09FA/pONVDaUQnfBH6m1OAXXXQRucprhBsW4ZYsWeJ4vEtpaWfMmEH3+NBNPiHE1KlTcdlll4XQsnaTTfQ30WmULv0/+rvCPdatWwd+8Fw/++wzjKGbkyRig0Cf9IaYv2l/9WSy8sRBWA1DnggBISAEhIAQEAJCQAgIgTASCNPKeRhnLF0LASEQkICVcvnt23c44DGR2nnioI6YeDKljYxglJVV0uJVMZo1SwufOFhYCPDDRzRtqDw14r8b92Dc/d+ipCwE0WjVKtCKXM0MuMYd10KMkdj5+++qxEHXtK2HD2MFOW0KyUUQ0WBh9YEHnK5Ft4EP0eLnanIaLNm4EUtowTGbXKOHOe8mBwkPuPFGQEmKUhaqNS60Owelfzn9qh7BqVyDBJtU9Qq1fSUmGvDppw1pwbkRxo61+DU7csbZa65JwqpVTUUc1OvNqmP9tKVaeIYQXLx8u8ceHVhYrr/e4YDS0lX5Bx9oaR5SW0OIrm07/bJX/Porii+9FIWjRqGIU1jHqDjoAmNbuxZlzz7reik/vQisor85xo8f7xAH2Yn35JNPYv78+fiDsgRcffXVjqNZjGNRrtDP30deXdZ66Z7SNoFuqOlPGQT6cs1klcHjj6U05i5xkOfH8+T58rx5/ixynnfeefj3339V9i6Hh4uAt4MwK7dGiA7XmNKvEBACQkAICAEhIASEgBAQAoA4COVTIATqGIG1a3eQSYnEoihHalICXrhpJBav2YlPZkZW7Pn77ywcd9w9jvSip58+RH8Su3b57fPozHRM+WY5yACFc9oaMb6tCW2SDUgmLekQ6YBrCmx4Z7MVKw46nYazFm/ByJum4ZunzkGb5gqEH1ZZ2DnIjsGhQ501A3nhm92DpWT35P3hck36PWvPHZXkalzz3HOeG1W8qqSFuzWUXmyIynRiKobwfSi7/O66C/bFi7HpjTewhFwv63furHWskaxqvWjBe9A996DDcceB3mpl0aoVkJur7Fh/R7FNrkMH7WIwi4wKhMYQNBV/M0cofXHasVNPTXQ8tmyx4ssvS5GXZ6NUdnY0amRAt25mXHBBIj2X+538gpcdSCQxvB1dU7a9+qpqGnwFaE4PujqEFEaqq2phl7HGsPE1P8JhaM5nrjzsdPNE+ZQpjoeNas/GW5STqyzx4YdhiCEnfqwwvPXWWx0pRM10IZ81axaOprSsrhhJdYW7detGX593Odx5L774Ih566CHXbsU/e5PT9pVXXnGkBD3yyCOpDHESHnnkEcqkXpVKXWFPnO6UXYIcz9HfInfeeWd1S573CSecgOPJ0cvuRz6vP//8s3q/PIkegT7pnu7dPYfLsOdwKVqmhXr1jd65yMhCQAgIASEgBISAEBACQiCeCIhAGE/vlsxVCCggMG3aPwqOCv8hRaUVGHPHV5j96gVIoxSah4s5UVvkoqSkHOee+wwtZD2G4cMz9R2YXG7+4tyjO2H3wCRMTLejVVJt6Wh4CyOu72LGwv02vLS+El/usGFhdh46n/cmHjm7F+4ePxDw5eziVKJZWQCP3aMHcOyxvqdADtKQg90i7HJjsVFD7amdtHjIIqGWyP/7bxxetAhpLIJGMAr37MHntGCYS2P7C04jm03uzexx49D5pJNw3pdfIqlxY3+H12xnAZL50hghB382OAVeixahv0cukVHBJNidp1do7atTJxPuvjtVr+lIP/WMQA+ukUYphPeRs01N8BV1L7ng2pPTSG1NPkPHjkiltIxci09r2AN872jt21d704gRMKSl+drlc5uNvjOKyf1tXbjQ5/542Mjvb8X06bBwTVmJagKcQtRV/+9KErvdxUHXQbfffjs+IJfrWnJivkypxe+9917Kqk1ptVUE1zLkh5aooFzWLDJycL1Bnpd38Pz5PN566y3Mnj0by5Ytw8CB9LeXRFQJdGqeihSLCcXlNX/HZlOa0ZY9RCCM6hsjgwsBISAEhIAQEAJCQAjUeQJyy32df4vlBOsbgW++IXdZjETu3sN47rPFOG9kz6jMqLy8EuPGPY2CgmJ9x+d0nr6CnG9J303H7Z3hUxx0b3JUMyO+ONqCKQPMMJGOWEpzveeL1fjomU+BTz4B5VUEKE0neDGbFizx8cdOUejEEwGu5eQvOO1lqFFOIi6Lg6mhizCcImzbt9+GOgOPdtuffx7YX1OPxmNnGF4covSh79HCYSBx0HvYzeQy/IBchMVK58m1mVQsunuMxyIkCQ6O6NLFKRR6HKDwhYq2VD7MUTpRYc8BD2MDpYQQiBYBI4kVR9K1qRWlFVQT6Zdcgt50PU4l57aRb85QGCZyGXNNO2NGhsIWgQ8zaLguB+7Z995ErsuqMOz03VdEImo8i4OuU62YNs31NOo/WexaQ+m2+VHG381eUUpZA1gAa0d/EySTCM0OvNdff93rKO0vv/vuu+pOLr/88urn7k+43vMl9LvCkZ+fXy0ouh8TiecsZB48eNAx1KWU4tZfHWr3WofffPNNJKYmYwQhYDJSdoY2DT2OypY0ox485IUQEAJCQAgIASEgBISAEAgHAREIw0FV+hQCUSLA4syGDXlRGt33sJ/MzMK8Vdt974zA1v37D5Pe9pe+I7EDyzu45s7XX6uus8RuwrcH1pi5r15agV9W7wbI6eJwDHKaLE5zeeaZAItL4Q52DubkhDxKIaWVK9QpFd7OmTNh5/Pn+XD9KhYwwxRl5M75bMwYHAzh3PeQs/Nzen+sSubH6V9pERdKHIfu58pKHYsTrs8e90PuCCq26X5U4Ofcpnt3VbUQuYkewh5rogoymgaev+wVAhoJmMjF2+/zz3EEiUCN/bmwq8ZoMnw4+n31FTI//BBGiwUmEtYbrFyJZLpZw3TUUX5nYqJ0iyn0XeAQFH25wf22DLzD2L594AN03Gto0wZm/s5RGMUXXwybyjSQCruO+GG2vNj5G+pbErS5Bh+nxOR0y95x9tlnO9x6uZS6mgVETqt5yy23OB7ex2p5PWfOHEfzVBKpAznthtPvjCvmzp3rehrRn6658qDu8/GexCAS8Pl8OKI1V+85yWsgM91TIMzKLRAsQkAICAEhIASEgBAQAkJACISZQM2qdJgHku6FgBAIP4GiolJYrbbwD6RihFJKFfTf9nwVLfQ/dMqUGbj++lN9LrCFNBoXU3MXg0iYxU8/OdN/htDhFZ3MWEk1CV/baEUldVXkbVCkNJZITw+h56om5ELQzQYWZBalWtJnevVdSaKrlWpamXnBlh+8QMpCGS+6N6RFJB8Lpl5dKH65mFwXLPSFGtvnzcOqqVMxQEmtMXZ5sri3m4RgFn/pHP0GL2DSYr0jpaj3+boEP3LvgOtisojqKzjNG6t8/AihthY3Y/xc3jLU0PLxDXVMaScEfBEw0O9NmwsucDwKVq3CTvq9Ldm6FVa63phIyU7u2BEZ5Dxq0KdPreYGEhgtJIbxw7piBSpJBLHz7x31aSCx3kxCjol/t8MQCRMnwrp0aRh6rt1lEqVjNShMD2kl0bTyxx9rdxKvW6guXazETL5Jhv6+OOecc2Ahkdo9fqbUtbyfhcO2bdti8ODBWEz1c1ksZBfhBfQZ95UK1L0Ppc85bShH165dqZas//869uxZky3C1UbpGHod5z6u+3y8++fz6EKi/7+UOti9jfdx8jqyBPpkNPIYMHsn/X0jIQSEgBAQAkJACAgBISAEhEBYCfj/X15Yh5XOhYAQCAeBxEQSAiRqEcjO3oaFC/+jxbKaxataB6nZwO4v90VESk3pEGjU9OF17N09zXh/ixUfDE7Aue1IQOKgRT9Hyk8Vae2cDb3+VbjQ69UqpJdWSnmmZ3B/7Prhmob8MFHqsAQSIQ1NmjgddTqcm43qNi57803N0146ZQr6X3GFMiGaxT4WOll9K6A75Nm5yaIz15BkAZGFPK5XyLY7b2HQfaa8jz+P/OAUdCxWsCDM/fBCLhf+Y1YsJoYYPBU2HrKZM5RgcVCN0TGUMaSNEAiFQMN+/cCPUMLUvz/4EamwkGhZSnXdAt5QoMNkEp94QlUNvvI33tBh1NjpwsA1XmMkli9f7vg+Of7442vNiOv9cXSnizMLg2kkbh+im0WOOeYYh5Pw3Xff1UUg5DSm+/btc4zFQmSgaELfNezKK6Lv6u38d1EUwjUuz6NxEKc+p2ZlgXAvff+yAzNRxQ00O3bsCHh2O/nmHwnVBPp4OQi3HyjBoeIKNEqR/9+ohikNhIAQEAJCQAgIASEgBISAQgIiECoEJYcJgXggkJBgJoNVGmkEh+NhuhGd45o12/QTCF2WKtcZkAtFa2QkG5B1sgUdU6uEnBYtgNNOAzZu1Np1RNubWZDSMTZTOr+ds2ahgl1yVWGiWkttyFXZjhwSDclZEYorztUX/9xAToxD27a5bwrp+U5azM2lhdq2Q4cqb88CHy9I67EozYub7DYMQ7Bxk0XCDRtAjhblA/B0OnRQfrwcKQSEgG8CBhI72LlY/vbbvg/QupVuTEh65RWoqj1INzeUc83cOhRq6kyG+7T3VDnyO3ulF7eRnft3qlHM7sEbb7zRIQ7yXBrR9wi/voHqR86fP1+X6R2m9NuuaKAgT7RLICzktOtRCNd8lc7VNUWerxqBkMVFCf0JdGuZBovJiHK3bCjZeSR8d22u/2DSoxAQAkJACAgBISAEhIAQEAIOAqFbCgSgEBACMUng7LOPisl5hXNSJLHgpFZG3N3DhKf7mvFYHzNu7GpCp1Te44yDB4tcT7X/JIGquoYcL4JR3T09oloc5IJtDz4IsLPFLWWXHmOEu4+UIA4DteNvoxpg7uIgt+e0ozt++AELJkzAIqoFVkgOAC2x9ptvtDT3aLt2+nSP13XpRXNan8vMVKZl8q8IZaNDp06BDZB1iY+cixAINwEj1U3TPUgYtNxxBxpQ+lLzKafASlZhG6UstivIKWzlm2Pc3fS6Ty7yHVquuiryg/oZ0eXcSyIXvXuspLSuBew8pziNbyRyi0y+SFO4nHRuu0J6yg5CV3inOXVtd//pEtlKAqXOdm+g83PXfNXMlacQrfnqfPpx353FbESP1vQ3sFtk5zk/626b5KkQEAJCQAgIASEgBISAEBACOhIQB6GOMKUrIRALBCZNGoP33vstFqYS9jk0oYxDV3Qy4fouJnRpUPt+h1eOtOPXXTZM2WRFzmYLyOCFI45wZl7UPDm+e9xV+02NpSrYwOxO5DRynH4yPz9itQODTUvp/pSMDDShtHv5VJ9LayRTbsp2Z52FpgMHIoFqDrJbooLcDPm0KL39u+9QlJODg7RQumjYMPSnGlhNhw8PachCrt+nUxRxXcE6HKxdc2k2XvtlbJTx1ZHRlH8FOKMpG0zCUCKyDhOVUxMCygjY6NpSesstyg5WcxSlabaS87mQ6g66h4Fu9rBccw1YMDP6cSY76i+6N4rz50YS10z0fRIrwSJXZWVldYpP17z++ecfx1NO+dmxY0fXZsdPTjXKYeU00zqEuzhZ7l572U/fnKqTI5nvEolCuOarZq48TbXzDSbAcorRIUOGRIFA/A+ZmdEQq3NrskZkkYNQQggIASEgBISAEBACQkAICIHwERCBMHxspWchEBUCAwZ0wdCh3bFo0fqojB+pQYc1N+DbYyxonljjEvQe20iC0pg2Jsdj4fa/8MozJyCxcQpGjQJOPFFjXTRehOvWDcjK8h5W2+snnwRIDHME15KLw2h/9tmaBMJGvXujy+WXozm5Aw1etfOSaaG6IeW67HDeeThAiu/2779HA7KpGdauhZUWUzn9qEOp4pSdLLbStmBR6eaQCHZssP0VUXJNBJuX3vsZM7sDJYSAEIgMgXKqKReW+oN0/bNWCU7uZ2KnGmtlDz2EssceQwLVP0x+9VUY3FJIs8PQRuJVXYpEEmD5RpRYCRb/1qxZQ3/PLaK/W+gPl6r4kW6I4Xn6qk14gOvQUrTgNOU6hEtw5K6UpA3l+oMcSlJ8Og7U+R/XfNXMNZT5BqvHqPNp1avueqfT32+oqWGZ5SYW1isQcrJCQAgIASEgBISAEBACQiBCBEQgjBBoGUYIRJLASy9dhREj7kN5ed1avHMxPLGlET8NS0CiSflC3lFJW9ESj+LRgw9j+vQUkK4EziQ2cqSrVx8/+U54TiHKC4ZsjXKJTVyT5++/nalFt2zx0VDDJleNn61bgdxcDR1Fr2nL449HUsuWKK2qn+SaSQq5LtuOHYvUjh2RQAKrlfiW7d+P3X/+ib0LFwK04My1BTPvvx9GcrUEi6YDBjjcitULumxjc6W74/RrtMBNRTkBTnuamuq3uyQ96v9V9Z5EdcIkhIAQEAJ6ErCTEFf+1lt6dqm8Lxq74r33YMvORtIzzzhqDlbSF6h93z51BUmVjxjSkUa6Ycdy3XUovf32kNonXHghEq64IqS24Wp0wgknIJu4v/baazibbrzp1asXfqD02n/99ZdjyDFjxtQaOqvqpqU2flyftRoE2cCOvOaUX5rTne7g79QAkU9ZD1wCYbRq9LFwx4Iqz+MgWdwbB/hOdrkAWUx1pUYNcHqyK0IEMtOrbpKrGm/zviIUlVUiNVGWLSL0FsgwQkAICAEhIASEgBAQAvWMgPylXc/ecDnd+kHg6KN74rPP7sD55z9HaaZsdeqkzaTVTTvKrEocdAHojC24FS/jGdxDabuMePNNgLW+M890HUE/WRScPx+YNQvYtMltBz1t357zUDm3h8M5wSIWC4+usTt3Bs44w3MOcfCKxb0jyHWy9OabYaOUZM2PPhodzj8fzQcP9jn7jFNPRQml48pfvRptyNrp7Rr02ahqY7U46OsgFgxJgHSkamW3Z7Nmvo5Cy7598R8tuuoRLavqP+nRl/QhBISAEGAClX/8Afv2GkdNNKhY6SaOohEjojF00DGNlDs89eefYeSbQSjUioTmceOQ/MEHqr57gk5KhwNuuukmvP3229hDN9twbcEmTZrQ11k+7PTdxkLYOJq3d8yivx/4e3GQjvUqWZicM2cONm7c6Eh5auZ80j5iHdWvdAW3iUb0pgwE06tqAfN8jqJMBL6CU7duqvobL1pz9TUv2Qb0atMQJqMBVhv9DUfBf8qt21WAgR3ohi8JISAEhIAQEAJCQAgIASEgBHQnYNS9R+lQCAiBmCAwbtwxmDHjYTRqlBIT89FrEpW0UPDnHueiQSh99sdKHIF/q5t++ikwbx695BUIthWSAwFvvFFbHOQW27YB//1Hq7VhcmZyfR/3sbnIG7nq4jGakOh2JKVL7X7jjRhIta38iYOuc+PUoemjR4dngZYZ8vtWlXrNNabrZ392jbBLVGOYyWlxxMSJGnuR5kJACNQXAnxpogySjvtCZs4EONMnayzel30bCTMStQkYqOBpIqVBbTB3brU4mHjbbUj59lsYKRV1sDCQwzzx8ceR8uWXMCQmBjs84vu70Y0tU6dORQqldmVRkNOH8k92xU2bNo2SGnim0N5FfzP89puzBvVJ5MbXK4ZV1WVkV96yZcv8dvs33+BUFccee6zraUR/uubKg7rPx3sSS5curXY7Rmuu3nOS104CSQkmdG1BWTvcIiu3wO2VPBUCQkAICAEhIASEgBAQAkJATwK+bwHVcwTpSwgIgagRGD26P3Jy3sVHH/2J55//ljJWkpuqDsTkjZUY384U8pmcjJlYhSOr23/0gQ1Hr3gTxn/+qt4WlSfeNQc5XSbf4c7utziMFsccA37ETKynupxH0vtOQp57NCGnZjdyMW6YMcN9s+rnmZSiLplTmkrEJQFeeC8ll1YFu04pEui9TKK0uGocrXF54jLpiBPgDMizZ4PEHJA7rPbwXD6V7pegVOEAl7u1cwOJagIG+k5MeuIJJFDaTYOPdNQJZ50FM6UGsFL66rLJk1H5yy9kKyx1tqe6tiZys1uuvx4J48fDwFkBYjjOo3q7w4cPx8/kkGQBkFOHnkGZBZr6+K75999/MWHCBMfZjAyYP13dCZ9FPJ9++mlHow/IaTl06NBaHdhI1f74448d21nA5PSo0YgR9EvTiNKGHzp0iP72/Qh33XWXw1HpPZcPP/ywehOnb5WILQJ9KM3of7spxUdVSB1CFwn5KQSEgBAQAkJACAgBISAE9Cdg1L9L6VEICIFYItC4cQPccssZGD9+WCxNS9Nc/tlnR05R6M66gViGzGoXoR3nFHwQfXHQH5GVKwFeHGbRgn+Gy73ob/y6tJ1tOZTK1FccRa4TLcEi0tBbbtHShbSNEoFycuXkvPAC5pDo8E+HDlhAtS358U/HjpjTpQs2P/ssyvfujdLsZNi6RoBdgpMmAexe9yUO8vnu3g1yjjmPY4e7IUAN1brGR8n58PXWwuKeD3HQ1Z7TbJpHjULqN9+gId1s05DqCTekmnQN6UacBpQu1XLppTEvDrrOpSXV9L388stx77334rLLLvMpDvKxo0lVZgGPHw0betZxc/UVys8hQ4bguOOOczR9j+pRLliwoFY3L9A1dO3atY7tt9B3YYLXe8N1E/k94QefQ7iCXZU3U3pzDp7P/yiDgXfw/Pk8OFh8Hewn/bl3O3kdOQJ9Mhp5DJaVJzdJeACRF0JACAgBISAEhIAQEAJCQEcC4iDUEaZ0JQRimcDSpRtieXqq57aVzHUdqWRfKMHJJB/CE1hNMuFa9MIp5CiM2cjNBd5/v2Z6JnJOcuo0qrkESq+mR2rMms7rwTNekedakszRLTrTQvLwRx7B3/QIJcaQS6V1v36hNJU2USJgt1qxkdIT5rz4Imwud5HXXEpycrDhnnscx7WndLndSSw0+qm/5dVUXgqBWgTIBEauplqb/W7gkrivvAJcNuh4RCdho9+pRXWHXaVgz6IURGTV9J69Qh9ETsVZUlLiECLvu+8+h0uQX3/++eeOWok8QHf6++T2228PeSx3Zx93spJvkqqKX3/9lbJi5LheomvXrnBPKeraceedd+KLL77AesoawA5Crp14wQUXUAnpZHLuzsZTTz3lqKXIr19++WVXM/kZQwQyyUHoHhvITVhWaUWi2fNvN/dj5LkQEAJCQAgIASEgBISAEBACoREQgTA0btJKCMQdgQMHCuNuzoEmfLgi9DqErn77Igv8iKsgUYNui3c+OA8d1xlq3jyuTiGqk2V++/YBzM4rhpNYZKUV+blVqdS8dvt9eRI5FAZx7UqJuCFgIxfRKlow3kPuIiVhp/qgW0lILKICcUdOnw6TV5paJX3IMfWbAJuu1IiD7rQ+WtYPqd0n4Mj1n7lvrrfP+fdRwklgN9lNf/rpJ/pa24dOnTrh9NNPdwhhevPp37+/Q3SbSHV2CyibAQuE3sHiIKdCTeO8uCEGOyX9xbN0g4Z7XEouUF8CIY/P8xgzZgw2bNjgEC/ffvtt96YOh+WnZOM9ktOOS8Qcgd5eAmGlzY71uwrRt62nszDmJi4TEgJCQAgIASEgBISAEBACcUhAUozG4ZsmUxYCagmUlJRRhsq6lZ6ngdze4MxD9+WXwI4daj8S9ft4SjXnK9hlMoqcBefQomHznj19HeKxrTUtLF744484RoNbwqNDeRERAlxrMPuaaxSLg+6T2kd1KrNoUdrO6WolhIBCAlxe1t0IrrBZ9WF2uwGfDpiCSoN88TEUQ5Mm1Wzq8hNOkTmeUqmef/75OEjpUb3jhx9+QBdKg3wNXc9YsLuQ6uD26tULXIswHMHiI/f9f//3fw6nYEpKCrje4KBBg8Di3YoVKxyuvnCMrbZPdhfyfHhePD+eJ8+3R48ejvnzeYwdO1Ztt3J8hAikJSWgY7MUj9Gy8g55vJYXQkAICAEhIASEgBAQAkJACOhDQFYa9OEovQiBmCWwb18BzjjjCezaVXtxKWYnrWBiHVI5UagE2ElBi4S0iihOQqUfhyB1HPtOmIBMWmjNoZpJS994A1upaFgJ1anjulcpzZqhE6UjHUxFxDKGDnXUU1I6rJLjKg4dcghXxVu2wHr4MEwNGiC5Y0e0GjcOCbTAKaGdwF76fcn78MOQO9pFonzLs85CG/qMSAgBJQQWLQLoV1tTHLQ2wr8ZZ2LAjuma+qkLjU3kZqsP8d133+Hrr7/G8ccf7xC43M95D6XLZjdfMdVXdI9t27Y5XIRr1qyhrKoh5mF379DreQeq0/oiuan5oSZGjBgBvjkjWCg5Jlgfrv18/pxilB8S8UeA6xDm7K/5fGeLQBh/b6LMWAgIASEgBISAEBACQiAuCIhAGBdvk0xSCIRGoKio9P/ZOw/4pqovjv+SNEk3tOyWsvcGEVnKUAQR3HvgRAX9u/ceuMU/DnCAe/zdOEDcbAQZsvcuLVBG90oz/uekTUnSjJfktU3ac/zEvLx377n3fZO8lPe75xycddaTWLVqZ3AOVO6VlBSH7OzCkL22p3te/xy3Qa+xoWWsCIV2kZAim3D11VKTUMmni+tR+TGOJmw7YoT94aepKofzKZphP9UxPEjRi5bCqt+RrbffjuYkXLa69VYkKkiJxhFux/78E+kkcOZQbkNzdjY0VHdRTwJno9Gj0YoEzsR6cpPd/Q3a/+ab7rsCfs0+RCAMGFu97fCrSmVuF456Gf0+EIHQcPPN9eKz9Cddw/m3yFOk24wZM1BA0fBRVBP1pZdewum0cOVX+qA9SDVTD1BWgZkzZ+LOO++sF5zkJOsmgR4pDTB3/cHKk9uYUbcyoVSemGwIASEgBISAEBACQkAICIFaJiApRmv5DZDhhUB1Epg8+a2wEQf5PAsKSlQ53V2kn1y2vAyt55bi/KUm/H7YomhluiqDh6sTinBDRka4zi685kU3VMPFOFpi15QpWNa7Nw5QjSRP4iDP1UJRIhmzZuFvEvV2Pvmkz897BhU6W0IpUlefeSayZs+G6dAhWKm2IvsoSU8v99OvH5YPGoRjf/0VLihqZB6F27bh2B9/hDxWzrJlyFu7NmQ/4qDuE+DMkPSxU8U2F7SF6T8PqeIrUp1oqM5e1JgxkTr9gObN0YBsven3wd2+o/qpLB5OmDDBLgT27NkT9957L2644Qb77wOnHxUTApFMoLtbHcItB/Ngtkh670h+T2XuQkAICAEhIASEgBAQAuFJQATC8HxfZFZCIGQCBw4cxaefLgzZj5oOysosaroD3yb4PtOKSavNqqd6VHWiNeVs3bqaGimyx6E6RGqaldK8Hvr6a2ym6L51l11mf/A2p6LkY75sG9Uv3PnYY76aVDm266mnsPWOO6qIhBw1uJVqQ2289loU7dhRpZ/7jtzly+0iYjpFmtQXy/z0U9VONfPjj1XzJY7qLgEPpeNCOtmS+55FNKd3jIlR5EfbrZuidpHSyEgiGKd7rg925MgR+2k2adLE5XSPHj2KTZs22fddQZHlznbOOefYXzqOOx+TbSEQSQTcBcJSsxW7jlTNsBBJ5yRzFQJCQAgIASEgBISAEBAC4UggfMIowpGOzEkIRDCBd9/9FVYSDOqDDWrkP2VkfeCAXbsAihSD0VgvTjfok+SojKSkkDmVUg0oTjV5gAQ2jtJzt3RKAWdo1gwtJ05Eq9tug5G2nY1Tiu7773+ddyne3v/GG4ht3x6tSSh02HZKLbdv2jTHS0XPNosFm2+6CToSG1KonlVdt+K9e1U7RTV9qTYpcRR2BPysEQh4vmVlGhhpIYCBFgKYKFrYRNcZq/uCAIMBeqpLa6BUwtoePZCfmgpQTdNIN/0118AwaVKkn4bi+TvqC5aUuGZfWLJkiX2BiJF+64cMGeLir0WLFvbXOWor0y6jyAshUP0EGsUbkdIgGpm5Jz7/XIewc/OE6h9cRhACQkAICAEhIASEgBAQAvWIQP1YgluP3lA5VSHABCx003/mzN/qDYxkgwiE9jebBWGqSSTmh4DZXJ7zj9J7Bmu5K1diGaV02/3MMx7FQYdf0+HD2M0pRKltzooVjt0wU53BHY88Uvk6mI2djz8Oc8X7nfXTT9j78svBuLH32Xj99Sh0FxmC9ha+HS0qiiTmPKmHFL7vdPjMTOWAZcRRDV42DS1yMFKNufitWxG/cSPiKF1wLNWijaP0t4m0YCH2k08QRWmEtQkJMJCwFummJ0E0hhZjcFrN+mLJycn2U3WkGnWcN9cmZOvfvz+tB3JdEGTm3zey+Ph4+7P8TwhEMoFuVIfQ2aQOoTMN2RYCQkAICAEhIASEgBAQAuoQEIFQHY7iRQiEFYHMzOM4dCg7rOZUnZOpH3GSCgmqHa6icNiIa8bCWpBiEQt9K4cPh4kiCJWaiVLFrRwxAjmU1pPt0BdfwJybq7S7x3YsUB387DP7sb1Tp3pso3SnrawMW0lssNJzXTadijfNo0h4ERMC/ghwdkg3DcdfF6/HOatoo0auhzndpq57d0TR9UV/1ll2UZDFQ2cL16i7qHHjAL3eeapVtrWdOiHmnXcQ8/770PhpW6VzhO9w1B78/PPPK8+kuLgYX1NKaxZKR44cWbnfsbFv3z77ZjO3iHXHcXkWApFEoEdqost0N1IEoZgQEAJCQAgIASEgBISAEBAC6hIQgVBdnuJNCIQFgZyc+lWj42hp8JFgYfGGqTmJenYDNSR0HtKC+vNXevAg/h0/HpaiIn9Nqxy30o3dNdS3OCMDnH5UDeM0pfkUPZS9cGHI7o5S9NHygQORv2ULCuiR9++/KNq9228dxZAHrkEHMW3aqDaamr5Um5Q4CjsCLA6eeqo606J1CYgKojiAjuoQ6q+7Tp1JqOgl5t13kZCeDuNzz0FH0XCaVq2gIWFL27kz9BdfjLg//rBHSBooDXJ9ihx0IL6MatraKNL9J4oQ5+03KaX1mWeeiSxanMI8Lr/8ckfTyucVFZHq7dq1q9wnG0IgUgn0cIsg3JyZR+UT5G/+SH0/Zd5CQAgIASEgBISAEBAC4UkgiNsM4XkiMishIAROENDr69dXe36WFWV0w0Cv1ZyAUF+3Qo2Q4rvPLDKSmFXn7dgxgCPmAhBV973+OjgaMFgrO3oUe55/Hnlr1gTrwqVfwYYNAdcddHHg9iKf5rWMxARni2rQACmU3q8V1f6Koxv3kWwpV1+N3c8+q8oppNSBtI2qgBAnfgmQpgPSukI29hOsxbz9NmwkxpnVmEiwk3DuFx0NDYVDaqheYvRDDwH8EHMhMGHCBLxPkZNcc5CjBvnhsOtI8O3SpYvjZeXzd9995zW6sLKRbAiBCCHQI9U1xWhBqRn7jxehTeOKXMsRch4yTSEgBISAEBACQkAICAEhEM4EJIIwnN8dmZsQCJJAkyauKXmCdBMx3Q6XAt9lSKJRcL2iYFKMsijI6RLbtwdOOgmoL5EHXIOwpMTj55zLObKGR/dmMW0a8OqrwDtvWfDHnEKYdZTnLwRzpAUNwYVL1/y1a11eq/2CU6Huf+01LKGb0RwBGYpAqvbcAvXHAmfy6acH2q1K+4ZU2y2xT58q+2WHEPBEgANXe/TwdET5vr59gdRU5e3dW7IQF0uRaHqKRFNstHgimi6ChrvuUtxFaUOeB89JzDsBLaWPnTdvHu6++260bNmSokejkJaWhsceewxvvfVWlY4cabh37177/lGjRlU5LjuEQKQRaJZoRKM41+uEpBmNtHdR5isEhIAQEAJCQAgIASEQ7gREIAz3d0jmJwSCINCoUSIGDaq6sjwIVxHTZcZOc/jPNTYWoGisarPjx4EPPgB+/BHgOkQsgPkzSumGAQOAnj0Brlmk0wGJJDBzsav6YGbXzw1nDv3hB+D224EXXgB++QVYtgzg0oF/ztdhQdfX8b9xGVje+1Xkx7YJipA5Jyeoft46lVDK0pqyI3PmYMXgwSiuuAldU+OqOU6r224L2V2aCj5CnoQ4iCgC/JFxrx+o9ASaNgUogDdk01DUXgzVs4ulC1vUOecAJEB5tIYNYSBRKn7zZhgpUi2GVkjE79gBwz33AHHqRO4EJFR6nGT92BlHvF955RX6Sd+H0tJSuwD41FNP2cVCdwJDhw7Fnj17sJtSQ/fk33QxIRDhBDiVbne3KMKNGXkRflYyfSEgBISAEBACQkAICAEhEF4EvNwZCK9JymyEgBAInMDkyWcF3imCeyw6asO8g5bwPgNe0a9C9JLPk2RRkG4OYvbs8px2Fh9MOGqwRYuq7uiGDDjkRaFtoJSZm156CTYOu4s0c7pBnpkJPPAA8NlnoBpP3k/EZEjCpk534bvRG7C/+VjvDWvoCEf41aQV7dyJ1WedBRML0hFoTc89FymUui9Ya0a10VoEEoUV7ECR3o/EDHCdT0prif37Af6C8WdVycKFSD93D/PnAG8K/AKLfYEYX6IffRQgzU4V4xvu+tGjEUcrIRLotyJ66lQY7rsPhltvhfHhhxHzySdIpEUHMbRf16FD5Zi8HUNCVQIvSEhKqtwf7EbxFVfATKkzxdQjkETvS+vWre0P9byKJyFQuwR6pNCiNSfblFmzf/M4DS2bQkAICAEhIASEgBAQAkKgThIQgbBOvq1yUkIAuOiiIWjc2PUf1YFyiYuLpqyVJCJFiF26vAxrc8JUpGLRjQXCM84oj9KrCaabNgE//wx4Eu44CoTrF3HEoCfjG8BNmng6UmVfLtXBO0A3m00qR8ZVGag6dlTUH2Qd4/HHgcOHlQ9ijorHH0N/xL4W45V3qmipMRoD7uOpQxSpDtZaqBdZuHUrdkRozTB7RMLMmWh6/vmekPrc15iE0Z4ffwyNk7Dss0N9O8jiH18H6POB1avLFyuwQHjgACj0CeBr0r//louFbtG79QFVSgrAJTBPOw0UAeb7jPnSNHw48MwzQPPmvtsGe1RLYpKRIgVjaIFHzJtvIpomZ7jqKmg42t2D2eg9M1MdPI0KAqGNFhgU0u+hecECDyPJLiEgBIRAOQH3OoSbMvNonYmCDBkCUAgIASEgBISAEBACQkAICAFFBEQgVIRJGgmByCMQHW3ASy9dG9LEn332KjzxRAD1ikIaLfTO+ZQtcvgCE/7K8hE1F/owwXngO70cOtK4MXDjjcH5CKbXrl2Ae5QGz4ELYlWIY17dck1CbwKiUydLhUB1ZOlSp70RsMlpVOnBZQifew7ICyJrlU2jw/yBX+B4YveATjiG6kipYc2CELnUGJd9ZFKkUVkkisI0dy3VPutDQkfbBx+EVoFYy7XSWlMdtr6UvldHaRrFPBDgaGUWBiktJXxFl/IXjsVCLvIZzJfOw9CRtIuzTHO6US4hR0F09rqCjo8gP/OlgTQ6vP02MHlyecbncDg/G71XRWPHonjiRNg4Sl0NoyjTwvPOg4V/p8R8ErDQ9+vbb7+lVLOTcOqpp6J79+72B2/zPj7GbcSEQF0j0N0tgvB4oQkHc+l3REwICAEhIASEgBAQAkJACAgBVQj4Wb+syhjiRAgIgVoicN11Z1C2yUOYMuWrgGdwxx3jqQ5beWTU1q0H6GbmvIB9OHfokajBLe11GNJIg4/2WTFtR/XcyMotA8YsKsPB8Vo0MlLUXjhYt26uoiCnGS0oKM9lWRPzW7u2vM4gC35cZ1CpwMFRUpxq1M/NW11FtMn+775Dy/GBR9PVBAKPY3BYDkV2LlxYngnRYxsFOy1RsVjf5SEM/4fu6iu0+F69UMQ32T1Fdyr0wXNvfsklyHjvPaU9VG3HkYuZH32E1nfcoarfmnKmIfG7E6XHbXPvvcig2p3ppNgUuwkf0RRhlXbLLUi9/noYA80NWVMnEg7jsDDB0YF8XVNqHEHIfbp2VS9/ptKxw6AdC4WkjdkfPB2+FIRrYKqNirMWUlpSCxdjVdso7WwpFXyNpaheMc8EfqGakTfddBMynOrNOiKoOCJ6GRXKfffdd9GyZUv782h6r8SEQF0h0Co5FgnRUcgvod+MCtuYkYuUhrTIS0wICAEhIASEgBAQAkJACAiBkAmIQBgyQnEgBMKbwNNPX0nlguJx330f0g1IugOpwJ5++gqqeXQp6Q/lAtubb96MxMRYvPjitwp6uzY5o6kWj3WLwmlNTgQs90nS4dwULV4jkfDHTCuUzcrVL79qF6fBTe10GNxIiyQDYKGMQ3llNiTQlS1sxMGTTgJYQHGP1qM6aPYUniSwIDu76smpuYff92PHgJEjA/dKgqKVbuDmzp+PMoogMVAhLAOllzNSaktHJFUMFckqpIig/O3bkUM3/BtSZEPYG9+JpxSqnKXqt99Cn+2etItxyrq7EFN6RJGzJpSqMorSvHIUXrDWnOrgJfTsGWx3VfplfPhhxAqEDgCGRo3QlkTCNpRqsYTSYZbxd4VMT5/x6FatJJ2oA5S3Z/4S0Xc/IHHQ4Yv7btsG8OfYS1pLR9O6/hyu4iBzLyZxqlrEwYo3tYwKv9pefhkatQot1qEPyyf0G3HdddfZUyo6RME2tHCnOS1w4deHKS/2vn377NvpdP06++yz8RH9XXHllVfWIQpyKvWZgD0tOEURLt99ou4xpxk9szst8hITAkJACAgBISAEhIAQEAJCIGQCGvrHJd2dERMCQqCmCRygmkxpFWkG+aYOr/yuTtu+PQMzZvyMDz/8C7m5hVWGio+PwYQJIyil2VhKW9WqynHesWjRRrz++hx8//1ySmXlX9a7s6MOU3tHQVshNHpyml5kw4d7LdiSZwVH/1lhwy+HfF+WTmuswYNdonBWC50nl+Gxr1MngFfxDxniOyyEo2hWrQIo3SG4Vld1GdeMmjFDUcpQxxSK9+/HAYpKOECRHaasLMdu+zNHXzUbNgxpF1yAUhJU1j/xhH1/w969cfJrr0HrLoi69Pb+wko8tP6Kc3nvrvxIu3b2wl5btoDS6Crv5qvlSRseRp+tz/tqYj8WRaFDwyoiQVYOH448fv8DtIS+fTFg0SLoSGRcTJGhxXv2BOhBneZRdEP/9OoWuNWZqnipLgKcZpbTioZiJNKic+dQPEjfaiJgIQG3gOvVVrNF//e/MN55p8so/E8UFiYtdI208eeMfle0tLAjaswYaFNTXdqG44tQ/85j4a8LsS+lVKxxdK1/iOq+3kgpypu6RTMfOXIEs2bNwvMUEV1AUbzRlCVgK6X7bUULHMRqh0Co733tzDp8R31mzma8t+TE3zlndG2KWdecHL4TlpkJASEgBISAEBACdZaA/J1XZ9/aen1iEkFYr99+Ofn6RKBTp1RMmzYRzz57NX74YQWVgDqM/PxisDDYqlUTSnN2ChISYn0iOe20HuBHRsYxfPzxX3j//T+wc+dBj31u66DDf/voPR5z3pkWq7FHGDrvm5NpwfilpBZ6MPb7Wh/foqOHbjW7i3PHTZmibEwWwwYOBCiFm73olLJegbdiEYdEaVDKRH9mo3SB2+lG5N6pU72mwOQ2h/76y/5IpBSBtrgEaArzkbNuHTZSQb+ejz0WcORVAYlc8W3b+pte6Mf5xjKnFyXbsCF0dw4PB5uOVCQQplx7rT16kPud9Ouv+JeiSXPc60Q6nHp4bjhokL0WXlR8vP0op8Dc/sADHlpW/y5Lfn71DyIjhDeBQ4dCnx/XLDSZAKr1KBZeBExcDLEGzEyh3A6B0EYil4miCk20qMW6fn3V0WmBShRdN41UpFFHkfGObAdVG0b2ntdosQ2Lg/F0rV9EC0L69Onj8YSakGjK4uFYqhHJNQkLCwvBfafyb7iYEKgDBHqkJrqcxcaMIIpGu3iQF0JACAgBISAEhIAQEAJCQAg4CFCONTEhIATqE4G4uGhcccUwPPzwJbTa/Bo88sgluPrqEX7FQWdGqamN6GbUxdi48U0KkuvrfMi+PTBZYxfxqhxQuGNcig4z+lZdvzCZahi+0ZciCHxEJCoconqb0Ur/sDQFYg5H8K2/4grspXRvSuvj5VEYXlFhceUpH6QbvWsffhhmqlGn1DLmzfMdaanUkb92KSkuImkgJdP8uS41JPtrAl1CgktKTgOlsez/++/o+OyziPYTRWykuXd45hn0//NPGBo3rhyL6+NpjcbK1zW5oasQKWtyTBkrjAiQeAEW90I1TmZBqRJr28xmG44csWL/fgtF2lM8ez1PssG1B01Un7MmzFaR2teyZg3yKZq0hBY+eBQHeTK0QMVMNW8LzzgDRSQUsqBYF+03+i1l8fO+++7zKg46n3dviuC/l9Il8+f2V1p8IiYE6gqBHim08M7JDuWV4GgB/f6ICQEhIASEgBAQAkJACAgBIRAyAREIQ0YoDuoCgf2USpFvqnSlSChO45RMN+0HDBiAV155hQK7KLJLzCMBo1GPH398lFJenely/N7OoUf4TeoQhW8H6dEjUYO+DTWYPTgKb3oQDV0GDpcX9PkJ2GpCaFFQg3IrpXg79NVXAU8/DmaU4sRPStbixVh86aXYSWnPSij9mSezlpWBxcQVkyahjNLHxSuIbvTkR/G+Dh0Aqt3kbGre/7dpTpy/8xiV21RkrM+33yKWoiRzcqyYPr0IN92Uh8snlOKJLbdizsXrEf3cV2hCN7wTKFIkhlKHJtAN3ybnnGPvd9revWj/6KPQxcRUuuQNFgvTiGFtWHR1v2e1cVIypnICnPZRLVNDaAxiLiymLFliooUzuRSpdYTSNx6lQOtjaNjwKKX+PkYpiAsoat4ShOfI72LhEGuqQVsTZqPFKeZly1Bw2mmwZWYqHtL8008oPP30OikS8t+mbGeQEKrURo0aZW/q6Ku0n7QTAuFMoF2TeETrXf/G4jqEYkJACAgBISAEhIAQEAJCQAiETqBqiE7oPsWDEIgoAnPnzsWVV15J98BO3ARjUXDlypX2B9d1+fnnn9GOa5aJVSFgMOgxc+ZtuPPOc/DWWz/jjy//wrkpFA2igl3QUofzU7WRlz5s6NDAz55rPHG6UbpJWm3mR4TMWbEC6dOnBz18tF6H+DYdkL9ju92HiSJCdlH0ye6PP0ZjSqMaT+JcFEXQWSnqiOsWsohootSn7SZMQOvLLgt6XL8dmSvzTXRNUcX9/CBBdEkWOu15D20yvkVMyWHoLCUo0zfA8QY9sa3dRBxoPgaUS9U+BaPJdyQVRw9m6DrjoYl5+OyzEngKsHwFw9Cr1+lUCzQG114bDaNR4/f0uEGnl15CAdWBO0aCa01aKr13YvWYAIn8qpmavhROaulSE33XCrB+vefrbmamFU8/XUSpuYtwySVGEvUTkJTkepNa4VAR2cxWg6KthurmFdFiCMqPGTAryz//oIh+Q2J//DHg1NYBD1aDHSwUKcmmo5SqSs3R1qpgQZBSn9JOCNQ2AZ1Wg64tEvHv/hOLUjZm5GJYpya1PTUZXwgIASEgBISAEBACQkAIRDyB+nOXI+LfKjmB6iCwjuqlXXLJJXZxkGu8PEtp/pbRCvY/KYXfxIkT7UNu27YNZ599NgrqaAortbh2794Kb755C9ZNOx9R9A95tSziagtxetEhQwI/fRavuBZhdRlHnXHtPR+WTvWeQrFoimQbMGM6UsaQaEbRcg7jeoVHli7FHqoptYPqWbFoeIBu5NroBmZ3qp3X8eabq08EZgWQ0tV5Egd5fhQ07NHiC/di2IqrcNncNJy88WE0yV6N+OIDiDEdRWLhLrTJ/B6jl5yNi+d1QLcdb4BOBs2PLPboy7HTQosQMk9vi0azrkW74hW027OQzmLFLbfkU73PbGRlWR3dfT5r9Xr0nT0bTc87z2c7NQ9q6YY+11MUq8cEVBQhqiudJ5dfXbUKWLAAVMcN+Pff8pKv33xTgpEjc7yKg87vKus0//tfKV3as5GeHj7RhDZaZWDNygKnAq0ufs4cqnOb04Q60owGM46ZFnuZ61hazdSK32z+u1SpOdqmcDptMSFQhwi4pxndlHliYWcdOk05FSEgBISAEBACQkAICAEhUOMEJIKwxpHLgOFE4E5Kp8jRglEUYcS1XgYNGlQ5vZEjR6Jjx464//77sXXrVrz66qt4/PHHK4/LhmcCxox0zwfqy97hw0FhX8GdLQtrS5YE19dfr2HDfM7LdPQoDn35pT8vPo9z6suo2Fj0fOwxdLjpJrsIeIDSv3EkobM16NYNrS68EM1GjIAuWFbODn1ts7C/aROQlgbKF0jRfq7idY8eQPPmwKFDJ5w0ObYCo5aOR0yp59SoJ1qS7li4B4PW3o5mR5egmR+BkPtpYcUIzLY/tuAkPItZOArPN3L/+ceMU0/NxtKlSWjc+ITg6jy+87aO2Pf55htkfPgh9r3+OgrWr3c+rPp2C6pVaWjUSHW/4jCCCHB0rkpWnJ6OQkpH3GTs2JA9cupg/tqzXkTJAKqUUz1+3IT58/Oq7Pc38JYtFpx1Vg5dppMoBan/76Q/f8Ect+7ejVJaaFH20UewkTjoME2TJtBTRK+BavfpOJ2yCqapqe83LSix0t9ZoZqJFrnozzorVDdh038E/Ubu3LkTL7zwgn0xmz/R78CBA/a2vLCK/4YVEwJ1iUCPVNcsEJJitC69u3IuQkAICAEhIASEgBAQArVJoHbubtTmGcvYQqCCAKcQXcAhBWQ33HCDizho30n/u+eee+x1Cfn1tGnTUFYLKdAcc4mY5yDSg0XMufmbKEXQoW/fcrUpmOJ2JEjb+/sbJ5jjZ57ps1fW99/bU3/6bOTnoNmpXmdMs2boSFG4wymq7TSquzeIRKshFEE4gqI8Bs6caY8yrHZx0Hm+JD6AavhRmI3zXnugozOapNwNGLPoTEXioLOjdge+QlzpQeddfre7YjX+i7FoiZ1e227fbsEFF+SSkOE6b28dNJSKriVdzwavXYsBFLXZkt6DBPpMcnpTNS2WBIhOL76opkvxFYkEOGJaJcslQXvNuHHY9wZF5IZgXBaR1/I8/TRAWZOriIBmsw2LF/N3KrhBNm2y4L77aGJQbHsAAEAASURBVOFBDZuV6tEVjh+PfPrumV5+2UUc5KnYSFw1TZ2KAvodKSSR1crXuxBN16sXNMHU1A1wXC2HcptMAfaq2pyjCNU476qea2fPf/7zH/qN0uIIvbennHIKvv76azjSjjrPiPd9RbWDeZFbFonG3Oe2225zbiLbQiDiCXRPaeByDvuOFSG3WMU01y7e5YUQEAJCQAgIASEgBISAEKg/BEQgrD/vtZypG4HvSRBx2HXXXefYdHnmmywTKmpsZVOeMoeg6NJIXrgSUDGixNVxBLyiKDxMmQLcfjuoKCMwZw4oN63yiXN02x13AK1bK++jpOWAAeXRcz7almRk+Diq7JCJ6lVZqL6gs7FgFUMheol005prEBoaNnQ+XLPbB0nAcw4VrBidAy04UEZrKcWoJefAYM6rsXk1wUFMwWVoAO/RiosXl1GEc2A3zzmCJGnwYHR/910MXrMGZ+TlYTAJMGmTJyOebvpHUzRldKtWMAaRhi6aPp/95s2DgQVxsfpNgFMjc/piFSydf5NJwN9K188D778flEe+BD/yCECZwb3a/v0lpEUpE9y9Ofn00xIcOxakwujNqY/9FvruFlAKajP/prgtcvDUzUzfT25v4XyqIRjXBdRff30IHhR05fp6AdTY8+mR2JQxozpiPSjE/ZlnnrGnj83MzMRlVGexadOmOOOMM+y1s6+66ir7Nu+7/PLLkVHxO859uK+YEKhLBDo2i4de55oFYnNmzf29VpdYyrkIASEgBISAEBACQkAICAFnAiIQOtOQ7XpFYPHixfbzjaMIiJNOOsnruQ/j1IwVtqS60j86BqgLz3zDWAxgMerjj0HF5ABK+6jkpq4dG6WJxBNPeC+OFyhbFhtvvdVvL4tT9J/fxl4aWCkC5BDV7wxr40hCLijmZIz8wQepVGHWN0go2ut0pGY2myMd1+AFn4PNmFHs87iSgwk9e6Lb9OkYQrVXhxGHYfv2YTjdUO5JkZ16hWJfI7oxPXD5csSplMJQybylTRgT4EUNnKM3RMuntJnZ9Ll02GaqS1oQYMpJvoQ9/zwo2srhpeoz1+nbtSv071JJCfDBB/S/GjCOiCukMGcb/6YEYLbDh1E4ejQsu3YF0KtqU05ZWp0WQ+lSQbUU1TKOpKxL9tBDD1F95zdJh4+xC4W8WG3+/Pn44osvqC7m/+zbvI8/27H0YzadrvEP8g+amBCoYwSMUTp0auaaDUHqENaxN1lORwgIASEgBISAEBACQqBWCIhAWCvYZdBwILBlyxb7NDrQjW6uQejNunTpUnnI0adyh2xUJeBDbK3auB7s4bRplPqL7tpVzXXn7fTj4wGq42cXF9u29dbK//7Oncv9KIjw0TdwTd3k37nnFunffef5QLjsNZtBoT9VZsM66siiGVX219SOkfgWccj1OtycOSbs2+cqbHptHOCBFKolyIJhz08+QUOnOqwONzpaRJFGIgFHIPb//XcYVRCEHL7luQ4QoNp30OtDOpG9JFI7m42+p/tJFAnEOHCM9X9flptrRk4OXQNUsPffV0/U8jWdoquvBot9wRiLZcUUZcbiUbCma98eei9ZFoL1ae/H0eUU4Wy48UbY3CLPQ/Krpq+QJqJe58kU+b2XhOIXKa3zqFGjSJNvDoPBYH/wNu/jY9xm0qRJ6g0snoRAmBHo4ZZmVOoQhtkbJNMRAkJACAgBISAEhIAQiEgC3lWRiDwdmbQQUEaghJb/H+VcZGQtKdWeL0tKSgJHGRZSbb10f3cfnRwdOHDA6VXVzYMBRgNU9RCme6hODlhsyvUudoTpzKt3WosWAVwH7pprlI3DojXnvhwxAhTyAvz9N0BRApRDrDxExlfqUko3Bi6sN2YM6A6iovFiWUxUwXJJeE/ftB1p3Tup4K2aXPB3jxk5Wf7GjShZs8xpT81uRqMYZ+BL/ICbPA7M9/cXLDDRx0eddI7ug+golWAKCQn8KCU+pSRIWCmqJ4pSwsaQeqrjMEsxIeCJAF+reCHNpk3KF0E4+dlPNUozf/nFaU/5ZiZFYHeikMAoBfUzWff/448qLqrsyM9XT2TfudNiF944nW91mYXSA1tCzFxgoYhfC9VcjuJU00FazFtvwUpRnpaFC4P04NSNUrdHnXMOjPffj6iKBQkaus7YqMaiKlabaaxVOQHPThpTlPd9991nf3huUb53N71PjmwXjhT5vtrLMSEQSQS6p1KWklUnZrwxQ/6tcYKGbAkBISAEhIAQEAJCQAgIgeAIiEAYHDfpFeEE8vPzK88gnqO1/JhDICzwJcq4+UhLS3PbU09eciQJC1uzZ9eTEw7gNOfOBVhAdYpK9dubbz5zOkfnlI6sFnGRrQULAI4s4dx6JPAgORk49VSgTx8qqKf169q5QdNx42CgSCCTCunZ7nn2W7zz9l1ISqQ5haOR2I+yMpeop/wQa3WpcZocRehNIGT/x48HHwUUyPyMLVqAH2JCQDEBFvG6dgU4LahbCl9fPlgc3DJtmscmFvqdPkTR1y1vuMHjceedpH9RZKDzHs/bZrN63yG+hHCwGl96q8tKSZhTw0zkJxSBUGM0Io5+v4qoBp69DqLCSWkoAl7LizFIwdXQYisd1UXliEGt299Hur59YaUIZTVMo0BQVmOccPXB6fO5rrZzDe1wnavMSwgESqC7WwThriMFKDZZEGPQBepK2gsBISAEhIAQEAJCQAgIASFQQUAEQvko1EsCHEHoME7T5M+MdHOMrVjFOjn+xozo42efDVrC7rsYVESfYAiT50iZQARCT0OxaMg+QvXj5FtLn/FUunG7h4t4hWAHEYtv91Gg4/1fY+7LF6NhQjXePQ9gnmV5eciYNw8Hf/0VxSSqWugawJFJMe3aIfXaa2Gm47VtjXDI5xSsVp+H5aAQqF0CHDneqxeOffQRkrp1g9ZH6u78nTuxl+qneYocdD6Jgs2bnV963eYAayWm09G1UyWjDJmo+NNAJY+ubmz090aZW+pV1xbKX5URa9sbb0CjYEGUN68ayqQQ+/33KKPad6YZM2BZ5iXimn6fonjByX/+Az2lvlRiBqo5WUafGzXMTL+xRgV1d9UYK5x9hJJWNpzPS+ZWvwl0bZEALV3GrRVrPfh5y6E89GuVVL/ByNkLASEgBISAEBACQkAICIEQCIhAGAI86Rq5BKKdlvybuEacHyutqGkTo6CWm8OVv3SknGJ0QAgpvxzjhOVzIqUAeugh4IknAKdozbCca01PasWK8lCXMEyDlkY3afdOnQqbgu+EWReDXWmXY3/quSg2NoVFa4SxLAcZ2fsQt2cFlm3IwOBbPsHHj41D/y4KotE44rEaFDAThRXteOcdZJIwaHWrTWUikZAjJnPpPeEImdq2aFAkqA9LTlZP3PAxjBwSAsEToN/I7TNnomTvXrQkkagpRTQbKHJMQ5HlZvotyNu+HekkMuUojBYzKwkLpNly9mUlFhsbWGS1L58pKVpUZ3pRK6cpV2tREl372J8uxEUlGlJFDVdeaX9Y1q6F6fPPYSO/NorK1tDvvpYi3Q2URlvbpo0vdFWO6QYOhJYi363kM1QzU6SjlT5/gc4h1HGlvxAQAtVPINYQhfZN4rEjq6BysE2UZlQEwkocsiEEhIAQEAJCQAgIASEgBAImIAJhwMikQ10gkOCUgkpJ2lCuP8imJB2pg4+/2oaOdnX2mWs7TpkCvPACUFfrLQbz5nH6PY68GDs2mN7V2odrzfWYNQsbJkzwOk6RsRnWd3kAO9pcC5Oh6optLWWTG9P5Wsp8ug7bt8/GyTd8hFO6p2Dy+f1w6eldYKSbOw4zW6wojUtAHGeGqviOOY6p8VxEN65X33MP+Nmf2dzEQ3/tq+N4EShNow8bMoTS94oJgTAnoKUFOKbjx7GbagjyIxTTKlyUo2BNg30ajRrpwSJhUVHo4bhXXlm90dE2lev42hSKrUrfLx0JejGczloFY6FVT5HcpXfeGbo3SsFtokUh0SFGw4c+EfEgBIRAdRDonpLoIhBuzKj9DBDVcZ7iUwgIASEgBISAEBACQkAI1BQB9ZZS19SMZRwhoAIBjiBs3Lix3dMBP+JBNoUmOATCeltXMFjmXMfs5ZeBSZMASuUoVkHg6NGwRZFy9dXo+uaboNCYKnM83qAnfjhjFTZ1usujOOjcoVmz3lQO8Ul063Y5VmzKxDVT5iBpzDR0uuw9PPr1RqzVNoRu0EDENW1ULeJg6bFjWHnHHYrEQed51+Z2Btp6Hf6MM/To1OmEuOq1oRwQArVMQM36lcbmzRWdTWysomb2iL/27WOUNfbRii+PN98cuh8fQ0Cj9KR8OXE6xilCw9k0CsVgJedQ9vPPSppJGyEgBCKQQI9USmftZBszc51eyaYQEAJCQAgIASEgBISAEBACgRKQu42BEpP2dYZA165dsXjxYuykWkhmsxlRXuolbd26tfKcuY9YgAS4xuOIEeWPXbsA5rl/PzB/foCO6lBzpxqY4XhWrah+U0ybNthG0XeF27bZp5gT3wk/D5uPUiMJegFY9+6Xo23bUSgoyIReH4cLLmhOenHFjWpOKXrId929AIZyabr+qadQUk2+XQZS8cVvuNyrt8mTFSogXj3IASFQMwSaXXQRDn/7rSqDsS8llpoKbNqkpCXoehRDbQtDymg8bpwBbdpw6HP1mZYX2LASSRFxapiG/YWx2VRcOKOmrzBGJlMTAvWSQPcUV4Fw++F8mMxWGKJk3XO9/EDISQsBISAEhIAQEAJCQAiETED+kg4ZoTiIVAJDhw61T52jA1evXu31NBYuXFh5bMiQIZXbshEEgfbtgbPPBjjNZhhaicWGxzeWYcT8Ujy4vgyv7zBj1m4zpm4zY2d+6CnpKk/ZqQZm5b4w22hC79OQLVvQ/88/0ejCy/D7qXMDFgcdpxQb2xhNm/ZCUlJ7nHeeUxQLFw5TmhvQ4UzBM9c5O+7jO63ARY03yUEjLMF4j+P26ROF8eNJaBcTAhFAoNkFF8DQrFnIM00eORLxCmvmnX668uGMRi369PGdzteXtyZNNJg2Lfj+vnw7H9NQ7caoMWOcdwW9HTVqFLQVWROCdlLdHWmhlmqmpi/VJiWOhIAQUINAN0ox6mxl9Lc7i4RiQkAICAEhIASEgBAQAkJACARHQATC4LhJrzpA4Lzzzqs8iw8++KBy23nDShFOH1fUUGrYsCEFwlEknFjoBDZsCN1HNXiI1mnw0V4LFhy14cVtFtyx1oyJq824d70Zy4+rE8Vhn3ZTKtQXAcZ1oRrRTfqC2/6HvPgOIc+Y7lHDJYglK8uvT2sQN3rTZ8/26zfcGvyN0SiDscq0UlO1+OmnBhThTJFEYkIgAghoKWq85cSJIc+UI5mVWtu2QMeOSlsDnGa0e3enxQoKuyYlaTBnTkPKmF290YOO6RgmT3ZshvSslp+QJuGnMwuiapmavtSak/gRAkJAHQINYvRoleyaVWGTpBlVB654EQJCQAgIASEgBISAEKiXBEQgrJdvu5w0ExgwYADVSDvVDuO9997D33//XQXM1KlTsYWiqNjuoHpmer2+ShvZEQSBgoIgOtVMlw1nGjG9bxQubZiL05CB05GOC7EL0XvXqTMB/gwNHuzT1549Fjz0UAGGDMlG167HqI7fMfqsZuPJJwuQkVEN0Zc5OcD69aAvAfDPPwCnFXUS5n77zed0FR+s+LqdaF9cfGLbw1YZfU60XlL/emhu32UmnwfVmrC3QTzsD0U+5m/DCHyHofgRWpx4f7t00VEa5CS0bFkzYoSH05JdQiAoAm0oPXGcwug/TwNwBHNTp0U8ntq47+Pg9ECsW7c49O+foFh8795dh2XLkuhvh5r7OyDqrLOgadMmkNOq0laTloaoceOq7A+3HTr6m0wt0518slquxI8QEAJhSKBHqmsU4caMvDCcpUxJCAgBISAEhIAQEAJCQAhEBgGpQRgZ75PMspoIvPbaayTCDEExiQpnnnkmHn74YXuUIL/+4osv8O6779pH7tSpE+6hG55iKhHg2nNhaokGDW5pq0HXzStgQumJWVKwW2F+R8QlhJhabtAgINH1xoZjkGXLyvDss4WYN8/koeyUBUuWlGHKlCKce64Rjz4ai759Q7hRzXWtuGgXi2krV1ZN+9qAarxQ3r7dnUZTnU51Ijv++ANw0Qx8pJoty8uDuagI+vh4Bx5Fz8UZGbDUcI1HToy3nR7R9KBAJgQS68dJsbjKaV+U4FFMRBZSsTjxOrSddAMmP9QKDRrIOh7CIxZhBPQUcd/v55+xctgwlKSnBzT7BgMHohf9/mq0gX32+dLKl7Tff1c+HNcjbNnSiH37SnD8eDE9nxDo2QtPgdP73nprLF0O9fQ6kG+38nl4a6nR6RA7axYKOdWo06INb+2r7Kf+MTNnQhPgQosqfmpgBwuE2j59YF27NuTRDJMmheyjph08/fTTqgy5VgV+qkxEnAiBaiTAdQh/3nCihrVEEFYjbHEtBISAEBACQkAICAEhUOcJiEBY599iOUFfBPr27Ysvv/wSV111FfJIkGCB0N1YHJw7dy4SQhWG3B3X59cs+uTmhh0B1swoqyYOk8hkKi0XBznNZhRH/dHB/Tt3oit9ZkIyLzWlZs0qxi235Pstz8ia2nffldJnshSffpqIiy5iWSpAO3YMeOUVYNcu7x35/fnuO+xAIbW5wXu7AI7s2OHWmG5eo6zMbSejtmHP55+j0y23VDnmbwdHHdaUldBAh+lB2rE97o/Xr5vo0ZoeMfTwZSyRH6HHXnpw9CGLjPyD3JSiVi/MmwLNf1/C0bhHkfjoo/SZrFlRgqYRkuXs24c9f/2FoqNHYaPFADHJyUijqNmm3buH5Fc6RxaBWMr7eQpFJf9LkYB5q1Ypmnyziy5Cz48+gi7WNX2cks78Nbn++nIdbf58JT3K2+j1Wjz1VCzGjYvB5s0WHDxopUVDNhLnNejUSYfmzWs3gjeKFmrEfPghiq+5pupCDl+nSepmDGVH0I8e7atV2Bzj65yRUqoW33RTSHPS9u4NHavFEWZPPvlkxF3rIwyxTLcOEejuVodw88E8WKw26Gp4EUcdQiqnIgSEgBAQAkJACAgBIVCPCYhAWI/ffDn1cgLjx4+n7IrrwdGELAQeOHAABqqh1KFDB1x88cW47bbbEBvEzUrh64NA587lqSx9NKmNQ99n9kK3xEMoPbgCLVq1Qlr79mjYqFHlTTsWrlg4NBiNwU3vggtAH6wqfT/8sBgTJ3IsmXJj/fLSS/Mwe7YG55wTwHwyMwGOVDh+XNFghQj8Rr03x1W0O/qewUO0Xy6l9W3QrZs3Nz73a2soDTDLkBs9zISStYIfFH+JZvRoSA8KQqo0h6jI4iCLgg5zlwBtJhN2Pv44ivfuRcfpM7HiHy020oAOhqyxczQmBUAj2I+jY2w1nlkI3EXRqCunT8d2uo56CIFFK8oxezLVlet6/vnQ8XsvVucJRKemYuCKFThGYX37Z8zAkTlzALcIcm10NFpccQXSKOqrQf/+ITHhNQe8rqA1qfQ//ABkZ/t217IlcPnlQHlGSg3VJYyih+8+tXHUcOWV0DRujOKrr4btCF89fJuGfrdiSGjVB5p31bfbaj+qp89ByTPPwBZg1KnzxIwPPlj5m+28PxK2+W8MMSEgBPwT4AhCZysps2L3kQJ0bBZilg9np7ItBISAEBACQkAICAEhIATqCQERCOvJGy2n6ZtAa7qb+Oqrr9ofvlvKUVUIUDpXe607VZyp5+Slrafj9Gbb8Wjfg4g2VI0a4QiHoMVBjuK49NIqk12zpixgcdDhhO+zX3ZZLjZsSEb79gou5xQli+efVywO8jhaqJcOlm/euxjd8KbQXZdd/GLR97vQ78bzcTRPi7hoM2IMyudgpGi1mjB3Qc99TI6P5Qe349NmkZAFQW9n4undK4puju//6YCd1xahyEqKoJstWAB88gkwfDgwdizQpIlbgxp6WUxi85ckfu9buNDniPsXLwY/GnftiitIREyiCDOxuk+AU4U2pusfP4pJ+MmlOqdl9Jnh+qJ6ErKSSDjWJyWpBoIjCfn7wD8zHLjIKUe3Uw7giqBwWvAD9OoFcDA3fRRJTFJt6Gp1xJGAURSdW/bVVzCR2GrherFuxrX3DBSFp6ffGk2Mvzhmt85h8FITF4c4EpELhg4F8gNbNMPTN9x7LwyXXRYGZxL4FOYHEvYauHvpIQTqFIEmCUY0T4zGoTxedlVumzLzRCB0wJBnISAEhIAQEAJCQAgIASEQAAFP9yQD6C5NhYAQEAJBEOjZE0hJATiaLUys0ByFie3+xvVtl9OM3JWs4CfJolDUtdcCZ53l8U701KlFQZWWcsyIymXijTeKMW2aglXT33xDOTEPO7oqek5A4DdpvTmukqWXFS264c15VS2knK3clYzPFqfhcOwgfPr5ibv2qUlFGNXrMIZ1zSLB0LVGmPtY0c2bI4HSAuezIlCNRhoDuAJk1QSproNyPAh/BnwZv3Pun7is5AH4fchPKIlu6l1VpH6FlAGWA/b++tOC++7XoUcPXyOpf6yQopk+pDpzRynqU6lx2/eozty1ixahMUcTi9UbAjFpaeBHTRiX3aOPmf3B43E2YxYDI6Acn1c8LPoZKNUoPyxbt8JKEcZ2IY1CirVt2kDHimeEm47U27g//0TRuHGwZWUpPhvjAw/A+NxzituHW8NhdB0VEwJCQDkBTjPqLBBuzMjFeX1TlTuQlkJACAgBISAEhIAQEAJCQAjYCThnPhMkQkAICIGaIcB3aTndZhhZvjm6QhwMfFJllA7S3XIpt93GlSuxg6PkOJzFQ5hKVpYVX39dXuvQvX8grz/8sISEIj+pyVhJ9BPh5WnMnthAUXDe4t489fC+j0pDuRqHFDZtikVbGmPyeyfh1bldcDgvjtqcEAe5Q0Z2LD5c2BY3z+qPjxe1htnietzZKUd5tqIUlg6zkS8/ZBxNA3rmGTQLqIf3xu5+WBycN+yvcnHQezeXI8UlOkx5qgyL3/3bXsPR5WA1vbDQ5/6Lc88NSBx0TKWQbvx/RqJ5EdfDFBMCNUCAsw9HsjjojkhHOYb1FAapp1Toevou1QVx0HGOURQJGU/hn/qJE2EP+XQc8PDM9QZjqV5u9AsvgKNVxYSAEKgfBLqnuqYZ3ZjJeRvEhIAQEAJCQAgIASEgBISAEAiUgEQQBkpM2gsBIaAOgdNOA/bvB378UR1/IXppHl0QtAc91VNbs2QJiimcy1Gn0CEadu/Xz6vfDz4otke1eG2g8EBurg1ffFGCG27wkVKOUjuCRcIArSmOoC/+xRqcFGDPqs1Hjaq67+sVrfD1r+7xc1Xb8R6TWYc5a1KRfjQW94zfhmh9VeGypEyLLS2vwU+jLkR2XGeU6ROhtZQituQg2h74Gl12vY3Ewt2eBwhwL8X24UCAfdybcxSic1LUImMze+SgOYqF0sDMqtHj7XndYPn9Npz68SvQVXOKwfWffYYDf/8d2CSdWufs2YO/p07F6REc9eN0OrIpBISAigS0FGka++67sL38Mkwff4wyKippr79IuWI1DRtC17cvDDfdZH9WcVhxJQSEQIQQ6EERhM7GKUb5b3BeKCYmBISAEBACQkAICAEhIASEgHICstRWOStpKQSEgNoErrwy7CIJgz3FFlTHsoDq6RVS3SSHOBiVmIjmPuoh/fOPv+STymezcqWfZJdLlyp35tZyNH512xP4S44ebNHCtR+nxvz6O2XioHPPdfuT8Nq8TuAajA4rM2vw6eLWuGVmf8xc1BVZDU+2i4N83KozoiCuDTZ0vg9fn7UDvw6di5yELo6uQT8bqKfbKQXsi5MtOv8Qb+5wW0CRg+4DlukbYMHOLlhD6fmsjqJr7o1Uer2K6qCFav++9x7M1TzPUOco/YWAEKg9ApoGDWD8z38Q/8cfSFi3DgmUWjV++XLEvPWWiIO197bIyEKg1gn0cIsgzC8xI/144Avhav1EZAJCQAgIASEgBISAEBACQqCWCTjfl6zlqcjwQkAI1DsCvMqXBbSHHwaq5J90oxHmqcOapabCEB3tMukUqhMVRbWhvFl2tpPC5a2Rwv3Z2X4SaR4/rtBT1Wa9sQ7dsbHqAYV7OK3fJZe4Nj5AoXcUFBK0rd6djN/WN7f3L+D0mrO74cfVqSgy+QmM12hxoMVY/DTybxxsEnrNp1Y0A+cIQCUnxMIifyra0oOjEB1moQjA7e1udLwM+nlHm2tweOEKbJ40KWgf/jpmUPrcTEoBGKpxqtEtlB5QTAgIASEgBISAEBACSgm0aBCNpFjOw3DCJM3oCRayJQSEgBAQAkJACAgBISAElBIQgVApKWknBIRA9RHo0wd45BHgtdcAqmkGSh2Gzp2Bnj2BoUOBe+8FqL6Qv1pE1TdB/561JGCmUhShw7RGI1pR1IMvU7Mell9fHuok+pqb8zEtVfG7B1ORhv3OuxVtswZ8661Ax46uzX/5BZQKynVfoK/mrW2BkjINXvqxK7ZkuNai8efLZGiI34bOwdGGTilggxChOZFVB3o4C32exuZ2jejRjR48Yg96uNcePNxoCIopxWioxmlVd7W+EhkffIACirapDtv6/fequd2moi/VJiWOhIAQEAJCQAgIgbAlwKlE3aMIN2ZIHcKwfcNkYkJACAgBISAEhIAQEAJhS8BPqEXYzlsmJgSEQF0kwDkoOe2oN3vySeDxx4GSEm8tAt6/pzAZbeOCj65zHjDWES1INy16fvop4txVMefGtN2kiXprNBo39uMrNhbIznabgfKX8SjEk/Tfy7gPW9FVUUcqzYg77gBOPtm1OZdCXLTIdV8wrw7mxGDGbx2xNdO1Do1SX+aoePwxZDau3jASrW+6DkXbtyMzgLDG2Fat0Or885FMgvZwSie76LHHsG3TJrhTTqIJcbQgRw76spSjC3D+773x18AvkZuojLE3fwebDEeX3e9iB82pz1dfBVyTp/TwYRTt2AEzpc3VUi3D6JQUxHbqVOmnkI6rZRxFKCYEhIAQEAJCQAgIgUAIdE9pgMU7jlZ22Uh1CMWEgBAQAkJACAgBISAEhIAQCIyACISB8ZLWQkAI1CaBNm2Adu2AzZtVmcXHe09G67hs1QTCKL0eGnr0eP99NL/oIr9zPPtsIz7/vNRvOyUNxo71Iz+x+JqRocSV1zYJKMATeArLxj6L33a1x7ZtnpsmJAAjRwJnnskiaNU2VD5KNY135a5AE3y6zqcwthWiPt6GdqdpcejrrxUJhEmUDrf9ddehkZvyOfiBB6CdOBHFVFPvGA1jokccPRrTgyMIlVhy7gaM/2swfj11Ho40Gqiki8c2pYZyLlnffINlFInb+s47kXr99dD4iJK0UVHHY1Tna//06TgyZw4Vb3RNgZtA5502eTJaXHEFLCrWDTQHKfhzdGQepTotI+FbQyG0BvqwJdMHz9CIYzXrppkoVXDZkSOwlpVB37AhjPS91ugCr+NZN+nIWQkBISAEhEB9ItAj1XWB2CaKILRRegqOLhQTAkJACAgBISAEhIAQEAJCQBkBEQiVcZJWQkAIhAMBzknpTZUKcH5/Hu6Ia1ZOwNyhbwXY03tzffPmGLBwIRoOGuS9kdORCy804s47NThyJLRcmx066DBqlB+BkBU7FWrG6Rok4NSrWuNU+vXYu7fcJQWZgfQKxJEaxhrugAEULedjOocOOUEIcdNi1YboAZjz8WFobzkdhQrScbYcPx5dKeWt1kNO14T27dH7qaewltLlplgsQc/LWJaDM5eMs9dJzEtwy80ahNcCimrcRMLl0d9+Q0+KkNS51cpklwUkuq+9+GIU+hDf89etw+abb8b2++9HNL/JKlk0CV1KzUqpcrMoJen+GTOQTd81d+PUvs2prikLmQ1VnKP7ODX52kpi7KFvv0U6nXPO0qUuQxuaNUNLem/T6H2JbtnS5Zi8qF8EjtGqhH//BXIpw57jety2LaU1przGPtYF1C9IcrZCQAjUKQIcQehsxwpNOJxXiuZUn1BMCAgBISAEhIAQEAJCQAgIAWUERCBUxklaCQEhEA4EODdlCMKL4xRWHGuN8Usn0UsNMoqVixOO/t6eG5HAAoXiIPswGjW44YYYKq9Y5M2lov2TJsXQDWA/q6X79aNQNoplO3oiFZMi5+6NTj8dqBDHWAzkR6DGb2M42d78FBzI0IBTgfqyFhQS2f3BB301QdNTT0XfF1/E2kcfhTXIyDgeINp0DIPXTMIvw/7wOZ63g0ZT1bS5hylC0krw+8ye7SJw5vz9N1afdRbMrCwoMG6n/f13UEwqDipo769JM65BqsAKKQXsv+ecg0IfiwRYTMv86CP7o/mll6IH1WHUUYrUSLUMEnS333cfTF7SsJoo1evuKVOw5/nn7ZGdXUlEjHKkOo7Uk5Z5KybAa2Y2bgR+/bV8sYZb0K/dD61bsUdzDx8OyEdDMVppKASEQAQQaJ0ci3hjFApKzZWz5TqEIhBW4pANISAEhIAQEAJCQAgIASHgl0DooRd+h5AGQkAICIHwIbDqeBrGLbkFxZbyELf/7T9JvckNGRKwr7vuikVqavCX4k6ddJg4UcFKaQ4hGTMm4Pm5dKD0qRSq6LIrmBcU5BV2lpNIYTZ+rIhStJYVFPhpRWlVSSTmFKShWmrWn2iQtyUoN82PLPTYj1OH7nriCfsxTimaT9GFa8aNUywOOjttTS9Icg7NKA1Yvxtv9Osjf8MGrBg82Kc46O7k0JdfYhWJupai0AR4d7819XonRaNuvOYar+Kg8zxstHAi85NPsHLYMJhCXQTg7Fi2w5YAZ/mdOhV45hngn3+qZASunDdHbHNpVa4HuyW4y0mlL9kQAkJACIQTAV4c1y3FLc2o1CEMp7dI5iIEhIAQEAJCQAgIASEQAQSCvysdAScnUxQCQqCOEeBIoCDrbZmsOry/ZyCGL7gTR01UJK/C5h/phK15zRwvg3/u0gVozZJJYNa0qRY//9wQDRtqAutIrZs312LevIZISFB4KT/7bKBv34DHqewwiaIuVajvpoKLyimptWEy+I8kzSUxjdOHcv03X8b1bw5RLT81bNg/ExBfuC8gV/qyPLTf/5nXPntefhkLUlLwGwm+y3r0QBnVtQvW2lLHUCrgdSDROplSs/qy0oMH7RGOZZxDMUDLWbIE66+6yl6TKMCutdp83+uvY9eTTwY8h7w1a7CG0uBGqiga8AnX0w4sDlLgqF0YVIogP79cTFy7VmkPaScEhIAQCH8C3d0Ewo2ZyrIhhP+ZyQyFgBAQAkJACAgBISAEhEDNEFB4V7lmJiOjCAEhIAR8EqBoI7AQF4DtK0zCIxvGodWcZ3DDqqtQaHEPX9Ng+q5TA/DopSlFKgVrvXpFYenSJNIXlV+Su3bV4e+/k9CuXQDyDIurd90F9OoV2FSZO0d5DR0aWD8vrQcODFrndfXI+fVUMp1FWd7T41THMZ1SdPqyQirOmL9jh68mio81yV6Fi39uizOWnoe4onRF/Tru/QgGs/dIRxsJnCy6wVM+QkUjnGjEn74mJ1563eJ20fSIpQd/A+kTZbdBd99dseX9aRcpIaUUvRmsZdH7dfSXX4LtXuP9CnfuxDYFXLxNLHf5cuyhNLdidZMAX/befDO4crxmysL36qvA/v11k42clRAQAvWPQA+3OoSbKMWomBAQAkJACAgBISAEhIAQEALKCSi/G63cp7QUAkJACFQfgQCEuG/Te6PTL4/hua1jcLjUNQWR8wTf3T0Ei4+0d94V2DbX96P0h6FYt25R2LAhGdOnx6NbN5ZTPFu/flF4770ErF6dTPX/vLfz3Jv2RpNMw3X0qJabfdtrw4oDqanAAw+UF7Hy11bh8YYUrHfKKQobe2tGd8kNZerdBIotOeRtpCr793/3HTg9pzcrprpwapoWNrTO/AHj/xyIpJz1Pl1z9GD3Hf/12Ubtg001euS0ugrHOt6BYx1up+0rURbT0j4Mx2V2pkd/evShB0vTfenBiX2HkFLclCIZfZmZwp4yOT9iiJb+1lsBebBStKKFah1aNm+GlcRUjgqtKTvw9tvglKGhWDr5sJpMobiQvmFCgK81tpwc2Oi7wJ9DThO6YkXwk+PSqF98EXx/6SkEhIAQCCcCPVIbuEwnM7cExwvl988FirwQAkJACAgBISAEhIAQEAI+CIhA6AOOHBICQiAMCZx8Migfp6KJXZi2DktGTMO5KeughXdBp8ymx5sxt6KgcStFfl0acUTjnXcCXOMvRONUoZMnx2LjxmQsXNiQUsjF4e67Y3DvvbF47rk4LF+ehFWrknD99TGIiXHEYAUxaFQUQGkX8c475ZGB7qlRudYgh/k9/nh5uAkLoCpbqOUQO+19Hx32hS4c8WlFlxxGs6NLFJ9hUXo6jlEkoTezFBd7OxTS/riSTIxZPJpSju716EdrNeGMZecjsXCPx+PVtTPWRhGJJA4e6jMNh/q+hoxTPkVB/5noYWwG+nYgiR7un1b6BMJCkW5Lu3fHqtGjUZKZ6XF6mZ9+CouCuo8eOzvtPDJ3Lor37XPaU3XTRrUKTR98gAK6xuQ3bowC+m4X0PzyScTMb9MGJc89B6vK4q/7LPizc+D99913B/zalJWFwyRki0UmARt9DkwffYQCqmeaR9fjvKQk5CUmIo/SbM99TPm1ytvZr14NSKlKb3RkvxAQApFEoH2TOBijXP8G3yRpRiPpLZS5CgEhIASEgBAQAkJACNQyAb5HJyYEhIAQiBwCLG5dcAGg8Cb6ycn78f2QmdhflASOFPwrqxOOaRrBlpSM5GQNTjvNgJtvjkb79uS36Ony3G0+xB8XUKedBtx0E2AwuOwO9YWGUnryvPhRrcY1HTkikx8csVRYCLA4yFGGnFa0Go11VdKF8OuvQQxCUTQ9t71i77i54+1BOHDt0nnPLOhIXAvEMillZeMBAzx2iYrlRJrVYxzpOHzFlZgzcqnLAHqKpjxj2QVIyfrLZX9NvYgrPoCj9jhBoMPej3HqquuhtSmLgjv2229YQYJ0/99/R1xnjjc8YYe//fbEi1C2KAor6/vv0fqOO6p44ags0/TpKCVB3JadXeU477BRTsZSqj9Z+uSTMEyciGjK06gxcrJUde3ITz/B7GUOgY6U8eGHaHHZZYF2k/a1SICjBUuffx4m+nzZPNQGzdEkYY0m1PBr+jxTQCyXSZWPRy2+2TK0EBACqhCI0mnRpUUi1qXnVPrbmJGHUzsqSYBe2UU2hIAQEAJCQAgIASEgBIRAvSUgAmG9fevlxIVABBNgZYmiuECCglJrFZuNKT3mgAr9AU89RcXQPIg4vO/++4G9ewESLbB4MVBa6joEtxk+HBg1CuD0m7VhLOZR+kPwDWROI8hzatGi/NyCnQ/XJ6QIlZq0a68F8vJAtRQDG7XFkQVomL/V3qkFCWIHm44MzIFTaw2JWJ13v+u0R9lmMdfw82Ix1fy5aHZsGRofX4mjySfDYMpBx70fUFrR15BQ5DtCzst0VdkdZSZxmSwtcw5OXXmdz4hdTwOW0PeZIwkHUlShsXnzyiYmFSP2Sj34YnGwhOpyml57rXJMnxtUv9E0YwYsmzYhjsQ8TUKCz+aBHizavTvQLl7bF++p2UhSrxORA4oI2OhaXkyR3WVff+21/aZmo2HR0iIOFYzXwYhAqAJIcSEEhECtE+iR4iYQSgRhrb8nMgEhIASEgBAQAkJACAiByCEgAmHkvFcyUyEgBBwEOLrthhsAjuCZQ6KfUuOwNRYAPYmDzj7atCmPDOQ0nCwWcopDHjM+HmjbVlntPmd/am1THSr8RRFiLIxSjbQq1r59eTQg10OshuimKuOFuIM1SQ7osvz5GVZGXwKbghvf8QW7MXz5FZUjn7L2LswdsQRl+uCEmj6bnyZhbX+lP6UbvtKIxrVsiYa9eiFn/Xql7gJuN2DdPciL74h26V9AbykKuL/aHUz6RESZCzDsnwkBi4OOuZRQCtCtlK63t1OBNCsXTFPJrB5Sv5bSYgHF4qDTPCwLF6Lo4osRyyIhR92qZBaqM6eWqelLrTmJH88EWKgupt80X+Ig98yPburZQRB7eXGGmBAQAkKgLhBwr0O4YGsWsvJL0DSBMmKICQEhIASEgBAQAkJACAgBIeCTgNbnUTkoBISAEAhXAlzzb8IE4KGHgN69fc+SBBsq3Ac89li5yOe79YmjLCR26wZwKkmufdi1a+2Jg/PmgQoUAiyeeBIHeda7dgFvvQXcdhuwefOJ8wjjreId29Bz7lW4bG4r9Nv4KOKK0qvMlqPTOu96B+f91geXzmuP2NJDlW0a5a7HyL8vgs4SeN2/Trtnoi8JhMGYvzSiaeefH4xbxX1aHF2MzlSHMRzEQZ50XnwHtN/3GYxlnlN0Kj2xw99841KPMKpBA6Vd/baLcqtdaqFCbCwQBmtmyo9r4u+biqbjRQgqmZq+VJqSuPFCoOzjj1FG9Tb9mUWjnhhtNvsbTY4LASEgBCKDwKB2jVwmWmiy4JVfKdOGmBAQAkJACAgBISAEhIAQEAJ+CUgEoV9E0kAICIGwJtC3L8CPQyQacXRdZibAkUJcFzApCRg6tFzYq+aaetXK6H//A2bPVj5Ebi4wZQpw991A//7K+9VCywOzZtlH5dp6fbc8i95bX8DxBj1RamgMqzYKRlM2GuZtgoGi07xZy8O/4awFI/Hn4O9QHEOpVv0YpxXts/kZEgefAsWFBmUxnNLVhzUfPhzb33wTpd7EXB99I+1QVvJA5CZ0wYjlode7s1H6XP5MdKB6gGzxPXsij4Q8NSyBfDlb6RtvOL8MattE77GBBHkNL1hQwaI5BbJKFt2qlUqexE11EuDowdJp0xQNEVt2osaWog4+GsXF+Tgoh4SAEBACEUSgTeM4XHRSS3yz+kDlrL+m7asHtkHPluotNKp0LhtCQAgIASEgBISAEBACQqAOEVDnjlYdAiKnIgSEQIQS4LplV1DqyXvvLY8UfOCB8jShHAEYyeIgRSkFJA463j4OD/nvf4GdOx17wvI5e+lSl3lpSbxrnLMWqVl/IO3QL2h6fIVPcdDRudnx5biEogu5/l3j41Rcy4NFlx5Bry3P4+KfO6BfCOIgu04ZO9bDCCd2aUmg7kXRaWqmnzzhPby2tnSYjOTcdeBoTjUs88MPK92k3XRT5XYoGwa6PjQZN67ShZWE2zKnVKaVBwLcsO7YAfOffwbYy3vzpuecA51KdQ1Trr7a+0ByJGwIWKgIq3XtWkXzaZW9RlE7JY04W7aYEBACQqCuELh/dGfEGSh3fYXR2gs89dMm8CIMMSEgBISAEBACQkAICAEhIAS8ExCB0DsbOSIEhIAQqF0CXPuQUs8FbWVlwHvvBd29JjqWHTmi2jBRlGa0094Pce6fJ9vTkQ5ffjkGr56EU/+5FqMXjcGlc9Jw8saHqebg3pDGjKPIrOR+/fz6SKbI1t4kErJYWFetyNgMe1pejASqDamWFVPdT5vVanfXYOBAJPTpE7LrlhMnurwPZV99BZSWhuyXHZQ5CZqhOoyiFKOp11wTqhvok5PR/JJLQvYjDqqfgCmAa3Q7WgiRmqOOED9qVPWfm4wgBISAEKgpAk0To3HryA4uw63al42f1h902ScvhIAQEAJCQAgIASEgBISAEHAlIAKhKw95JQSEgBAIHwILFpD6QCJfKMZ1CcM5irCaojsbUURb+/Qv0HX32+i07yO0PPwroqzqCEJpF1xAQanKkpM2GzYMvZ9+GpqoupfR26I1YP7AL2DRRUNvzg/lU+ral1b7WwoL7fuYc5t77nE9HuArHdUSTbv5Zpde1t3qCZrWPXtcfIf6Im3SpJCjnlNvvBG6mJhQpyL9a4CAdds2xaPwVWf4rhmK23trmJIC9Ojh7ajsFwJCQAhEJoHrh7RFWrLrb98LP29BMdUkFBMCQkAICAEhIASEgBAQAkLAMwERCD1zkb1CQAgIgdolwBFUv/2mzhzU8qPObFy86LlOZARZIwq7SeNUtgotn4SoLZTq1cYpX+uQmbVGuzh4qOlw+1mZo+LVOzsSBVnUc1iLK69Ey2BTjZKvXp9/jujUVIe78ud89QRNm4q+eHLxlBa5A4nKwVpC795o/8gjwXaXfjVMwJaXF9CIA/d9igbFmQH1cW9MmWxrJPO2Zc0aFFH0bn737sijuq159D3Mp89nMYn+FkrPKyYEhIAQUJNAtF6HR8ZSaQEny8wtwTuLaLGcmBAQAkJACAgBISAEhIAQEAIeCYhA6BGL7BQCQkAI1DIBSrOIQ4fUmQTVuKIiLOr4UtlL8umnq+yx+twlnXYa+nz7LbQnnQR06gQkJnodLHvdOqx/8kn8ff31KDl82Gu7SDxwrEEvzBs+H/tSz6+cfkFs68rtUDeiW7aERneijhBHEXadPh2pN9wQkGuu/9jz00/R9Nxzq/ajVJ5qmUZFX445tSOBr/WddzpeKn6O69IF/ebORZSPz6ZiZ9KwRghoAoz0jLYU4ral51Bt1vIo20AnecYZwIgRgfYKrH3ZnDkooPTABXStLJs1C9bNm2Gj3zNbZias69fD9OqrKKBraOHo0TAvXx6Yc2ktBISAEPBBYHT3ZhjUrpFLi7cX7kJmTrHLPnkhBISAEBACQkAICAEhIASEQDkBEQjlkyAEhIAQCEcC2dnqzYprrZWUqOdPRU9tH3ywZkJZQpkziVUcwdafIjGjEhIALf10Nm5cnqOPomHQti2Qllb+oO0iavPPrbfi4O+/wxZqithQ5q1iX1uUATtbXYGfRizB96PWIqvRIBfvR5NOQnZCV5d9wb5IufrqKl21lKK1+8yZ6DpjBqJb+xcjWcw9ef58pHiJ9tRSHUm1TE1fjjmxKNqZRJQur78OnUIBssn48Thl2bKq0ZIOp/IclgQ0nO8zQGuTvRp3LB6DWNPxgHqyOMg6O328qsVstBCl5PnnUUSfRcuKFX7HMNM1tZC+q6bPPvPbVhqEB4H9+/fj3nvvRdeuXREXF4dkqnc6YMAAvPLKKygqKlJtkr/88gsuoHTeLWnBiNFotD/za96v1JaT+HwDfeA7d+6MeLqOsp8WFMk6ZswYzCLh2mQyKXUl7SKIAP9+Pj6+G7RO17mSMite/GVrBJ2FTFUICAEhIASEgBAQAkJACNQcAQ39Yz48w0pqjoGMJARqhcCBAwdIUyBRgSw9Pd1+86NWJiKDhicBjvqj1JSqGYkraNBANXdqOlo2aAjyly9T06VPX1ZoUWpoBL2lADpLMZzuIVXpl0wpRXt99BGMdFMxENv51FPYRRGEkWw6Ejp7f/01Evv2Ra45Cf+5S+8zELXbjjcwaO3toZ0yia+nUVrWGB8ioM1iwVG6SZz+9tvI/ecflJGYrqVoQX2TJmgydizSJk9Ggp8Ca1aKZspnkVAFATf2p5+gHzcutPP20dtMKUwzKRIynaIoCzZtcmnJkYIp11wDrlsYTzfsxSKPgOnLL1F82WVBTTwrrj1+6PE0Vre8CFwT1JuRxgLS7DB8ePWJgzx26csvo+T++71Nw/t+uqEfS9ca/YUXem8TgUfq2t95cyk6+UpK+Zybm+vx3WAh7ueff0a7du08Hleyk/9Zesstt+Ddd9/12vwmWrDzNl3/WQjyZOzj7rvvxrRp0zwdrtzXs2dP+3xZhFTb6tp7rzafmvD3yOwN+GzFfpehvp00CCe1TnbZJy+EgBAQAkJACAgBIRAIAfk7LxBa0jZSCIhAGCnvlMyzzhGQH5U695aqe0KUohLPPqueT47QIBEl3Iz1jlkP/YvRP/XzKdT5mjeLQ1aFQs/elPOwYOD/YNFF211GlxxGx70focuut5BYtNdlGI7ganPXXS77lL7gG5Rb/vMfu6ijtE9YtSOhriN9/tpxhGeFvfQSsGqV41XVZ31ZLi6bk0apD4Ov79f0vPPQd/bsqs6rYU/R5Zej7IsvQvKsISEzYdcul5SoITn00Zk/U8WUeth05AhsFPnC9Ttj6Ea8LsAUlT6GkEO1QIDfSxarbSGkIs4zNsWys17B2i5Xg0sacslTLuPZpg1Aaxwo2qt6hUHGZl6wAIWh5C6Njkb8xo3QtW9fC+9C9QxZl/7OW0d/kwwePNgeJcjReA899BClqh2B4uJifEHX0Zm8CImsC6U5XrlypT1iLxiqj1B65eeee87etS8tTrmfBOf29JnYRdfZl+hH6N9//7Uf43ZTpkzxOAS3e+CBB+zHEmihC4uFQ4YMsc9p27ZtmDp1KjbSZ42tV69eWL16NaIoSl1Nq0vvvZpcatLXsYJSjHhlAfJKTtSA7tWyAb6fPIQSQXgWl2tyfjKWEBACQkAICAEhEJkE5O+8yHzfZNa+CYhA6JuPHBUC1UZAflSqDW3dcHz0KECRUKpYs2bAG2+o4kpNJ3RPjtKSARQQhlP+vR09dgY+xwSqY9WPbgYeoVRi6VT3Kr/ipp/zPK06A3anXowt7SeXp8b0FHVgsyLt4FwM2XAn2o8dYK//1vCUU5zd2LetVqCQSn859EgulUdZ1ujmYpWmFG1nwz4SGXc89hisdBM1Eq0N3ZztRCkDNSQYUhkx+AuKbJv+FUYuvzSoU+UozVPofYxRMf2nr4mYly5F4dChvpr4PRb94oswBhMx5dezNKhPBEoefxylzzwT0inHzpsHPaVOrC0rpChaM0WYhWIGqrsZo2bkfCiTUaFvXfo7j8XABSQCs5C2aNEiDBrkmmb6ZYoeZTGP7SmKoH+cPtOB2s6dO+2pS82kcPfv398+TozTAghOYTps2DBaqLLKPo+tW7faxUPnccrox7kZ/c2TTZHlBoMBKyjVbZ8+fZybkIBuxlC69vMxtm+ptjCnL1XT6tJ7ryaXmvb13pI9eGYO/fHiZC9f1AsX9y/P4OK0WzaFgBAQAkJACAgBIaCIgPydpwiTNIowAlRISUwICAEhIATCjgDXuHO7qRX0HE8/Peiu1dVxz57yDKosDrKt6PMadqZdXv5C4f+bUETAyVSjTUt1hdKobt0giiwYSCknu73zDjq+8AIaP/IqVgz7BP8bm46Fp3yKrMaDvYfRaLRITxmPXy/ZjqQX/wd3cZBLOFLwlj2CbsMGgO5L2h8cAclRdXRfEwUFrhPn9Gdt7rkHwzMy0Jluesd27OjaIAJe7SXxdesdd9jFzm7dAC8l/SrPZE/aJVjWdzpsAcaDGuiGbj8SOGpKHOQJR9HnhwWJYE1HN5gNt4eYUjXYwaVfnSJgpJpuWj9pcX2dsJ6+mFGjR/tqUq3HrHRBN1NqyVDN9MEHsPEKDLGwIsARgSwOsnFNP3dxkPffQ791XJeQjVN7slAXqP2XfidZvGN7gxY1OYuDvC+WwmJ5Pxu385RCdMuWLXZxkNuMI9HaXRzk/SxyPvzww7xpt2VUu1WsbhKYMKg12jehVVxO9tKv21BQeiKq0OmQbAoBISAEhIAQEAJCQAgIgXpJQATCevm2y0kLASEQEQTOPDP0aXJoWyhp30KfgUcPVFINlFnvhJGYtvCUz7Cm6+PgGoHeTEPn03jgQHS67TY0POlkmDhKgOqwIScHGlLrGpComkb1iWImPIAZB+/CxqZXoSS6qTd3VfZn5+oobRlAWRztxhGDlNkMa9YAmZnlqfvcO3GbrCxg/XrYo+wq7m9WNuNUkG1IiBpKquIQunkZTynNIsn2v/kmDlSkjzv3XODii33PfkuHyfhz8HcoMlLkqgJrQO/nQIocTOzdW0FrdZtEUwirfsKEgJ1q+/VD7PffQ0NpEcWEQKgENHQNiyOBTRtE7TYWBmPef99rPbZQ56akv2nWLPgsUKrECbeh2nZlVItQLLwIfE/XOoddd911jk2XZy1FmU+ouJZy9J5DUHRp5OMFR9y0B7lYAABAAElEQVT/8MMP9hacpnQg/S54Mt7PtQ7ZeF7cz9lMTn9Y+KqFyGlLHVZaWurYlOc6RkCv0+LRcbS6ycmO5Jdi+nxa1SUmBISAEBACQkAICAEhIASEgJ2A97uwAkgICAEhIARqlwCJEKC0iyEZp1Bs0CAkF2p3ZqGNo/CqGImE//Z4Cl+evQ9ruj2OYmOTKk1spL6t3ZOI6WvOwF/J9yDOOR0lR56QAGczW0CBhcjPr9Jd0Q7SGjF9OsDC3//ZOw/wJqvvj3+TNuneUKBllL0UkC1LpmwBRUQUQVEQFBeunwtRQFHUP6iAqIjiQEVR9pC991L23rN7Zv7PTUiaNGma8aZN2nN8Yt73vveee+8naenzft9zjogUdKU0mBgr9mZxj9I8p0jTqYiNRaZQEv3MTlP6Qx2xp4/IIBBSSSdYoi+4nXOJ/fFrn/M43O8XKO5qX/CyIeozYdgwtKIUb60oeiNEFEsrAZNRjtiQuXMRRCnx4IzYRwAUgwcjfMMGyOPiSmDFPGVpJSCvUgVh27YhoEMHp7eopIchQhcvhoyiqEvStLfrwkmxBq14GoPNpwhs2rTJsJ4wyqfdrFmzQtcm0n+abPPmzaZDp97PUBTqJYq2F2bpx95g03WR3umsCO23sNoUqS+i94WdPn3a4or1oahpaLI6lKqcrfQS6FQ3Hp3qWv89+e2mMzh/K7v0bpp3xgSYABNgAkyACTABJsAEXCBAoSVsTIAJMAEm4JMESFCivF0A1bCDOzXsqlUDhg/3ua2tWuV4SdmhlbGv4QTsr/8WKtzahuDcawjU5SJPEY2UqDuQGVbd4OD6cS2GdLiE0CBtvkPK83li6w2K+quY3+bGkai3J+6JKhSuDxYflRAWGzYERI1CS1NdvWp56jfHuXQj9gbVeKzQv79hzSK4Q5RoPHYMEJ8n6bKGFKviKxsebtx79+5Kqg81mPoPRi6pwmLvWqohFUiCtRAEAyMifGL/4mZyMNXLUlJUqprEQtXMmdCJnLEWJiNhV/HEE1A+/TQCLCJPLLrwIRPwmIA8Ph5h69dDS6K5asYMYzRdgVSNMopINnwXR41CQDGnLc6lCL/kEycg3gNJUA+vWBExFPWop4gxqUwvnrJg8ykCIm2nsFq1ahnScxa2OBH5ZzLTGNN5Ue+W/S392BtneV2Mq17d+DeB6BtF/7489NBDmD9/PpbQv1kH6YGcRgWi9kV60g+otq6wSIreffhh19KbGwby//yKgIgi3HRiIzQ6Y8SpSqvDpGWH8dXQ5n61D14sE2ACTIAJMAEmwASYABPwBgEWCL1BlX0yASbABKQiIMK0RK2cKVNsi9w5mkOIg2Ic1ezxNaNyRk6ZXq7A1fKFR9PkqgPw74UotKyVbOVvxRo3VD0rD8YTChJD1652LjjRJOoRXrkCVK5s3VnrjtBr7aLEzi6QYGESCMUiRJCGuB9scU+40LUFJyRAvHzZ5CQCBlFopKhLqKPIE/3Nm4acskKQkdON8ZKO0vJldrw26QgIwVrUxxQv3bRpENF5QoAT6ZVlVJs2oEULyEJCpJuwCE8iheMlivTdTcL5v7/+Cm2BdIwVKDVwfxL+JVuRUlnEivhycRLIpQK8N8XvQrLKBf9BK7CQGPpdKaIMsyia/8KFCwWuOj617F/UPFUo2tZkluNMbaKW4VF6Smf//v1o3769oT5imzZt6OGVcHqo5RjE9QMHDhhqHM6lh0LKiZrPLpqIXnRkV8QfAGw+Q6Bm+XAMa5OEbzefMa9p5X/XsPXkTbSp5frnb3bCB0yACTABJsAEmAATYAJMoBQQYIGwFHyIvAUmwARKOQFRb2fSJIBuZNEdL8e1nsTNVZGibuhQ0N0vnwSTnm5cVlzKHlS6vh5BqluGhjxlHK7Ed8KtmKZOrzs9x1oMzM4LwPbjMU6Pd9RRRAF27AiKmHDUq/BrIjVpYqJRSDP1UkRHmw797j1l40ZDvSdT+ja/24CTCxapYANIEKRwGSdHcDcm4B0CchIu5N26ece5E17TSOT5g1LqXqCIxsLsGgktQgqqU1gHF9vl5a1TAbo4nLtLTCDDIle3ENiKMpNAmCmeknHBXJlHzGEye/NUpMhWkeJ09uzZ9GzVFIwfP97U3fAu/g0bMWIEXqIHQho0sK5PZ9XRwYmlSOmgG1/yIQLPdamNhfsuITlLZV7Ve0sOY8nYdgikWoVsTIAJMAEmwASYABNgAkygrBJw87ZnWcXF+2YCTIAJFA+B8+dv0M2tlXST6zBSUjINNXXi4iLwQLvuGJaoQ9i+PUAyRc5RdIdBwRK1Cjt3FsV7jDkei2eZLs+ipWiEasfno96JGSifYj+U8HpsSxytORqnqwyGNiDY4RxqLYWxWdjNjCBotNLc6KEsZBQJ4X4JRxFoIzLvUWCa2YIpIjSAbm5qhWM/Mx1tSEcRkAE+GJXqZyh5uUzA5wncoNSN8yiEOkMUjS3CRKU3qQTCwH79ipiNLxcnARFBaDKlE9GdQbfrYea4GC3vyjymOcS6Cptn/fr1hjSj1+wUERZRsYupdmc8pfR999134cy+TAz43X8JRIUoMO7eOnhz4b/mTRy9moH5uy7g0dbVzG18wASYABNgAkyACTABJsAEyhoBFgjL2ifO+2UCTMCnCezYcYxq4yygm1e7oNPpbNa6du1BvKAIxMCBbfDm26+hYR1K2ygK5Yl8jz5u2ZS2cW/v3mgvQvMcWHzyTojXnUc/wqr2S801B+0NCbOsP0gdslUFiv7ZG+RCW4Fsei6MNHYVmdksBcIAiuqs9OijuPjVVy774gFMgAkwgeIgkEn1Qn/q2dMpcVCs5wS96NEUj9OMyu+6CwGiuCmbzxAIplqTJlOp8iOvTG0F3/Nu/6MZ4mIGA1fmMc0h5rY3zzRKzSuiA8XfUB0oo8LbVMe5ZcuWEHOcOnUKc+bMMaQZFXUIt2zZgmXLlhlSoxbci6Nze6lNLfuLFKNiTjbfIjC4RVXM23YOQhg02SerjqFvowREhVpnpDBd53cmwASYABNgAkyACTABJlDaCUgTZlHaKfH+mAATYALFQGDu3DVo1+51/P33DrvioGkJarUGv/yyES1bvYLFKynlqB+Ig1knT2LH3Xcjqwhx0LRH8R6TcQR919yNiMxTls1Wx0nlrSPxggJtRVWrAS6eCO3VE7N3P7XK6NGeuCyxsXK6uSp38aZviS2WJ2YCTMBtAqtffRVp5845PV5LPf9zunfhHYPGjDFEyxfeg68UN4GIiAjzlPbSeZov3j4Q9QeFOZOO9PYQw5sr85jmsDePqC1oEge7UgTs2rVrqZZwV0RGRhoiBevXr4+PP/7YkH5UjN9IqbNFFKGrJuokOnpVElkd2HyOQIBchnf6WqeVTclWY9oa8ZgDGxNgAkyACTABJsAEmAATKJsEWCAsm58775oJMAEfI/DDD2vx+OPToNGIW63OWXZ2HgYMmIzlyyndqA+bOjUVeykaRXX9usurDM27hu6bekKhTrMZW6dSOqqWy7ZqjwkrOsLBakARJ55m09Ta+TgjGzdGbJcuRczse5djO3Xim/e+97HwipiApASy6Pf0f7/+6rJP8a/Q7fKyLo8VAwKaN4dC1M5l8ykCIuquHNXCFHbx4kWHa0uhnNom8c7VGn1CbDNZUfNYRu8VnGfu3LnmB6wmTJiAgAD7WQWeeOIJ1K5d2zCliCgUaUfZygaBNjXLoUfDilab/WHbWZy87lrdTCsHfMIEmAATYAJMgAkwASbABPyYAAuEfvzh8dKZABMoHQT27TuFESM+d2szWq0ODz4wGWdPXXFrfHEMOv/ll8imCEJ3LSrzBOqfmmkz/N5GV23aosPUuKNKqk27Ow01agC3yym5M9wwJrCQRN53/vADgqtUcdtvSQysQtE9zprIjrufgluXLgUWLAAWLQI2bTLWdHTWB/djAkyg+AnsI7FEay/0uYil5ND1v+gl3l01ea1aCF2yBDJPf+G6OjH3d4qAiLoTdpL+HdeI4ryF2FGLDAGmMYV0tWlu0CA/qsvSj01HarC8XnCeI1Q702RNmzY1Hdp9N11PpnrO1914gMmuU270CwJv9KoPZWD+bRCNTo+JSw/7xdp5kUyACTABJsAEmAATYAJMQGoC+X8ZS+2Z/TEBJsAEmIBTBKZO/culyMGCTrNy1Phi2HgqwGcdTVewX0mc6+hmohT19uqdmgWZPj8cr1xEHlrXvmV3S/aEQ7sdi2hs1KiIDk5ctijfZNU7OCEBzf/5ByFChfQDC65WDeUpCrQoS6NAzz//BJ59Fpg8Gfj+e+C334AffwQ+Jw386acBUX7x7NmiPPF1JsAESoLAQfHD6qYl0zgRe5jiwvgASj0dRnXg5BUquDCKuxYngXbt2hmmE9GBe/YUnrFgw4YN5mW1bdvWfOzMQfXq1ZFA/y4Ks/Rjb6xICyosMTERSUlJhmPT/wItnspxJGaK/mq12jQMluPMjXxQaglUjQvFk+2qW+1v/bEbWHfU9UwXVk74hAkwASbABJgAE2ACTIAJ+CEBFgj98EPjJTMBJlB6CFy/norff9/i8Ybm7LyKnHcmALm5HvuS0sFNCiHLvXDBI5d6yBCRfQ6Vrywz+AlRavDqfUfo6W/7KcGa18lAbKz9a84uJCpSB9LEPLb4+MJdhNWpg1bbtiFBpNXztNhh4dNIcqUWpWqTFZKqzTTBzp1GYXD+fODmTVOr9XteHrBmDUAlzkCZ4CgVnPV1PivbBM6f1+KttzLRoMEtxMXdQFjYdRINbqJHj1T88UcuPUjh2c912abr3O7TPfx9LeK3hcS4kl65JPoUZoHduiF04UKEkdgjd/SLsjAH3F5sBPr372+e67vvvjMfWx7o6Jf5DxQZLyw6OhqdKCW1KyajWsr9+vUzDBERgtu3b7c7XLSbIghFfzHO0oTQaLJNImy9EBPi4Db691dYVFQU/c0QW0hPbi6tBMZ0qoX4iCCr7b1PUYQqDf9hYgWFT5gAE2ACTIAJMAEmwARKPQEWCEv9R8wbZAJMwJcJfPfdGnqKvfCUXc6uPYUehP99I6XxpHSevmTXKW2cq5aniMGh2i/iz24H8P2ATMx5UIe5A7JwI7YFiYJajO1+AknlC4+WDKxUniLVZJC7+S+cXKdCp4iVdOPR1ZVb9w8PByIirNsKngXRjXGRbrTxL78UvOQz59Vfew2Jw4Y5XI8IHPnkE0AIgM7aMtJ7RVQhi4TOEiu9/Y4d06B//1RUr34LkyZl48gRLZKT9Yag6CtXdFi5UoWBA9ORlHQLH3+cBa2WhUJvfRtUmZ7X4RKx3iLR49kXXkA4RZwFT5+OIHrIIOj99xE8YwbCjx1D2KpVUJDwJLOI+PLWntivZwRatmyJ9u3bG5x8++23ZmHN0usn9A+AKb3n888/T8+8KCwvY/369QYxTwh6w4cPt7pmOnmBvi+mSL6xY8ciJ8c6Ya04F+3CRD/Rv6D17dvX3PT6668jPd1+Zczx48fjypUrhr69evWyERrNTvig1BIIDwrEqz3qWe3v9I0s/LDtrFUbnzABJsAEmAATYAJMgAkwgdJOILC0b5D3xwSYABPwZQI7dhyTbHnbk3V4bMcO4PRpwEdSV6pv3HB6f3mKaOxqNAUnqw2FNiDEapw2MBTiBdJSP1laF61q3cJjHc4iNjw/RZhhQCj1qVwZTZKM6SxnzgT0LmgJMp0a9+wYivKHTgAPFp1S02qRBU4qVSrQ4OA0QKzbB63We++hxltvOVzZv/8Cs2a5xtnkkDILolw54JFHTC38XtYIrF+vInEwDWlpRf+gXrqko+jTLGzdqsbPP0chJMRDFb+swXZiv0p6qiFP5AqWwISvAKoDJ15s/k1g2rRpEGlDhUh377334o033jBECYrz+RQ2Pnv2bMMG61Bk/Lhx49zarBj78ssv48MPP8Tu3bsN871GD6jUrFkTp06dwpQpU7Bv3z6D71deeQW1a9e2mUesrXPnzli7di0OHjyIJk2aQAiWQuQMppzfoo7iHKqzuWLFCsPYsLAwCLGQrWwSuP+uRMzbdhYHLub/zpu25gQGUHtcuHV0YdkkxLtmAkyACTABJsAEmAATKAsEWCAsC58y75EJMAGfJZCSkiXZ2lJUt2+wU2SGodibZJ7dd6SzqPHjyEtmSBWs7LACqZENHHUzXNPq5Nh6vDyOXY7EGwMOo0rc7SgDUfCvfn0RWmDo17GjMYKPAlaQkVGkWwTnXkPHHY8i8fo/EAmm8ma8hqAxU4oeaKdH+fJG4cvOJbtNgZTizNcsmm4Gx1DUSMEUbpbrFOLrvHmgiC7LVteOFy0C3XAGBDO2skVg9241evdOdbl86l9/qfDww2mUdjQKAQEsEkr5rYklMebK3r2SuIzxkQdVJNlMGXdy11134ddff8Wjjz5qiMoTAmFBEwLfUkorHlFU6HzBgRbnkyZNwvXr1w0inhADBw8ebHHVeDhixAhMnDjRpt3UsGDBAjzwwANYt24dzpw5YzfSUPQtT//o/Pzzz6hbt65pKL+XMQJyuQzv9G2IB2ZuNe88I1eDT1Yfx+QBd5rb+IAJMAEmwASYABNgAkyACZRmAm4mYCvNSHhvTIAJMIHiIxAQIN2v4UDTffLNm4Es6YRHT2goqBZRUZarjMWKe1Y7JQ5a+rqVGYT3/2iIG+n0lLeY5066mRNk/cR3s2aAEAgfSPwb5ZJ3Ww43H8ff3EpRg4/ioaXVDOKg6YLqp4+Q9/0k06nT73FxoIgHuJSiNKxePcgKpGRzekIvdUyl8L5dVEfqJEUR6gsJwzxBgZZ0/9UjE67/+ccjFzzYDwlkZ+up5liay+Kgaat//62idKOFpxo29eN31wg0LiT9o2tegBj6JVjtdlpKV8dyf98kINJ3iqi8F198EUIMDKXId1FvsHnz5ubovlq1anm0eDnlBhdpTIXQKGoMJiQkQKlUGt7F+TLKTf3NN99QCvHC/3aKiYmhWrdr8Ndff2HQoEGUurg6RRuHGNKeClGwIz09JKIRRS3Drl27erReHuz/BJpVi0H/JglWG5m/8zwOX7afntaqI58wASbABJgAE2ACTIAJMIFSQEBGN/1uh5yUgt3wFpiAHxG4ePEiqlSpYljxhQsXKCtiZT9aPS9VKgKDB39MT+VvksTdC7UD8FkThdGXeLqfUmuVtJ397DMce+klh8tYc/cCnK38gMM+ji7WqaXF+5MCHApyWZRWbDOlI0uNqIfM0GpQB4ZDoclARNYZRGWSyuXAAns+hqCnJkJewfjzWljXgADQTUxDhlOHayls/IEhQ3DVR2sRVqdaTnU++MBm6aKG4CYJvr4igFIIuT6mkdrslxukIzBnTg5GjMjwyGHFinKcPx9H3xvT0xEeuePBRCCX0ot+mpgItYcPmXSbOhVt3Ew1WVo+CP47r7R8kq7vgz9715mV5IgraTnoPHUDctT56RBa14jFL0+1dphFoSTXzHMzASbABJgAE2ACJUOA/84rGe48q3cJFP74pXfnZe9MgAkwASZABHr2lK42U0+6WW42Z3Jqmjt77yDhsccgF6k/C7GM0CScTRxQyFXnmo+fDICIZHNkYRTVENe9O6IzjqLytZWofukPel9VpDgofGqW/4CsB6sj5/X+0B8Waaisn6uhEkaGiEEKoiDRPz9yUJOZiQtUl+ng0KHY26cP9lL0xSGKzrlMOTm1ubk2y606ZoxNm680nKGaUJd/+slmOVQmShITJc+K+gwlmYid+AQB8Wzal1/eTg3swYquXtVRlFCeBx54aEECwaTWNx42rGCzS+cK+qXYhH7XsTEBJsAE/IFApagQjO5Y02qp208nY+V/V63a+IQJMAEmwASYABNgAkyACZRGAhZ3k0vj9nhPTIAJMAHfJjBoUDvExkZIssgRVM9r7D41DqdTBT0fCQ5XUr7NinZqCJk2fKTm06Soef5PkSi7WJRVfeaZoroUfp2K7Gk2/Y0qqv24+24ZWrQApVUDWrcGGjcGKlQA1UIzDs8+fRpHxo7FegonPDxqFK78+CNuULq0G0uW4PL33+MQiaYbKGL42GuvIffyZfOcouZfVKtW5nNfOzg1YQL0OlGd0WiivGSO5xqPyR3VtTIf8kEpJ7BvnwZ792ok2eXs2RJ+CSVZkf876TJ5MuJFymZ3TCZD/7lzESpyLbMxASbABPyEwMgONZAYHWK12olLjyDXIqrQ6iKfMAEmwASYABNgAkyACTCBUkLA87uypQQEb4MJMAEmUBIEQkKC8MQT0tTAuUj3yb84qUXDlSp0emkB9uw5WRJbspkzieoVyQIDbdp1kONE9Sds2t1p2EqBfdlFlCMrT1F8lR591B33hjGxXbqg8lNPGdKHilSYVBaJ6iBZu7u5ciW2kmJ4/osvoHUQxam+dQtnP/rI0Dd1xw6DExndWG/8228IqlTJ2qmPnGVTiF/y2rXm1QiBUEqT2p+Ua3PV1969JNaPzUCvXqlo1y4F3bun4skn07FunarQeo6uzuHP/Q8elEYcFAyk9OXPTKVcu4gifIRqvZWrX98ltzL6hdhn1iw0GDjQpXHcmQkwASZQ0gSCFQH4X696Vsu4mJKDbzefsWrjEybABJgAE2ACTIAJMAEmUNoIFLi1Wdq2x/thAkyACfg+gZde6kcRaNGSLnT9nrNo3/51/P33dkn9uuMsolEjNPz2W5uhucHxyA0qb9PuToOG9IarRWSCEgLcHbQOIRS6aiKyr8kff0DuoEjejRUrDKlEtZRa1FlT37yJ3Z07I3XnTsOQkKpV0fyffxBM775o50WhwNsmMscSUslMpGr1ZxNpM3/5JZeiSpPRrFkKvvgiB8uXq7BlixqrVqnw7be56Nw5FQ0bJlN6zWyoVNapav15766uPTVVur1L6cvVfZTm/pEU5fzEli1o8OCDTv2gRyclYfCiRWg2cmRpxsJ7YwJMoBQT6H1nJbRMirXa4ZfrTuJaum1aeKtOfMIEmAATYAJMgAkwASbABPyYAAuEfvzh8dKZABMoHQQqVYrF0qXvICKUQtIktJwcFR588COsXXtAQq/uuUqktJoNv/4aMlMeTnKTHSxtpFxREYRi5XIK+2uycCGqvfCC1Voc7arSI4+gBUXOKSiqpjDLoui6A3QjXS+UShdNSwvfR/UJ865dM4wMb9AArbdvRwUfjMJJ2bjRvDsRPUmagCQmhMZq1SRxVSJOhNg3bFgGhgxJx44djr8DR45o8eyzmVR/NBWpqfkpW0tk4SU0aVCQdBMHBUmoUku3rFLhKSQmBg9SVPPzZ86g3RtvICw+3npf9INbu3dvDKEUymNPnkQdOmZjAkyACfgrAfEg2Tt9G1g9/JSt0uKjFcf8dUu8bibABJgAE2ACTIAJMAEmUCQBFgiLRMQdmAATYALeJ9CsWS1s2vQhEsOk/bWsVmswcOAUEiKcj2rz1m4rP/kkmq9bj+xe47Cu1c9Y3EXa6EaR8tMZk1O603qffYYO58+j5rvvIohqBRY0RWwskl5+Ge1I+GtENQQDQkMLdrE6Pzt1KlyJHLQaTCeq69dxYeZMc7NIMxresKH53FcO1CkpVnUIu3WTZmVNmwL+WrJMq9Vj8OB0zJvnWoTB2rVq3HtvKrKypIumk+bTcM9LcrIOJ05ocPy4Bjdv6hymUq1QQbrfc/Hx0vlyb+elf1Q0qfddJk3CuCtXMI5CtZ85ehTPnz2LNyhaegjVVq3dqxfkFg9/lH4ivEMmwARKK4E7EqMwqFkVq+39sfci9l9ItWrjEybABJgAE2ACTIAJMAEmUFoI8F2V0vJJ8j6YABPwewKNm9bCv/OexKeNA1ErXLqomJSUTHz/fX7tuJICRfeT8cGidvglbCpOV30YOrmTip6TC452MUtrMAmDtcaPR4dz59Dm4EG0WLcOzdeswd379uGeS5dQ9+OPEVarVpGzq9PScIVERE/t4uzZ0N0uxKejSERx7uvWrh0QEuL5Krt399xHSXmYMCELCxfmuTX9rl0aQ21Ctwb7wKDMTB1mz85BkybJJPDeRJ06yahbNxnly99E/frJmDYt226UZNeuSoSFSfM77v77JQxH9AGmvrwEUWMwvEIFlKtbF0I0VBTx4IQv74XXxgSYABMojMDL3esiPMi6dvaExf85fPClMF/czgSYABNgAkyACTABJsAEfJ0AC4S+/gnx+pgAEyhTBKL798aLz/bCsR5KrOqgwKDK0vyanjFjeYne2Pj3X+CddwDS4rxi1auDRAn3XIuIwog770Rsx46Io3qAkU2aIEAU2HPSLs+bB5Em1FPLo+ic63//bXCTQyn9xLmvmYisFCKByQQmT8U9UW6RylT6paWn6/DZZzkerX3+/DxD1J1HTop5sKi3OGVKFhITb2HUqAwcOGCbVvXYMS1eeCGT+tzEm29mQkRamiwyUo6hQ53/GTONK/guUtOOGuW5n4J++ZwJMAEmwATKLoHyEUEY29n6AbF951Px9/7LZRcK75wJMAEmwASYABNgAkyg1BLIv8tXarfIG2MCTIAJ+BEBccf7iScgv+8+dKsQgEzb++5ubeb48UvYuPE/t8Z6OkhEDn70EZDrWgZGl6btnvs3ZFs2A7cj8Fwa7GHnlA0bPPSQP9zkS52cnN/oQ0exnTrZrGbQIPcFvshI4JVXqDakn/418uOPucjMzBe+bOA42TBrlmcio5PTSNJNCH1PPJGB11/PQnp60XsX2vnkydmU6jgNeXn5/ceM8Tz0tAc9SFGzpnWUhySbZCdMgAkwASZQpgkMb5uEanHW6eU/XH4U2SqJ/jAv03R580yACTABJsAEmAATYAK+RMBPb8n5EkJeCxNgAkxAYgJCLRk6FHQHHv/mSHfz+7//zku80KLdUaARvvjCu+JgGDLR9srvwPTpwOjRwJ9/Ajpd0Ytz0ENESKVu345/Sazdetdd2Eghipvr1cPODh1w9pNPoLIQ8NS3bjnw5NolszAohGIJTdDYhc74AF/heSzHSGzEc1iBifga29AdWjj350DVMWNsVkUBmKByjZRm0uaSwwZRc1BElVLGQr81qYS9777LRU5Ovnjmy0BeeikTc+e6rvb/9ZcKI0ak04+mcZ933hlIP67ui4ThlIZ5ypRwX0bFa2MCTIAJMAE/JRAUGIC3ejewWv3V9FzM2nDaqo1PmAATYAJMgAkwASbABJiAvxOQ7s6zv5Pg9TMBJsAEfI1A06ZIg4JWpZJkZampmZL4ccXJkSPAeS/rksMxF0HEKBkxOJuehMz5lxCwaykiH+yBOg0VCHKhRJkQBq/On48zVH8wg2oRFrSsY8eQsmkTTrz1Fio9/DBqCoVLAtPJApATVAE39Qm4fh1QRLqZL7WQtbyEpTiK5jZXj+MubMZ9iMcF9CGO/UkwVMJ+Pb2w+vURc889Nj5Eg0g1+tprwOLFwIoVgIV+atNfCIqU0RX9+wNVqthc9puGtDQdDh3SSrLe1FQ9jhzRoGlT8fPuu7Z6tYp0ePejHX/6KQ99+uRh8GBjWtDp08Nx+bIWf//t2u848X37449I+h7xn7G++23hlTEBJsAE/JtA1/rxaFerHDafvGneyFcbTmFQ88qoHGMdXWjuwAdMgAkwASbABJgAE2ACTMDPCPCdFT/7wHi5TIAJlC0CwcEKpKVJs+eQEBeUMmmmxMqVEjkqxM2AFhdQ4XIgPrr0MvaQAKY3RcKdogEfAmFhenTsKEO3bkBCQiFObjfrtVocfeklnBeRiEWYjvKlXvruO1wnRUzUL3TX0sJr4WiNp3G8+uNQKWMBkbnqWUAmS0JSt1Woc3AqEq+thox25q6dQ10SB5s5HH4dVTAHb2MHumE8hiMSKTb9a733Hq2r8MjGgACj6Ne3L7BnD7BmDUj8ATIyQPXnSOYmDUhElGpoj0J7Fa/ERGMNQwrMRKif3WtLTnb/M7GBSw23bknrz94cnrZ9/rnntTY//zzHLBAGBsqwYEEUnnsuEzNnOic8xsfLsHBhNNq08W0x1VPWPJ4JMAEmwARKloD4m+ftPg3Qa/omaG9Hv+dpdBCpRr8Y0rRkF8ezMwEmwASYABNgAkyACTABiQg4l1NMosnYDRNgAkyACbhGoGLFGNcGOOhdoUK0g6vSXxK1x3bulN6v0aNRTFm4qwreufQ0dqNlvjhoMWVWlgxLlwIvvAB8/bVRnLK4bD4UkYNHnn/eKXHQPIgO1DdvImXrVssmp45zgspjddu/saDnCfxbd5xRHLQYqdfLcCa6G1Z2WIkFPY7hcnxni6uuHS4hwY8kR6cG/YfWeBPzkQNrtU6IgxUHDnTKhxAKW7YERo0CYknzFN+DPApKFOJgQbt0CZgzx9hXRB76k0ldN1Fqf1KzPHdOiyVLXIv0s7eGrVvV2L9fbb4kRMIZMyJIMI7ByJHBhQrFd94ZgFmzInDqVByLg2Z6fMAEmAATYALeJFC3YgQeaVXVaoolB69g5xnfrBVttVA+YQJMgAkwASbABJgAE2ACThBggdAJSNyFCTABJlBSBAYObCPJ1CEhSvTq5TiKTJKJLJyINJMickxaM6lMzglelnOvXg1Q5lBDBJtluzi+/MMPuPDllwWbnTrXC/XLQWRdQSfpYTWwuPN2nE+4r+Alu+fpEbWxov0KnKz6iN3rjhqF0LcGDzrqYnPtBJpgGj4xt9edOhU1KKWqK3blCvDmm6C0mc6NEgiFUPjjj/aFROe8FG+vuDjXv4OOVii1P0dzuXNt3rxcuyKvO77s1TBs0kSBr76KpKjTcvj110h8/nk4PvooDLNnR2DbthgcOBBLonMIwsP5T1d3mPMYJsAEmAATcI/Ai13rICrEOmp9wuL/zFGF7nnlUUyACTABJsAEmAATYAJMwDcIcIpR3/gceBVMgAkwAbsEnnzyXkyYMJ9ELc+Utkce6Yjo6HC7c3irkbJwesE8E2VEWsuvvgLGjMnX9ET04JkpU8xrzVNE4US1YThX+X7kBFeERh6MIHUqYtIOou7pr1Hx5ibbeDx74XFmj/kHOcpyWEliX0Z4jfxGJ470cgU2tvweSlUKql5d5sQIY5ffMBbZiHC6v6njBqpGOIxytFYtn42Exx93mFrUNMb0np4OTJ4sUmaaWpx/X7QIiIwE7nNOO3XesRd6CqGqVatA7Ngh8sJ6ZhUrytGwoW//SXbqlGe/gywJnT5duK+oKDkGDTLWKLQcUxqO9ZRnV71wITTLlkF/44bhCQpZdDQCWrWCYtgwyOPiSsM2eQ9MgAkwgVJFICZMiRe71sa7iw+b9/Xf5XQs2HMBD7Wwji40d+ADJsAEmAATYAJMgAkwASbgJwT4MWw/+aB4mUyACZRNAiLF6AMPeB5FOHp0z2IHGOyj9/g3bAAO59/jQQo1ZFGoW1ZIIjY1m41f+lzGjrum4Wr5e5AWURdZYdWQHN0Yp6oNxbJOG/HnvYdwPOlx26qATuSI3NHkU4iIQHdMLwvAhlbzoA4Ic3r4NnR3uq9lR1HLcRmGQk0ixkWhqLpgv/wCXLvmwoACXX/+2bPxBdx59XTMmBBJ/D/1VDAUCs/Eb0kW4sBJRoYpetdBJycvSenLNKU+KwsaSverprqg4iWO9SK/rQ+YjlIR5779NjKqVkXO4MFQU8SyZvlyaFatgvq335A7bhwyqCBn9vDh0B486AMr5iUwASbABJiAJYFHWldD7XjrB+0+XnkMGbn5KbMt+/MxE2ACTIAJMAEmwASYABPwFwIsEPrLJ8XrZAJMoMwSmDx5KMqVo7AqN02Ig02b1nRztPvDRP05JzQz9yfwYOTKlfmDL5AAdjO6CRZ12YnjNZ6CNtC6/l5+T+NRatQd2NRiDjY3/wY6mUXUl04HWaDFeYGBOUHxOFPloQKtrp2qlLEkVDqfarQ3fnBtAoveKzEEaihwYdYs6J3MFZuZCWzaZOHEjUPCCJEO1h9MRLrFxnom7ImajSNHSiM0epNZeLhn+7Rcm5S+tEePIue555CekICstm2RTeGn4iWORVsO1RYVfUrKtP/+i8ymTZE3cSL0jpRzyrOr/v57ZDZvDtVPP5XUcnleJsAEmAATsENAESDH230aWF25manCF2tPWrXxCRNgAkyACTABJsAEmAAT8DcCLBD62yfG62UCTKDMEahRoyIWL36LUi86Fq7sgenfvzWmTx9p75LX20JpuS1aeH0atybYtQsQNRKFnT2UgmX3rEF2SIKxwcn/H68+gkTCr60iCauMHo1AkSPTjh2r/iR0cqWdK641Hak5xmpOR6O74HeqQpjhqEuh19IRh6uohtzz53Fj6dJC+1leENGZlEXRY1u3rmg/QkjcvRv4+mvgEyqZ+NFHgCgjuWIFUFyBY8HBMowf73xEpz0wo0eHoHJlUgl93KpWle5PxipVPN+viA7MfuQRZNavD9XnnwMit21BS0uDavp0Q5/sRx+FPienYA+vnmtFZHKHDtBfuOD8PGo1cmitKooyZGMCTIAJMAHfIdChTnl0rR9vtaA5W87gzM0sqzY+YQJMgAkwASbABJgAE2AC/kRAurs9/rRrXisTYAJMwM8ItG5dD5s3f4ikJOsbE4628eyzvfH7768hMNDzm/GO5nF0rbt7GS4duZTkmgiI27wZoFJg+KXe31BTZJ47diJpOI7UfMY8VBkfjzaUIrDaiy8ikGqLWZroK4WJdKe3ou9yylUostAMa53qa69TBox7uLVmjb3LNm2CqRSWQZrmgQP2PQnxj8q44dlnjaKgiDbcscMoFgqBcs4cYNQoYPZs4OJF+z6kbB07NgRC5HPHevZU4tNPrVOWueOnOMY88oh0OYOHDvXMl57EwKwuXaAW+WidNDVF5YkxYmxxmJ7CabN69YI+JcWt6XJGjIBGfLHZmAATYAJMwGcIvNm7ARQB+RH1aq0ek5Ye8Zn18UKYABNgAkyACTABJsAEmICrBFggdJUY92cCTIAJlBCBO+9MwtGjM/Hzz+PQrp11miPTkkSU4dixfajG3pf4/PNRJSoOijU1bAiKjjKtzrfehbYwdy4oqi/Io4UdqvsydFSzT5hcqURItWqo9+mn6HjpEu4gtSrx8cdRvl8/ZITX8Ggey8EZYc77isU1y6EuHQdAY+ivvnXLqXFUak0yszfl5cvAa6+RqEt1Dh3NRdka8c8/xr5Uis6rJpPJ8MUX4fjf/1yL8H344SASOqN8vvagCV7duoHo0kVhOnX7vUmTQLRuXXgq3qIc6zUaZA8cCO327UV1tbmu3bYN2YMGQfjwtqkoXaj+7Fn3p6E15k2e7P54HskEmAATYAKSE6heLgyPt61u5fefI9ew6cQNqzY+YQJMgAkwASbABJgAE2AC/kLA/Ts0/rJDXicTYAJMoBQRCApS4OGH7zG8Dh8+T1FWZ5CamgXRHh8fjY4d70B4uG00k06nR1qaHno9EBUlQ4DF08/exEPaCZ6hALt33wWEaONLJlJUSmGZYUm4WKknql5ZCmX58maXAZRjVYiD4iUyG+qHmS95fKBWRDjtIwi5Tvct2DEKxjyseidh5bo/VcGpUdDX1augdJ6g77FN10IbKFsj/u//SASmz7pdu0K7eXxBLpdh8uRwiIjA6dNzSPjLQ2FlG7t1U1D0Yyj69lVCiIv+ZGLda9a48AHY2dwzz4Q43Leod6mhIqFCzNOLPMBUyFRWrhwCu3VDwN13Q02pNzUeFKkUvtXz5kFJP5feMj39olXNmOGxe82SJdCdOwc5PXTAxgSYABNgAr5B4NnOtfDHnou4lZWfU/39JYex7Ln2CKRahWxMgAkwASbABJgAE2ACTMCfCLBA6E+fFq+VCTABJmBBoEGDqhCvwkyj0WPRojzMnJmDdevUZsFCaBLt2ikMaRHvvz+IxEXvihQ1awLjxhnrxPmaSFgYO1fbj9Z4GtWur0CcRU5VoW0cOwbs3Gl8uerTUX+FJtPRZatrWbBfE9Gqk52TyjiBCjhvuKKIdS4Fq6g7KdVnLHyZTNQ1/PBD18RB01jxLmoTVqwI1Kpl2Sr9cfv2SojX5cta/PhjLk6d0lJpPD3CwmQUSSvHkCHBqFPHf//06tdPSXsIoihm99R+IaAOH24/vaiOfmDUVFAyb9Ysu5F3efSUgbxJE+jthZa6+FEK8c6bAqF20yboDh92cVV2upOyraJcucGTJtm5yE1MgAkwASZQEgQigxV4pXtdvP7nIfP0x69l4ued5/HY3UnmNj5gAkyACTABJsAEmAATYAL+QMB/71L5A11eIxNgAkyghAh8+20ORVtl4dIl2zA5EUW4aZPa8CpfXoY33gjD8887jurxdBt0Xx8TJoDSnoLWVLS3hJhsXE6xUIiKHlKiPW7GNkf8gAFQVkzA/v0ABSlhzx7vLSks+4LTzs+jjtN9RUchIIsI096auTBJxzFOht9VqAC4WXLNZo1UztFspLeQ6GY+dflARPP9+Sfw6qsuD3VrQEJCAM0V5tZYXx4kIh7nzImkKM40LF2aHznhzJrbt1fgt98iKe2x6VuVP0qzaxey+/aF/prjdLg68cMlgWl374aYM7BFCwm82brQbNxo2+hmi5S+3FwCD2MCTIAJMIECBB5sXgU/bDuHw1fy69p+uvo47mucgOhQZYHefMoEmAATYAJMgAkwASbABHyXAOfA8N3PhlfmZQKZmZnYSDfxpk6dikFUk6h69eqGtGfiBmhSUpKXZ2f3TMA7BERqu5dfzsCTT2bYFQcLznrjhh4vvpiJp57KoAhDUg69aDVqAFSaD2+/DbRsacgcaDWbXK5Hy4aZeHvgYXz22H5EhbomQFg5K+aTPEUMwgePMQhQomyYN8XBqIxjKJ9MYYlO2EXUxL9oXWTP6OhANG8egX79ymHgwHgMGFAeKQ/MxF9d9+BE45cR2WNAkT5Eh44dnepWZKe4OODOO43dhKAtBFdPTXwmjuoWeuq/rIwXEcd//RVFvzdCSOwreteUIRRPPBGMVauiKf2x7Z+dGkolmkVfnKLEwaJncq2HWhQh9ZJJEeVoWpohzarphN+ZABNgAkzAJwgEUGrx8X0bWK0lNVuN//vnhFUbnzABJsAEmAATYAJMgAkwAV8n4MStHV/fAq+PCbhHoC9FK6xfv969wTyKCfgogQkTsvDJJ1TwzkX79ttchIbKMG1auMP6YC66telO+rtB+BHiT1YWILIFilpzwZR1MC5ORqkYwwFNHUMuybYt1Fi2QWnjwxcbAuRafLC0IzKcz/zp9jbqnZpFkX3OiblLMIx62ooyYnKFQkbMFWjYMAyxsQqb9ejlgbgV0xQb6bXreaBrV2DwYDHOpqu5oW1bgMq7GT5bc6MbB1RujqIYjQNP0L22s2fdcFJgiBAa//nHuIcCl/jURQIiCvDTTyPwyiuh+OabXHz1VY7NAwkiOvmpp0IwcmQIqlW7/WEWmEd3/jyy77sPyM4ucMX7p3pnQpndXYb4RcfGBJgAE2ACpZpAqxpx6H1nJSw9dMW8z3nbz2FIq6qoUyHC3MYHTIAJMAEmwASYABNgAkzAlwmwQOjLnw6vzasERKSVyWJiYih6pjm2USSDiCxkYwL+SGDzZhWl8XT/Rvvnn+eQCKTEffcFFcv2wygDo3jZmAhLohCyeylobdkGm6s+2aCVB5M46H1RIFCThdpn5zrFIANR+AcPWfUtX16BmjVDkJAQZEgjanXRwUkOac6LFwOnT4NEIZCYbL9zEH11unQB1b60f92ZViFAdu6c31OKUm4mb//+azridykIVKoUQBHBYfjf/0Jx/LgWyck6UNk8EpzlVGsxAEql45+JvI8+gr6Ewjr14gkFL5lMhMBKZFL6kmhJ7IYJMAEmwARuE3i9Zz38c+Qa8jTGlP5anR7vLzmMH55o6dUH7vgDYAJMgAkwASbABJgAE2ACUhGwH1YglXf2wwR8mMCQIUPw008/4QSFpyQnJ1P6s1WkSUh3U8+Ht85LK6UEpk93PXKwIIrp090XGAv68vQ8ISE/zaSnvrw9Xqcvnn9O2+4ZhSB1apHb0SAQk/E1MhFt6FuhghLdu8dSCtAYVKkS7JI4aDnZf/+BIlQpyFNj2Wp9PHAg0KDceZS/tR2Vrq9DXMoeKFUp1p0cnI0eDUQbl23olZHhoLOLl/j5DxeBOdldRBQ2aBCIdu2U6NBBiTvuCCxSHNTTB6v64QcnZ5C+mywyUnqntz0Gduokme9AS7VcMq/siAkwASbABKQgUCU2FCM71LBytenETaw5ct2qjU+YABNgAkyACTABJsAEmICvEuAIQl/9ZHhdXicwcuRIr8/BEzCB4iJw+bIWCxfmeTzdmjVqHD2qQb16vvHPw+OPA2+9VSIZCD1mKbWDVvtfRK3zPxXpVoUgTMEM7MM9hr5JScFo3iwCMqqXI4UdOgRKKwlUrgyKuDbWkoygTFoNa+cicPsCXJgxA3dTNLalaWUKnK38AI7UHINr5dqBHqu3vGw4FlHd585lIT6e8s2SwMlWugmo6AEdSKn+uohLXodSGXvJAu6+G/LGjaE7cMCzGSjPrvKppzzzwaOZABNgAkzAqwRGd6yJ33dfxNV0ypl/2yYuPYwOdcpDGVg8D5CZ5uV3JsAEmAATYAJMgAkwASbgKgH+i9VVYtyfCTABJuCDBObNy3UY1eXKkr/7Lv8GhyvjvNFXiFCvvmqsUegN/37hk4SzLor5aHJjbpHL/Q8t8Cr+xBb0MfRNTAyi9MnSiYOmBaxdC4jgrz//BBYsANa9/wf23lEVh4YORWoBcVCMCdCrUfPCfPRZ3wH9/mmOyIzjJleG96wsLbZsScOuXdkU6ZiKq1e15uvhVJZSKpPSl1RrKqt+NCtWlNzWSaBWDh/utfll5D9ozBiP/Qf27w95YqLHftgBE2ACTIAJeI9AqDIQr/WsazXB2VvZmLv1jFUbnzABJsAEmAATYAJMgAkwAV8kwAKhL34qvCYmwASYgIsEjh3LF1RcHGrT/fhxjU1bSTY0aACqrQiKLCvJVZTg3CQ2NHh6MDpeuoQ75sxBVKtWkAeLKDsyuiaLKY+VgY9hNNZiHJbgKJobLgUFydCyZaTXa+DUP/kFumwbiJC8G4Z5i/pfudS96Lu2DWKTd+PKlTxs2pSKZctu0bHKMPTsWR2eeYZCE2+b+PylMil9SbWmsuqnpGoPCt6BPXtCXr26V9ErHn0UHkUpUkHP4Dfe8Ooa2TkTYAJMgAlIQ6Bf40TcVdUiPzq5/XzNSdzI8Dy7hzQrZC9MgAkwASbABJgAE2ACTMA+Ac7hZZ8LtzIBJsAEfI/AxYvAnj1AWhqgJUEwLAwQN7mbNqUmvWTrldKXVIsS25w2zbj9lSsBkeayoJUvD9xwTqMqONTnz0UKz4DQUCRSzlXxEqbLy4NMoaDUoXIEbVBhTj+qTUhfDZNVrx4CURvOm1b9wm9os2+sy1MEq26h45qe+AvLcA3VbMb/9Vcezp/XomrVAIhMkNWoy7lzNt1caiAtFV27ujSEO3uTgPgdVkKmfO45r88so5/X0GXLkNWmDfTXXaxFRV/WEArRDaDf7WxMgAkwASbg+wTklMZ9fN+G6P/lFvNiM/I0+GTVMXz4QCNzGx8wASbABJgAE2ACTIAJMAFfI8ACoa99IryeUkPgohBzHNiVK1ccXOVLTOA2AZ0O2LkTEKrYf//ZxxIbiyHKdtgS1BrX8iLt93GhNTTUu6KSC0ux6krluCgizvgS99svXzbWJlQqgZgYo4j0zDNASorVsFJxIsq1qSjArkkT4OBBkHgmyrcF4dYtQK0G4hRp+CBiFL5KG4oDoBp/ZDVrhnh373od2u12vz5aDG5iDN7AeNDmCpj42s+enYOJE8NFkCSlHRXnBTq5eHrXXWU4CtVFVsXRXSZ+aEvAlC+9BIX4QhWDBdSsibDNm5FNEYu6U6ecm5Gig0N//hmKAQOc68+9mAATYAJMwCcINKkSjfubJuLPvZfM6/l19wU82roa7kiMMrfxARNgAkyACTABJsAEmAAT8CUCLBD60qfBaylVBKpUqVKq9sObKQECmZRm8ZNPChcGTUtKTsYDWISuPVfhga1PYs31eqYrbr0nJPh+9mmRbtReytEuXYw18dzauA8PEtFzU6cCFCwIIZ7ZmD4OFWs+j2fUH0F37XUsr/AelKGDbbpJ2SDTa6HUpHvksgXWoBLO4gqSbPyIWphCIBTWvj2waBGoNqFNN6cahLh8//1OdfVKJ7Hu1auNEbDphEwEz1GAGYm4wL33AnfcYfxsvTK5jzoNoFS5muXLi3V1ypEjEfzxx8U6Z0Dt2ginyG/VV18hb+ZM6M+etT8/hQkrhw2DiG4UY9iYABNgAkzA/wi81qMeVvx7Fdkq+oeejMpI473Fh/HrqNZeT/nuf7R4xUyACTABJsAEmAATYAK+QMD37wL7AiVeAxNgAkyguAkIcfDdd4sWBy3WFaXIxfL2M9C7kp38mxb9ijp8+OHb9e2K6uiD10UKSSGilVazKw6KzcrkuBrfEas7LMOeexeiVdwRryNQaDI8nkMOPXrhB7t+Ll/WITfXmDqXyrHh9dcBkWrVHRs1CoZUpe6M9WTM6dPApEmAyGi5eLEx6lX8aOfkwBD9KYKDJ04EXngBWLPGeCPRk/n8aazyyScpby4pt56aCDEtwmSUfziYchQHz5plSMlbRHfJL8uiohD06quIOHkSoUuWQDlmDBSDBiGQVGvFiBEIIfEwkmqMhnz+OYuDktNnh0yACTCB4iNQITIYz3SqZTXhzrPJWHbIzSecrDzxCRNgAkyACTABJsAEmAATkJ5AKb6NKj0s9lj8BDQajeFpSxndAPTkNXfu3GJf/IULF+DotVPcGWZjAvYICBXo00+NeSTtXXfQppDr8Gvr73BnVH56IwfdbS7VqxeAjh0VNu3+0kDZVnHPPf6yWu+sMzWqIfY1GO8d5xZec7M1OI/aFi3uHXbB74UOTE83CoSiQ0ICMGECIGpNOmtCfxpLJRI7dnR2hHT9du0C3nkHOHCgaJ8iwpA0Inz9dSERokW78Lse8sREBPbv7/G6la+9hhDKwRvQtq2NL9EmrkXQv8dBpNKKvyNK0mT0hVT07o2QL79E6K+/IuyPPxD6zTcQkY0yd9XvktwQz80EmAATYAI2BEa0q47KMdZp3icvO4JctTGq0GYANzABJsAEmAATYAJMgAkwgRIkEFiCc/PUTKBUE6hcuXKp3h9vzosE9u8H/v3X7QnCAlWY0HAp7t860mUfY8aElPhNdJcXXWAABeSAgnFw/HiBC2XplCIKvW3/XQjFH9hMFQ8X4yU8j1BkuTVlLK4jAGpoYRSmFQoZqlULRmysggQzGUT0YCSV1mzWDBB1BKdMAVasML5Euk57piBXQi/q0weoWtVeD++2CVFQaPwilagr9s8/xghY8R0uYS3LlWW73VdE1Wn++st1UKYZRWQeKcByUo+VQ4ZAT4U69amphquy6GjIRIFSNibABJgAE2ACxUggWBGAN3vVx+if9ppnvZSag683nsbYLp4/WGV2ygdMgAkwASbABJgAE2ACTEACAiwQSgCRXXiPQGBgII4c8TxVXqVKlby3SPbMBKQmsHKlxx7vSziEyiEpuJgT47Svdu0UGDnS+olnpwf7UEehCfzvf8byjR7orD60I99bik6nx5kzuYaFbUZfqiFYDR9gECKR4tZig5GNgMhYSgMaSoJeMGWeNEZ6WUbfCfGsXDmgYUMghabJKCTDaTiVLezZE+jRw/mUpCJo9+BBYO1ao7gsUoCK75GISG3TBmjXDgh2MvNuWpp74qAJ3KpVxnSoHTqYWkrve2DLlgihtJ85Tz3l+iZJBQ5buNAgDpoGC0FQZq84qakDvzMBJsAEmAATKAYCPe6oiFbVY7HjTLJ5thnrT+HB5lVQMcrJPyjM9rVi2QAAQABJREFUI/mACTABJsAEmAATYAJMgAl4jwALhN5jy54lIlCvXj2JPLEbJuAHBK5dA0QEoYcWINNjVI3NePu/vk55atw4EH//TdE4QSWbgs+pxTrRKSwMeOMNYNkyQOitN244MYi7OE3g4sU85OWRqnbbTqERJmAuiYQPQgmVqdnhuw4y7EEnHEIblKtWHs2bR1H9SMffv5s3gQ0bHLqFqPH3++/A8uXAyy8DDRoU3l8IgyIaUfQVP3oF7fJlYzDvvHnG1LUDBxqjGQv2szwXtQSFwOiJUZk6tG9fNqIIDbUI6YPIobp8TodckgocSuk5Azt18gSzX4w9dy4DmzZdRXJynmG9cXHB9N2oSEI6KeFsTIAJMAEm4JMERErrd/o2QN/PN4OeqTJYDqUYnbLiKD57qIlPrpkXxQSYABNgAkyACTABJlA2CbBAWDY/d941E2ACvkpAhEzp8+uuebLMnpUOOyUQ9uunxLx5kYiI8H5aSk/24+pYCkDGffcZ00wKzXX9eqNQmEuBbyIaTESjiXqFQkC0jFRzdR5Tf5ESUqKPzuTSJ99zc7U4dCjTZm3/oTWW4HHcj69srlk2ZFKc4XI8imUYRpGHSUhKCkbLFpGWXSQ5FkLhxIkAlahD48a2LvNIb5k+HRC1AosyIfgJIXHfPqPwXFhQuhAcV68uylvR18+eNabIrVu36L7+3kNEo66o2B3/NPsId+/8Ad1xkFLO2v8dqKd8s8rBgxFEIcIBpRiOYLJy5UXMmHEYS5desPm9IoT0Pn2qYMyYBujWLbFIYd3fvyO8fibABJiAPxJomBCFh1pUxS87z5uXv3DfJQy9uxqaVnU+w4d5MB8wASbABJgAE2ACTIAJMAEvEGCB0AtQ2SUTYAJMwG0CV6+6PbTgwEZJOXjuuRDMnZuL9HTrG+6hocCjjwZj9OgQNGlCBdtKsclJ92za1Piyt83zdN9GCoGwLIiDKpWOopnSkJ2dHz1oyXQxCYT9MRvyQgSeM6iPt/ETbiLRMCwuTkG1BSMsXUh6rNEYU81OmgRUqZLvWtQG/OwzYG9+eaD8iw6ORJThhAmA8BcXZ9tRCNG3btm2u9MiUqqWYg3MgCQzU43Bg9caRDCKzcTneBZVcAuPYSPJzacQTXUtdfRtSkYY1qAh/pR1wIwB96Ff3WruIPWLMdev56B//9XYtu16oesVAuKiRecNr3btKmDhwm70wAOnrCsUGF9gAkyACZQQgZfvrYMlBy8jI5f+ILltExYfxsLRbfjhDhMQfmcCTIAJMAEmwASYABMoUQIsEJYofp6cCTABJkAETAXQRIiSq4qFA4AKuRbTpkVg8uRw7NmjphR1eppKT3XV5CSYBSIysnRFDDpA4fBScn55GIf9yvrF9HQNtm5No9p/pK4VYldQHXtxD5pjvU2PUyTwvIK/qNpgfrRggwZhXr9BJiJGf/sNGDcuf0kLFrj/oya+L1Ongn6ubFOAisg/qezcOak8+aafrCw1unZdhh07rPP/XkAcJmGA/UXTZzlgwGr8+GNHDBlSy34fP269dCkLHToswenThRTYtLO3zZuvUZ3MRdi4sQ8qVqQnP9iYABNgAkzAZwjEhQfh+S61MXHpEfOaDlxIxV/7L+H+ppXNbXzABJgAE2ACTIAJMAEmwARKigALhCVFnuctcQInT57E5s2brdaRKXLSkYn3uXPnGo5N/+vRowfdfKtoOuV3JiANgdRU4OOPgRMnpPFn6YXqdAkLC5PRTWel5RU+tiCQnm5xwoc2BK5ezcPJkzm4csW52oLr8ICNQJhKos87FDloKQ6GhwfQ79Ti+V6KNKJC2IuNBYRgKGoOemKnTgH//QfccYe1l+xs63NPzrKyPBnt22P1FG47dOh6G3HQmVWLSN3hwzdSatoIEsYqODPEL/oIwbR375UuiYOmjZ04kW4Yu3lzX4SE8J/2Ji78zgSYABPwBQKP3Z2En3ecx+mb+f+wf7j8KLo3rIiwIP6d7QufEa+BCTABJsAEmAATYAJlmQD/RVqWP/0yvnchDj7++ON2KdyiHHEFr61bt44FQru0uNFtAjdvAuPHGwvjue3E/kCNTo7FGe2x8LF0XLumQ16eHtHRMqrFFogRI0JQtWqA/YFlsLUspAZ15WPNyNDg1KkcelBCi7Q0TaHpRAvzeRMJNpcWYQQljqxk1V6jRojVuTdPRJCuSNk5aBAoChK0J89nE7UrCwqEou6lVKYoxZl/t269Rmkx3Q+RVKt1eOONXVRXtI9UuEvcz6xZRyjVsfvhzHv33sI33xzD2LENS3wvvAAmwASYABPIJ6AMlOOtPvXxxNzd5sbrGXmYuf4UXu5eBooNm3fNB0yACTABJsAEmAATYAK+SEDCW1m+uD1eExNgAkzARwnk5AAffCC5OJimDsb0Ex3x1em2uJQTQ5uncCkL+/tvFSZOzEafPkpKuRjKkYXE5nagpQWlsneo12lx8ZLaIAzeuKH2CEAurNMcqqHAcjxq4zM+vngVMJG9VwiEq1fbLMWtBhGVmJICxIgfs9sWHW068vw9KspzH77qYcaM/FRr7q5xw4arFMWZgoYNLT4Ad52V8DiR+nnmzKMer2LGjMN49tkGkMlkHvtiB0yACTABJiAdgU5143FPnfLYcPyG2ensTafxUIsqqBJr/XeTuQMfMAEmwASYABNgAkyACTCBYiDABaiKATJP4ZsEhg8fDpHmzNlXx44dfXMjvCr/JCByHF64IOnaT2fGodWal/HOf31ui4P23YtoqkWLVLjnnlRMmZJl+Bmw37NstNYt4w9vx9/YjLwlX2D79nR4Kg6Kb0worOunbUNPpMA2FaRSWbx/gohUsuK7f/q0NN9r4atgncBmzWzrEro7W4sW7o707XHXr+fg99/PSLJIIYi5Y4Z/9+kLoRf5Zn3AVq++ROK857mOjx5No6jKKz6wI14CE2ACTIAJWBIQD268TVGEAfL8BzhUGh0+WO75AzOW8/AxE2ACTIAJMAEmwASYABNwlUDx3p1zdXXcnwkwASZQGgkIZUGqMKbbfC5kR6PD+hdwLKOiS8Refz3LEFHo0qBS1vnuu0WdxlK2KRe2kxVWFW11i/EE3sdo/A/D8AE6YwGCqGKgOxYXSd9veX4K293o5I4bycdoNMb6g1KmlL1dtta81vLlASESempKKs1YWp9JWbv2MkSKUCls+fKLTrvRU1FH1TffIJOU13TK35pOIZrpISFIi4xE9iOPQLNlS4k9LLFw4Vmn91FUxz/+kM5XUXPxdSbABJgAE3CeQK34CDx2dzWrAcsOXcX207es2viECTABJsAEmAATYAJMgAkUJwFOMVqctHkuJsAEmIAgIHIdUp1LqUzUG+y7+WmHUYOO5nrnnSyqpRaIAQOCHHUrtdeCaNudSMNasqTUbtF2Y6SSVbqxDvVPfolql/+GXK9FE6yx6peBKKzGQ1iCx3EZNayuOTqZ9EktNKuxCqrUVGgple77EykUzk72RJVKR8JsvpDoyKcU14QIHCDxdPZqDt57L7A7v8yQW0tv27b0pr69cUO6qL2bN4v2pSdlOO/dd5H3xRegopq2n0dGBtQ//2x4yRs1QsgnnyCwa1fbfl5suXo1RzLv165J50uyRbEjJsAEmAATMBB4oUsd/LXvElKy89O5T1h8GEvGtrOKLmRcTIAJMAEmwASYABNgAkyguAhwBGFxkeZ5mAATYAImAtu3m44keV90+U4cSKvska/33ivbqUZ79gQomKhMmFynQoddw9FrQxdUv/SnQRy0t/EIpOF+zKb/2qM7frLXxapN1BTs2S0M83fXxzM/tsWry3tiyq6+KFc1DgpFfkot0yAp0pmafDnzXoM0ThGZJwRhqYyCz2yscWOgeXObZqcbRO3BBx90urvfddRq9ZKtWUPp2RyZiBrM7tsXeZMm2RcHCwzWHTyIrB49oPr22wJXvHuam6uVbILcXAqVZWMCTIAJMAGfJBAVqsBL3epYre3IlXT8ukvasgNWE/AJE2ACTIAJMAEmwASYABNwQIAFQgdw+BITYAJMwCsEKLJKSptxqr3H7vbv12DHjrJ7Y1mkhnzpJekjzDz+YCR2INNp0GXr/ah97genPQdCgxfxEvrjK7tjatQIRvfusVTTMgbh0WG4mByK5MwgXEsLwbHLkVBGxaFPn3KUejOCRNj8PztOnSreSKdu3Yz1AZs2tbsNlxsjIoBatWyHUZkhPPcc4E5tSyFSv/YaUK6crd/S0hIbqqOKlKkoh3SI75YnFhNTuNqr12qR/fDD0KxY4doUNC7nqaegXrDAtXEe9I6OJuVaIouOLpyJRFOwGybABJgAE/CAwMMtq6JuBfojwsI+WXUMaTn5UYUWl/iQCTABJsAEmAATYAJMgAl4lUD+nTqvTsPOmQATYAJMwExALd0NgONBDbDmej2za08OZswoXsHGk7V6Y6yI/BLiTHCwN7z7hs/W+19A1StL3VrMSIxHa+SLLUIIa9kykoS/SERGOs5YHhgoQ40aIejSJQbR0ca+mZlaXLumcmstrg6qnLofNZO3GYZ17+7qaPv9O3c2RiTauyq+Q2+9BbRqZe+q/TYhUr//vn3R0f4I/2nVUxrPvFmzkNGkCfqNaoxjeAUnMQ7XMAYL8Sn6YC8C4HoUXcuWBK0QU02bBs3ixYVcLaKZUvBmP/YYdJcvF9FRmsuNGsVK44i8SOlLskWxIybABJgAEzATCAyQ452+Dczn4uBWlgqfrzlh1cYnTIAJMAEmwASYABNgAkygOAiwQFgclHkOJsAEmIAlAQlzWW6S3W3p2aPjzZuLR6zxaJFeHkz6BT780FiTUKHw8mTF7D4i8xQanPrS7Vnl0GMESMGid2FCHKxWzTU1NSQkgCINoxERYSwGePhwFnQ66VJOGhZm5389jn4E1YcfGK7Urw9UrmynkwtNQhwVEYmOTKQyFVGp770HtGtXeHSqiEIcMwb47DOgalVHHv3vmojiy337baQnJiJ39GjoDhyw2kQAfZc64Qh+xEwcwP8wCNutrhd1Mno0fZh2TMybN326nSsuNFH9TNXXX7swwP2uw4fXpvqYtml4XfWoUMgxbFhtV4dxfybABJgAEyhmAm1rlcO9DSpYzTp361mcupFp1cYnTIAJMAEmwASYABNgAkzA2wQcP/Lv7dnZPxNgAkygLBIQKsDevZLsPEVZeASNqxMkJ3tfqHF1TSXRPyEBIC0DQ4cC69cD//4LXL0KFFMwkde2XO/ULI99V6G4r+FNDiA1qSvVFXTvGSOlUo62baOwcmUybt5U049CBtXss1PMz+PVGh10Pf4ZWl34BZqLMujOnoU8KQkUHIYPSC+kQDG3TNSsjI8veqgQEuvVM77EnIcOAenpgIYya4aFATVrAtWrF+3HH3voc3OR/dBD0Cxa5NTyKyOF6l1+i5oUV/gB7qMxBM+B1aoVia5dE+320CxfDv25c3avudKomj0bQW+8AZmXnxZISAjDgAFJWLDgjCvLs+n74IPV6XsZYtNeUg1CqBUmCzA+EFBS6+B5mQATYAK+SODN3vWx/tgNqLQ6w/I09MDUpKVHMGd4C19cLq+JCTABJsAEmAATYAJMoJQScO/uXimFwdtiAkyACRQLAZGbUApTKiGrIZ26IOd/Eaw+lfBwUO084PXXgU8/BUQKSH+1AG0O6pydI8nyW9z8xm1x0LSAiIhAJCZSiB3ZmTO52Lkz3SuRhN2OfYIHD4wzTktqoCkiTESKjhhhbHb1/y1bwiAwujouOhpoT+VCe/cG+vUDiVulWBzU6ZBDiqiz4qAly9ewBM9gtWWT3ePXX28Mudy+iKj65hu7Y1xt1NNTAUJsLA578cU7PJpGiNEvvOCZD48WQIP19LmriVfWffchjb7w6YGBhldaTAyyHngAmjVrSJR3U5X3dHE8ngkwASbgYwSqxYXhiXbWf8evPXqdRMPrPrZSXg4TYAJMgAkwASbABJhAaSbAt4NL86fLe2MCTMA3CVSsCAiFwlOjvIVxCa6leHQ0ZVwc/5NQGB8hngphx1+tfPJOBKuSJVq+NN+TmjXzI53OncvFP/+kkFiYA622cAEhKvsSqqTsQ4A2z+FeqqbswchtD2HQwZchUqOaTLN5s+kQ994LPPccSOw0NxV5IES9F18ECVNFdi3THdRz5kD9++9uM3gfC9AQFwsdL1KLPvFEnUKv644eLfSaqxd0x465OsSt/m3aVMDHH5P67Kb93/+1RosWJfcUg+r775FZuzaye/Uy1n5MS8vfSWoqNH/+iSz6AcqkHL+q337Lv8ZHTIAJMIEyTODZzrVQLtz4wJQJw/tLDkN9O6rQ1MbvTIAJMAEmwASYABNgAkzAWwQCveWY/TIBJsAEmIADAiI0bf9+Bx2KuCTCRSjPYSdQFCEdShGU0bmzC0pJEcsrjZd79ACE7rB9u//tLjjvhmSLPl79CUl8xccrER4egMxMYxrCtDQNdu/OwMGDmYbahjExCgQHy3BPuaMof24HGl1egkZXlpLgp0OGshy2VH8c+xP6ISOoPLRyJUJVKaiaug/3UCrVpJRddhNU6m/dslq7qA0oUoCupoA1Cm4ypP+06kAnFASFu6nUZ/fuQJ3CNamCw8rsuYgQ87T+nxB1n8Q6vIihNhzHjm1A9Rpb0+89+sVXiOlFHleJTG8pdEnkszA348bdCZVKhzff3F1YF7vtU6a0ILG7ZKIHxeed++qrUE2dandtBRuF4JpDqWeFiBtE9SkdfY4Fx/I5E2ACTKC0EQgPCsSrPeri1QUHzVs7dSMLP24/h8fbWkcXmjvwARNgAkyACTABJsAEmAATkJAAC4QSwmRXTIAJMAGnCTRqBFDKNfzxh9NDrDo++SRIRUE1auzdW4klS1RWl905GTMmP6LLnfHOjsnO1mP+/FzMm5eLc+e0JBDpEREho1psARg+PAQDBwYZhCFn/RVXPxE19uCD/ikQyvRGEU4KVrdimknhxuAjJibQLBCanKpUevpe5KB1az1eey0UNb7+Dqo/p5kuG94jVDfR49jHhpfVhaJO7IT+lSsHPPww6HsH7NljrDWZnQ0E0QP9lBkRIqVoZGRRjvm6iYB2yxboRLFFD20QtmM8HkA6Qg2eRHTcuHF3YNCgGkWKSrKQEIu4UQ8XEmqc30MvTg0XYtkbbzRB/frRePfdvSSWO476bdIkDu+91xR9+4p/CUrG8saPd1octFyhGCejQpxB426nALa8yMdMgAkwgTJEYGDTypi37RwOXcqPvP5s9XH0a5KI2DBlGSLBW2UCTIAJMAEmwASYABMoCQIsEJYEdZ6TCTABJiAIDBoEqNXAokWu8aDaXujWzTxGCHueCoRt2yrQuLF3IwjT03V0MzsL336bi9TU/LSPYiM3buhx+rSOorjUlMJRhpEjQyiKJgyhoYVHCZkBFONBMWUblHxHKgUVwJPAxKemUkrjSyxn0KBgHD6sR3KyzrC62Fg5OnZU4PHHQ6jmozGPZ25cnOGaFP+TOfAlUo22bi3FLGXbh4rSi0phYVDhpepHcanrw/T7oB6aN3c+faYsIQH0C0WKZUBeqZIkflxxMmBAEvr3r4Zt265jxozD2LDhKv2M5BlcxMYGoVOnShgzpgFatSpfpFjqyryu9hU1BfPef9/VYeb+uS+/jAAqzBkoVHg2JsAEmEAZJSDq6Y7v2wADZ20zE0jP1UCIhO/3L5nocPNC+IAJMAEmwASYABNgAkyg1BNggbDUf8S8QSbABHyWgEiR9+ijhkhALFgAXLnieKlJSQClZkMz6wiu7t2VaNdOgc2bSWx0w0RQ1bvvhrkx0vkhFy9q0atXKg4dKjqS7eZNPSZPzqaadCosXhyN+HijUOT8bN7rKWHmQu8t0o7nW9F3QScLhFyvsXPV+SYh18p0GugDApwf5KBnjx5BmDqVQvUcWCDVLct75x0HPZy/JHyxeZeA7vhxySZ49YEwhHzc3mV/Cvo9qbWoN+myA9OA4GAE3nef6axY30U0oahLKF6+anmffebx0lTTpiHwp5889sMOmAATYAL+TKB5Uiz6Nk7A4gOXzdv4acc5PNK6KupV5DQGZih8wASYABNgAkyACTABJiA5Ad+56yr51tghE2ACTMBPCFAEBf7v/wCqx0QhIUBUFCAEGFH8TOQ47NABmDgRmDLFRhwUOxRPHi9cGEX10dwTbb74Ihxdu3ovhdGtWzry75w4aPmJ7dypobpvqcjI0Fk2l+ixzneW4hKH3OB4nE2836UxhXX2VGS09OtM6s4ACuuTN2liOcy9YwoRVI4Y4d5YHuU0ASnr/9ktCunESpRDhwKUvtJTUwweDLmDqFNP/fvzeN2ZM9AsW+bxFtS//w7d9ese+2EHTIAJMAF/J/B6z3oIVuTfntFR2ob3Fh+mOuPWWTf8fZ+8fibABJgAE2ACTIAJMAHfIpD/F6hvrYtXwwSYABMoWwRENOGdd4KKbAFffw38/DMgoiq++gp49lmQ+kehWyJ+y76VKyfHxo0xVC/N+cBwoT/OmROB0aO9W2Nr+PB0HDumtb/wIlr379fg6acziuhVfJfDw4tvLqlnOlJrjMcu8xQx0ModR/w5O4mo8XeHE5mzRCRV0OjRzrottJ/i/vshr1ix0Ot8QRoCoq6cZOamLxk9ZGEQCT1ciHKM5z8zHi7BZ4cbUslKcdOa0myrf/jBZ/fJC2MCTIAJFBeBxOgQjOpQ02q6raduYdXha1ZtfMIEmAATYAJMgAkwASbABKQkwAKhlDTZFxNgAkxAKgJCDHQgCNqbpkIFo0g4a1YEGjUqXCgMJT3wqaeCsW9frKHWmz1fUrUdPqzxuD7i/Pl5uHDBPYFRqn2Y/NSrZzryv/er5TrgWlwbjxb+b50XoZcX/t1yxbkInBXfRWdMQal45Z7ADwlB0JtvOjMV9/GQgCwx0UMP+cPlHvgKevddyKpVy3fm4pHymWcQ2KKFi6PKTnft4cOSbVZKX5Itih0xASbABEqAwNP31ESlqGCrmSctPYI8jW/8HWy1MD5hAkyACTABJsAEmAATKBUEWCAsFR8jb4IJMAEmYCQQFCTDqFEh2L8/Blu2xFBAYgiGDg3GoEFBGDkyGCKd6KVL5TB7diRFb0kj9DhiP3NmjqPLTl0TaT1nz/bcj1OTFdEpKQmoXbuITr56mQTnNW3+QEaoe6LJGUpReqD+G5Lt7t57nXclIyUxjNIZytyJAKR0vaG//IIAEaHL5nUCyiFDpJmDiqMqBg1y25e8QgWELV8OGb27aoEDByJYpH1mK5SAPi2t0GuuXtCnpro6hPszASbABEolgRBlAESqUUs7n5yNOZvPWjbxMRNgAkyACTABJsAEmAATkIyA9+8OS7ZUdsQEmAATYALOEhBpGdu0URheBcdkHj2Ks98uherGDehUKiiozmFks2Yo1707ZKL2oUSWk6PH99/nSuLt669z8e67Yf/P3nmAR1F1b/xN7wmB0GvoXWlKBxXpIMJfsSAiNop+9v6pWD57L2BHxV6QLsVCkY5I7y2EQALpved/7sQku9mSLbNJNnnv8wyZuf3+ZpMd5r3nHAnNaNnNqi4D2dCJCod39KgNFathlSz/RlgxdC1GbBiFOmmHbJ7h8ebXYUOf+Sjy0OfzMXQooMRWe5JnZCSCN25ExqhRKDxyxLam4qIyUGKc+Ugbpsoh4D1+PDyaNEHR2bNODaj68Wze3Kk+vDp1QvCWLci4+moU7tpVcV/yd9P3/vvhL/Fe9fxbWPHA7lfDw9/YwsWZFXiIhS8TCZAACZBAMYHxFzXBgs1R2BGVVIrkvT+OYlKvpmgQot/f3tLOeUICJEACJEACJEACJFCrCdCCsFbffi6eBEigthAoKihA7M8/Y/sVV2CjvDQ//OCDOCkvwaPefBPHnnoKO8eMwfo2bXDixRc14VAPLidOFCAtrUiPrhAXV4izZ8WUsBqk8+erwSScmEJ6UCssvXwzdnd4BNm+9az2lBTaBX/1+ghrL/0aBV76vJTq3h1izWp1WIuFnq1bI3jHDvi/8QY8rZhyFoTWQcL1M3Bh8SYUXWGHqaLFkVlgKwEPCW7qO2OGrdUt1vMTF596JE9RooP//huBq1bB+6qrALFMLJ88IiLg9+ijCDl+HAGvvUZxsDwgM9cejRubyXUsyyHLYMeGYisSIAESqPYE1Ca/p8Z1NppnRm4BXl152CiPFyRAAiRAAiRAAiRAAiSgBwGPIkl6dMQ+SIAE7CNw5swZNP/XOiI6OhrNmjWzrwPWJgEbCeQmJmLXxIlIWrfOphaZXnWwdtBXGP3oMFx5pa+8T3fMau+vv3IxaJB+ruP27KmLbt2q1vA9M7NY3BLDS6eSt1ch8gtMhQqnOnWgsVdBNlqd+Qmto79DYNZZeBdkItcnDCkhHXAk8jbERkigQHlRpVcaOBCYORPw8XG+xyLxPZv/++/IFzeSuefO49SxRPxzohCrE5vgF/RGNny1QerW9cP06e0xY0YntGkT6vzA7KFCAkUZGUgfPBiFO3dWWNdcBZ9p0xDw2Wfy0dPvs1cyTmFMDAp270ZRUhI8fH3hUb8+vPr1g4efX0kV/rSBQN7q1cgUq3M9UpBYeXpfeqkeXVWrPvicV61uR6VOhve+UnHX2MEe+nE3fvz7TOn61Ffi4tkD0L1ZndI8npAACZAACZAACVQuAT7nVS5vjlY5BKr2TWvlrJGjkAAJkECtJZAnL8G3DxmC9H37bGYQWJCMYWsn4vG1C3BX2ytw990BmD07wG73nr6++r7cl3f5VZ6U10RnxUG1CCUOXj/gFNKyfLDjeF3EplSNiz1lFXi85RTtcBVc5bVWvftXMQfFeFU3vdFDLMG8hw3D+/sb48mP/0Z6ep7ZJSQm5uC11/ZqxzXXROKTTwYhNLQafJjMzrZmZHqIa9egZcuQcfnlKBSXxvYk73HjEPDRRy4RB9U8PJs21Q575sS6pgTU755n27YoPHbMtNCOHM+ePeF1ySV2tGDVqiRw+vRpvPPOO1i+fDnUuZ8I623lc3CtxAudNWsWAiVerB5p5cqVEnv4I2zbtg0XxB16fRHyL5HPyR1ifj5y5Eibh1D7YBcuXIjvvvsOO8T6PDY2FgHi0rahxCbtJa7VrxCvClOnTpXnG33cZ9s8MVYkARsIPDSyA1bsPQdlPaiS2tb97NID+HGGbGpxwQYaG6bEKiRAAiRAAiRAAiRAAjWQQNWbL9RAqFwSCZAACVQHAsqt6K5Jk+wSB0vm7YtcPIFbkXvsAO65Jx0TJ6YgI8M+g/MGDfT9iqlfX9/+StZqz08xjNItLfu7KYZ1i4O3l31cdZuAiztS1oLKlei8ecC99wKdxVuWnu+z1Ivf//xnM+67b4tFcbD8En/88SQGD14mL5yzyhfxWmcCnuKCMkhiRnrb+jJfBF/fe+5BoLzM99DDxFTn9bA7YwJKoPdV5sBOJj8Rlfii20mIldRciYLdxUf066+/jkMi/GeKSX2S2oS0fTseeugh9BSx98SJE07NRv1dv/POOzFK4sb+8ssviBGL31zZlaN+qmuVr8ptcYCjBMzBYsn8f//3f/jpp59w6tQpZGdna3NW8//666/Funy6uEJPc2rObEwCriKg4g3edXk7o+5VXMKle84Z5fGCBEiABEiABEiABEiABJwhUPVvW52ZPduSAAmQAAlYJHBBXuYl/vmnxfKKCoKQjhvxmlZtyZJcTJqUgrw828Wsli09xSWoPrvyBw3yQd26Vf+VpaeRQVq2D576oSvOJOpjcVHR/azMcnnnKuIdIEZGqOMiT1jPPLMT7713wO5l7d6diLFjxT1iZr7dbdnAPgKedesiSFzAqhiAPrfdBjHdMenAo0ED+D3xBELk5X3AW29BxTBkcg8CSiD0EissR5OXiDc+Yr3FVP0J7Ba3vMpKMCUlBcHBwfjf//6HTZs24Xdx83z77bdrCzh8+DDGSDzj9PR0hxf03//+V7McVB306NED3377rWZFqH6qa5WUZeGTTz6pnVv6R7nuHzp0KP766y9xk+6JG264AT/88IPW19q1a/HFF19o846Q+KNMJFCdCUwf2Aot6ho/J7644iCy/rUqrM5z59xIgARIgARIgARIgATcgwBjELrHfeIsayAB+q2ugTe1mi1ph1juJKxa5dSs8uGNqdiJRDTU+nn44UC8/HKwzX1+8EGWxJxzfnf+t9+G4rrr/G0e11UV5Z0jHnjAVb27f7/K8GvGDEjsSdeuZffuBFx88S9ODTJnTk88/XRPp/pgY/sIFKWmomDPHi3+H0QI9KhXD14XX6zFArSvJ9auLgQKxWVjhggxhSIO2ZM8xRItSDawKBG5pqaa9Jx32WWXQQlr3vJ7u379evSTuJ2G6dVXX8XDDz+sZT3zzDN46qmnDIttOj8m7mo7iR/q/Px89O7dWxtHuQMtScpicYi4TFeuQtU8lBVgmzZtSopLfyrrQiUOqnmGhIRgyZIl2nVpBYMTNZZyL6q3FWtNuvcGuHhaRQRW7Y/FnQv+Nhr93mHtcO+w9kZ5vCABEiABEiABEnA9AT7nuZ4xR6h8Ap6VPyRHJAESIAEScDWBDHnR5qw4qObojXyMxFel0507NwupqYWl1xWd3Hijn7ygcy4WYYMGHuLi1K+ioSqlXMKXSeyiShmqWg2iXIOqOIISWs5sUu/4xbhELPpcLw6qCbz/vv2Wg+Un/sEHB8Ui1vpnOScH+OMPiLVMsTB8993AI49AYnAB+/cXxwMq3y+vLRPwCA2Ft/ie9ZE4gz7iKtBbYop5VIfgopanzJIKCHg2aqS5kvUSAcnW5D16NII3bKjR4qCtLNyhnnIhqsRBlW699VYTcVDlPyA7Z5S4p9JbYgmcl2c+JqxWwcI/b775piYOquJ3331XixVoWFXFN1T5KilhT41jLinXoUocVEnVV2KhpaSERr3FQUtjMZ8EHCUwvHND9G9Tz6j5B+uO42xyllEeL0iABEiABEiABEiABEjAEQIUCB2hxjYkQAIkUM0JxItbP71SH/xW2lV6ehG++iq79Lqik5AQT7z4ogVVqaLG/5a/9lowfH2dExltHKrCap7yrXnllRVWq3EVxo4F7r8f+PDDYqFs2jSIRSckfhPw2GNKsIPEeQLCw12/9JSUXIkdddzpgWJjs7Bo0Smz/aiQVJ9/XmwN+cEHgHjXg7IejYsDTp6EuK0DxEhGY/Kb/HoUWtcZzY7BTBKoKQQ8xRI0SFxNBq1bB5/Jk2VniRk3sSIE+9x0E4I2b0bgsmVQYjGTexBYtGhR6URvueWW0nPDE+XGc+q/7mJVXMISQdGwjrVzZfW3ePFirUrHjh3Rt29fs9VVfocOHbQyNS9zsQjfUztVJEVGRpbOScvgPyTgpgSUiP3UuM7wNHgUzpYNTi/9eshNV8RpkwAJkAAJkAAJkAAJVCcCZv4HX52mx7mQAAmQAAk4QiD3wgVHmpltE4YEo3zlNnTWLON4KEYVDC/kpd/syZk4uS0Dr39pv1D47LNBuOmmMhdjhl1X1bkylPn+e4iFRFXNoPLHXboU2LcPEl+q2EJQWRRWVVq48JRu8QO//PIorrmmtdFSzp0DXnihWAw0KjBzEROj4mEVs5k9G1AuVplIoDYSUC+wvSWmoDqU29GCLVtQlJgIMc8qdiXbvz88Ge/NLT8aG8TaU6UgMSHvZSXmpHL/WZJU7L8r7dhNc1J2XsSoP6iSDPsp6c/wpypX8Q6Ve6dTErtUCYEl6fTp09i6dat2+X+ya6XEOjBHzMFV/74iVDcSq1dlOchEAu5EoGOjUNxwaQt8teV06bSX7D6Lqf1aonermuuquXSxPCEBEiABEiABEiABEnAZAf7vyGVo2TEJkAAJVB2BInG/pVdSbkYN0969BcjOLoK/fwUqkYiDOHFCU1pevQOoH1QHj31QT3b8V9BOBlPv7t5+O9h2IdJwgi4+l5BGmrXct9+6eKBq1r2ynFOGGStWAO0l7I36iInHN7RoUex+1M+vciZ86pSY9+mUoqLSjXqKjwfmzAHEAMautGlTsRXhvfcCysqUiQRqMwHldtRzwoTajKBGrf3gwYPaetq2bWtVWFOWfyWppE3JdUU/Desb9mOunWG5amcoEJaIg6qdipN45MgRPP7441gqu1xyc3O17oKDgzFKXBw//fTT6NKli7khmEcC1ZLA/Vd2wJJdZ5GaXfZc/szSA1g8e4A8e1T8bF0tF8VJkQAJkAAJkAAJkAAJVDkBCoRVfgs4ARIgARLQn4CPjr4e0xFmMsGkpEI0buxlkl+aocTB4+IG8vx5LUtZnD0yJRkTBmXgg0VhmL8iBCnppu3rhRXg1qsyMOPxpojsUL0sB0vXJifq3bdamnjVq3VJab7qMEzz5yurD2DkSIh1hmGJ/ufp6fqZbqanl71kU25CX3vNfnGwZIViMCUuSyHxMkty+JMESIAE3JtAdnY24tXOCUnNmjWzuphwee5QVoYZGRnikll8MtuRDOtXNE7z5s1LezZspzIPHCiLT6usCadMmSIW55ml9dVJeno6fvzxRyxZsgQLFiwQK/JrjMptuVDWi9bSOWWKzkQCOhOoG+SLe4e1x7PLyj7ne2NS8PPOM7imd9nvhc7DsjsSIAESIAESIAESIIEaToACYQ2/wVweCZBA7SQQ1qePbgs/jB4mfVUYE1BczJWIg4aNO7TMw5v3xOP5OxKwbGMQTsd5Iz3LEyGBhYhsnIcx/TPh7yfiYo680CvqprmnM2xfXc6V4Hn77YAKo/XLL9VlVlU3D3kfrFkWrloFzJwJiJdBl6WQEP38eBr2tXevqfBp7yKWLwdUvEbxYsdEAiRAAm5PIE0FZP03Kcu7ilKJQKhEOHuSPeOoMUpS+XESlVvbf9MjjzwC5VpUxU188MEHoSwgz8vOHiUKzhFTcVV2k8TFbC8m8RdddFFJM5t+GoqUNjVgJRLQicBN4lL0661ROH5BHrz+Ta+sOoxR3Roj2I+vdkqY8CcJkAAJkAAJkAAJkIDtBOgIy3ZWrEkCJEACbkMgXMy5gjp00GW+y3GzUT9eYvgXFmbFlZEyxapgd31QQBEmD0vHQzcm45nbEvHgDcmYdFlGsTioRlMvF1NSjMatbhfKleT11wOvvAIMG0ZRSN2fgoJiN6S//ea6uxUZKT5edUqtWpW98FbiprNJvUtXloRMJEACJFATCCgLwpKk4vdVlPz+9TWdlZVVUVWjcnvGKRlDdVB+HGW9WJKUADhbgsN+9tln6Ny5sxZ/UFknPvbYY/j888+1aqrOE088UdKEP0mg2hPw8fLEk2M7G83zQloO3v/zmFEeL0iABEiABEiABEiABEjAVgIUCG0lxXokQAIk4EYEPMTErfmsWU7P+JBYDx6D8c76kSN9JQ6RFYFQ7eDP08ENpLJCdIPUqhVwxx3ARx8Bl13mBhOuhCl+/DGwZ49rBrr66lYIDtbHinDatPbaJNVH9u+/9ZmvK8VRfWbIXkiABEjANgL+/v6lFUti+JVmmDlRgptKAQH2uQi3Z5ySMcyNY9iPmsPzzz+vzaf8P9fL7p7evXtr2b/++qvsR7JvQ5JybWrt2LZtW/kheU0CuhEY2qEBLu/YwKi/TzecRFRCmUBuVMgLEiABEiABEiABEiABErBCgAKhFTgsIgESIAF3JtDk5pvh62RAuB/wHxMEs2dX8OIvLs6kjUMZSrXJzXWoaVU0CgwEpk+HxGCqitGr15gqBOW337pmTqGhvuIWrq3TnTdtGohx41po/aiYimrOeqRjsolfr770mE9N7yMvrxCLF0fhuef+wQMPbMHjj2/H22/vw6lTZa4RazoDro8EXEUgJKTMYru8O09zY5ZY8NnijtSwvT3jlIyh2pcfx7Cfvn37ok6dOobDGJ2PGDFCuy4Urwd/27lDRFkiWjsaN25sNBYvSEBvAk+M6QRvz7LNerkFhXhhxUG9h2F/JEACJEACJEACJEACtYAAHdXXgpvMJZIACdROAj5hYei5dCm2i7vRgkyJ6Wdn+hb3YBNGG7WKjPTEiBEVuBkziFlk1NiRC+VqtG5draUSXZTn0gsXlFsxQBk21K8PNG+uf6jCIhns6NECGatQM4asU8dD4hR5IzCw7GWMueUo72rKmvDNN82V1q6848cBJZZJ2Cfd0+zZnfHBBwedEuJmzpSXa97F+6QMvNI5Pdf8/GJd+19Pe073xw7MEzh3LlOsdg9px9mzpn/f7rtvC8aMaY5Zszpj5MhmUFbVTCRAAvYRUBZ5ERERiI+Pl+9f+QK2kpKSklAi3tkbo0+JbSWponGU5V5JKj+O4bVhnyX1DX8a1lWxCZlIwJ0ItKkfjGn9W+GTv06WTnvV/jhsOhaP/m0jSvN4QgIkQAIkQAIkQAIkQAIVEaBAWBEhlpMACZCAGxMIExdavSS42s7x45EvL+9sTd/iXnyBR02qv/pqMDwNdiybVFDxB9WhVxK1RXks27gRUDHiTpa9BykdITISIloCAwYAzooySUmF+OKLbMybl4UjRySgnkEKCfHA1Kn+mDkzAF26WP767NcPUKj/DXFk0EPxaUZGgZTniXFkkbCEvHz1EatDL6cFDCVqmhNBfMQbpxKtqsKqbfVq1wiEXbqE44UX+kgsqe0mfG3JGDiwoVibdSut6pWjBCYxAdUpqTidTK4jsHz5aVx33Z8SqtSyK2P1eV+2LFo7rrqqJb755jIR+C3/3rputuyZBNybQKdOnbBhwwbZ8HFMvkvyZWOF+d+jQ4cOlS5UtbEnqRiBJcmwn5I8w5+G5eXH6dKlS2nVAhUU10oyLLe0JivNWUQCVU7g7ivaYeE/MUjMKPO28eyyA1h290B4S6xCJhIgARIgARIgARIgARKwhQCfHG2hxDokQAIk4MYEwgcORL8dO9Bk2jR4GsQTMrek/eiDOfhcxMHHpNjY4uaVV4IwaVJZPCJz7UWhMpvtaObO/X5iAQSxFjMvDqp+lWioymfOdDyOnBLXXnklA02bxuO++9JNxEE1TlpaEd5/PwtduyaKa8pkJCZaFkJHjwbuvRdQ4pxKhYVFiInJwfr1yVixIgGbN6eKS7M0bN+ehpUrEzWxsLim4//m54siYpB8xdDzrruATz+FuF3U38rSYCiLp7t3WyxyuuCRR7rjoYfKRD5bO+zdO0JcUl4pFqj/vuROSEBo4ilbm1dYT7matfD+vMK2rFAxgR9/PIHx49dYFQfL96JckA4f/qtYHotSzkQCJGAXgYHyDKGSsg605opz3bp1pf0OUDt27EiRstOnSZMmWgvDfsx1sX79ei27adOmaNWqlVGVPn36lMY/PK7M2K0kw3LVFxMJuBuBsAAfPDi8g9G0D8Wm4eWVh5Cbb/kZ1agBL0iABEiABEiABEiABGo9AQqEtf4jQAAkQAK1gUBg69boNn8+hsTEoN0rryG+yUBEoQNiEInDuBhLMQ2z8DsewDJswSgjJErs+PDDEBFjbAiupwRCncyn1h2oj5c/CBVhzmg6Fi+UN9JXXgHWrrVYxWyBEgdnzUrHI49kaK5LzVYql7lsWa5YLCbh7FnLFgrKklC5QE1Pz8eaNYnYtCkFcXFlu7xLumzWzE8sH53/Ovb2NhZnVfhG8QynuWK96CKgYcOSESvvZ2qq6ywXlbXkK69cKtaeAxAeLr5dK0jK8nXq1Hby+RgjXmv/FbrVh+voUXRonIpAX33Eox49KpgIix0msHXreUyZslYT3O3tZOPGOLn/68SS1lhIt7cf1ieB2kZgwoQJpUueL88R5pKK4/fll19qRSru32WXXWaumsU89ff8qquu0sqVheCWLVvM1lX5JRaEqn55q/lA2aExcuRIre0O2Rhl6I7UsEM138WLF2tZqk2vXr0Mi3lOAm5DYHKf5ujUONRovh9vOIlx7/6Fnadt9xxi1AEvSIAESIAESIAESIAEahUB599I1ipcXCwJkAAJuDcBX4nn1/qhB3BjzAZ03bgXf9z4Dx7yXYX38TJOoKvR4sLDPXD//QE4eLCuxNUL0MqUW8yYmAKxsMtHbGyBuBsz87JdXg46m3ZHhWHemrbyMt9Y9KqoX/XuX1kT7tpVUc2y8qeeypA2EtTQznToUAFGj04RAdP8Lu2DByHs8vH770lITbUsJLZsWYFVpo3zMnxR6u1ViL7t4hGafBo4cQI4dQrhQTk29qRfNT29zVqa1YwZneQzeb24dB2MPn1EkS2XGjUKwJNP9kBU1HXiPnaIuHP916xT1RMuyiWun08hhnY+X66lY5fK3S2Tawgol7K5ueZ/32wZ8aefTorwoM99tmU81iGBmkDgkksuwaBBg7SlfCom6Zs3bzZZ1uuvvy7fd/KlJ+mee+4R63mDv7OSt1Z27qjvKHVME28G5tK9YnZf4urz7rvvlg07xt/L6lrlq6Tqqfrm0qOPPqplKxeis8QFgXKLWj7973//Q4kF4S233AJfXzG5ZyIBNyTgJZufnhrb2WTmh+PSMGneJjy1eB/Ssi274zZpyAwSIAESIAESIAESIIFaR8B8EIlah4ELJgESIIHaR6B/fx/07x+GN94oxMKFOZrwl5FRhNBQD7Rv740JE/wkZpeHZnGzZUse5s7Nwg8/ZGsxAUtoqbh8N93kLy/hDOLyNWoEiNtGR5MSlT78rS0K7RQHS8bT2n8IvPdexcaMu3fn4fnnVfw5x9Lu3fkSCy8TL74YbNLBwoUFErcpuUL3oYGB+gWsCxAruKt6x+DyLudRJ+jfF0KxxVPr16IQB4+3NpmnKzOCxOhU3ge7PAUEeOPmm9trx/nzWYiPz9aEpPBwXzRrFiRGrWb2QymTUwPz1OHdY7FiV7GLO0cn3KIF0KGDo63ZzhqBgweT8Oef56xVsals7tyD6NevoU11WYkESKCYwNtvvy1W8wM00W748OHitvpxzUpQiXbfffcdPvroI61i+/btJb7rAw5hU20ffPBBvPTSS1DWf2q8Rx55BG3atNHEvJdffhn//POP1vdDDz2Edu3amR1HCZpKGJw7d67EIF2GIUOGaGKi6uf8+fNYsGCBxCT9RmvbvHlzzJkzx2w/zCQBdyHQr009PD66I1769ZA8O5fNWm2a+3JzFFbvj8OzV3XB8C7yfM5EAiRAAiRAAiRAAiRAAuUIUCAsB4SXJEACJFDbCDRo4IkZM4otBMuv/Z9/8nD77WkSd8h0B76qq+LyKeFQHZdf7iPx7kLRqqW4OgqQ/srt/i/ft6XrnSfDEZ/mZ6nYpnylT+7cCbEos15dzdvZ9MknWXj66SBx5WmshP30k3JZWrG1U3nXoM7M5zIRBideEmO2i8EdL+Drv1oiJ08/QdLsQAaZFt7fGtTQ/7RBgwCoo8IUF2dUpUndbAzrFovf9jr2Ak0JoVOmVI4gajTxWnIxb16xdZKzy/3hhxOyKeJScf9rw2fE2cHYngRqCIEe4jv5+++/l79xU8QiPlUTCMsvTQl8y5cvR0hISPkim6+VZZ8S8T777DNNDLzuuutM2t56662ysed5k3zDjHfeeUfce6drbk83bdokLr43GRZr523bttUExAjli5uJBNycwB2D26Bv63p49Oe9OHBO/LsbpNjUbNyx4G+MFIHwGREKG4bq47nCYAiekgAJkAAJkAAJkAAJuDEBM1vq3Xg1nDoJkAAJkIBuBNasyRW3YskWxcHyA/3xRx769k3CLrGqQ8uW5Yttvl61xzGBpvwAq1eXzzG+TkkpxFdfZRtnOnAVH1+En34ydt+p+j5wwDjPUtdm3bRaqlxBfrC/eSFXNQv0K4ASCSsziaFJ9U1mrFynDz2JHq0ci9kj76xx8cXVd7nuPrMVK87osgTlovT338/q0hc7IYHaRGDcuHHYs2cP7rvvPvEy0F48DARCxRvs3bs3Sqz7lOjmTPL09JSNRp9qQqOKMdikSRPN/af6qa5XrFiBTz75BKqeteQlsZC/+OIL/Prrr5g0aRKaNm2q9VNX3Kwrd6lvvfUW9u7dKxbfNPm2xpFl7kWge7M6WHzXADw2qiP8fUx/R1buj8Ww19fhqy1RDsXydS8anC0JkAAJkAAJkAAJkICtBGhBaCsp1iMBEiCBWkRgx448cTGajEw7vW/GxRVi5MgUifMVjlatWhXHeLODW3q2N3ZHhdvRwnLV3buLPUhaMmZYtCjH7vVZGm3BgmyxrCjbka2u8/IM/DxZaij5GRkFCAvT5+u4Qah1UfKavtH455TzFppWllNaVF/CAYrRSfVMyg+tmbhU3l5FeGjcIXzwWxusP9jAprmreI8zpmRg8HDHrWZsGqiWV7pwwXlr3xKEygUtEwmQgP0EWsrmnzfeeEM77Gk9dOhQzV25rW1Gjx4tMX5H21rdYr2RI0fKM8lIi+UsIIGaRsBHXKrfOaQNRnZthP8u2ocNR+ONlpiWk6/lL/onBi9O7IZ2DfnsYgSIFyRAAiRAAiRAAiRQCwmYbi2rhRC4ZBIgARIggTICBQVFuP76VIfFMyUS3nabuDeSHf/vLG9sTocpG6zc2U/rxD2pjinJijHY6dMVu/+0dSrR0QVGVX/4wbpQZ1j51Cl9xAoVf/CSNuJb1UpScQkfn3AAYYG5VmrpUyTGHmLloU9fuveiAvNYSEoknD38GJ6atA9928XD08N83RD/PIn3eAZvTf0Hg/voJ15ZmFatz87L0+/3VVkRMpEACZAACZBATSXQsl4Qvpx+Cd6cfBHqBvmaLHNHVBJGv7MBb6w5gpx842dYk8rMIAESIAESIAESIAESqNEE9DFZqNGIuDgSIAESqF0EVq7MxbFjzr0s+P33POzfn4+Pl4Ri3tc+mD0pBVNHpiI0yLzYsmprAOb9EoYN+8NwxRX68c61ooOlp5ufiyOjl+/r3DnbBYizZ3MkVmGBhG10LjbgkE4X4O9b8bjN6mXhf5P34tWlHREVH+TIcitsM2wYcOWVFVarugoVKJcqnmDX5qnakZjui23H6iIpw1deonki0LcAzepmonebRPh6//sZ8ubjlKtvZni4n/ye2GnSbGFSqi8mEiABEiABEqjJBDzkYebqHs0wpH0DPL/8ABbuNI5RnScbAt/5/SiW7TmLF6/uhkslhiETCZAACZAACZAACZBA7SPAN1q1755zxSRAAiRglcDcufpYQ82blwUlnJ2K8sXdb9THYx/Uw7WXp6NTy1zUCSlEZrYHziV44ac/g3HsTPHu5uBgq1Ozu1BCJFlMISGiAumUyveVnW27+KiM2Y4fz0LXro4v3gNFGH5RrM2raRCWg5dv2I1dUXWwWmI+/nMyXHow5qE0r8aNgcRE5QbV5q4xYgRwyy2AEtmqbVKTCxJx1IaF1Q3OxciLK2Br7YNWbSG418T69KmPxYujdJl0nz4RuvTDTkiABEiABEiguhNQFoRvXHsxJopY+Pgve3E60XizzYkLGZj80RZcf0lzPDqyk3iZ8KnuS+L8SIAESIAESIAESIAEdCRAgVBHmOyKBEiABNydQGxsAX791YrZnR0L/PLLbLRqVeZjMj3TE58ts+5CVFnS5ecXwdvbeXXJR95v1K1recJt2jhnsWfYc/m+wsI8cOaMYQ3r54cPZ6JRI19ERJi6gbLesrj0hoFRYtVmn7CrjOh6RiZrR3yqL075tEO6V5iwB0LlNnXoAPiJoVVaGkSYAf78s/jc0nzatQPGjAH69avm4mDJAho2BE6cKLly/GdYGOBfFn/S8Y7Y0hqBO+/sqItAOGBAQxHjrfxhsDYJlpEACZAACZCAmxIY2C4Cq+4djLfFavDjDSdQUGi8me3bbdFYc+A8nhnfBaO7NZKNXs4/i7spKk6bBEiABEiABEiABGoVAQqEtep2c7EkQAIkYJ3AqVOFsBKezXrjchS8rfUAAEAASURBVKVpaUVo1swLe/fa7q60QKqePp2N1q0DyvVm/+XAgdZ1m6uu8oMS8lJSjF+Q2D+Sspgznm+XLt7iYtX2dReKZ9CNG1MwaFAdETXt27k9QeLgje911pFpl7aJCM1FRKTsKG8sYle5FBICTJkCXHstsGVL8ZGcDCj3rcoIr3lzaG5hIyPLNazul/XrA6dOAQq+M6lRI2das62NBEaMaIbIyBCcPCmKtRNp1qxOTrRmUxIgARIgARJwXwIBvl54dFRHjL+oCR5buAe7z6QYLSY+PQezv9mJKzo2wHMTuqJJHePnW6PKvCABEiABEiABEiABEqgRBMpMO2rEcrgIEiABEiABZwikpjoplpQbfPx4+y3ilLtNPZJydWktBQZ6iLDnvOVX06aeGDfOeJ233Wb/C5Xc3CKsXZskmlWWiLS2iZa3DD2BGwae1sedZwVj+soSBw8GHn4YeOEF4LXXgGeeAW67DSLcWCNdTcu8xILUWXFPWQ5aM1Otpkt3x2l5enrgkUe6OzX1Nm1CMWmSO35YnVo2G5MACZAACZCAEYHOTUKxcNYAPDW2s8RWNvWo8fuh87jyjXWYv/GkiaWhUUe8IAESIAESIAESIAEScHsCFAjd/hZyASRAAiSgH4GgIA/9OpOexozxRYsW9n3VJCfn48IF59ycKveYrVtXvJRZswKgdCJn0syZAZpL1PR04Nw5ICYG6NXLB+3a2d+xsqDcvj0Ny5cn4MCBDOnXVLA1nO+lbSVAoF5J+RatbalFi2J/qo6sW92Ijh3dxJ+qIwusfm3uuKMjbr9dfrkdSOHhfli2bLi4zbX/99KB4diEBEiABEiABKo1AS/ZeDN9YCTW3D8El4vFYPmUkVuAZ5YewMS5G3HgbGr5Yl6TAAmQAAmQAAmQAAnUEAL2vbWtIYvmMkiABEiABMwTaN5cv5fnKn5dw4ZemD3bfmu6bdtSkZ1tu4tOw9WokHB3322YY/m8XTtvvPlmsOUKFZQMHuyDPn0C8fjjwPTpwD33APfdB8yY4YHGje1fd8lwWVmF4koxA6++WoS5c4EXXwQuvri4VImIJelorONzL+mj9Gewjn2VdlrNT1QgRiXyqaCL9iQlpnbuDAQG2tOKdZ0koOIhzZ07QH6/5J7ZkRo3DsQff4yWW13HjlasSgIkQAIkQAI1n0BTcSP66c298d4NPRARLA/v5ZJyQzruvb/w8spDyM4zeAgtV4+XJEACJEACJEACJEAC7kmAAqF73jfOmgRIgARcQqBFCy+Jg2dfDDxLE5k82R++vh64995ADBtmX5+ZmYVYvz4ZWVn2vYgIDweeeAJoYLoR2tI0RUwMxJw5EkzPztSxo7fEWAzDJ5944Ngx08YNGgRIfD7TFy2mNU1zRAfBN9+EoWlTL9SrB7EoBHbtMq23Zo9O8e+UQFZbxa4Ssa9ZM4jJpink8jnKpWh3cXWpgjMy2U1gz54EfPnlUbzzzj58+OFB/PLLKaSm2m4x7O3tqYmECxYMEdFcfjmspKAgb9x5Z0exyr2qwrpWumERCZAACZAACdRoAmoDztjuTfC7WBNe16e5yVoLCoswb+1xjHhrPf46Gm9SzgwSIAESIAESIAESIAH3JWDDmzD3XRxnTgIkQAIkYD8B5XZzw4Y8+xuWa6H6UUmJhD//HCbuRlPw11+295uSUiA6TLYYdwVh505IXL5yAxhcKkGtZ0/g1luBiAiDAhtPn346SOLoeUp8vQzExZm69TTsxke0zn79/GWcEOTlWXbJql629OkTisLCVHE7mmPYhdVzpVF99lkIrrqqWFxcswbCzXyTvafDcC7JH43Ds81XsDXX2Vh8to5TXespS0LlbrRpUyAhAfIhgKjTgDLXVGUqAKNSahs2hPiorK6rqLbzys7Oxw8/nBRh7wC2br1gMs/gYB/cdFNbzJzZCd26iQBbQVK/W1OmtMONN7bV+lNC4969SUhOzpXb4ym3KQATJ7aSPtshLMw4PmgFXbOYBEiABEiABGotgbBAH7w0qTsm9GiKxxfuxYn4DCMWUQmZmPLpVkzq2QxPjOmEukH8jjUCxAsSIAESIAESIAEScEMCHkWS3HDenDIJuD2BM2fOiHVR8Q7N6OhosUQS6xUmEqgGBHJzi9CqVYLE07MulFmbau/e3ti2LRzqRX5Jys4uwgMPpIvFXRZyKzAYatTIEy+/HISpU4tFxvPnASWUrVsHEQFKeoS8/AeGDAGGD7fParCsB+MztfZFi3Lw/vtZYsFoLGY2b+4p1kgBYp3oL3Ox3RWr+po9fDgTR45kIifH+lfuJZd445VXgmVNvkjZvh3Rn3yK3SuOoCgjFfleQcgMaIrjLa7HmcajUeRRPIfLusRh5pXHjRdiz5W/f7H/UiWEMZGAzgR27UrA+PGrER1t/JLR0jDKfei77/YXY05+Hi0xYr57EOBznnvcJ1fMkvfeFVTZZ2UTUO5E5/55DPPWHUdegenzqxIHnxzbCRMubmr0vF/Z8+R4JEACJEACJFCZBPicV5m0OVZlEaBAWFmkOQ4JlCPAL5VyQHhZrQisWpWjWfwZxruzdYLBwR5i8VYHF11k3q3ohQuFYiGXJe4FsyTOnrEIOWSIjxazcMIEP/j4lImLhmPniW6njLsCRDtU1nyuSqmphYiPLxIxswjh4Z4iDHrIfD3w6KOOjVgo7pnOnMlBZmYWTpzIR0ZGkWacFhEhbp3G+kFZXPbs6Y1zX3+NqLffRuqOHRYHSg9sgYNtZmJ/2/+gwDsAt152EiMuirVY32KBMlfs1q0YpsVKLCABxwhs3BiLkSNXIT3dWGyvqLdx41qI1fEw+f2mSFgRK5ZXXwJ8zqu+98bVM+O9dzVh9l+ZBI7EpeExsSb8OyrJ7LCD2kXgfxO6oUU9xmU2C4iZJEACJEACNYoAn/Nq1O3kYv4lQIGQHwUSqCIC/FKpIvAc1mYCCxZk4ZZb0jQvi7Y2CgryEAu8MIk5WLHLIWVZl5BQhKSkQs0Nab16HggOrt6CwNy5wNq1ttIwX69JE+DNN4H8/CJ4iRGgp2exEFooZpX7b7sNZxcsMN/QTO6F8N5YPXAZcgIaYOrgUxjT45yZWqZZ+QUe2B9bD0mhLZHr4aeJrcq7ZmQkZBe4aX3mkIC9BI4dS8Glly5BYqLt7nUNx7jjjo6yiWCgYRbPScCtCPA5z61ul66T5b3XFSc7qwYE1Ca3r7edxiu/HkJaTr7JjPxlQ899w9rj1oGR8Paq3s/yJpNnBgmQAAmQAAnYQYDPeXbAYlW3IcAYhG5zqzhREiABEqhcAjfdFID69T0xfXqaTe5G27XzwrffhqJXL9vM+pT7UWU9FxFh+iKhQFwZLV+ei6VLc3D+fKHE+gPq1PEQCzsf3HyzvzavyqUBpKUBGzc6P+rZs8C+fcpwr0yJKyosxJ4pUxD34492DVA/aQdGr7scyy7biC/WReLw2RCM7XkW7Runm+0nPtUXa/Y3xh/7GyIl3fQRoFWrYnetA0WXUZ5HmUjAUQKPP77DYXFQjfnRR4egRMJevRwIKuropNmOBEiABEiABEjAhIDazHZT35a4slNDzFmyHyv3G3utyM4rxIsiHi7edRYvSwzDbs0kBgATCZAACZAACZAACZCAWxCgBaFb3CZOsiYS4K6TmnhXa+aalIvNX37Jwbx5WRID0NhVoLI2GzPGV3OPOWKEb6k1nKMkUlIKtfh/yv3o6dPG7kdL+vQV48TJk/1x330B6NHDNjGypK0zP1etAj791Jkeytr27Qvcf3/Z9fHnn8exJ58sy7DzLLrRKKwetKK0VWSDdAzrFifunjIR5JeP3HxPbDjWGL/+XR+FhWXCZGmDciehocXz69y5XIHBZUICsHcvkJoK6RNi/Qm0awe0bGlQiae1ksDZsxnyOfhOs5J1BsCtt7aXmKWDnemCbUmgygjwOa/K0Ff5wLz3VX4LOAEXE1glAuHTi/cjNjXbZCTlGGP6gEjcd2V7eQY13Yxm0oAZJEACJEACJOBGBPic50Y3i1O1mQAFQptRsSIJ6EuAXyr68mRvlUMgOroA0dGFElOsEGFhnmjVygsNG5paADoym+PH8zFqVAqOHi2wqblyz/nhhyG49VYJRuhAOnw4H1u25ImL02JXn8qS8bLLfNCokXRsJn35JbBsmZkCC1nKhWpqaoHEMCyEnGoxFUNDvcWtqAf8/JSFVHHov4LMTKwV/575yckWerIt++t+WxEb3EOL2+bv76mNY1tL87VUeMIHHoBYcJWVq3UoUXD1amD7dmjrKistPmvfvtgKsV8/18aILD8ur6sPgWef3Ymnn97p9IQCArwQE3ODxACVXxgmEnAzAnzOc7MbpuN0ee91hMmuqi2BtOw8vLrqMBZsiTL7PNi0TgCev7orLuvQoNqugRMjARIgARIgAXsJ8DnPXmKs7w4EuKXLHe4S50gCJEAC1YRA8+ZeUIfe6dSpAgwalGyTK9OSsQtER7zttjQR4Iowc2ZgSbbVn3l5RVi8OAdz52bhzz+NrSFVQyWKTZrkp1lEDhrkI/H4yqztsk03SZsdKzu7ECdPZuHEiSxkZhpbQSpRs0kTP3TqFCgWmT6aFeG578TSyklxUE2k7uZ3sABvaHPy9vYQCy5/tGkTIEKuY1/1+RJiRsVKfOYZSD+QtRRf795tdtmlmUeOAOr46Sfg0UfVekuLeFJLCPz88yldVpqVVYBff43GDTe01aU/dkICJEACJEACJKAPgRB/Hzx7VVdcdXETPLZwL47EGbu3j0mWWObzt2PcRU3w1NjOqB/CzT76kGcvJEACJEACJEACJKAvAX3MPvSdE3sjARIgARKoRQRycoowdqx94qAhnrvuShexL9cwy+z5yZMF4pI0Eddck2pWHFSNlCj2/fc5GDIkGePGpUjcwTKBr6KYfMpicO/edLEyjJcYgxkm4qDqX4ma0dE5YoGXhOeeS8TOnWKROW+eKnI6XYaFCIT4+5SUn1+E48ezsH59srj/FLM/B1OuYP38cyAjA2IRBlQkDhoOEyvhaf77X4irWMNcntcGArGxoibrlOLisnTqid2QAAmQAAmQAAnoTaBXy7pYdvcgPDi8PXy9TV8vLd19FsPeWIcftkeLpaHjz6R6z5v9kQAJkAAJkAAJkAAJFBMwfYIjGRIgARIgARKoRAI//ZSD/fttcytqbloq/t1zz4mCZSUpd6L9+iXZNc7y5bkYOjQZKi6iShERlgdQItzWrak4dCjTrJslcy2Tk/PRp08Cluxoaq7Y7jx/ZKE19hu1i4z0dzou5OHDgIRIRFSUUdc2XaTLZvIXXwR0MJC0aTxWqh4EsrMd/30uvwJlRchEAiRAAiRAAiRQfQkoYfCuy9th5T2DcGlkXZOJpmTl4eGf9+D6j7fgxAVjS0OTyswgARIgARIgARIgARKoVAIUCCsVNwcjARIgARIoT0C5+3Q2KXehBw+K+Z+ZFB9fKLENkxEXV2YNaKaa2aydO/PF5WiKWP4VicAIietXVi0pKQ87dqRixYoE/PLLBc0ysKzUtjMlbr6ED7EK19nWoIJaIUgqraG8o7Zu7Vh8xtJO/j05frx8ju3XCQnAokW212dN9ydQp45+bsTq1PF1fyBcAQmQAAmQAAnUAgKt6wfjuzv64pVJ3REW4GOy4i0nEjHy7Q1474+jyM23/7ncpENmkAAJkAAJkAAJkAAJOE2AAqHTCNkBCZAACZCAowR2787Dpk2msQAd6e+DD8wLja+8kikxAR1/CfH773n4+ecchIcDl1wCERpz8fvvifjttyTpN1vcbxaIG09HZlzSxgNvSezAXRhYkuHwz0KUKZh16/ogMLDs2uFOdWi4di1gawxHHYZjF1VMoFs3+WXRKXXrZmqJoFPXWjdpabniBjddBP50+V3W52+RnvNjXyRAAiRAAiTgTgRU/O5r+zTHb/cP0eIPlp+7EgZfW30E4979CztPl21sK1+P1yRAAiRAAiRAAiRAApVDgAJh5XDmKCRAAiRAAmYIrFun3wv5tWtN+8rKKsKnn5oXDs1Mx2LWvHnFfSQmZmpx/RITzVsrWuyggoIiEfbewmsohJj9OZFSUSam1PNJRttTX6L7oZfRY/8cdDnyFprGroZHUeW7bMyUkHQbNzqxMDZ1KwK3395Rl/l27BiGgQMb6tKXYSfp6Xn48MODuPjihQgN/RItW36HFi2+Q3DwF2IpvAQLFhwVQVvf33HD8XlOAiRAAiRAAjWdQP0QP7x7fQ/Mn9YHTeuYerQ4HJeGSfM24anF+5CWbfoMX9P5cH0kQAIkQAIkQAIkUF0IeFeXiXAeJEACJEACtY9AYqJTpndGwMz19eOP2UhMLDKq58iFEh+ffTZdYvGJ0uWiFItI7MQQ9MZah0ZIEXHwGLqhvfQyDp9jaOwi+MTmmPSVFtgSh9rMwJHIW5HtV9+k3FUZ27cDV1zhqt7Zb3UiMGZMczRvHiRWedZjg1Y051mzOkNZIuiVVKzQ5577B6+/vhdpaeZfRm7Zch7quO++LXjyyR74z3+66DoHvdbCfkiABEiABEjAHQhc1rEBVt83GG+sOYL5G09CvopLU5Gcf7k5Cqv3x+HZq7pgeJdGpWU8IQESIAESIAESIAESqBwCtCCsHM4chQRIgARIwMUEzLn5XLjQVCBzdBpz5rhOHCyZ0zLcUnJq9881mIyZ+C/ewShcie/hA/NrD8mMQp+9j+GaFW3Q7NwKu8dxtEFKiqMt2c7dCHh7e+Lee7s6Ne169fwwdWo7p/owbJyTk4/hw3/FnDk7LYqDhvUTEnJkDVswe/YmcSFs8DbTsBLPSYAESIAESIAEKiQQ5OeNJ8d2xqLZA9C5cahJ/djUbNyx4G/MkCNOzplIgARIgARIgARIgAQqjwAFwspjzZFIgARIgATKEahXT7+voXPnCjF9eir+/jsP+flFWLQoB3/9Zd5KqNw0bLpUu5xdnbaJtJeFQLuHUa5JI7Efo7HA5ra++Wm48q9xiIz+weY2zlTMp8dGZ/C5Xdt77umCiRNbOTRvX19PLFw4DGFhvg61N2yUmpqL99/fj0aNvpHYoWcNi2w6nzfvIJ54YodNdVmJBEiABEiABEjAMoHuzepg8V0D8NiojvD3Mf0/wMr9sRj2+jp8tSWKm3MsY2QJCZAACZAACZAACehKwPSpTNfu2RkJkAAJkAAJWCYwdKjzAkBJ70rAmz8/G717J0kssQu4+uoUJCRUgqpXMgEdfhZKLMJk2O/2Mwod0Avr7Z6Bp0Q9HLLtJjRI2Gx3W3sbBNqve9o7BOtXIwJeXp746quh8nvY0q5ZBQZ6a+Lg4MGN7WpnrvLixVFabMG77tqM5ORcc1Vsynvppd1Yt+6cTXVZiQRIgARIgARIwDIBH3k+uHNIG6y+dwgGtYswqZgmFv//XbQP1364GUclTiETCZAACZAACZAACZCAawlQIHQtX/ZOAiRAAiRghUC3bt4YONDHSg3HinLMe9d0rLNKbpWDALtG3IdLxXrwkF1tDCt7Febikt0PGGa55LylfTqRS+bATiuXQECAN3788Qq89FIfNG5csUJ85ZVNsXHjOIwZ08Lpic6ff0TEyTVISXFcGDScxLvv7je85DkJkAAJkAAJkIATBFrUC8SX0y/Bm5MvQt0g0w2DO6KSMPqdDVrswuy8AidGYlMSIAESIAESIAESIAFrBCgQWqPDMhIgARIgAZcTmDXLPkHM5ROq4gGCkGrzDNZiAo6ji831LVVsKBaEdZN3mS32ys8ym29v5rBh9rZg/ZpAQFkSPvLIRYiKuk4TC4cNa4KICH94e3sgKMgbkZEhuP/+rjhy5BqsXj0KF19cz+llr1wZjdtv3wA93QIvWhSFmJgMp+fGDkiABEiABEiABIoJeHh44OoezfDb/UMwsWdTEyx5BUV45/ejmlC49USCSTkzSIAESIAESIAESIAEnCfg7XwX7IEE3JPA6dOnsXz5cqxduxa7du3CmTNnUFBQIC8uI9CrVy9cd911uOaaa+QlJn9N3PMOc9bVkYD8mmHLFiApCciT8IDK7WSjRn646CJv7N7NIHWhoR4YtXopzn34Hs59+y0Ks7NNbqOKN7gDl2MZpmE3+uNr9DCp40hGp+PzsLHXh0ZNGyRuQYMLf2FfhweN8u29aJK+XVw99rG3Ges7QSAtLVdcfB7DF18cxalT6UhPz0NIiA9atQrB1KltMWVKW7k23bHvxJBWm/pIrKH/+79I7bBa0cnCgoJCzJixUb7P9XUvrPpbsOAYHn30IidnyOYkQAIkQAIkQAKGBJQF4RvXXoyJIhY+/stenE7MNCzGiQsZmPzRFlx/SXM8OrITwgL19z5iNCAvSIAESIAESIAESKAWEaDyUYtuNpdaRuCpp57C888/L9YFpi8QY2JixEogBkuWLMEbb7yBn3/+WV5sO+/urGx0npFA7SJQWAhs2wasWgXsN+ulzwNdu4YjNDRbRMJMpKbWXjdC06b5o96lPeX4DO1few2xIhJmHDmC/NRUHDrlh5/X1pVIgxNwDq20D1Fv/I5gOywOrX3yWsQsNhIIIxumov/iscj3CsTBNrNQ4F2xi0hL/XfZ9SwKsn6AVwCtRS0x0is/OTkHTz75Nz7//KgmChr2m5GRj9jYLBHpz4tV33ZMm9YOzz3XG2FhlScUGs7HFecrVkSLtWK6K7rG8eO2W/e6ZAI6dpqQkK0Jx8oFa3CwN5o1C0KTJkE6jsCuSIAESIAESMA+AgMlJuGqewfjbbEa/HjDCRQUGv9f/dtt0Vhz4DzmjO+MMd0aQ1kgMpEACZAACZAACZAACThHgAKhc/zY2k0JnD17VhMHg4KCJEbR1bjiiivQrl07+Pv74+DBg3jnnXewfft27NixA8PEL97OnTvlBVqwm66W0yaBqiOgDODefBP45x/rc8jN9RBLwgA0bOgvv29pOHHC1HLOeg81o3TmzDIBzbduXbSYPbt0YZHphbijeQKSk8teltRBfGm5syf+OReAokIEBHri8suBy4PW4sB7yp1TAi7bej1+6/8L4GG/Z/KLDv4PLc4tQ35yMgVCZ29SBe1Pn07H6NGrRIhPqqAmkJaWh3ffPYA//zyHFStGoHnzmvEdN3fuwQrX7miF1FR94hk6Or6z7QrlRetvv8Vg3ryDsgnqNNS1YbrsssaYNaszrrqqJZTFJxMJkAAJkAAJVDaBAF8vPDqqI8Zf1ASPLdyD3WdSjKYQn56Du775B790jMGzE7qiaZ2yZ2ejirwgARIgARIgARIgARKwiQD/928TJlaqaQTq1auHl19+GefOnROXYQvEimIaBgwYoLkWnTJlCjZv3oxrr71WW/bRo0dF4BCFg4kESMAuAjk5EEvdisVBw07VTuBevULRt6/j1mqG/bnT+YgRvujY0fK+neBgT8yfHyq7pctW5Q39BAtPFOK26QX4ULyM3nwz4B9YNpeWZ5dg6NYb4VkgN9WO1O3QK+i1779aCw/fmmOlZgeCSqsaH58tG1pW2CQOGk5q374kXHnlr1AWZe6elDXcypVnXLaMoCD3dWm2a1eCWGr/jBEjVkLFUywvDipoSiy+5prfJS7kdxIP0nUcXXaD2DEJkAAJkECNIdC5SSgWzhqAp8Z2RqCIhuXT74fOY/gb6zB/40kTS8PydXlNAiRAAiRAAiRAAiRgmQAFQstsWFKDCShx8OGHH5b4SyFmV+nl5YW5c+fC998X2j/99JPZeswkARKwTOD99wHxjulQUtZMEybUHpGwRQsl/pn/e2QIcMIEP3z0UQg8//32Tkcdw2Knzr3Eonr4KB+xpC7uxrdBA6P+2kR/hzFrh6BJ3BqjfHMX4Sl7MXTL9bhk7yMSMVEMD3184FNHv7maG7O2591yyzocPeqYC8zDh1Mwffp6t0cYF5fl0jUoN5zumNauPYtBg5aJh4Rkm6YfE5OpWaJ+/fUxm+qzEgmQAAmQAAm4goCXpwemD4zEmvuH4PKOxs+laryM3AI8s/QAJs7diANnHXsGcsW82ScJkAAJkAAJkAAJuBOBMvMAd5o150oClUBAWRl2795dczN6/PjxShiRQ5BAzSFw4gQkzplz6/HzUy6DMp3rxA1at2rlKfEZ66BxY9Pd0eamf9ttAeKO1RN33JGG4+e6mqviUF5Ijx5G7UIuvhj+zZsjOzq6NL9B4laMWj8cycHtcajNDJxpNBI5fhEo9PCGX24SGiRsRsfj89AwYaMmDJY0rD9uHDxk4wWTawgcOpSMZcvK7pMjoyiXk0eOpKB9+zBHmleLNllZ+S6dx403tnFp/9Y6VzGT09PzRMD3LnX/WVBQqIl+Fy5ko6CgCOHhfujUqQ4CDax/9+1LFJeha0ziUVobS5Wp/qZNW4f69f0xfHiziqqznARIgARIgARcRkC5Ef305t5Yvvcc5iw5AOVm1DApN6Tj3vsLt4qYOKlnM7RvGMz4hIaAeE4CJEACJEACJEACVghQILQCh0UkkKN8JEryLDHXIRISIAGbCKxebVM1q5UKC70QEeGD+Pg8q/XctVDpZZMm+eHtt4NF8LNPPBs71g9RUb7iKjAYp+4YglbJ65zG0HzGDKM+PL290ezOO3Hsv8UuQg0L66QfQd/d9wPqsCG1mDXLhlqs4iiBDz7QJ+6e6ueNN/o6Oo0qbxca6jo3tldc0QQdOlSuFWxycg6+/PIoPv74MA4cSC51Cxoe7ou2bUMRHZ2B2Fhjq8mQEB9MndoOM2d2Qpcu4bjttg1ITXXsb2h+vhIJ1+PUqcniUcG+v1FV/mHgBEiABEiABGoUARWGYGz3JhjUtj5eWnkQ326LNlpfgcTV/Wj9Ce1oFOqPQe0iMLh9fQxsG4HwINc9HxhNghckQAIkQAIkQAIk4IYEPGRHcpEbzptTJgGXEzh//jyaNm2K/Px89OnTB9u2bdN1zDNnzqC5WOeoFC0WOs2acYe+roDZWZURyMgARFdCrg7h8U6fzsbWre7tMkjtLygsLLsdLVt64pZbAuTFvb/8jXH+pXvcwoXYNWlS2QAOnPlERGCo/E3y9PMzap0TG4t1LVqgKM8xgUF1Fti+PQYeOsSd3EZk9bvIySlAw4ZfQ8XfczbVqeOLuLgb3VYMyhVXYxERXyEtzfHPqyWGCxcOw9VXt7JUrGt+ZmY+Hn10Gz799AjUuaNp8OBGWL8+1tHmpe2+++4yTJ7cpvSaJ7YR4HOebZxqYi3e+5p4V7mm6kZgy4kEPL5wL07Ey388rCQVu7t7szoY8q9geHHzOvD2YqQdK8hYRAIkQAIkYIUAn/OswGGR2xKgBaHb3jpO3NUEXn31VU0cVONce+21dg+nvjSspXPnzlkrZhkJuC0B0YJ0EQcVgEaN3HvHb48e3liyJExzCZqZWYSgIA94eamofPql+uPHayJcpqMBH2UqLe+5x0QcVDP0a9QIrcWC8PjTTzs2YVFHO775JsVBx+jZ1CoqKl0XcVANlpycq1mltWkTatPYelVSe9W2br0gsX8P4I8/ziIhodh6v25dPwwZ0lizhhs4sGGFnyNl5Xbzze3w3nsH9Jqa1s/kya0lJmpLXfu01FlCQjbGjl0tLprPW6pic74e4qAa7Kmn/qZAaDN1ViQBEiABEqgMAn1b18OKewZh7p/HMG/dceSJa2xzSW2H3x2drB3v/HEMIeKqe0CbYuvCwe0j0Cy89sQ8N8eHeSRAAiRAAiRAAiRAgZCfARIwQ2Dr1q146623tBJl2TfLAfd4JdaBZrpnFgnUaAJpafotz9e3oh2+6mVAohxKcNcvHp+zK7jySh/5uxEoL/p94e1dLAiGhuorDJbMUbkC7blkCbb274+8RMXCvqQExtaPPWaxUZsnn0T2qVOImT/fYh1LBZ3ffx/1R4+2VMx8HQgoN5R6JiUSVmZaujQKTz+9E//8k2Ay7Nmzmfj22+Pa0bVruNTrif/7v0iTeoYZyrWmngLh8OFN8fnngysUJw3n4Oi5iqGolzjo6BzMtTtyJBXz5h0QobazuWLmkQAJkAAJkECVEPD38cL9wztg/MVN8NnGU1h3+AJiko1db5efWFp2Plbuj9UOVdY6IkhzRarEQiU6BvryFVl5ZrwmARIgARIgARKo2QT49FOz7y9X5wCBuLg4eQH5f5r1oIp18MUXXyAwkDsLHUDJJrWUgKE7TXsR5OQUivVQnlggFvvkVAKhcg1k6gxblavYIyflKHFB2knOnXfZKZ04nZRHzp49vUvFQac7rKCDoA4d0HvNGvw9ZgxyxS2oranBhAno/s038FABES0k9XewyyefwLdhQ5x86SULtYyzPf390fWzz9D4+uuNC3ilOwE/P8v3zpHB/PwqEuUd6dV8m9de24OHHtpmvrBc7r59Sbjmmt8xZ05PsWjrYVGw69w5HCNGNMOqVdat+Mt1b3Lp6ekhrpI7SozQfvDxcZzJ+fNZOH48VbPyDAz0FrfCQbBkofnkk3/rYjloshgdMu69dwvGjWsp7tCDdOiNXZAACZAACZCAfgTaNgjBC1d3k/8vFOH4hQysP3IB649egHJDmp1n4OffzJDKRak6Pt90Cr7ierR3q/BiwbBdfXRqHGLxecNMV8wiARIgARIgARIgAbckQIHQLW9b7Zm0iv/n4+Pj9ILni+XLtGnTKuwnTUyfxsgL9hL3oC+88AIuv/zyCtuZq6DiClpLysXoJZdcYq0Ky0jALQkEB9s3bfWf+cTEfHmJniXuDbON4vWpnpR2VVBg2KeKL7ZdjnjDzH+vG5bLq5rLtWvz0LdvElavroOuXSvnqza0Z0/0lVipx8TiL/a771CYY9myzL9lS7T8z38016LWxMESeh7iKrT9iy+i8Q03IHrePJxdsAAF6eklxaU//Zo0QbM77kCz22+Hv5wzuZ5Aw4YBug7SoIG+/Vma3Hvv7bdZHDTsY86cnQgI8MLDD19kmG10Pn/+YPn9W4LTp00/o0YVzVyomKGzZ3fGgw92R4sWdv4x+7e/wsIirFkTo7lMXbYsWv6mKUvnstSnT32xMO4kbjtby1qK/z6kp+fh44/FP3M1TWrTxocfHsRzz/WupjPktEiABEiABGo7AbWprW2DYO2YPjBSxMEC7DiVpImFSjQ8FGvdzUluQSE2HU/Qjpd+PYT6IX4YJLELh7Svj4FtI1Av2DhWd23nzfWTAAmQAAmQAAnUDAIe8mLW+K1FzVgXV1FDCFSmQJidnY3R4grvzz//1Ojdf//9eP31111GkoFtXYaWHVcxAeXlUnnltcWSME929W7dmopz52x1a5gvq9ssR5KZVTaQvL5m8qsuq2lTT7EICherG32tvCpaUW5CguYSNO7HH5EjmxEKsrLgU6cOgjp31sS7+qNGWbUarKj//NRUnF+2DDkxMSjIyIB3aKgWBzFixAh46rCpo6LxWW5MoF+/JbpYnvXv3wAbN4437twFVzt3xqNPn8Umwpk9Q23YMBYDBzay2OTIkRSMHLkSJ09afxlo2EH9+v5ieTgSPXpEGGZr54cOJWuuTqOjM5CRkYfQUF+0bRuKKVPaalaBJQ327UuUuMV/4ODB5JIsiz/r1fMT0W0gJk2K1MS3GTM2WqxbHQqUGH369HVQsR6ZKibA57yKGdXUGrz3NfXOcl3uTiAuNftf68J4/CUWhkmZatOhbUl5NOnaJEysCyV+oVgX9mwZDh+xOGQiARIgARKoXQT4nFe77ndtWS0Fwtpyp914nYcOOb+jvnHjxggLC7NIQQmREydOxNKlS7U6t912m+zk/9hifT0K+KWiB0X2UV0JvPYaIMZsVpOySFm3LhnJyUr0szX9LRVjrFS+Qsqqlwu88eN9sXhxHStzZhEJOEdgwYKjmDp1nXOdSOuvvhqKG29s63Q/FXUwdepaLFhwrKJqVssnTmyFn38eZrVOXFwm7rtvK3788YS4Dbe8H87LywMTJrSU2MP9jFxoqj10ixZFaTEN//jjrNmxStrec09XKOvD0aNXITXVvheO777bX9yZH8X27RfMjlGdMn/++Qp5XoqsTlOqtnPhc161vTUunxjvvcsRcwAScJpAgVj374tJ0QTDDUfj8ffpJKg8W1OQbJbp10ZZF4pgKBaGLetVr/9/2LoO1iMBEiABErCPAJ/z7OPF2u5BgAKhe9wnztKFBArFzOnGG2/Ed+KST6XJkyfjG4nJ5ane9Lkw8UvFhXDZdZUT2LMHeP55y9NQLvfWr0/GhQu2v0gHlLvAPyx3qpU0k397VlCncovVjuPjx+shMpJWN5VLvvaMlp2dL8LWtxK/07Jb2YpoKOu56OjroXdMw/LjXriQpc21JM5o+XJbr5Uwd+rUdUaCnqW2sbGZ+OSTw/j886PSJk1cFhfJd7yHuBANEmG1HW6/vaNJP4rpzTevww8/nLTUrUm+v78XsrON/CGb1LGU4UxbS326Iv/553vhiSd6uKLrGtcnn/Nq3C21eUG89zajYkUSqDYE0rLzNNeiJfELoxOz7Jpby3qBmmWhEgv7tamHYL9iF+J2dcLKJEACJEAC1Z4An/Oq/S3iBB0gwKcWB6CxSc0icOedd5aKg2PHjhWrhgUuFwdrFkGuhgRMCXTtCrRvD+zfX4ioqGzEx+chN7cISizz9fXQXs7bJw6qMU6ZDmSSc0Zy1A7eDiYlVZWhHHl/+GEWXnrJsXhmVTVvjus+BPz9vfHmm32dsiJ8441LXS4OKqLKctBZcVD1o0S+zz47jKeeqnhDQKNGgfjvf3toh7IKzMoqgBLklEhoLinXx1ddtUZiiFqzVjZt6ag4qHpypq3pTFyXk5Jiqzto182BPZMACZAACZCA3gRC/H0woksj7VDPCqcSMovdkUrsws0nEpCZa30DUJTUX5AQhQVbosT1qAd6tgjXLAtV/MLOjUMtPnPovQ72RwIkQAIkQAIkQAL2EqBAaC8x1q9RBFScwU8++URb0xVXXIGffvoJPoyfVaPuMRdTNQSOHctHTEwmli3Lseraz/bZqf+UR9tY/bDUUy6COtpY3/XVPv88mwKh6zFXygjnzhWI28kcnD1bKEJTkcSi80THjl4YP95PRCfzglNlTOymm9pJfLh0EcGUG1770v/+11ti6bWzr5GDtffskSClOqW9e83FIrXeuYfsUggMtP74e889m+0WB62PWnNKK2JXc1bKlZAACZAACdRWAupZITIiSDtu7t8KOfkF+DsqSQTDeE00PHAu1SqaPNnEtPVkona8uuow6gX5YlC7YlekA+VngxB/q+1ZSAIkQAIkQAIkQAKVScD6G5LKnAnHIoFKJjBnzhyxuHhTG7V///4SI2yxWE/4VfIsOBwJ1DwCShScPDkFmZl6rk39R9xWd6StpW5bPQd3uq+4uEKxECqqUgHJ6UXU8g7Wr8/F++9nYeFCJXqbwoiI8MCttwZg5swAtGxZNe5klevHevX88Z//bIaygqso+fh4Sny9/rjjjsoT0/W0QEtOdtylqiU2UVFp+OCDg5aKa32+ssZkIgESIAESIIHaRMDP2wv9Jd6gOh4d1REX0nKw4egFOeK1n/Hp1q3rEzJysWjXWe1Q3DqJReFgiV04pF199GoVDtU/EwmQAAmQAAmQAAlUFQEKhFVFnuNWKYF3330XzzzzjDaHpk2b4pVXXsHJk9bjDHXo0IHWhVV61zi4OxBYujQHEyakQEJ76pys/8e7bDDxbQolEFa/lJamj0CYlFQo1pmFSE8vQkiIB5o399Ss2KrfimvGjPLyijBjRpq4s8y2uqD4+CK8/HKmbDzJxPz5objhhqrZHT5jRieMGdMcH398GB99dAhxcaYxdLy9PdCmTSgeeaS7FoPP6sJ0LlSuPfVKAQH6P8YqZsotMJMpAfW5mTChpWkBc0iABEiABEigFhGoH+KHiT2baYeKq64sCteLYKjiFypLQ2VBaC0dlPrq+HDdCQT6eqFv63oSv7DYwlBZLioLRiYSIAESIAESIAESqCwC+r9ZqayZcxwScILAzz//XNo6JiYGAwcOLL22dKIExFatWlkqZj4J1HoChw7l47rrXCEO2opWgh5WU3FQrSA42PH/7KuXD6tX52Lu3CwsX55rJMB6yzf5pEl+mDUrAIMG+fClgq0fFxvq5ecXYeLEFHGVa6tADYmvB9x4YypSUgrFmrBqrK2aNw/GQw91k7nkY968Q0hNNba+Ves6fDgF06dvwJw5/2D27E64996uEh9UP/HOEt6GDQMsFdmd36CBPiJsbGymuBs/LPERj+LIEetuw+yeZA1qMGlSJGhBWINuKJdCAiRAAiTgNAEVz7hr0zDtmDW0LTJy8rH5eEKpYKhiGVpLKrbhH4fOa4eq1yw8QItdOFisC/u3rYdQiY3IRAIkQAIkQAIkQAKuJECB0JV02TcJkAAJ1BICRWJy88AD6Tq7FTWE52t4YeY8RPIqz02imQlYzVLuJwMCHBMIN23Kw7RpqTh6VMVhNE3K3eX33+dox0UXeeOrr0LRtSu/3k1J2Z9z773pdomDhiPcdVc6IiO9MHJk5buuPns2Q6wIV2PXrgTDKZk9V3ELH3lkO9asiZE4vMMQFlbR75rZbmzOnDixFd5+e7/N9a1VVIKVMykmJgMPPrhV1n1Sp1ipzsym+redPbtz9Z8kZ0gCJEACJEACVUggyM8bwzo31A41jdMiEJZYF24S4TBdBERr6UxSFr7Zelo7vER87NmijsQvrK+Jht1EiFR5TCRAAiRAAiRAAiSgJwEPealr3f+BnqOxLxIggVICZ86cEdeAzbXr6OhoNGvWrLSMJyTgTgS++iobL7yQjoMHdfcraoBBiWNr5LBkydVNypwTCwwG0/30/vsD8PrrSsS0Ly1ZkoNrr01Bjh2h1kJDPbB0aRgGD3at0GPfStyv9tGj+WjfPtGpiXft6oU9e+pWqlVnYmI2BgxYikOHUuye+6BBjcRSdaTEynSdwKweO7t1W4j9+5Psnp9hg8jIEBw7di3Uzn1H0r59iRg1ahXOnMlwpHmta+Pl5SEi6q21bt3OLJjPec7Qc++2vPfuff84exJwFYG8gkL8czpZc0WqRMO9MSl2uTUPD/TBgLbFrkiHtK+PhqH6eFJw1XrZLwmQAAnURAJ8zquJd5Vrct0bILIlARIgARKo0QSU28v7708XayDTGGf6L1y5PlSC+nEzXauvsmKx3UxhtciaOdN+t4obN+baLQ6qxaamFmH8+BRs3BiOLl34Ne/oB+CDD5z/XO/bV4C//soT16+VJ9ZOnbrOIXFQcdqwIVYsgbfi/fcHOIqtwnYqrs6sWZ3EremmCutaqzBzZieHxcFTp9IwbNivZuMzWhuzNpd5e3vW5uVz7SRAAiRAAiTgNAEfL09cEllXOx4c0QGJGbnYoMUujNd+nk+zviMwKTMPy/ac0w41mQ4NQ8SysFgw7NOqLvx9XO8q3mkI7IAESIAESIAESKDaEeCbw2p3SzghEiABEnAPAg8+WFniYAmPVnJiTiBsJPnV9+ts1ChftG1r3/wKCopw002pdlkOllBSP1NSiiS+XCq2bq1rmM1zGwlkZRVh/vxsG2tbr/b++1mVJhDu3ZsoMSqjrU+oglIVi++ZZ3ohIsJ1u9KnT2+PL744im3bLlQwG/PF3bvX1URG86XWc5UF4+TJf1ActI7JpDQ8vPJEbpPBmUECJEACJEACNZBA3SBfXHVxU+1QzyeHYtNKrQu3n0xCrlgcWkuH49Kgjo83nBRxUImP9dC/TT30a11Pi4lId6TW6LGMBEiABEiABEighIB9byxLWvEnCZAACZBArSbw/ffZePNN5y2s7IMYJNVbyHG6XDP7rfPKdeCyywYNPMQay37XoitW5OLkSesvBSqa9LZt+di+PQ99+vhUVJXl5Qhs3pyHpCR9PLCre1lZad68g04PlZtbiM8+O4yHH77I6b4sdaBcmC5dOlzc4C7D4cP2uUJVrkWXLx+BoCDHPtebNsU5LExaWk9tyB8woKG4GC3UBOgvvjgif5/SkZaWh+Bgb7RqFYKpU9uK5XJL0NKwNnwauEYSIAESIAG9CSgPC50ah2rHnUPaICu3AFtOJhQLhkcu4PgF6y7Rs/MKS+uquYVILMRLW9dFXxEL+4lo2KlRqMOeF/ReK/sjARIgARIgARKoXgQoEFav+8HZkAAJkEC1J6B2uL7ySmYVzVPFGlT/QU4wGL96fpVFRHhgxYo6iIy0393P3Ln6iK+qn/nzHRNSDADXutMLF5wTZw2BpaUViSVoEfz8HIuVZ9iXtfOMjDwsWHDMWhWbyz744BAeeqi7S2MnNmgQIG5wx2HSpN+wbl2sTXPr27cBFi0ahoYNA22qb66SHiKquX5rel6dOr5o3fp7REebvqDcvTsRixdHoWnTQMyY0Unc1HZDQED1/Ltc0+8T10cCJEACJFAzCAT4euGyDg20Q63oTFKmuCGN10TAv47FIy073+pC03Ly8dvB89qhKqr4hZeKhaESC5WVYdsGwS59zrM6ORaSAAmQAAmQAAlUKwL833u1uh2cDAmQAAlUfwLr1uVi507r/yl13SqU2HapHDvlKBEVqmoullfZp483vvkm1C7XogkJhfLyvQBRUYVYuVIfq7PvvsvGvHkh8Pd3rThlSOL06QLNPee+fflITi4UocADjRp5ihDkhyuv9HWL3ct5eYYrcv48L8/1AuGhQylIT9dn4idPpiExMQf16rnOzaiiqvr/448xcpzF3LkHRGQ6DRXb1DDJhnqMHdtCcyk6fHgzpz4/yck5+PHHk4bd89wGAgEBXvj00yMV1oyJycSTT/4tGyOisWTJcJe6qa1wMqxAAiRAAiRAAjWIQLPwQFx/SQvtyBfXo7vPJGPdkWLBcI+cl3t8Mlm5il+4cn+sdqjCiGA/TSxU7kiVaNiqXiAFQxNqzCABEiABEiCB2kGAAmHtuM9cJQmQAAk4TeDo0Xy88//s3Qd4VGXWwPGT3ntCDb2E0BRBmg0EFQV0VRTBgl3BtXddOxZ07X5ir7vq2pVFXV0VsaGgKLoUqdIhhEB653vPDTNMSJvJTCZT/vd5rjO55S2/OzLl3Pe8j5XI0097ZnRb8xukb10Hm3WzWfXH/rojWszGVlsuuCDaGCU49SVbgyGfflpugiMlJnVfuQmOeLbZpWYave3bq6VzZ9dHMbraEg0cP/xwsUkdWX8/nn22VHr0CJPp02OsNTbWe0FLV/uSkuK5toWbl2tcnOfKa6gveXllDe1q1nYtr6UDhNqw0NAQGTu2o7Vu3FgkX3+9VXJza+Z/TE2NEk1t2aWL62l66+v0ypX5oilUWVwTKCmpcumE77/fLkcf/bHMnz/BpCBlBLNLeByMAAIIIIBAEwLhYaEyuEuqtV51VG/ZVVwu367Kle/X7JDvV+c2mY5Ui99RWCZzft1srfp3u8Roa2ThcBMs1KBhp9TmZ2vQ8lgQQAABBBBAwH8ECBD6z7WipQgggECrCbz9dqmccUa+SZXYak3Yr2INuHTcu+ocZvoDdssHwUwlTS4aCBs0KNwEwRr/Yv3jjxVy5pn58scfrv343mQD9jugsLD2iKz9drv9p6acnTmzWG69telA7erVVXLNNYXyz3+Wyr//nSQdOvjGNdsfQa9fmGlalQcujY4m1XllWnqJiAj1aBWRJrWVt5fMzDiZMqVHi1WrIwhZvCOweHGuXHjhN2Yk9WjvVEgtCCCAAAIIBKlAcmykjB/Y3lqVYFt+qSxYkyvfWUHDXFm/s+mpIbaac95dvMlatYxOqTFWoFBHF47oni7tklo2q4TWyYIAAggggAACrSNAgLB13KkVAQQQ8BsBDeZoIMvEgXx0SfK5dl12WaEcd1yUGflUf5Dlk0/K5KSTdkuJFwZjJiS0bHDqlluK5O67m/7hwfEiLV5cKUccsUu++y5FMjI8G9hyrKe5zzVwOXFipJnvzv1UrxddFNPcZrh0XkaGZ3+4SUuLcql+fziYefFcu0oa13bn3/033lht/m0YYuZh9cwIUNdaz9EIIIAAAggEp0BbMxrwhAM7WqsKbNpVYo0s/G71DllgRhhu3l2TqaExnQ07S2TDzo3y5qKN1mHd0+NERxfq/IXDzQhDTVHKggACCCCAAAKBIUCAMDCuI71AAAEEWkRgwYIKOeccXw4Otki33S600kyL+MwzJebH8fg6ZS1cWOG14GC0iRm1adNyATid49DV4KANZNWqKsth/vxkr4yws9Xr7OOMGbFuBwhTU0Pk1FM9E7hbuXK3GXW53qSMLTVpMqskJSXKjFRNk3HjMs1ox1Dp0ydZundPkDVrCpztYoPHjRrV3qRFDbzUkO3aeSdY2yCsH+3o2TNRVq3Kd6vFGlx8+ullct99Q90qh5MRQAABBBBAoPkCHZNjZNLgTGvVzB86ovA7EyjUdKTfm5GGOQVNZ1hYs6NIdH3th/VWQ3q3jTfBwnQrWDi8e6roKEYWBBBAAAEEEPBPAQKE/nndaDUCCCDgFYE77iiSigqvVBVwlTz3XIlJuxknUVH7RvDpl/Kzz873yshBBZ0yJbpW/Z5E1r7cfnvTaUUbq/Obbyrkiy8qZMwY3/tRYcyYCBk8OFx++slEe5u5XH55rMTE7Lv+rhajc1RqUPDJJ5fJf/5Tcwf3/mV07hwvF13URy64IEsuvjhbrrvux/0PcfnvSy7p6/I5/nBCjx6JMnBgqixZstMfmtuqbVy92r3goK3xzz23Qu66a4h4OgWurXweEUAAAQQQQMB5AU173yUtzlqnDO1sMgXsMXMWFtqDhRo0zCtu+svfH9sKRdeXvltnbvQTyW6XaI0u1JSkQ7ulSkJ04N1o5rwyRyKAAAIIIOBfAgQI/et60VoEEEDAawKrV1eaoIT7KRa91mAfq2j79j2W3/HH70vB8+WXFbJ0qQcmtnOyrzNmtNyIKe3LihXu9+XJJ0t8MkAYGhoi772XJMOH58nmzdVOiu877C9/iZSbb258Hsp9R9d9lp9fLqec8rl8+ummujsdtqxfX2jqWSQPPLBEXnzxcBMQDjNzhTb/urRvHysnnNDFoYbAeao/is2YkW0Cqd8GTqdaqCfm90KPLLm5ZWZUa75kZSV7pDwKQQABBBBAAAHPCehno55tEqz1zBFdRW9OW7GtYG9K0lz5YW2uFJQ2frOcfmZYuiXfWp/7Zq2Yj9AyIDPZPofhwV1TJDaSnx49d9UoCQEEEEAAAc8K8C7tWU9KQwABBAJG4OmnS92afypgINzoyCuvlFhz2emXb100GOatZdiwcBkypOXu3vVUXz74oEw2baqSjh3rn6/RW1711dOpU5jMm5csxx67W1avdj7oduqpUfLyy4km9WfNda+v7Ma2aXBw1Ki5snhxbmOH1dq3a1e5SWf6hUye3F3+8Y9Vtfa58sesWQcH9Giv00/vaY2yzM9v+u54V9w4tmGBvDxuNGlYhz0IIIAAAgj4joDeIJfdPtFazz20m1SZgOHSzfkmJekOKx3pwrU7pcikum9sMafIrxt2WetTX62WCPN5+AATMLTmLzQjDA/qnCLREb73ub+xPrEPAQQQQACBQBYgQBjIV5e+IYAAAm4I/Pe//KjrBp916jvvlMukSbvlX/9KkuLiPWZOu6bn+HC3Tj0/xgwcfOGFRE8U1WAZ8+d75vVRZX5j+O67CjNazjd/KOjVK1x++CFFZs4sMiP0SmX37oaHVmVlhcnll8eYlJ8xoj+wNGfRO7c10OdKcNBWT0VFtRn1uM4aAfjBB3/aNjv9ePvtB8mZZ/Zy+nh/PDA+PkIefni4nHfe1/7YfL9sM+lF/fKy0WgEEEAAAQQkzHyeHZCZZK0XHdFDKqqqZcnG3bLAzF2o6UgXrtspZZWNZ9qoqNoji/7Ms9bHvlglkeG5IDvZAABAAElEQVShJkioAcN00ZSkGjzUbSwIIIAAAggg0DoCBAhbx51aEUAAAZ8XyM1t/Muez3fARxr47rvlcvzxu+XBB+NEg2HeWI49NlL69m25t3idr2TnzoYDZa720ZNluVq3M8enpYWaoFKCCRLGy+uvl5qAb6ls2VJtBX2TkkKlT58wOf/8GDnyyAgzD0vzAoO2dnz22aYG5xu0HdPYY1FRTRqoa68dYNKO/tbYofZ9Gsx86KFhJrjZ374tkJ+ce26WbNxYJLfd9nMgd9Nn+paREe0zbaEhCCCAAAIIINB8gYiwUBncJcVaLxnd0wQHq+SX9bus0YXfmYChPi83QcTGlnITUFywZqe1ymfmxkYzmnCISUOqwcIR3dNkQMckCTf1sCCAAAIIIICAdwRa7tdD77SfWhBAAAEEWkiguvHvdi1Ua2AW+/HH5ZKe7l7gyBWZNWta/uK5GQdzpTs+c2xcXIgVCNRgYEstTz651O2i58xZL+vWTTYjEbubtLbLTFBztZSW1o1OJyZGyNln95bp07NNkDO45oi79daDpG3bGLnssu+lvLzl/39x+6L6aQH9+6dIp05xftp6mr2/wPr16+Wxxx6TuXPnij6PioqSnj17mn9rTjXze86Q2Njmz7vqWNcnn3wizzzzjPz444+Sk5MjGRkZMnToULnwwgtl3Lhxjoc6/XzJkiUyePBgqaysuYli2rRp8tJLLzl9PgcigAACCNQViAoPk2EmqKfrFWNFSkz60Z/X59WkJDUBQx1tWKk5RxtZSiqq5OuVO6xVD4uPCpeh3VLtcxj2NSlPm5uZo5Fq2YUAAggggAACewVCzCiAxt+toUIAgRYR2Lhxo/nRrJNV9oYNGyQzM7NF6qFQBJorcMABO2XJkpof0ppbBuftEwg1N8J6M+g6blyETJ0abVJ3Rkt0tOeDk23b5sj27Z75CPHWW4kmFSujjNavL5Ru3f5lXifuu95884FmxOMQ6wW4c2epvPvuOtmwoUgKCyskMTFSevRIkBNP7CpxcS03T+W+V7/vPtu6tViee26FPP30cmtUoe+21D9b9uSTI00Auq9/Nt7NVgfa5zwNCp5++ukmzfLuemWysrLko48+ku7du9e735mN+rX04osvtoKDDR2vQcKnnnrKpdHa1ebNd+TIkSZd9A/2YlsyQBho196OxhMEEEDARYHCskorDekCEyz83qQl/X3TbnH1Y25STIQMMwFDncNwhElL2rttvEvvAS42mcMRQACBRgX4nNcoDzv9VIARhH564Wg2Aggg0NICI0eGEyD0ILIGB3XUnbduy/nkkwrR9corC818azFy3XWxoqkyPbWMHh1pUm26P6diuPkkcsghwR2ksl2TTz7Z6JHgoJb30Ucb7AHC1NRoM/Kxj60aHh0E2rWLlb/9bZDccMMB8tVXW+S++36V//53s8MRPG2ugM73eMYZPZt7Ouf5kMCvv/5qjRIsLi6W+Ph4ufHGG2X06NFSUlIib7zxhjz77LOyYsUKGT9+vCxcuNA6pjnN/9vf/mYPDg4aNMi8b11nbmboIatXr5b777/fzM262NqvIwpnzpzpdBVPPPGEFRxs06aNubFlu9PncSACCCCAgHsCOhpwdFYba9WSdpdUyI9rd1rzF2rAcNmW/CYr0HM+XbrNWvXgtLhIGW5GLFopSU3QsHt6HAHDJhU5AAEEEEAAgYYFCBA2bMMeBBBAIKgFsrNb7i1Cs5CZ3xlZvCCQm7vH/LBaLG+/XWqCRsmSleWZ6zpjRoxHAoQnnRQl7duHeUHC96vIySnxWCNzcko9VlYwFBQeHmoCB6UEBz14se+44yBJSIj0YIkU1VoCV1xxhXnPLpZwc0fHp59+KiNGjLA35cgjj5RevXpZwbzly5eb+UwfkltvvdW+39knq1atsoKAevyQIUNk/vz5EhNTk8754IMPNnP5Hi9HHHGELFq0SGbNmiXnnHOOFTxsqny9y1sDjzo/7AMPPCA6cpAFAQQQQKB1BHQ04FF921qrtmBnUbn8YAKFGizUOQxXbS9ssmG55py5v22xVj24bWKUlY50aLc06dshUbLaJkhMJN8tmoTkAAQQQAABBPYKeOZXQjgRQAABBAJOoKzM/TSHNpTMzFAzF1qC+XFRzFx8oWaUTpFJeVhu2x00j94aPVgfqM5LeMQRu2TBghTp2tX9L82HHRYh/fqFyf/+V3duu/rqb2ibBhpZagQqKjw3F54ny/K366MpWj/9dKPMm7dFcnNrRrmmpUWZ1397OfrojhIWVnckraY2vP/+Jf7WVZ9t74wZ2Wb0cn+fbR8Nc15ARwTOmzfPOuG8886rFRy0lXL11VfLiy++KMuWLZNHHnnEGmEYEeHayPCHH37YPj/g448/bg8O2urQ+Q11uwYndR5BrUf/bmq55JJLpKCgwAooHn744U0dzn4EEEAAAS8KpJrRgMcOaG+tWu32glJZsEZHGO6wRhmuy236jtJt+WXy/i+brVXLCDUZW7qaUYXZZu5Cnb8wu32C9bxdYjQjDRWIBQEEEEAAgf0ECBDuB8KfCCCAAAI1Avn5ngsQVlSITJwYZac1v+2xtILAtm3V5jrsMmnaUk2w1r15CXU0xl13xctJJ9U/H5Uz3Rs7NkIOP9y1H5GdKddfj0lJ2ff/iLt9SEkJvpFbu3aVWfMJzp69TNasKahDOGvWEjPHY4KZ46yPXHBBH3H0/uGHHPnll9w657DBdYFbbhkkOnpQ/41g8X+B999/394JHbVX3xJqJtk966yzrMBgXl6eFVA86qij6ju03m0aoP/ggw+sfX369JHhw4fXe5xu17kONZ2ptuuxxx5r9HX29ttvy4cffmjSa6dZoxMLC5semVJvxWxEAAEEEPCKQJuEaDn+gA7WqhVu3lViT0f6vRlhuMn83dSicxyuySmy1rlLttgPT46NkOx2idJnb8BQg4c928RLdIT7N07aK+EJAggggAACfihAgNAPLxpNRgABBLwhEBfnuR934+Jqtzg1te4IntpH8FdLCfz+e5X5wbTMBPai3a7ixBOj5O674+Tmm4tcLqtv3zB5882kRn/cdblQPz/h4IMzPNYDT5blsUa1YEG//bbTzH/2H9mwofHX4tq1BXL99QtNYOF/MnfuMXLAAWlWq55+elkLti7wi46JCZPTT+8pOnJw0KD0wO9wEPXw66+/tnobZ97IBw8e3GDPNf2nbfnmm2/ElQDh2rVrZdOmTdbpjuXYynN81P0aINTUoevWrTNB/26Ou+3Pd+/eLZdddpn1t85fmJ6eLgQI7Tw8QQABBPxCoENyjJw8ONNa9WaSDTtNwHBNzehCTUm6vcD5+dB3FVdYqUw1naltCTPDDXtk1Iw27GOChzraUAOHGQlRfEexIfGIAAIIIBDwAgQIA/4S00EEEECgeQKZmZ67m7Jjx9pljR0bKS+9xBxpzbsy7p/15JMlHgkQaktuuinOpIILkauvLhRnU6iOGBFugpTJZgQXgWLHqzliRBsZMCBFfvstz3Fzs55ffHF2s87zx5OWLMk1I1Hnyu7dzqct3rSpWA477N9mnrMJcuCBaWZU7b4fi/zRoLXaPGpUOyswePLJ3WqNyGyt9lCv5wU0baguPXv2tOYgbKgGHflnW2zn2P5u6tHxeMdy6jvPcb+e11CA8Prrr5ctW7bIoYceaqUXra8stiGAAAII+I+AZibonBZr1s4y+eDO5nvHHlmzo8gaYbjABP2Wbs6XtblFTn8f0Z5XmeGGf2wrtNYPZLMdI82kPtUUpbb0pPq8R0a8RJo5q1kQQAABBBAINAEChIF2RekPAggg4CGBiRMjRUcRFhW5n2p06tTao9WmTo2S888XKSVG6KGr5Voxn39eIX/8USm9e3vmY8CVV8bKIYdEyKOPFsvbb5dJeQNxmoEDw2X69BjzY220REV5boSqa7333aP1h48ZM/oao2/dauQBB6SaebrauFWGv5y8Y0epHHfcf1wKDtr6VlBQYZ37yy8nyq5dDbxobQfzWK9Av36p5t/yfYGheg9io98KlJo36R07dljtz8zMbLQfKSkp5jNDnPnMUGRG8m5o9Nj9dzoe31Q9nTp1sp/ueJ59o3ny7bffyjPPPCM6D+JTTz3l8VEgOnqxsUUDkywIIIAAAi0roJ+bNWin6xnDu1iVFZdXyoqtBbJsi6751rrc/F1Y5tr8FrlF5fLNqh3WautFRFhNfTXzGmrwsCaAmBbvuSkCbHXxiAACCCCAgDcFPPPLoDdbTF0IIIAAAl4RSEoKNSNDosyPbO5F8RISQqxyHButX+gmTYqSf/zD+bQwjufz3H2BBQsqPBYg1NYMHRoh//xnkjz8cLW88kqp/P57pQm6VFujC9u2DZVTTomSkSMjPP5DrfsSvlXCGWf0lJkzF5t0e8XNbtj11x8QNM5PPPE/t6y2bCm20o1GRdUe5dxs/CA7sbjYtR/cgozH77tbULBvLs/4+Pgm+2MLELqaytOVerQO21JfPeXmDpULL7zQGlly1VVXSb9+/WyHe+zRMUjpsUIpCAEEEEDAbYHYyHAZ1DnFWm2F6UjDjXklsnRvwLAmcFgg63e69lm7omqPaLBRV1lckxZb62hj0pH22RsstAUPu6fHSXgYow1t14BHBBBAAAHfFiBA6NvXh9YhgAACrSowY0aM2wHCadOiJSGh7hekhx6Kl9deK5Pq6lbtYtBWnpfn/sjQ+vDatAmVa66JrW8X25wQiI+PkDlzjrZSZhYWVjhxRu1Drrqqv0yZ0qP2xgD9q6KiWh5/fKnbvXv22eUmfWKiGVXrdlFBV0BSUmTQ9TmYOqwjCG1LZGTT1zoqqmYURUlJie00px5dqcdWhxZcXz333XefLF26VLp06SK33nqrU/VzEAIIIIBA4ArojamdUmOt9Zh+7ewd1VGFK7bmm8DhvtGGOvqwuLzKfowzT3QexO0FOTL/jxz74ZqKtHfbeMm25jVMNAHEmrkNk2Obfi+1F8ITBBBAAAEEvCRAgNBL0FSDAAII+KPAAQdEyPXXx8qsWa7dYWnra8+eYXLbbfvu9rdt18eMjDAzcide/vrXQsfNPPeSQDifALwk7Xo1gwaly2efHSsTJ35q0vvt+4G+qZKuvXaA3Hff0KYOC4j9ejf41KlfyM6d7o9C3r69VI46qqN89932gLDxZieyspK8WR11eVkgOnpfenAdmdfUUlZW8/9jTExMU4fW2u9KPbY6tID961mxYoXcc889VtlPPPGExMa2zM0qDaU2tXVKU4wOHRoc/xbb+swjAggg4G8C8VHhMrhLqrXa2l5t5iT804ws1FGGy81qCx5u2uXajS/lldXy+6Z8a7WVrY/tk6LtqUlrUpQmSte0OAkLZeoFRyeeI4AAAgh4V4CfB73rTW0IIICA3wncc0+cbNtWLS+95HygQjuZmRkqH3+cJOnpdUcP2hAuuSTWBECq5fbbmxeAtJUTyI+aonXcuEh5770yqfRgNr+0tIavSyB7+kvfhg9vIwsXniB33rlYXn99tZmvs+G7mYcNyzCjNgeatL3d/KV7brfzllt+MvNdrnO7HFsBJt5o5sUMk7Kyhp1tx/JYIxAbGy6nndYdjgAWSEhIsPeuvnSe9p17n+j8g7o4k4507ynWgyv12OrYvx69aeCiiy4y/w+XyYknnigTJkxwrMKjz5uaJ9GjlVEYAggggIDXBEJNoK6bSQ+q63ED2tvr3V1SYQUMbelJl5uRh5pqtMwEAl1ZtuwuFV2/WL7vprToiFDJapuwN3BYM7ehjjhMjI5wpWiORQABBBBAoNkCBAibTceJCCCAQHAI6BelF15IkK5dw+Tuu4ukwomshyNGhMtbbyVJx45Nz+t1223x0qtXmFxwgUnpQpywzouqvHyPvPlmkjWf3/nnF8g777g/YirCfN8cNYovnXWwfWxD164J5v+9w+Xvfx9qAvQr5cMP/xQd7VZeXi3JyZFy0EFp5gfxbBk8ON3HWt6yzXn77bXm36JfPFpJQUGFFex6+eWVHi03kAs7/fQe5nVYk1IykPsZzH3TkX3p6enmRp4dsnHjxkYp8vLyxBa8c3WOPseAW1P1OI7ec6xnwYIF8tVXX1ltHDlypLzxxht12puTsy/929q1a+3H9O/fX3RlQQABBBBAoD6BpJgIGdY9zVpt+6vMaMO1O4qs0YY1gcN887xAtua7dlNtqUmZ/+vG3dZqK1sfM1NiaoKG7fYFDzubVKn63ZwFAQQQQAABTwoQIPSkJmUhgAACASqgczdoqtCLL46R558vkaeeKpENG2rfManTE02aFCU6b+HIkRGi5zi7TJ0aIyefHC3/+lepzJxZLCtXMorHZpeUVOOYnBwqf/tbrEcChCefHCXt2jUdvLW1gcfWFUhNjZarrhpgra3bktavXUcJeTo4qL2qqtpj/o07SP797/WSm+t+EL71pVq2BfrP+6WX9mvZSijdJwSys7Pl66+/llWrVplR7JUS3kB+6uXLl9vbq+e4svTt29d+uGM59o0OTxz3O9bjmHr02muvdTij/qfz588XXXW57bbbCBDWz8RWBBBAAIEGBDQtaM828dY68YAO9qPyisplmRlhqMFCW+Bw5bZCKa+q/d3ZfkIDTzbmlYiuny3dZj8iLjJMshwChtlmpGGWmedQ06WyIIAAAggg0FwB3kWaK8d5CCCAQBAKtG0bKjfdFCfXXRcrv/5aaUYzVZvRTCIpKSHSr1+4uJO2MioqRM46K8Za162rksWLK+SUU/LND/dBCO3Q5aysfW/VBx4YITo68/vv3cs1qkFcFgT8UeCHH3Lkl19yPd701NQo6dYtQT744CgZM+Yjk6bQtR9xPN4gHy9QR7UOGJDq462keZ4QOPTQQ60AoY4O/Omnn2TYsGH1Fmsbvac7DznkkHqPaWhjt27dpEOHDrJ582b7KMCGjrUF9Tp27GgyG3Rt6DC2I4AAAggg0CoCKXGRMrJHurXaGlBhgoNrcopMWlKd13Bf8DCnwLWb0orKq+Tn9bus1Va2PnZJi5VsEyjsbYKHmq5Ug4hdzbbwMKaUcHTiOQIIIIBA/QL7fnWsfz9bEUAAAQQQqCMQHh5i0hq2XIpKTWeq6wMPVJtRU4V16g+mDeefH12ru3ffHS9HHbWr2YHTIUPCrSBjrUL5wysChYUV8tprq+Wf/1wl69cXmnR8lZKQECE9eiTKOef0NqNou0qkuTOYpWGB2bOXNbzTjT0jRrSRtWsL5K9//Z7gYBOOhxzSxvy7PLCJo9gdKAJ/+ctf5N5777W68+KLL9YbIKyurpZXXnnFOiY5OVlGjx7tUvc148AJJ5wgs2fPFh0hqOlChw8fXqcM3W4bQajHO2YqGDVqlOgI48aWdevWmRsBulmHTJs2zaRufqmxw9mHAAIIIICARwQiTKBOg3a6nnBgR3uZOwrL7KMMl5sRhxo8XLW9UCpN+lJXlj9zi0XXT/631X5apKmzhxnhmNU2vlbgsGNyTK33T/sJPEEAAQQQCFoBAoRBe+npOAIIIOD7AldcESNbt1bL/fcH5+SEmsnN/G5q/ehp+yF09OhIefrpBNH5CJuzLFpUaQJSO83cdTHW6s6oz+bUH4zn5OWVyR13/CwvvviH5OfXnsQzJ6dU1qwpkM8+2yRt2kSbNL7ZcuONB0h0NB/R6nutLFiwvb7Nbm2LiwuXIUPSTeD8Q9m2rcStsgL9ZP336IUXjgj0btI/B4GhQ4fKYYcdZo0ifP7550UDayNGjHA4QuTBBx+UZctqgveXX365ROhEtw7LvHnz7EHDhgJzV1xxhTz77LNWGtNLL73USv8ZE7NvtHtJSYlJa3upVaqmOdXjWRBAAAEEEPBngfT4KDmsV4a12vpRXlltBQlt6Ult6Up3mtSlriya0tRWhuN5mo60lwka6kjD3ntHG+pjRgLzSjs68RwBBBAIJgF+fQqmq01fEUAAAT8T0KDYrFnxkpkZatKaFkqpa3O++1lv6zbXTPdkUq4WyMcfl5vgUqJoGlZdzjsvRmJjQ+Tss/OtFK91z2x8y/r11XLzzUXy+OMlMndukhx0UO0fcxs/m72uCKxbVyDHHvuJGfWyu8nTtm8vlTvvXCyff77ZSnWZllZ79GiTBQTBARps9fSiIzdPOeULgoNOwN5//1Dp3TvJiSM5JJAEHn30USttqAbpjj76aJNq/CYr4Kd/v/HGG/LMM89Y3e3du7dcffXVzeq6nnvNNdfIfffdJ4sWLbLqu/76680NLT1k9erV5rPALJN6fLFVts4x2KtXr2bVw0kIIIAAAgj4skBkeKj07ZBorbZ26gh5TUfqmJ5Ug39rdhRJlYujDQvLKmWxSVOqq+OSZlKjOgYMs9rFm0BigiRG8z3R0YnnCCCAQCAKECAMxKtKnxBAAIEAE7j00liZOjXapAMrNSnISsyPhbUnJtRRLccdFynTp0eb9I2l8vrrrt1h6etcr79eJoWFu+Xdd5NE07vqMmVKtBx8cLg8+WSJCR6Wyq5drqWi0TJ0dObhh++SL79MNmXx5U9NPLls21YsRx75kZW60pVyv/12mwkq/sdcl+MkLo7r4mgXFlbz+nfc5s7zmJgwc+NBtZXy1Z1yguHcG244wAR/BgRDV+njfgKDBg2Sf/3rX3LGGWeYUdD5VoBwv0NM4Li3ueFkrkmbnLD/Lqf/vvvuu83cxtvNKNUXrGDgaaedVufc8847T2bOnFlnOxsQQAABBBAIVAG9abZNYrS1jspqY+9maUWVNdpQA4crthbIH9sKrMftLs5tqAXmmhGK36/JtVZ7BeZJh6TomhSle+c31CBiT5O6NDqCaREcnXiOAAII+LMAAUJ/vnq0HQEEEAgiAU2FefXVsXLllTHyyy+VVnCrpGSPJCebOR2ywswow5ovKePHR5v0m+UmLWmRfPpphUnPGRhIc+aUmx9li0y/4u0d6tkzXB56KMGYxMrAgTubFSQsKtojEybsMqap0r49X/TsuB54cvrp81wODtqqXbgwRy677Ht5/vnDbZt4NALp6dHm/33PpAHVGwuee+4wueCCb7BtRCAiIlQee2y4SX/bt5Gj2BXoAhMnTpQlS5aIjibUQODGjRvNnKmR0rNnTzMC9xQzf+dfzcj2WLcYQkNDzb95z5v5WE+2RiUuXLhQduzYYf6/Tzc3sRxs0mJfZG6eONatOjgZAQQQQACBQBHQIF3/jknW6tinPBPs02ChrssdAof5pSY9jYvL5t2louu8FTn2M0PNZ+iuaXH2EYc6t6IGDrumxUq4mfuQBQEEEEDAvwRCzFD1APnp1L/gaS0C+sNKp06dLIgNGzaY4EYmKAgg4GGBP/+skjFjdtUZcejharxWXJSZGmLTpnTZf97AO+8skttuK3KrHdddF2ulc3WrEE62C/z88w4ZPPh9+9/NeaKBmfXrT5N27dz70b05dfvqOddc84OZ7+w3jzTvqacOkaKiSnPjwQ8eKS8QC8nKSpLvvpsoqamku3X1+vI5z1WxwDmeax8415KeIIAAAoEioD/9bssvkxUaODRBQ33UUYcrtxdIaUW1R7oZaYKDPczowiwzx2FvEzTsszdw2DE5RnQUJAsCgSDA57xAuIr0YX8BRhDuL8LfCCCAAAIBI9ClS5gJnMVa8/gFQqfKzPRrL7xQItdeG2fvTkXFHnn6afdHVGm5d9wRJ9HRfHmz47rxZPbsZW6cXXNqhfmy/vzzK8x8kYPcLitQCrj44j4eCRAefnhbM5dnlrRp849AofF4PyZP7mbSOh9h/k3g64LHcSkQAQQQQAABBBDwooAG6NqZdKG6HtE7w16zzmG4YWdxncBhc+Y3LK+qFp0bUVfHJT4q3MxnqIHDmpGGOuJQ1/R4c/crCwIIIIBAqwvwjb/VLwENQAABBBBoSYFTTomWq64qNGnKAmPAvM7BqKlWQzW3i1nmzCmTzZvdv+tTfd5+u8zMMcVIIXdfjwUF5WYuzFXuFmOd//TTy01q2QO563avZs+eSXLMMZnyn/9sdMv3hhsOlOuv/1Hy8gJrvlK3UMzJOmr15JO7mvlcs+Www9rxunMXlPMRQAABBBBAAAEfFggz3ym7psdZ6zH92tlbWlZZJWt3FDnMbVhogoj5Jpjo+o2phWWVsnj9Lmu1V2CepMVF2tOUaorSrHbxJpCYIInRzMHu6MRzBBBAoKUFCBC2tDDlI4AAAgi0qoCOiLvgghi5997iVm2Hpypfu7bapBmtNimKa+YL/OyzCk8VbYIuBAg9gblixW4pKanyRFGyYUOR5OaWWXPveaTAACjkzjsPki+/3Czl5c0LjI8e3V4OOCBVJk78TwBoeKYLp53WTY4+upOZ3y2TlLaeIaUUBBBAAAEEEEDAbwWiwsNMitBEa3XsRJEJ9q3cXmhPU2qb5zCnwKS6cXHJNXMlfr8m11odT9WUpL33pim1jTrsaVKX6pyLLAgggAACnhcgQOh5U0pEAAEEEPAxgRtvjJWPPy6XX35xfWJ2H+uK1ZydO/cFCHfsaF6QpL5+5eYGxijL+vrmzW15ea5/QW6sfVpeejojO21GQ4e2kVdfHSVTpnwp1SYtkitLv34p8s47Y+XRR3+XKs/EcF2p3mePjY+PlHPO6e2z7aNhCCCAAAIIIIAAAq0vEGfShR7YKdlaHVuz0wT7NFioq85taAscFpS6/v17064S0fXLFTn2KjR5Tte0OCs1ac1ow5p0pV3TYiXczH3IggACCCDQfAEChM2340wEEEAAAT8RSEgIlblzk2TMmF2yfHlgRQWqPRcfNMEWP7mgPt7MyEjP3t0aFeXZ8nycz6nmnXpqdzM3XpgVJCwudu6HB02Z+f77YyUhIUIeeOA3p+oJloM0Je4DDwyV5GTmggmWa04/EUAAAQQQQAABTwmkmnShw7unWautzD179sjW/FJ7wHDFVjPycG8QsazStS+eek+gzouo68e/b7VVIZEmONjDjC7UEYfd082aESfdTMpUfYyN5CdvOxRPEEAAgUYE+NeyERx2IYAAAggEjkCHDmHy3XcpMm1avpm3r/nzjvXtGyZbt1bLzp2ujVzypGRq6r67JFNTa+Yi9ET5KSmeK8sT7fHXMtq08dxovxBzSdLSCNrU91o4/vgusmzZJJk9e5k899wKM89oaX2HyaGHtrXm1NOgYnh4qAkSrhNng4r1FhiAGzUl7rvvrpNzz80KwN7RJQQQQAABBBBAAAFvC4SYLzLtk2KsdVRWG3v1VSbat35n8b7AoQYNzahDDf7pPleW8qpqWbYl31r3P69dYrRDwLAmeNjdBA8zU2JF515kQQABBBCoESBAyCsBAQQQQCBoBFJSQuXDD5Nl6dJKE1QokZdfLpWCgtpfQtLTQ6yRdI4BwOTkEJk6NdoEGWKkf/9wGTdul5mvr/lBRnfAe/YMk8zMfQHCESMiTHCk/sCIq/UccggTwrtqVt/xWVnJ0rt3kvzxx+76dru0bezYjhIXx3VpCK1z53gzv+jBcvvtB8l7762TxYtzZdeucomKCpWMjBg54YTOMnBgWq3TX3zxj1p/80eNgM53yYIAAggggAACCCCAQEsKaHBOR/npOq5/O3tVZZVVsianqFaa0hUmeLhhZ4n9GFee6OhFXb9bnVvrNB112NmkJtVgYTcz0rCHGXmoj/q3joTUwCYLAgggEEwCBAiD6WrTVwQQQAABS6Bv33B5/PEEuf/+eFm5skry8qolMjLEBBRCpEePMOtLQUXFHsnP3yOxsSESE1P7S0J8fO2/vcmqQUrHLy2nnRYtV19daIIitQOdrrYpNlbkzDM9N/LN1foD6fhQ86V3+vRsufLKBW53a8aMbLfLCIYCNA3raaf1sNbG+rt7d7l8+ummxg4J2n2FhRVB23c6jgACCCCAAAIIINC6AlHhYZLdPtFaHVtSVFZpT01qS1OqgcOcgubN+66jDldtL7RWx3r0eWJ0uBl1qOlKa9KUdtubtlTnP4zx8DQS+9fN3wgggEBrCRAgbC156kUAAQQQaHUBDfwNHFj/W2FERIhJ7Vh/ILBDh30j+LzZiWgTvzv77NpBPA1gnnNOtDz8cPPurLS1/4wzoiUpqXX6ZWtDID1Om9ZLbrppoWjqxuYumZlxMmFC5+aeznn1CMyfv0VKS5t/TeopMmA26dyMLAgggAACCCCAAAII+JJAXFS4DOqcYq2O7dpZVG5PU7o6p1DW6hyFZgTipl3N/16cX1opv2zYZa2Odenzjskx9vkNa+Y5rAkkdjDbSVm6vxZ/I4CAPwnU/6uoP/WAtiKAAAIIIOBlgcmTo8wIxOZ/8Whuc6+8MlYc5x+0lXPZZbHy7LOlUljYvFGEMTFiRruZIYQsHhNISYmSu+4aLNdc82Ozy3zooWHWnHnNLoAT6wjk5HgmHW+dggNgQ48eiQHQC7qAAAIIIIAAAgggEAwCmg50RI80a3Xsb0l5lazLLdobMCy05jbUwOEaE0TUAGBzFw086vrNqh21iog0c5x3MyMMa4KGtsea4GGKaSMLAggg4OsCBAh9/QrRPgQQQAABnxMYOTLCGnm4ZEnzv2C42qmTTtKAU1y9p3XtGiZvv51oRpvtlkoXmxRqBg2+/nqS9OnDR4J6cd3YeNVVA2TdukJ54omlLpfywAND5ZRTurt8Hic0LlBZ2bwgeuOl+v/epKRI+ctfuvh/R+gBAggggAACCCCAQFALaCrQ+lKV7tmzR3TUoW2k4RprxGHNyMM/c4tFU482ZymvrBZNearr/ktKbMTewKGZ59CkLe1h5jrUtKVdzByI0RFh+x/O3wgggECrCPBrYKuwUykCCCCAgD8L6ByAl1wSIxddVPdLQEv0a9q0aHnmmQQJC6s/5anWecwxUTJnTpIJKuU7PZJQRw6+9lqSnHBCVEs0O+jL1NfJY4+NMHNbRsvtt/8s5jtpk0tERKg8+eRIOf/8Pk0eywGuC6SkcBdvfWpnn91L4uJIMVqfDdsQQAABBBBAAAEE/F9Av5ulxUdZ65CuqbU6VGmCg5t3lcrqHWbEoRltuNb+WCRbdjc/A0lecYXkrd8lP5vVcTFNsVKWOs532N0EDruZAGL7xGjROe1ZEEAAAW8JECD0ljT1IIAAAggElIDOBfjaa6Xy1VcVzepXv35hJnAUIvPmNTzkb8yYCPnrX2NNAC9S9AtNU8u4cVGyaFGK3H9/sdW20ga+y0SZeODkydFy3XWx0q8fHwWacnVnv163W289yARuu8ns2cvk5ZdXSn5+3deMBhEvuCBLLrywj3TpkuBOlZzbiMCQIRmN7A3OXfoDxPTp2cHZeXqNAAIIIIAAAgggEPQC4WGh0tmM6tN1dFZtjuLySvuow5rRh/vmOywoa/i7fO1Sav+lN45uzCux1vl/5NTaGW1uGO1qUpb2yKgZddjdGnUYJxpMTIrhhr5aWPyBAAIeEQgxQ6yduJ/dI3VRCAIIOAhs3LhROnXqZG3ZsGGDZGZmOuzlKQII+INAXl61jBmzSxYvdu2LwaBB4fL558mSkhIqy5ZVyquvlppUlFXWyL+EhBDRlKFnnhldK+1nVdUeKxi5YkWl7Nq1x4z2CZGOHUNFg4L6fP9l585qE4wqlS+/LJfc3Jq3+tTUEDn88Ag555wYSU83uUVZvC5QWFhhRnqulw0bCs31rpTExAjRud+OO66TREWRZsYbF2TcuE/kP//Z6I2q/KKORx4ZLpdf3t8v2upPjeRznj9dLc+2lWvvWU9KQwABBBBAwBcF9Of0HYXl1tyGVuDQSllq5jo0ow/Xm5SlldWe/7k9zcxp6BgwtKUt7ZQaK1HhfJf0xuuEz3neUKYObwsQIPS2OPUhsFeANxVeCggEhkBBQbUJ5uXLBx+UO9UhHQ346quJkpDgXIBux45qeeGFEnnqqRJZu7buvAiJiSGioxmnT4+pFVB0qjEchEAQCnz44Z9mVO5nQdjzul2+7bZBJv3t4Lo72OK2AJ/z3Cb02wK49n576Wg4AggggAACHhHQlKUbzAhBW6pSx/kOt+WXeaQOx0LCTUaQnm3irbkX+7ZPlL4dEq3nqSagyOJZAT7nedaT0nxDgLxivnEdaAUCCCCAgJ8KaKDv/feT5eefK8zccSVWas+Sktqd0bn+Tj+9Joh30EHOpwX58MMyc17jcwrm5+8x89yVWOtNN8XKzJlxTqUjrd1C/kIgeATGj+8kPXsmyqpV+cHT6f162rZtjElFPFTOOqvXfnv4EwEEEEAAAQQQQAABBNwR0JSlOrpP1yP3m1q+0KQlXWdGG67O2Zeq1Ja6tKi8qlnV6mjF5VsLrPW9xZvsZbRPiq4VNNTgYWcz2pA5Du1EPEEAASNAgJCXAQIIIIAAAh4Q0MDfc89FyN//Hi8LF1aKpvjUJTU1VA4+OFySk50bMWhryj/+USrTpuVLdd1Bg7ZD6jzec0+xbNtWLc8+m0CQsI4OGxCoEQgzX9g/++xY6dXrTams9HzqH192Tk2Nkv/7v5Fy0kldJTKSNES+fK1oGwIIIIAAAggggEDgCcRHhUv/jknW6tg7TVmaU1BmAodFe+c83BtANMHE9TuLpaoZKUu37C4VXb9Yvt1eVZz5DtBHRxnuHWmoj1ntEiQ6gu8GdiSeIBBkAgQIg+yC010EEEAAgZYV0EDgUUe5l8pj3rxyM0+ga8FBW6+ef75UunQJk1tuibNt4hEBBPYTWLOmQMLCQoIqQDhuXKa8885YiY3l4/9+Lwf+RAABBBBAAAEEEECgVQVCQkKkTWK0tY7okVarLRUmZakGCdea4KHOcagjDjWQuMasOwpdS1mqoxR/+jPPWm2VmAylZm7D+FpBw2wTOMxIiLIdwiMCCASwAL8QBPDFpWsIIIAAAv4noHcOXnFFoQlcNL/td95ZJOefHy3t23MXYPMVOTNQBRYtypGJEz+VsjIXhuf6MUZ6epRceeUAuf76gSYo6tpIZj/uNk1HAAEEEEAAAQQQQCAgBCLMZ/geJoCnq0jbWn3KL62QP0x60aVb8mXp5nxZZh413WhZpfPfdXRw4qrthdb64a+b7eVrgNBxpKEGDTVtaphGFFkQQCBgBAgQBsylpCMIIIAAAoEg8N13FfLrr25EBw2CBheffbZUbr2VUYSB8JqgD54TKDd3zJ588udSXOze/2Oea1HLljR8eIZ89dUE0om2LDOlI4AAAggggAACCCDQKgKJ0REypGuqtdoaUGlGHOooQ1vQ0PaYW1RuO8SpR015+lVBjnz1R479+OiIUOnTzqQo7VCTplSDhn1MitI4kzqVBQEE/FOA/3v987rRagQQQACBABV48skSj/Ts6adL5MYbYyUigrv7PAJKIQEh8P77f8r69YVe7UuoGbTnylyinmpcu3YxMm/eeIKDngKlHAQQQAABBBBAAAEE/EAg3Iw47NU2wVpPOLCj1WLbHIf/cxhpqIFDDSSaJEZOL6UV1fLLhl3WajvJZEeVbmlxkr03aGgbddjGjEDU1KksCCDg2wIECH37+tA6BBBAAIEgEtAP7e+/79ocAg3xbN5cLYsWVcqIERENHcJ2BIJO4Mknl3qlzxHmztpzz+0t06dnS4X5Ej1p0ufy55/eC0xmZETLkiUnSRR38nrlelMJAggggAACCCCAAAK+LOA4x+HorDb2phaXV1opSTU1qaYo1aDh8i0FUlJRZT+mqScaYFxjAo26zl2yxX54WlxkrZGGOuqwu0lRqgFMFgQQ8B0BAoS+cy1oCQIIIIBAkAsUFe0xqQ89h7B9u/PzDniuVkpCwDcFVq7cbdJtbvVK495/f6wcd1xne10rV55qRvT+KA8++Lt9W0s9GTYsQz7+eJykpES1VBWUiwACCCCAAAIIIIAAAgEgEBsZLgd1TrFWW3eqzKSE63KLrPkMbUFDfdxuUo66smhK069X7rBW23mR4ZqiNMGa21DTk2rQUP9OMKlSWRBAoHUECBC2jju1IoAAAgggUEegzLXP23XO339DWZkLuUL2P5m/EQgwgWXLdnmlR716Jcq4cZ1q1aUjCk87rUeLBwh15OD33x9PKp9a+vyBAAIIIIAAAggggAACzgqEhYZIj4x4a50wsIP9tB2FZXWChqtzCsXEE51eyiurZcnG3dbqeFKXtNh9QcO9gcP2SdF8r3FE4jkCLSRAgLCFYCkWAQQQQAABVwWSkjybnz852bPludofjkfAlwTy88u90hxNKxpqvlTvvwwenC7Z2cnSkoFKLZ95PvaX528EEEAAAQQQQAABBBBwVyA9PkoO65VhrbaySk0q0j+2FdjTk+pIQ01XWlTufIpSLevP3GJr/fj3fRlfkmMj6gQNe7aJlwhSlNr4eUTAIwIECD3CSCEIIIAAAgi4LxAeHiL9+oXJ//7n2ofp+moONWn9+/Xjbb4+G7YFp0BMTMv//5CVlSQXXNCnXmAN3M2YkS2XXvp9vfs9sVFHELIggAACCCCAAAIIIIAAAt4QiI4Ik4GZydZqq6/aDCnckFdcJ2i4eXep7RCnHncVV8h3q3Ot1XZCpAkOapBQU5P23TvSUFOVJsWQotRmxCMCrgq0/C8lrraI4xFAAAEEEAhigQsvjJHLLy90W2DChEjp2DHM7XIoAIFAEejQIbZFu9K+faw19198fMNfTs88s5fcdNMiKSioaJG2VFW5kN+nRVpAoQgggAACCCCAAAIIIBDMAppNpUtanLUeO6C9nSLPzEmoowuX2lYz2nDV9kKpdCFHaXlVtf18e8HmScfkGJkxuoecPqyL42aeI4CAEwIECJ1A4hAEEEAAAQS8JTBtWrTceGOhFBe7V+Mll8S4VwBnIxBgAkOHZkjnzvGyfr37Afj9afr2TZa5c4+Rrl0T9t9V6++kpEh54YXD5ZRTPq+13VN/VJo5PVgQQAABBBBAAAEEEEAAAV8TSImLlJE9063V1rayyipZua3QCvpZwUMTNNQAYkFppe0Qpx437SqRMJOxhQUBBFwXIEDouhlnIIAAAggg0GICSUmhoqMIH3mkpNl1DBwYLmPHRjb7fE5EIBAFwkw6mosv7mON4PNU/3RewUsu6SunndZdnE1hOmlSN5k9+xCZPv1bTzXDXk5KSpT9OU8QQAABBBBAAAEEEEAAAV8WiAoPk/4dk6zV1s49e/bIxrySOkFD3dbYomlHWRBAwHUBAoSum3EGAggggAACLSpw773x8tNPlfL1166nIczICJF3300UTevBggACtQXOOy9Lbr/9Zykvb/5Iu8zMOCsoeOSR7WXo0Da1K3Dyr4svzpb09CiZOvVLqajwXFrQkSPbOtkCDkMAAQQQQAABBBBAAAEEfE9A527vlBprrcf0a2dv4O6SCitFqeNIQx19qGlHw8zvH73bNp7NxV4QTxBAoJYAAcJaHPyBAAIIIIBA6wtER4fIhx8myYkn7pZ585wPEnbsGCoffZQsPXrw9t76V5EW+KJAmzYxMmvWULnyygXNap7OL/jvfx8tBxyQ1qzzHU+aNKm7jBnTQY48cq788kue465mPde2nX56j2ady0kIIIAAAggggAACCCCAgC8LJMVEyPDuadZqa2e5mWJhdU6hrN9ZLNERYbbNPCKAgAsCoS4cy6EIIIAAAggg4CWB5ORQ+eSTZLn11ljRUYGNLVEmq+BZZ0XLDz+kiKYXZUEAgYYFLr+8n1x77YCGD2hgT0xMmLzzzhiPBAdtVaSkRMuiRSfK6NHtbZua/XjmmT0lIYHUws0G5EQEEEAAAQQQQAABBBDwK4HI8FDJbp8ojiMN/aoDNBYBHxAgQOgDF4EmIIAAAgggUJ9AVFSI3HFHvGzYkC7//GeiGWkUITpKMD4+RNq0CZEDDwyX+++Pk40b0+XllxPNPu6Yq8+RbQg4CmjKmvvvH2bm+RwukZHOfRTu2DFWvvxyvBx9dKZjUR55rnMjPvXUoWYOw+b//6vnXnFFf4+0h0IQQAABBBBAAAEEEEAAAQQQQCA4BBhmEBzXmV4igAACCPixgAYKp06NtlY/7gZNR8CnBC6/vL+cdlp3ef75P0yAbpkJxBfVad8hh7SVGTOy5eSTu0lUVPMDeHUK3m9D795J8vrrR8pJJ/1Xqqtdm5NQ5xvVc7UMFgQQQAABBBBAAAEEEEAAAQQQQMBZAQKEzkpxXMAJzJ07VxYuXGita9askZycHNm9e7cZmRMv3bt3l1GjRsmFF14oWVlZAdd3OoQAAggggAACIm3bxspNNx0o1103UBYs2C5bthRLcXGlJCVFmvf/JMnOTvEa0wkndJE33zxSzjhjnpSWVjlVb3R0mBldPEr0XBYEEEAAAQQQQAABBBBAAAEEEEDAFQEChK5ocWzACFRWVsqECRPq7c+uXbvk559/ttbHH39c7rzzTrnhhhvqPZaNCCCAAAIIIOD/AuFm7opDD23X6h3RkYq9eiXKffctkbffXisVFdX1tikiIlQmTepmPp8MNPOOptV7DBsRQAABBBBAAAEEEEAAAQQQQACBxgQIEDamw76AFkhKSrJGCQ4bNswaMdi+fXuJjY2VzZs3y7x58+SFF16wRhTeeOONkpycLBdffHFAe9A5BBBAAAEEEGh9AQ34vfbaaHn44WFW+tN587bIzp1lVsNSU6Nk9Oj2cu65va3Rj63fWlqAAAIIIIAAAggggAACCCCAAAL+KhCyxyz+2njajYA7AlVVVRIW1vB8QmvXrpXBgwdLXl6eZGRkmLRjWxo93tW2bNy4UTp16mSdtmHDBsnMzHS1CI5HAAEEEEAAAQQQ8EEBPuf54EXxUpO49l6CphoEEEAAAQQQQMDLAnzO8zI41XlFINQrtVAJAj4o0FhwUJvbrVs3mTx5stVynZ9w+fLlPtgLmoQAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIuCZAgNA1L44OMoG4uDh7j0tLS+3PeYIAAggggAACCCCAAAIIIIAAAggggAACCCCAAAII+KsAAUJ/vXK0u8UFSkpK5IMPPrDqCQ0Nld69e7d4nVSAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCLS0AAHClhamfL8SqKiokPXr18sbb7whI0eOlFWrVlntP+eccyQhIcGv+kJjEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIH6BMLr28g2BIJJYN26ddZ8gw31eezYsfLggw82tLvB7TpxbWPLli1bGtvNPgQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEECgRQQIELYIK4UGgkBaWpo88cQTcsopp0hYWJjLXerUqZPL53ACAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIINDSAgQIW1qY8n1eoGPHjvLbb79Z7aysrJRNmzbJJ598Is8//7zMmDFD1qxZIzfeeKOEhIT4fF9oIAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCDQlAABwqaE2N+qAhqwi4iIcLsNL774opx99tn1lqPl9+/f377vwAMPlPHjx8sFF1wgo0ePlptvvtmai/CFF16wH+PMkw0bNjR6mKYYHTp0aKPHsBMBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQ8LQAAUJPi1JewAgMHDhQZs6caY0i1ADjaaedJkcffbTT/cvMzHT6WA5EAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBLwlQIDQW9LU0yyB8PBwWbZsWbPOdTypffv2jn86/fyEE06wAoR6wttvv+1SgNDpSjgQAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEPCiAAFCL2JTVfME+vTp07wTPXBWRkaGvZQ///zT/pwnCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggIC/CoT6a8NpNwLeENi0aZO9mvj4ePtzniCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAAC/ipAgNBfrxzt9orAW2+9Za9nwIAB9uc8QQABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQT8VYAAob9eOdrtlsD7778vW7ZsabSM+fPny5133mkdo3MhTpkypdHj2YkAAggggAACCCCAAAIIIIAAAggggAACCCCAAAII+IMAcxD6w1WijR4X0ADh5MmTZfz48TJmzBjp16+fJCcnS1lZmaxevVrmzJkjb775plRXV1t133LLLZKVleXxdlAgAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOBtAQKE3hanPp8RKC8vl/fee89aG2pUTEyM3HXXXXL11Vc3dAjbEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAG/EiBA6FeXi8Z6SuDvf/+7HHfccfLFF1/Izz//LFu3bpXt27dLaGiopKamWiMKjzzySDnrrLOkffv2nqq2VjmVlZX2v5tKd2o/kCcIIIAAAggggAACPi/g+NnO8TOfzzecBrot4Hi9HV8HbhdMAQgggAACCCCAAAKtKuD42c7xM1+rNorKEXBTIGSPWdwsg9MRQKAZAgsXLpShQ4c240xOQQABBBBAAAEEEPAXgR9//FEOPvhgf2ku7XRTgM/4bgJyOgIIIIAAAggg4AcCfMb3g4tEE50SCHXqKA5CAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIGAEGAEYUBcRjrhjwKlpaXy22+/WU3PyMiQ8HAy/vrjdXS3zZqewDaSVO8+aqmUtu62k/MRaEyA13FjOuzzFwFex/5ypfyjnZpyKCcnx2rsgAEDJDo62j8aTivdFmiNz/j8++X2ZfPrArj+fn353G48199tQr8ugOvv15fP7cZz/d0mdLkAPuO7TMYJfiBARMIPLhJNDEwB/aGIdFOBeW2b2ysNDmZmZjb3dM5DwCcEeB37xGWgEW4K8Dp2E5DTLYGuXbsiEYQCrf0Zn3+/gvBF59Blrr8DRhA+5foH4UV36DLX3wEjCJ9y/b130fmM7z1ravKOAClGveNMLQgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgj4hAABQp+4DDQCAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAe8IECD0jjO1IIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOATAgQIfeIy0AgEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEvCNAgNA7ztSCAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgE8IECD0ictAIxBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBDwjgABQu84UwsCCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACPiEQsscsPtESGoEAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAi0uwAjCFiemAgQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQR8R4AAoe9cC1qCAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAQIsLECBscWIqQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQMB3BAgQ+s61oCUIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIItLgAAcIWJ6YCBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBHxHgACh71wLWoIAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIBAiwsQIGxxYipAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAwHcECBD6zrWgJQgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgi0uAABwhYnpgIEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABs0qOIgAAIIlJREFUBBBAAAEEfEeAAKHvXAtaggACCNQRWL9+vcyePVsmT54sWVlZEhcXJ9HR0ZKZmSknnHCCvP7661JZWVnnPDYg4C0BfY1ec801kp2dbb0+U1NTZejQofL3v/9diouLvdUM6kHAZYGff/5Z7rnnHjn22GOlU6dOEhUVJfHx8dK7d285++yz5euvv3a5TE5AAAEEvC3A+7C3xVu/Pt6/Wv8a+GILrrvuOgkJCbGv8+bN88Vm0iYPCuzYsUPuv/9+OeSQQ6Rdu3bWZ9kOHTrIsGHD5Nprr5Xvv//eg7VRlC8JlJeXy/PPPy/jxo2T9u3b27/H6G9G5557rixYsMCXmktbEEDAxwVC9pjFx9tI8xBAAIGgFLj11ltl5syZ0tQ/00OGDJF33nlHOnfuHJROdLr1BObOnSunn3667N69u95G6BeUjz76SLp3717vfjYi0FoCRxxxhMyfP7/J6s8880x57rnnJDIyssljOQABBBDwtgDvw94Wb/36eP9q/Wvgiy349ddfRb8TOt44+uWXX8qoUaN8sbm0yQMCb731lkyfPl1yc3MbLE1vKH7//fcb3M8O/xTYsGGDjB8/Xn777bdGO3DllVfKgw8+aN000OiB7EQAgaAXCA96AQAQQAABHxXYvHmzFRzUUYMnnniijBkzRnr16mWNIFy2bJk89thjsnDhQlm0aJGMHTtW9G5iHf3CgoA3BPSHiFNPPdUaJaivuxtvvFFGjx4tJSUl8sYbb8izzz4rK1assL686OuU16Y3rgp1OCuwadMm61C9y/qUU06Rww47zLrJoqqqyrrbWr9M6zGvvvqq9WPba6+95mzRHIcAAgh4RYD3Ya8w+1wlvH/53CVp9QZVV1fLBRdcYH1eadOmjWzfvr3V20QDWlbglVdekXPOOUf02us110DhoYceKprJZevWrbJ69WqZM2eOREREtGxDKN3rAnoTgGNwcODAgXLVVVdZ2aYKCgrkm2++sYKCRUVF8vDDD1ujC3U0KQsCCCDQmAAjCBvTYR8CCCDQigLXX3+9pKWlWR/4ExIS6rREf8ieOnWqvPnmm9a+O++8U2655ZY6x7EBgZYQ0GCgpi4KDw+3RmKNGDGiVjUPPPCAaKojXe644w7REbEsCPiKwIQJE+Sss86Sk08+WcLCwuo0S1M2abqmP/74w9qnow01iMiCAAII+IoA78O+ciW82w7ev7zr7Q+1PfLII6Ijhfr06WPdVHrvvfdazWYEoT9cPdfbqDcKDxo0SMrKyqzPphoITEpKqrcgTUNJFox6afx2o2aOmjRpktV+/f6tUyLs/13mp59+Et1XUVEhKSkp1k0D+p2dBQEEEGhIgABhQzJsRwABBPxAQFOK6AgY/fCvd4/p3eQsCLS0gI4I1HkGdbnooovkqaeeqlOl3tHav39/0S+x+sVk27Zt3MVaR4kNvizw73//WyZOnGg18bLLLpNHH33Ul5tL2xBAIIgEeB8OoovdjK7y/tUMND89RVMN9u3bVwoLC0UDgnrznt6YpwsBQj+9qE00WzMHff7555Kenm59z9JHluAR0NGCOjJQlw8//ND+XWV/gZNOOknee+89a7OmItXv5SwIIIBAQwKhDe1gOwIIIICA7wvoCEMNDOqiqURYEPCGgONcFprepr4lNDTUGqGl+/Ly8qwfLOo7jm0I+KqA47w9/Pvqq1eJdiEQnAK8DwfndXe217x/OSvl/8fNmDHDCg5OmzZNHK+7//eMHtQnsHz5cis4qPv++te/WkHC+o5jW+AK6I3htqV79+62p3Uee/ToYd+mo01ZEEAAgcYECBA2psM+BBBAwA8EbB/4NCDDgoA3BDSViS46P+bgwYMbrPKII46w79P5EFgQ8CcBxy/g/PvqT1eOtiIQ+AK8Dwf+NXanh7x/uaPnP+fqNBM6WlTnndPU/iyBL/DWW2/ZO6lzaNsWvRlz5cqVotmFWAJboHfv3vYOrlmzxv58/ye2mxtDQkKkV69e++/mbwQQQKCWAL8m1+LgDwQQQMC/BHQSek3hqIvOO8GCgDcEbK+5nj17WnMQNlSn42vSdk5Dx7IdAV8T+Oqrr+xNcnwt2zfyBAEEEGglAdt7Ku/DrXQBfLxa3r98/AJ5oHm7du2Syy+/3Cpp1qxZkpGR4YFSKcLXBRYsWGA1UecczM7Oln/+859ywAEHWEFiDRxpulEdVaZpZjXtLEvgCUyZMkUSExOtjun/+1VVVXU6uXjxYpk7d661/bTTTrMfX+dANiCAAAJ7BQgQ8lJAAAEE/FhA7xatrKy0enDqqaf6cU9our8IlJaWyo4dO6zmZmZmNtpsnXtQRxnqonOksCDgLwI6h+Z9991nby7/vtopeIIAAq0swPtwK18AH6+e9y8fv0Aeat51110nW7dulZEjR8p5553noVIpxtcFli5dajWxa9eucumll8oZZ5whS5YsqdXstWvXyu233y4jRoyQzZs319rHH/4voDcDvPTSSxITEyPffvutHHzwwfLKK6+IBo//+9//WsFhzeKjI8kPPPBAeeihh/y/0/QAAQRaXIAAYYsTUwECCCDQMgI//PCDPPLII1bhGqjROShYEGhpgYKCAnsV8fHx9ucNPbEFCLmLtSEhtvuiwMMPPyw//vij1bQTTzxRhgwZ4ovNpE0IIBCEArwPB+FFd6HLvH+5gOWnh2ra/ueee87K4vHUU0+JphBkCQ6BnTt3Wh3VuQj/7//+T5KTk0VfA5pVSG8eWbhwoRx77LHWMb///rtoGlK9aYAlsAT0u8miRYusmwN++eUX0TlINSB81FFHWcHh2NhYKzCo/1a0a9cusDpPbxBAoEUECBC2CCuFIoAAAi0rsG3bNpk0aZI1elC/FL788suiHwRZEGhpAf3yaVsiIyNtTxt8jIqKsvaVlJQ0eAw7EPAlAU3NdsMNN1hNatOmjcyePduXmkdbEEAgyAV4Hw7yF0Aj3ef9qxGcANmlo4IuvPBC2bNnj1x55ZUyYMCAAOkZ3XBGoKioyDqsrKxMwsLC5OOPP5aLLrrISjGr37n0hjadl9IWJPzuu+/k3XffdaZojvEjgYqKCnnttddkzpw51r8F+zddfyt6/fXXZd68efvv4m8EEECgXgEChPWysBEBBBBwXkBTfGqQzt1VU0U4s+id4+PHj5eNGzdah99zzz1y5JFHOnMqxyDgtkB0dLS9DP2RoqlFv8DqomlQWBDwdYH//e9/onfl6r/r+kPLm2++KW3btvX1ZtM+BBAIIgHeh4PoYrvQVd6/XMDy40P1e5/OQdq5c2e57bbb/LgnNL05Ao7//uvowOHDh9cpJjQ0VHQaEtuigSKWwBHQIPHYsWPl7rvvltzcXNF0w/pvgn7n3r17t3z66ady6KGHWqNJJ06cKI8++mjgdJ6eIIBAiwkQIGwxWgpGAAEEPC+gd42fcMIJ8tNPP1mFX3XVVfaRLp6vjRIRqCuQkJBg3+hM2lDbna7OpCO1F8wTBFpBQOdsOfrooyUvL8+6K1t/UNE5PFgQQAABXxLgfdiXroZvtIX3L9+4Di3dCk0ree+991rVPP744/Z5vlu6Xsr3HQHHf/9towTra12/fv2kY8eO1i5NO8oSOAJ6Y8D8+fOtDj3//PMya9Ys6dOnj2hmn8TERCvN6JdffimjR4+2Rhfq70X7z1MZOBr0BAEEPCUQ7qmCKAcBBBAIVoHw8HDrri13+9++fftGi9ARLaeeeqroBz5dzj//fHnwwQcbPYedCHhaQO9cTU9Plx07dthHsTZUhwZabAHCTp06NXQY2xFodYHNmzdbd+Pqo44Gf+GFF6yRhK3eMBqAAAII7CfA+/B+IEH+J+9fwfMC0PklNXtH9+7dpbi4WN544406ndd552zLF198IVu3brX+1JFEtnnBbft59D8B/T5lu6aZmZmNdkCP3bRpkzU/YaMHstNvBDS18Isvvmi1t3fv3tbcg/U1Xn+fuuuuu6yRhDoHpZ6j/36wIIAAAg0JECBsSIbtCCCAgAsCetdWSy76we7MM8+08sxrPZMnT5ann366JaukbAQaFMjOzpavv/5aVq1aZaVi1C8h9S16p7Nt0XNYEPBFAQ12H3XUUbJmzRqreXpX/llnneWLTaVNCCCAgCXA+zAvBBXg/Su4Xge2tP36eWXKlClNdl4DBLZFR5kSILRp+O+jjgy0jQisqqpqtCO2/Q19T2v0ZHb6pIDOLbhz506rbYMGDWq0jYMHD7bvd/xObt/IEwQQQMBBgBSjDhg8RQABBHxVQCcft90lOmHCBHn11VdF5xdgQaA1BHReA110dKAt3W197fjqq6/smw855BD7c54g4CsCOlfHMcccI0uXLrWadN9998kll1ziK82jHQgggEC9ArwP18sSVBt5/wqqy01nEbAEDj/8cLvE6tWr7c/re2K78c2WarS+Y9jmXwKOwV7NLtXYUlFRYd/teJ59I08QQAABBwF+XXbA4CkCCCDgiwKaN/65556zmjZmzBh5++23JSIiwhebSpuCROAvf/mLvae2NCf2DXuf6KjXV155xforOTnZmgdh/2P4G4HWFND0XOPHj5eff/7ZasbNN98s119/fWs2iboRQAABpwR4H3aKKWAP4v0rYC9tox176aWXrDnFNM1gQ6vOT2ZbdFoK23Fdu3a1bebRjwWOP/54++8A7777boM90Zs0c3Nzrf2HHXZYg8exw78EUlNTrXkGtdXff/+9lcmnoR443qjbrVu3hg5jOwIIIGAJECDkhYAAAgj4sMDtt99uzxc/cuRI+eCDDyQqKsqHW0zTgkFg6NChYvuyqZOj6xeU/RedH3PZsmXW5ssvv9z+ZXb/4/gbgdYQ0Dl8TjzxRPn222+t6vU1OnPmzNZoCnUigAACLgvwPuwyWcCcwPtXwFxKOoKAywJpaWly/vnnW+d99tln9gxDjgUVFBTIFVdcYd+kmYhYAkNAM0jpzY266Pyzd999d70dy8vLq3XTo2agYkEAAQQaEwgxdxTtaewA9iGAAAIItI6AzoN12WWXWZVrapB//etfkpSU1GhjsrKyCMQ0KsROTwksXrxYNG1oSUmJxMfHy0033WSNEtS/NR3uM888Y1WlE6gvWrRIEhISPFU15SDgtsDJJ58stjuvjzzySHnkkUckJCSkwXIjIyNFX8ssCCCAgK8I8D7sK1fCu+3g/cu73v5Wm95cescdd1jN1hGEo0aN8rcu0N4mBHJycmTIkCGyfv160dSRF198sZx00knWyLLffvtNZs2aJbY556ZPny5PPvlkEyWy258E9Nrq/II6klyXiRMnyrRp06R79+5SWloqCxYssL7X6OtDF81A9d///td6zn8QQACBhgQIEDYkw3YEEECglQX0C51jaghnmqMT0JNCxhkpjvGEwJw5c+SMM86Q/Pz8eovTgMrcuXOlZ8+e9e5nIwKtJdBYMLC+NnXp0kXWrVtX3y62IYAAAq0mwPtwq9G3WsW8f7UavV9UTIDQLy6T243ULC2abnTVqlUNlnXuuefKU089xc3DDQr57w4N+E2ZMkV27NjRaCf0JkidniYlJaXR49iJAAIIkGKU1wACCCCAAAIINEtA71hcsmSJXHnlldboqtjYWNH5BvWuVr17VUc3EBxsFi0nIYAAAggg0KQA78NNEnEAAgggEHAC2dnZ8ssvv8gDDzwgw4YNE52bTrNdZGZmyuTJk+WLL74QnQYiIiIi4PpOh0TGjh1rjRLV79t6U3lGRoZ1rWNiYkTnGzz11FPl/ffft0YOEhzkFYMAAs4IMILQGSWOQQABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQCBABBhBGCAXkm4ggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggg4IwAAUJnlDgGAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAgQARIEAYIBeSbiCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCDgjAABQmeUOAYBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQACBABEgQBggF5JuIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOCMAAFCZ5Q4BgEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIEAESBAGCAXkm4ggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggg4IwAAUJnlDgGAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAgQARIEAYIBeSbiCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCDgjAABQmeUOAYBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQACBABEgQBggF5JuIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOCMAAFCZ5Q4BgEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIEAESBAGCAXkm4ggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggg4IwAAUJnlDgGAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAgQARIEAYIBeSbiCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCDgjAABQmeUOAYBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQACBABEgQBggF5JuIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOCMAAFCZ5Q4BgEEEEAAAQQQQAABBBBAAAEEEEAAAQRaVGDdunUSEhJirS+99FKL1kXhCCCAAAIIBLsAAcJgfwXQfwQQQAABBBBAAAEEEEAAAQQQQACBVhWYN2+ePTBmC5A19XjFFVe0apupHAEEEEAAAQT8W4AAoX9fP1qPAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgEsC4S4dzcEIIIAAAggggAACCCCAAAIIIIAAAggg0GIC06dPlxkzZjRZfnp6epPHcAACCCCAAAIIINCQAAHChmTYjgACCCCAAAIIIIAAAggggAACCCCAgJcF2rRpI/379/dyrVSHAAIIIIAAAsEmQIrRYLvi9BcBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQCCoBQgQBvXlp/MIIIAAAggggAACCCCAAAIIIIAAAoEg0LVrVwkJCZGzzz7b6s7ChQtlypQp0qlTJ4mOjrYedd+yZcuc6u6cOXNk0qRJkpmZKVFRUZKWliYjRoyQ++67TwoLC50q4/fff5dLL71UBgwYICkpKRIbGys9e/aUcePGyezZsyUnJ6fJcj777DOZOHGitGvXzmpHt27dRNOwbty4sdFzN2/eLDfccIMcdNBBkpSUJJGRkVYZ2hZ1eemllyQ/P7/RMtiJAAIIIIBAIAuE7DFLIHeQviGAAAIIIIAAAggggAACCCCAAAIIIODLAvPmzZPRo0dbTbztttvk9ttvd7m5GiD8888/Zdq0aXL44YfLRRddJJWVlXXK0WDfyy+/LJMnT66zTzeUlpbK1KlT5b333qt3v27s0KGDzJ07Vw488MB6j6mqqpJrr71WHn30Uamurq73GN2obdVAnW1Zt26daABQlxdffFGWL18us2bNsu2u9ZiRkSFfffWVZGdn19quf3z99dcyYcKEJgOAGgTV41gQQAABBBAIRgHmIAzGq06fEUAAAQQQQAABBBD4//buLKSqrg3g+GMTRSYVDZZEkWQpkjaooJWBFWREp6wImmimkoiEQm+66MK7CiLKMikvwkYJyqJsDidKGiRIGmiSEjMlMEzKr2d97M0+x3PUeH179Zz/Ajlrr7X22nv99kXQw7MWAggggAACCCDglwKPHz+W06dPi55lmJmZKfHx8SboV1RUJAcPHpTm5mZZtWqVCcRpn2fRoJ0VHIyJiZGMjAwThKuvr5eCggIT0NPsvJSUFHn69KmEhYV5TiGbN2+WvLw80z5q1ChJT0+XxMREk8mnWYMVFRVy/vz5Nvc5G44fPy4lJSWSnJxsgp0RERHS0NAg+fn55k/nWb9+vZSWljpvM+tbsWKFCQ4OGjTIZBtq8FU9WlpaTBBV77lw4YLbfVwggAACCCAQaAJkEAbaF2e9CCCAAAIIIIAAAggggAACCCCAAALdSsCZQajbZ27btq3D95s4caL07dvXHmdlEGrD2LFjpayszGypaQ/4Xbl9+7bMmzfPZBZOnz5ddBtSZ9GsQCujTgOAGlTUrTmdRQN3GgDUsnz5cjlz5oyzWy5duiQul8u06ZakOsfgwYPdxlgXuk2obmFqFWcGobZt2rRJcnJyzNap1hirPTc31zRVVlbKlClT7O5bt26Z4KU2tJchqNmVTU1NEhISYt9LBQEEEEAAgUASIEAYSF+btSKAAAIIIIAAAggggAACCCCAAAIIdDsBZ4Cwsy/35s0b0aCgVZwBQs3OS0tLs7rcfjX4qOf/adFMvri4OLs/NTVVrl69agKPr169MucW2p2Oyty5c6W4uFj69Okj7969E80StIoGBTU4qecNVldXe80wtMZ6/joDhDqnrlG3RPUsL168kEmTJplm3cZ0x44d9hDNnly5cqW5bmxsJABoy1BBAAEEEEDAXaCX+yVXCCCAAAIIIIAAAggggAACCCCAAAIIINBTBYYMGSKLFi3y+fq6LadVNMhnFc2o0zP9tGgAcMyYMVZXm1/N7NOi92hw0ypfvnyR8vJyc6nZhd62H7XGdvS7dOlSr8FBvU+zJ4ODg80Ur1+/dpvKGazUcwwpCCCAAAIIIOBdgAChdxdaEUAAAQQQQAABBBBAAAEEEEAAAQQQ+OsCe/fuldbW1g7/nNmDzpfU7TY1s89XiY2NtbcNraqqsodpoE233NSSkJBgt3urOPudc+j5h/ruWmbNmuXt1k63WRmCvm7QQKiWb9++uQ2ZMWOGjB8/3rTt3LnTnMGYnZ1tzjP88eOH21guEEAAAQQQCGQBAoSB/PVZOwIIIIAAAggggAACCCCAAAIIIICAXwmMGDGi3fVo8HDo0KFmTH19vT3WWR85cqTd7q0SGhpqNzvvq6urs9udmXx24x9UdIvS9kqvXv//b82fP3+6DdNzGfXswcjISNOu5yxmZWVJUlKSOQtx/vz5otuQet7nNgkXCCCAAAIIBIAAAcIA+MgsEQEEEEAAAQQQQAABBBBAAAEEEEAgMASCgoI6XKiV5edrYGfm8HWv1d4Vc1hz/elvVFSUPHv2TAoLC0W3VA0PDzdTfP/+Xa5du2bOKNQsyNra2j+dmvEIIIAAAgj4jQABQr/5lCwEAQQQQAABBBBAAAEEEEAAAQQQQCDQBT5//twugZ4b+PXrVzPGyiTUC2f906dP7c7h7HfeN2zYMPu+mpoau/5fVHr37i0ul0tOnDghL1++FH0frU+bNs28zqNHj2TLli3/xavxTAQQQAABBLqFAAHCbvEZeAkEEEAAAQQQQAABBBBAAAEEEEAAAQT+uYCeA6hBQF/lyZMnYp3FFx0dbQ/Tc/usbT3Ly8vtdm+ViooKu9k5h55/aGUO3rt3zx7THSq65almE5aWlsrUqVPNK12+fFk0q5CCAAIIIIBAIAoQIAzEr86aEUAAAQQQQAABBBBAAAEEEEAAAQT8UkDPBNQz+HyVvLw8u2vOnDl2Xc8mTE5ONtc3btyQ9+/f232eldzcXNOkWXqzZ8+2uzWbMDEx0VyfPXvWZO3Znd2komcUWuvUQGpDQ0M3eTNeAwEEEEAAgb8rQIDw73rzNAQQQAABBBBAAAEEEEAAAQQQQAABBP5VgV27dom3rUbv3r0rx44dM8/WrTbj4uLc3mP79u3muqWlxWTbWZmGzkEaYLx+/bppSktLE83Mc5Y9e/aYy6amJlm2bJk0NjY6u93qHz58cLvuiov79++bLUV9zaVrUgctwcHBMnz4cF9DaUcAAQQQQMCvBfr49epYHAIIIIAAAggggAACCCCAAAIIIIAAAj1IoLa2Vqqqqjp84wEDBkh4eHibcTExMfL8+XNz1l5mZqbEx8dLc3OzFBUVyYEDB8z2o5otePjw4Tb3LliwwAT1zp07J8XFxZKQkCAZGRkSGRlpzi0sKCgQKwNRswX379/fZo6FCxfKhg0bzHl/JSUlEhUVJenp6ZKUlCQhISFSV1cnDx8+FM0wnDx5spw8ebLNHP+k4ebNm7Jv3z6ZOXOm6Hr0GRoE1K1Eq6ur5ejRo1JZWWkesXHjRlELCgIIIIAAAoEowL+AgfjVWTMCCCCAAAIIIIAAAggggAACCCCAQLcUOHLkiOhfR0UDgXreoGeJjY01AbmtW7eaX8/+fv36yalTp0zwz7NPr/Pz800QsbCw0My/evXqNsNGjx4tV65ckbCwsDZ92pCTkyMawNQgZE1NjWRlZXkdp8G7f6P8+vXLZAlamYLenrFkyRLJzs721kUbAggggAACASFAgDAgPjOLRAABBBBAAAEEEEAAAQQQQAABBBAIFAHNjIuOjjYZgw8ePDBZe5pFl5KSIroFqGb1+Sr9+/eXixcvmnMMNbuvrKzM3D9w4ECJiIgQl8tlAo+6PaevomcTHjp0SNatW2eChXfu3JGPHz9Ka2urCSpOmDBBFi9eLLpFaVeX3bt3m+CnnqNYWlpqApSalaklNDTU9K1Zs0ZSU1O7+tHMhwACCCCAQI8SCPr9D3Nrj3pjXhYBBBBAAAEEEEAAAQQQQAABBBBAAAEE3ATGjRsnb9++lbVr13b5tp1uD+ICAQQQQAABBPxCoJdfrIJFIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIBApwQIEHaKiUEIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAII+IcAAUL/+I6sAgEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIFOCRAg7BQTgxBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBDwDwEChP7xHVkFAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAp0SCGr9XTo1kkEIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIINDjBf4HkHZP3EsQAhMAAAAASUVORK5CYII=" width="900"></p><pre><code>WARNING:tensorflow:sample_weight modes were coerced from  ...    to    [&#39;...&#39;]</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Create-a-Siamese-Network-with-Triplet-Loss-in-Keras&quot;&gt;&lt;a href=&quot;#Create-a-Siamese-Network-with-Triplet-Loss-in-Keras&quot; class=&quot;headerlin
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
  </entry>
  
  <entry>
    <title>Image_Super Resolution</title>
    <link href="https://zhangruochi.com/Image-Super-Resolution/2020/08/11/"/>
    <id>https://zhangruochi.com/Image-Super-Resolution/2020/08/11/</id>
    <published>2020-08-11T10:34:27.000Z</published>
    <updated>2020-08-11T10:35:58.937Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.rhyme.com" target="_blank" rel="noopener"> <img src="https://www.rhyme.com/assets/img/logo-dark.png" alt="Header" style="width: 150px;"> </a></p><h1 align="center"> Image Super Resolution using Autoencoders</h1><!-- <img src="images/high_res_v_low_res.jpg" width=550px> --><h2 id="Task-1-Project-Overview-and-Import-Libraries"><a href="#Task-1-Project-Overview-and-Import-Libraries" class="headerlink" title="Task 1: Project Overview and Import Libraries"></a>Task 1: Project Overview and Import Libraries</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br></pre></td></tr></table></figure><h2 id="Task-2-Build-the-Encoder"><a href="#Task-2-Build-the-Encoder" class="headerlink" title="Task 2: Build the Encoder"></a>Task 2: Build the Encoder</h2><!-- <img src="images/autoencoder.jpg"> --><p>Credit: Autoencoder Schema by <a href="https://blog.keras.io/img/ae/autoencoder_schema.jpg" target="_blank" rel="noopener">Francois Chollet, 2016</a>.</p><!-- <h4 align=center>Encoder Architecture</h4><img src="images/encoder.png" width=450px align=center> --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">input_img = Input(shape=(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">l1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(input_img)</span><br><span class="line">l2 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l1)</span><br><span class="line">l3 = MaxPooling2D(padding=<span class="string">'same'</span>)(l2)</span><br><span class="line">l3 = Dropout(<span class="number">0.3</span>)(l3)</span><br><span class="line">l4 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>),  padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l3)</span><br><span class="line">l5 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l4)</span><br><span class="line">l6 = MaxPooling2D(padding=<span class="string">'same'</span>)(l5)</span><br><span class="line">l7 = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l6)</span><br><span class="line">encoder = Model(input_img, l7)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         [(None, 256, 256, 3)]     0         _________________________________________________________________conv2d (Conv2D)              (None, 256, 256, 64)      1792      _________________________________________________________________conv2d_1 (Conv2D)            (None, 256, 256, 64)      36928     _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         _________________________________________________________________dropout (Dropout)            (None, 128, 128, 64)      0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     _________________________________________________________________conv2d_3 (Conv2D)            (None, 128, 128, 128)     147584    _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    =================================================================Total params: 555,328Trainable params: 555,328Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="Task-3-Build-the-Decoder-to-Complete-the-Network"><a href="#Task-3-Build-the-Decoder-to-Complete-the-Network" class="headerlink" title="Task 3: Build the Decoder to Complete the Network"></a>Task 3: Build the Decoder to Complete the Network</h2><!-- <img src="images/decoder.png" width=450px> --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input_img = Input(shape=(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br><span class="line">l1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(input_img)</span><br><span class="line">l2 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l1)</span><br><span class="line"></span><br><span class="line">l3 = MaxPooling2D(padding=<span class="string">'same'</span>)(l2)</span><br><span class="line">l3 = Dropout(<span class="number">0.3</span>)(l3)</span><br><span class="line">l4 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>),  padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l3)</span><br><span class="line">l5 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l4)</span><br><span class="line"></span><br><span class="line">l6 = MaxPooling2D(padding=<span class="string">'same'</span>)(l5)</span><br><span class="line">l7 = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l6)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decoder</span></span><br><span class="line"></span><br><span class="line">l8 = UpSampling2D()(l7)</span><br><span class="line"></span><br><span class="line">l9 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">            activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l8)</span><br><span class="line">l10 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l9)</span><br><span class="line"></span><br><span class="line">l11 = add([l5, l10])</span><br><span class="line">l12 = UpSampling2D()(l11)</span><br><span class="line">l13 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l12)</span><br><span class="line">l14 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l13)</span><br><span class="line"></span><br><span class="line">l15 = add([l14, l2])</span><br><span class="line"></span><br><span class="line"><span class="comment"># chan = 3, for RGB</span></span><br><span class="line">decoded = Conv2D(<span class="number">3</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l15)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create our network</span></span><br><span class="line">autoencoder = Model(input_img, decoded)</span><br><span class="line"><span class="comment"># You'll understand later what this is</span></span><br><span class="line">autoencoder_hfenn = Model(input_img, decoded)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model_1&quot;__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            __________________________________________________________________________________________________conv2d_5 (Conv2D)               (None, 256, 256, 64) 1792        input_2[0][0]                    __________________________________________________________________________________________________conv2d_6 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_5[0][0]                   __________________________________________________________________________________________________max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_6[0][0]                   __________________________________________________________________________________________________dropout_1 (Dropout)             (None, 128, 128, 64) 0           max_pooling2d_2[0][0]            __________________________________________________________________________________________________conv2d_7 (Conv2D)               (None, 128, 128, 128 73856       dropout_1[0][0]                  __________________________________________________________________________________________________conv2d_8 (Conv2D)               (None, 128, 128, 128 147584      conv2d_7[0][0]                   __________________________________________________________________________________________________max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_8[0][0]                   __________________________________________________________________________________________________conv2d_9 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_3[0][0]            __________________________________________________________________________________________________up_sampling2d (UpSampling2D)    (None, 128, 128, 256 0           conv2d_9[0][0]                   __________________________________________________________________________________________________conv2d_10 (Conv2D)              (None, 128, 128, 128 295040      up_sampling2d[0][0]              __________________________________________________________________________________________________conv2d_11 (Conv2D)              (None, 128, 128, 128 147584      conv2d_10[0][0]                  __________________________________________________________________________________________________add (Add)                       (None, 128, 128, 128 0           conv2d_8[0][0]                                                                                    conv2d_11[0][0]                  __________________________________________________________________________________________________up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 128 0           add[0][0]                        __________________________________________________________________________________________________conv2d_12 (Conv2D)              (None, 256, 256, 64) 73792       up_sampling2d_1[0][0]            __________________________________________________________________________________________________conv2d_13 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_12[0][0]                  __________________________________________________________________________________________________add_1 (Add)                     (None, 256, 256, 64) 0           conv2d_13[0][0]                                                                                   conv2d_6[0][0]                   __________________________________________________________________________________________________conv2d_14 (Conv2D)              (None, 256, 256, 3)  1731        add_1[0][0]                      ==================================================================================================Total params: 1,110,403Trainable params: 1,110,403Non-trainable params: 0__________________________________________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.compile(optimizer=<span class="string">'adadelta'</span>, loss=<span class="string">'mean_squared_error'</span>)</span><br></pre></td></tr></table></figure><h2 id="Task-4-Create-Dataset-and-Specify-Training-Routine"><a href="#Task-4-Create-Dataset-and-Specify-Training-Routine" class="headerlink" title="Task 4: Create Dataset and Specify Training Routine"></a>Task 4: Create Dataset and Specify Training Routine</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage, misc</span><br><span class="line"><span class="keyword">from</span> skimage.transform <span class="keyword">import</span> resize, rescale</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_batches</span><span class="params">(just_load_dataset=False)</span>:</span></span><br><span class="line"></span><br><span class="line">    batches = <span class="number">256</span> <span class="comment"># Number of images to have at the same time in a batch</span></span><br><span class="line"></span><br><span class="line">    batch = <span class="number">0</span> <span class="comment"># Number if images in the current batch (grows over time and then resets for each batch)</span></span><br><span class="line">    batch_nb = <span class="number">0</span> <span class="comment"># Batch current index</span></span><br><span class="line"></span><br><span class="line">    max_batches = <span class="number">-1</span> <span class="comment"># If you want to train only on a limited number of images to finish the training even faster.</span></span><br><span class="line">    </span><br><span class="line">    ep = <span class="number">4</span> <span class="comment"># Number of epochs</span></span><br><span class="line"></span><br><span class="line">    images = []</span><br><span class="line">    x_train_n = []</span><br><span class="line">    x_train_down = []</span><br><span class="line">    </span><br><span class="line">    x_train_n2 = [] <span class="comment"># Resulting high res dataset</span></span><br><span class="line">    x_train_down2 = [] <span class="comment"># Resulting low res dataset</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> root, dirnames, filenames <span class="keyword">in</span> os.walk(<span class="string">"/home/rhyme/Desktop/Project/data/cars_train"</span>):</span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">            <span class="keyword">if</span> re.search(<span class="string">"\.(jpg|jpeg|JPEG|png|bmp|tiff)$"</span>, filename):</span><br><span class="line">                <span class="keyword">if</span> batch_nb == max_batches: <span class="comment"># If we limit the number of batches, just return earlier</span></span><br><span class="line">                    <span class="keyword">return</span> x_train_n2, x_train_down2</span><br><span class="line">                filepath = os.path.join(root, filename)</span><br><span class="line">                image = pyplot.imread(filepath)</span><br><span class="line">                <span class="keyword">if</span> len(image.shape) &gt; <span class="number">2</span>:</span><br><span class="line">                        </span><br><span class="line">                    image_resized = resize(image, (<span class="number">256</span>, <span class="number">256</span>)) <span class="comment"># Resize the image so that every image is the same size</span></span><br><span class="line">                    x_train_n.append(image_resized) <span class="comment"># Add this image to the high res dataset</span></span><br><span class="line">                    x_train_down.append(rescale(rescale(image_resized, <span class="number">0.5</span>), <span class="number">2.0</span>)) <span class="comment"># Rescale it 0.5x and 2x so that it is a low res image but still has 256x256 resolution</span></span><br><span class="line">                    batch += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> batch == batches:</span><br><span class="line">                        batch_nb += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                        x_train_n2 = np.array(x_train_n)</span><br><span class="line">                        x_train_down2 = np.array(x_train_down)</span><br><span class="line">                        </span><br><span class="line">                        <span class="keyword">if</span> just_load_dataset:</span><br><span class="line">                            <span class="keyword">return</span> x_train_n2, x_train_down2</span><br><span class="line">                        </span><br><span class="line">                        print(<span class="string">'Training batch'</span>, batch_nb, <span class="string">'('</span>, batches, <span class="string">')'</span>)</span><br><span class="line"></span><br><span class="line">                        autoencoder.fit(x_train_down2, x_train_n2,</span><br><span class="line">                            epochs=ep,</span><br><span class="line">                            batch_size=<span class="number">10</span>,</span><br><span class="line">                            shuffle=<span class="keyword">True</span>,</span><br><span class="line">                            validation_split=<span class="number">0.15</span>)</span><br><span class="line">                    </span><br><span class="line">                        x_train_n = []</span><br><span class="line">                        x_train_down = []</span><br><span class="line">                    </span><br><span class="line">                        batch = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train_n2, x_train_down2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Task-5-Load-the-Dataset-and-Pre-trained-Model"><a href="#Task-5-Load-the-Dataset-and-Pre-trained-Model" class="headerlink" title="Task 5: Load the Dataset and Pre-trained Model"></a>Task 5: Load the Dataset and Pre-trained Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train_n, x_train_down = train_batches(just_load_dataset=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &#39;constant&#39;, will be changed to &#39;reflect&#39; in skimage 0.15.  warn(&quot;The default mode, &#39;constant&#39;, will be changed to &#39;reflect&#39; in &quot;/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:24: UserWarning: The default multichannel argument (None) is deprecated.  Please specify either True or False explicitly.  multichannel will default to False starting with release 0.16.  warn(&#39;The default multichannel argument (None) is deprecated.  Please &#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.load_weights(<span class="string">"/home/rhyme/Desktop/Project/data/sr.img_net.mse.final_model5.no_patch.weights.best.hdf5"</span>)</span><br></pre></td></tr></table></figure><h3 id="Task-6-Model-Predictions-and-Visualizing-the-Results"><a href="#Task-6-Model-Predictions-and-Visualizing-the-Results" class="headerlink" title="Task 6: Model Predictions and Visualizing the Results"></a>Task 6: Model Predictions and Visualizing the Results</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder.load_weights(<span class="string">'/home/rhyme/Desktop/Project/data/encoder_weights.hdf5'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoded_imgs = encoder.predict(x_train_down)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoded_imgs.shape</span><br></pre></td></tr></table></figure><pre><code>(256, 64, 64, 256)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We clip the output so that it doesn't produce weird colors</span></span><br><span class="line">sr1 = np.clip(autoencoder.predict(x_train_down), <span class="number">0.0</span>, <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image_index = <span class="number">251</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">i = <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_down[image_index])</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_down[image_index], interpolation=<span class="string">"bicubic"</span>)</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(encoded_imgs[image_index].reshape((<span class="number">64</span>*<span class="number">64</span>, <span class="number">256</span>)))</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(sr1[image_index])</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_n[image_index])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_28_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.rhyme.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; &lt;img src=&quot;https://www.rhyme.com/assets/img/logo-dark.png&quot; alt=&quot;Header&quot; st
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Deepfakes with Keras</title>
    <link href="https://zhangruochi.com/Understanding-Deepfakes-with-Keras/2020/07/30/"/>
    <id>https://zhangruochi.com/Understanding-Deepfakes-with-Keras/2020/07/30/</id>
    <published>2020-07-30T07:00:32.000Z</published>
    <updated>2020-07-30T07:03:40.057Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Understanding-Deepfakes-with-Keras"><a href="#Understanding-Deepfakes-with-Keras" class="headerlink" title="Understanding Deepfakes with Keras"></a>Understanding Deepfakes with Keras</h1><p><img src="DCGAN.png" alt="DCGAN"></p><h1 id="Task-1-Importing-Libraries-and-Helper-Functions"><a href="#Task-1-Importing-Libraries-and-Helper-Functions" class="headerlink" title="Task 1: Importing Libraries and Helper Functions"></a>Task 1: Importing Libraries and Helper Functions</h1><p>Please note: If you havenâ€™t already, please install the required packages by executing the code cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip3 install tensorflow==2.1.0 pillow matplotlib</span></span><br><span class="line"><span class="comment"># !pip3 install git+https://github.com/am1tyadav/tfutils.git</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tfutils</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Flatten, Conv2D, BatchNormalization</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Conv2DTranspose, Reshape, LeakyReLU</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">print(<span class="string">'TensorFlow version:'</span>, tf.__version__)</span><br></pre></td></tr></table></figure><pre><code>TensorFlow version: 2.1.0</code></pre><h1 id="Task-2-Importing-and-Plotting-the-Data"><a href="#Task-2-Importing-and-Plotting-the-Data" class="headerlink" title="Task 2: Importing and Plotting the Data"></a>Task 2: Importing and Plotting the Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = tfutils.datasets.mnist.load_data(one_hot=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">x_train = tfutils.datasets.mnist.load_subset([<span class="number">0</span>], x_train, y_train)</span><br><span class="line">x_test = tfutils.datasets.mnist.load_subset([<span class="number">0</span>], x_test, y_test)</span><br><span class="line"></span><br><span class="line">x = np.concatenate([x_train, x_test], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><pre><code>(6903, 784)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tfutils.datasets.mnist.plot_ten_random_examples(plt, x, np.zeros((x.shape[<span class="number">0</span>], <span class="number">1</span>))).show()</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAB9AAAAPoCAYAAACGXmWqAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAH0KADAAQAAAABAAAD6AAAAADMYby4AABAAElEQVR4Aezde5CWdd34cdZdEaKaPKAWCgVM4qmkUBqjQe3gITymNXkqGzKK6TCOJSqUmZIYTqUJ2jh4mBIdTcUstbTCcBqlMcwkcYqwRAOVLCXFA/tcV/YoPAqfD/vc9+59Xferf37s7nu/9/V9fb/erXye7dfRXfynn/8QIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIE2F9iszfdv+wQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBA4D8CBuguAgECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKAQM0F0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQCBiguwYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAQMEB3DQgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQCFggO4aECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBQsAA3TUgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKFgAG6a0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoBA3TXgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFAIG6K4BAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAoBAzQXQMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAIGKC7BgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBAwQHcNCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAIWCA7hoQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFCwADdNSBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAoWAAbprQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECgEDdNeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUAgborgEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECgEDNBdAwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUAgYoLsGBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgEDBAdw0IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAhYIDuGhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgULAAN01IECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEChYABumtAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQKAQN014AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQCBuiuAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKAQM0F0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQCBiguwYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAQMEB3DQgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQCFggO4aECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBQsAA3TUgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKFgAG6a0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoBA3TXgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFAIG6K4BAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAoBAzQXQMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAIGKC7BgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBAwQHcNCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAIWCA7hoQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFCwADdNSBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAoWAAbprQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECgEDdNeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUAgborgEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECgEDNBdAwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUAgYoLsGBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgEDBAdw0IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAhYIDuGhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgULAAN01IECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEChYABumtAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQKAQN014AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQCBuiuAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKAQM0F0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQCBiguwYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAQMEB3DQgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQCFggO4aECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBQsAA3TUgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKFgAG6a0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoBA3TXgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFAIG6K4BAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAoBAzQXQMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAIGKC7BgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBAwQHcNCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAIWCA7hoQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFCwADdNSBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAoWAAbprQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECgEDdNeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUAgborgEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECgEDNBdAwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUAgYoLsGBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgEDBAdw0IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAhYIDuGhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgULAAN01IECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEChYABumtAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQKAQN014AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQCBuiuAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKAQM0F0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQCBiguwYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAQMEB3DQgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQCFggO4aECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBQsAA3TUgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKFgAG6a0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoBA3TXgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFAIG6K4BAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAoBAzQXQMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAIGKC7BgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBAwQHcNCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAIWCA7hoQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFCwADdNSBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAoWAAbprQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECgEDdNeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUAgborgEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECgEDNBdAwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUAgYoLsGBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgEDBAdw0IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAhYIDuGhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgULAAN01IECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEChYABumtAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQKAQN014AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQCBuiuAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKAQM0F0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQCBiguwYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAQMEB3DQgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQCFggO4aECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBQsAA3TUgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKFgAG6a0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoBA3TXgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFAIG6K4BAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAoBAzQXQMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAIGKC7BgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBAwQHcNCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAIWCA7hoQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFCwADdNSBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAoWAAbprQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECgEDdNeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUAgborgEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECgEDNBdAwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUAgYoLsGBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgEDBAdw0IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAhYIDuGhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgULAAN01IECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEChYABumtAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQKAQN014AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQCBuiuAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKAQM0F0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQCBiguwYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAQMEB3DQgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQCFggO4aECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBQsAA3TUgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKFgAG6a0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoBA3TXgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFAIG6K4BAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAoBAzQXQMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAIGKC7BgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBAwQHcNCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAIWCA7hoQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFCwADdNSBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAoWAAbprQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECgEDdNeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUAgborgEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECgEDNBdAwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUAgYoLsGBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgEDBAdw0IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAhYIDuGhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgULAAN01IECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEChYABumtAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQKAQN014AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQCBuiuAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKAQM0F0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQCBiguwYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAQMEB3DQgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQCFggO4aECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBQsAA3TUgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKFgAG6a0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoBA3TXgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFAIG6K4BAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAoBAzQXQMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAIGKC7BgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBAwQHcNCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAIWCA7hoQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFCoIsCAQL1FXj22Wf73Xffff/Z4ODBg/t1dflHvr6nbWcEei7wwgsv9Hvsscf+s8Duu+/eb8CAAT1frIW+03tgCx2GRyHQwgJ1fA/0/tfCF86jEWgxAe+BLXYgHocAgV4TqOP7X4nn58Beu0JeiEClBer6HljpQ/HwLSdgmtZyR+KBCDROoBye77XXXo1b0EoECNRe4O677+6355571mKf3gNrcYw2QaBXBeryHuj9r1evjRcjUBsB74G1OUobIUBgEwXq8v5XbtvPgZt4+HICBPrV6T3QcRJopID/CfdGalqLAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBCor4DfQK3t0HpxALFD+z7b/73/K/0uyN7/5zf/7of+XAAECLws8+uijL/+vVaz7vvFyUNE/rLsX74EVPUSPTaAXBOr4Huj9rxcujpcgUBMB74E1OUjbIEBgkwXq+P5XIvg5cJOvgm8g0JYCdX0PbMvDtOmmCRigN43WwgT6XmDd/z/Py+H5Djvs0PcP5QkIEGhpgXXfN1r6QRMPt+5evAcmwCQECPRb932jyhzr7sP7X5VP0rMT6F2Bdd87eveVG/tq6+7De2Bjba1GoK4C675vVH2P6+7Fe2DVT9PzE+gdgXXfN3rnFb0KgWoI+J9wr8Y5eUoCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaLKAAXqTgS1PgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAtUQMECvxjl5SgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBosoABepOBLU+AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC1RAwQK/GOXlKAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiygAF6k4EtT4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLVEDBAr8Y5eUoCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaLKAAXqTgS1PgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAtUQMECvxjl5SgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBosoABepOBLU+AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC1RAwQK/GOXlKAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiygAF6k4EtT4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLVEDBAr8Y5eUoCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaLKAAXqTgS1PgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAtUQMECvxjl5SgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBosoABepOBLU+AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC1RAwQK/GOXlKAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiygAF6k4EtT4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLVEDBAr8Y5eUoCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaLKAAXqTgS1PgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAtUQMECvxjl5SgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBosoABepOBLU+AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC1RAwQK/GOXlKAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiygAF6k4EtT4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLVEDBAr8Y5eUoCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaLKAAXqTgS1PgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAtUQMECvxjl5SgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBosoABepOBLU+AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC1RAwQK/GOXlKAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiygAF6k4EtT4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLVEDBAr8Y5eUoCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaLKAAXqTgS1PgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAtUQMECvxjl5SgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBosoABepOBLU+AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC1RAwQK/GOXlKAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiyQFeT17c8AQIECFRc4Lbbbgt3MHfu3LApg5/97Gdhd+utt4ZNGeyyyy6pTkSAAAECBAgQIECAAAECBAgQIECg3QUWL16cInj00UfD7qabbgqbMvjOd76T6no72nXXXVMvOW/evLAbMWJE2AgIEKiegN9Ar96ZeWICBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaIKAAXoTUC1JgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAtUTMECv3pl5YgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBogoABehNQLUmAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC1RMwQK/emXliAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiCgAF6E1AtSYAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLVEzBAr96ZeWICBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaIKAAXoTUC1JgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAtUTMECv3pl5YgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBogkBXE9a0JAECBAj0ocDMmTNTr/7UU0+luuuuuy7sFi9eHDbZ4IADDkilv/jFL8Ju5MiRYSMgQIAAAQIECLSTwFlnnZXa7lVXXRV2999/f9iUwRZbbBF2RxxxRNiUwbhx48LuM5/5TNiUQWdnZ6oTESCw6QIPPfRQ+E2PP/542JTBmDFjwm6zzXr/d4Tuvvvu8LmywbBhw1LpNttsk+pEBAi0vsC8efNSD7lw4cJUl4luuummTNbv97//farLRB0dHZms15vs32WecMIJ4bNddtllYVMGw4cPT3UiAgRaQ6D3f7psjX17CgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgsJ6AAfp6HD4gQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgXYVMEBv15O3bwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBYT8AAfT0OHxAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAuwoYoLfryds3AQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECKwnYIC+HocPCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKBdBQzQ2/Xk7ZsAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE1hMwQF+PwwcECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAg0K4CXe26cfsmQIBAFQWWLVsWPvbMmTPDpgxWrlyZ6gYMGBB248ePD5symD9/ftgtX748bMpg+vTpYTdnzpywERAgQGBjApn3raVLl25siT772oQJE1KvPXjw4FQnIkDgJYGTTjopRfHDH/4w7DLvMeUiL7zwQrjWcccdFzZlcO+996a67u7usBs2bFjYlMEWW2wRdnPnzg2bMsh03/nOd1Jr3XzzzaluxIgRqU5EoOoCjz32WLiF66+/PmzKIPPP6p133plaa7PN4t//6ezsTK3VyGivvfZKLZd5tqOPPjq11t577x12J554YtgICBBorsDtt98evsCkSZPCpgxWrFiR6kTNEViwYEG48IMPPhg2ZTB8+PBUJyJAoDUE4p9AW+M5PQUBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiqgAF6U3ktToAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQJVETBAr8pJeU4CBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaKqAAXpTeS1OgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAlURMECvykl5TgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBoqoABelN5LU6AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECVREwQK/KSXlOAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiqgAF6U3ktToAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQJVETBAr8pJeU4CBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaKpAV1NXtzgBAgQIpASWLVuW6s4+++ywe+yxx8KmDAYNGpTqvvzlL4fdFltsETZlMH/+/FSXibbccstMpiFAoM0Esu+B1113XUrmlFNOCbt//vOfYdMXwb777pt62ZNOOinVvfOd7wy7HXfcMWwEBFpZ4L777gsf7/LLLw+bMli1alXYnXHGGWFTBr/73e/C7sEHHwybMhg9enSqmzFjRtiNHTs2bMqgs7Mz7LLvy5dddlm41i9+8YuwKYMPfehDqW7u3Llht9dee4WNgECrC0yaNCl8xB//+MdhI9h0gSuvvDL1TZlu5cqVqbWmTp2a6kQECGy6QObnwBUrVmz6wr6jJQUOOuig1HOtXbs21YkIEGgNAb+B3hrn4CkIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoI8FDND7+AC8PAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAi0hoABemucg6cgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgT4WMEDv4wPw8gQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECDQGgIG6K1xDp6CAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBPpYwAC9jw/AyxMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAawgYoLfGOXgKAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEOhjAQP0Pj4AL0+AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECrSFggN4a5+ApCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKCPBbr6+PW9PAECBAgUAgceeGDK4cEHH0x1mWjChAmZrN9Xv/rVsNt+++3DptHBiBEjGr2k9QgQ6COBu+++O/XK99xzT9jNmjUrbMrgvvvuS3VVjn75y1+mHj/b7bTTTuF6X/va18KmDD7+8Y+nOhGB3hY45ZRTwpdctWpV2JTB3Llzw27GjBlhUwaZnwEPP/zw1FqzZ89Oddttt12qa1R03HHHpZY67LDDwi773wVTp04N1yqDgw8+OOwuuuiisCmD7DmlFhMRSApk//m64YYbwhU7OzvDRtC3AjfeeGPqAbLvganFRATaRGD16tWpnV5xxRWprh2iT37yk+E2Bw8eHDZl8K1vfSvViQgQINAIAb+B3ghFaxAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBA5QUM0Ct/hDZAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAo0QMEBvhKI1CBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKDyAgbolT9CGyBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBRggYoDdC0RoECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUHkBA/TKH6ENECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAjBAzQG6FoDQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBCovIABeuWP0AYIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBECXY1YxBoECBAgsGGBL3zhCxv+4n+/smTJkrDJBtOmTUulX//611NdJrryyiszWb8PfOADqS4T3XHHHWH2uc99LmwEBAj0TOCxxx5LfeM111wTdp///OfDpgzWrl2b6kTNEcj8d9Xy5cub8+JWJfD/FHj44YdTK/zmN78Ju6FDh4ZNGTz66KNht2jRorApg3e+851hd/HFF4dNGQwePDjVtWr0hje8IXy0U045JWzKoKOjI9WdfvrpYTd58uSwKYMPf/jDYde/f/+wERAoBSZNmpSCyP77Wm//rLXzzjunnv8Pf/hDquvtaM8990y95OOPPx52f/3rX8MmGyxcuDCVZv99edasWan1RASqLvCtb30r3MIVV1wRNmVw//33p7pWjd7xjneEj5a1GDZsWLhW9mef7u7ucK2ZM2eGjYAAAQIZAb+BnlHSECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEDtBQzQa3/ENkiAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECGQED9IyShgABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgRqL2CAXvsjtkECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQyAgYoGeUNAQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQewED9NofsQ0SIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQEbAAD2jpCFAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACB2gsYoNf+iG2QAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBDICBugZJQ0BAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI1F6gq/Y7tEECBAg0SeCee+5JrfzLX/4y7Do6OsKmDLbddtuw+/SnPx02jQ7GjRuXWnKfffYJu/nz54dNGfzkJz8Ju0WLFoVNGeyxxx6pTkSgXQT+9re/hVvdf//9w6YM/vjHP6a6Vo0GDhwYPtratWvDpgzWrFmT6kQECDRe4Gc/+1lq0SeffDLstthii7ApgwsuuCDsNt9887Apg0svvTTsBg8eHDaC9QW+8pWvrP+JDXzU3d29ga+88ukpU6a88sFG/jRx4sSNfPWlL11xxRVhIyBQCmT/PbKzs7NhYNm1Mv+OePnllzfsufpioYULF6Ze9te//nXYTZ48OWzK4IEHHkh1IgIEXhF45JFHXvlgI3+64447NvLVl750//33h00rB2PHjk09Xubv3LbaaqvUWo2Mtt9++0YuZy0CBAhsVMBvoG+UxxcJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoF0EDNDb5aTtkwABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQ2KmCAvlEeXyRAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBdhEwQG+Xk7ZPAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIENiogAH6Rnl8kQABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTaRcAAvV1O2j4JECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAYKMCBugb5fFFAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGgXAQP0djlp+yRAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBjQp0bfSrvkiAAAECGxT47Gc/u8GvrfuFxYsXr/vha/550KBBr/n5//vJm2666f9+6lUf77DDDq/6XLM/0b9//9RLXHjhhWG37777hk0ZrFy5Muy++93vhk0ZXHrppalORKDqApdccklqC+edd17YPfDAA2HTysEJJ5yQeryTTjop7JYvXx42ZTBz5sywu+2228KmlYNbb7019Xgnn3xyqhMRaJRA5p+/7GutWLEim4bdiSeeGDZlMHr06FQnao7AscceGy48Y8aMsCmDG2+8MdWJCNxxxx0hwoIFC8JmU4KhQ4eG+emnnx42ZTBu3Liw23HHHcOmDsH73ve+cBvvfe97w6YMGvkzePb+ZLrMeac2KCKwCQLPPfdcqp44cWKqu+WWW1JdK0Z777136rHmzJmT6rbaaqtUJyJAgECdBfwGep1P194IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIC1ggJ6mEhIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAnQUM0Ot8uvZGgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAmkBA/Q0lZAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE6ixggF7n07U3AgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEEgLGKCnqYQECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUGcBA/Q6n669ESBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEBawAA9TSUkQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgToLGKDX+XTtjQABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTSAl3pUkiAAAEC6wksXLhwvY839EFHR8eGvvTy50eOHPnynzf2h3e/+90b+3LLf23nnXcOn3Hw4MFhUwYrV64Mu2eeeSZsBATqIHDqqaemtjFjxoxU193dnep6O5o4cWL4kqNHjw6bMpg0aVKq22yz+P/edLfddkut9YEPfCDsDj744LApg5tvvjnV9Xa0Zs2a3n5Jr0cgJbB8+fJU18howIAB4XKf/exnw0bQ9wJDhgwJH2LKlClhUwbTp09PdSICDzzwQIiwZMmSsNmUIPPvYpmfxzblNbV9J5C5Y+XTZbpx48b13Ua8ctsKHHPMMam933LLLamuVaO3vOUt4aP96Ec/Cpsy2G677VKdiAABAgT69Yv/RpASAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBoAwED9DY4ZFskQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgVjAAD02UhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAGwgYoLfBIdsiAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECMQCBuixkYIAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE2kDAAL0NDtkWCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCAWMECPjRQECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAg0AYCBuhtcMi2SIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKxQFecKAgQINB+AlOnTu3VTff26/Xq5rwYAQL/L4Grr746/P7zzjsvbMqgu7s71WWi/v37Z7J+l1xySdjtt99+YVMG22+/fdh1dnaGTV8FmWe75pprUo/35JNPht2ECRPCpgwWLVqU6jLRIYccksk0BBomcN9996XWWrNmTaprZHTKKaeEy+2xxx5hI6iGQEdHRzUe1FNWRiDzc9uLL77Y0P1kXrOhL2ixlwWy9pkzX7t27cvr+gOBqgrcc8894aPfe++9YdPKwd577516vLFjx4bddtttFzatHKxatSr1eL/97W9TnYgAAQKNEPAb6I1QtAYBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIVF7AAL3yR2gDBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQINAIAQP0RihagwABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQqL2CAXvkjtAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQaISAAXojFK1BgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABApUXMECv/BHaAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAg0QsAAvRGK1iBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBygsYoFf+CG2AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBBohYIDeCEVrECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEDlBboqvwMbIECAwCYIrF69OlVfffXVYdfd3R02ZfDRj3407I444oiwaZfgYx/7WGqr06ZNS3UiAlUXuPfee8MtPP/882GzKcHAgQPD/Pzzzw+bMjjuuONSneglgUGDBqUonn766bB79tlnw0ZAoOoCy5YtS21hzZo1qW7IkCFh97a3vS1syuDII49MdaJ6CBx66KGpjUyZMiXsbr/99rApg/e///2pTlRNgY6OjvDBOzs7w2ZTgsxrbsp62rxA1r6RZ97ItfI7Vba7wOLFi1MEn/rUp8LuT3/6U9i0cpD9d+XPfOYzrbyNhjzb5MmTU+tk/r42tZCIAAECCQG/gZ5AkhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBA/QUM0Ot/xnZIgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgkBA/QEkoQAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE6i9ggF7/M7ZDAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEEgIGKAnkCQECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUH8BA/T6n7EdEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEBCwAA9gSQhQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgfoLGKDX/4ztkAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQSAgboCSQJAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECNRfoKv+W7RDAgQIvCKwcOHCVz7YyJ+WLl26ka++9KWOjo6wKYORI0emOtFLAldffXWKIuufWkxEoA8EVqxYkXrV2bNnp7pGRnvuuWe43MSJE8NGsOkCF198ceqbZs2aFXYPPPBA2GxKsPnmm4f5tttuGzYCAo0UWL16dSOX6/epT30qXO/MM88MG0H7CcyfP79hm/ZzbsMoLUSgMgLHHHNM6llvvfXWsHvooYfCpgzGjRvX0C61mKjtBcaPH58yeOKJJ1Jdq0aHHHJI+GjHHnts2NQhmDRpUriNa6+9NmxaORg6dGgrP55nI0CghwJ+A72HcL6NAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBOolYIBer/O0GwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBDooYABeg/hfBsBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI1EvAAL1e52k3BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQINBDAQP0HsL5NgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBCol4ABer3O024IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoIcCBug9hPNtBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAvAQP0ep2n3RAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBADwW6evh9vo0AAQJtLzBs2LCUwcSJE1OdiACB9hL47ne/m9rwk08+meoaGY0YMaKRy1V6rVWrVoXP/69//StsyuC0004Lu2uvvTZsyuD5559PdZlo8803z2T9Tj755LA7/vjjw0ZAoJECN954YyOX6/fGN76xoetZrH0Esu/fb3jDG0KU/fbbL2wE9Rc44ogjwk3edtttYVMGN9xwQ6oT9Z3Ar371q9SLP/HEE6kuE40aNSqT9ct2qcVEbS/w+OOPpww6OjpSXW9HAwYMSL3khAkTwm7QoEFh01dB5u8hrrrqqtTjLVy4MOxefPHFsOmrYMyYMeFL/+AHPwgbAQEC1RPwG+jVOzNPTIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQJNEDBAbwKqJQkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgegIG6NU7M09MgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAk0QMEBvAqolCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKB6Agbo1TszT0yAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECTRAwQG8CqiUJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoHoCBujVOzNPTIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQJNEDBAbwKqJQkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgegIG6NU7M09MgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAk0Q6GrCmpYkQIBAWwiMHDkytc83velNqU5EgEB7CXzzm9/s9Q1n37dOP/30Xn+23n7BZcuWpV7ygAMOCLslS5aETV8E/fv3T73sl7/85VR31llnpToRAQIE6ibw17/+NdzSXXfdFTYCApsisM0224T5VlttFTZl8OKLL6a6hQsXht1RRx0VNmUwe/bssMvsMVykJsHDDz+c2snTTz8ddmvXrg0bAQECPRM499xzU984ceLEVNfb0fe+973US/70pz8Nu1tuuSVsWjnYaaedUo83Z86csHv7298eNgICBKon4DfQq3dmnpgAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEmiBggN4EVEsSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQPUEDNCrd2aemAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgSaIGCA3gRUSxIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBA9QQM0Kt3Zp6YAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBJogYIDeBFRLEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgED1BAzQq3dmnpgAAQIEAM97bQAAQABJREFUCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEmiBggN4EVEsSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQPUEuqr3yJ6YAAECrSFw++23px7k3nvvDbvx48eHTR2C5cuXh9v4xz/+ETbZYLfddsumOgJtIXD00Uen9jlixIhU14rR97///dRjzZgxI9UtXbo01fV2NGrUqPAlf/7zn4dNGeywww6pTkSgFQV23HHHhj7WnXfeGa538sknh42gXgKrV68ON/TPf/4zbMpg1113TXUiAhmBjo6OTNavs7Mz1WWiefPmZbJ+gwYNCruZM2eGTRlss802qa7KUV+cZZW9PDuBvhLYbrvtUi+9aNGiVJeJPvGJT4TZww8/HDZl8O9//zvVrVmzJtW1YpT9+4wFCxakHn/rrbdOdSICBOon4DfQ63emdkSAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECPRAwQO8Bmm8hQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgfoJGKDX70ztiAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgR6IGCA3gM030KAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC9RMwQK/fmdoRAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECPRAwAC9B2i+hQABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTqJ2CAXr8ztSMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQ6IGAAXoP0HwLAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECNRPwAC9fmdqRwQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECDQA4GuHnyPbyFAgEDLCTzzzDOpZ7rssstSXXd3d9hlmnKRVatWhWtVPXjqqadSW9h3333D7pFHHgmbMthll13C7oQTTggbAYF2Epg+fXpqu0888UTY7bbbbmHT6ODss88Ol8y+h6xduzZcq9HBu971rnDJL3zhC2FTBh/96EfDbuDAgWEjIFB1gcMPPzy1hW9/+9upbvHixWG3evXqsCmDQYMGpTpR6wvMmzevYQ955JFHNmwtCxE45phjUgi33nprqnvooYdSXSa68sorw+zf//532JTBO97xjrCbOnVq2LRLMHTo0NRWjz766FQnIkDgFYGPfexjr3zgT70uMGbMmPA1L7/88rApg6233jrViQgQaF8Bv4Hevmdv5wQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCwjoAB+joY/kiAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC7StggN6+Z2/nBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQILCOgAH6Ohj+SIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLtK2CA3r5nb+cECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgsI6AAfo6GP5IgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAu0rYIDevmdv5wQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCwjoAB+joY/kiAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC7SvQ1b5bt3MCBOokMHDgwNR23vKWt6S6jo6OVJeJrr766jA7/PDDw6avgr/97W/hS3/7298OmzL485//HHZZ+/POOy9ca8iQIWEjINBOAi+88EJquxdeeGGqa4do7Nix4TYnT54cNmUwYcKEsNtyyy3DRkCAwCsC73nPe175YCN/yv6s+OCDD25klZe+9Lvf/S5symDcuHGpTtR3AjNmzEi9+LRp08Ju9913D5syOO2001KdiEBGIPs+89Of/jSzXL9dd9011TUquuGGG1JL3XzzzWHX2dkZNmVw6qmnprpGRtddd124XNYiXKgIBg8enMn891RKSUSAQG8IXHXVVamXyfy8tfPOO6fWEhEgQCAS8BvokZCvEyBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEBbCBigt8Ux2yQBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIRAIG6JGQrxMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAWwgYoLfFMdskAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECEQCBuiRkK8TIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQFsIGKC3xTHbJAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAhEAgbokZCvEyBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEBbCBigt8Ux2yQBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIRAIG6JGQrxMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAWwh0tcUubZIAAQL/FTjxxBNTFnPmzAm7lStXhk0ZzJs3L+y+9KUvhU0ZDBw4MOw++MEPhk0ZrFq1KtV99atfDbslS5aETTaYNWtWKt1///1TnYhAqwrMmDEj9WjTpk0Lu+eeey5s2iXYcsstU1s9//zzU92BBx4YdltvvXXYCAgQ6FuB0047LfUAU6ZMCbtjjz02bMrgzjvvDLshQ4aEjWB9gaeeemr9T7zGRxdeeOFrfPbVn8r8d2z5XV1d8V+dZNfq37//qx/EZwg0WWDUqFGpV7jrrrvCbuzYsWHT6ODpp58Ol5w6dWrYlEG2yyy2du3aTNZvs8169/eXuru7U88lIkCAwIYEXve6123oSy9/fo899nj5zxv7Q+bvFbN/l9nR0bGxl/I1AgQINFSgd3+Ca+ijW4wAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECDROwAC9cZZWIkCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEKCxigV/jwPDoBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQINE7AAL1xllYiQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgQoLGKBX+PA8OgECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAg0TsAAvXGWViJAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBCgsYoFf48Dw6AQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECDROwAC9cZZWIkCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEKCxigV/jwPDoBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQINE6gq3FLWYkAAQKtL/DWt7419ZAHHXRQ2P385z8PmzJ45JFHwu6CCy4Im2xw7rnnptLu7u5U19HREXb9+/cPmzI444wzwu7jH/942AgI1EHgK1/5Smobo0ePDrvp06eHTRmsWLEi1bVqNG3atPDRhg8fHjZlMHbs2FQnIkCgHgKTJ09ObeSiiy4Ku2XLloVNGUyYMCHsMu9r5SL77LNPuNZWW20VNpsS/P3vfw/zF198MWzKYM2aNWF37bXXhk0ZZM7oL3/5S2qt7H9nnHPOOeF6Rx11VNgICLS6wODBg8NHHD9+fNiUwYIFC1Jdo6LOzs5GLdXwdXr72Y444oiG78GCBBolMHXq1NRS8+fPD7vefp8JH6gCwaGHHpp6yszPnl/84hdTa4kIECBQVQG/gV7Vk/PcBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQINBQAQP0hnJajAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgSqKmCAXtWT89wECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAg0FABA/SGclqMAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBKoqYIBe1ZPz3AQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECDQUAED9IZyWowAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEqipggF7Vk/PcBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQINBQAQP0hnJajAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgSqKtBV1Qf33AQIEGimwJw5c8LlFy9eHDZlMHv27LBbuHBh2JRBtkstloxGjhwZltOmTQubMjj22GNTnYgAgVcEPvjBD77ywQb+lGk28K0+TYAAgbYQeP3rX5/a5zXXXBN2hx12WNiUwaJFi8LuIx/5SNiUwbbbbht2O++8c9hsSnDXXXeF+bPPPhs22aCjoyOVvvGNbwy7448/PmzKYPr06aluyJAhqU5EoOoCw4YNC7cwa9assCmDBQsWhF32n8GHHnooXKtdgtNOOy3c6qmnnho2AgJ9JfCNb3wj9dJLly4Nu0MOOSRsyiD793epxfoguuKKK8JXzf6skv15cfvttw9fU0CAAIG6C/gN9LqfsP0RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQErAAD3FJCJAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBugsYoNf9hO2PAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBFICBugpJhEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI1F3AAL3uJ2x/BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJASMEBPMYkIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoO4CBuh1P2H7I0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIGUgAF6iklEgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAnUXMECv+wnbHwECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAikBLpSlYgAAQIEXiWwyy67vOpzr/WJCy644LU+7XMECBAgQIAAAQItJjBmzJjwiZYsWRI2ZXDqqaeG3apVq8KmDJYtWxZ2v/71r8OmDNauXZvq9thjj7Dbddddw6YM+vfvH3ZHHnlk2JTBQQcdlOpEBAg0R2DUqFGphTPduHHjUms988wzYXf99deHTRmcc845qa6R0eGHHx4uN2XKlLApg5122inViQhUXWD48OHhFv7whz+EjYAAAQIECPRUwG+g91TO9xEgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBArQQM0Gt1nDZDgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAj0VMEDvqZzvI0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFaCRig1+o4bYYAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEeipggN5TOd9HgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABArUSMECv1XHaDAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAj0VMAAvadyvo8AAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEaiVggF6r47QZAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEOipQFdPv9H3ESBAgAABAgQIECBAgACBdhMYNGhQasvnn39+qhMRIECgXQVGjRrVsK2PHj06tdaZZ56Z6kQECBAgQIAAAQLtLeA30Nv7/O2eAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBP4rYIDuKhAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgULAAN01IECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEChYABumtAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQKAQN014AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQCBuiuAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKAQM0F0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQCBiguwYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAQMEB3DQgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQCFggO4aECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBQsAA3TUgQIAAAQIECBAgQIAAAQIE/oe9ew+yurzvB342rgsooAEjwRukE6kSjbcEvES666SZ1EuKY3AitpmAtabGar20WqfN7lKnMTSo0XZq03ptJ+OkRoMWaybBXUaroUaIktZ7Kq4KVhAjXrjV/fHFn5slsPs83+V895zvc17849nveZ/P93len+PDbj6eDQECBAgQIECAAAECBAgQIEBgq4ABurcBAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBDYKmCA7m1AgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgS2ChigexsQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIGtAgbo3gYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQGCrgAG6twEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIENgqYIDubUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBLYKGKB7GxAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAga0CBujeBgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAYKuAAbq3AQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQ2CpggO5tQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEtgoYoHsbECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBrQIG6N4GBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEBgq4ABurcBAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBDYKmCA7m1AgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgS2ChigexsQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIGtAs0UCBBIV2DLli19m1u1alXfYw8IECDQX6D/+dD/3OifKePj/nvpv8cy7sWaCRAoTqD/+dD/3CjujsVX7r+P/vsr/s7uQIBA2QT6nxH9z46y7aP/evvvo//++mc8JkCAQP/zof+5UXaZ/nvpv8ey78v6CRCorkD/86H/uVHdu6hGoNwCBujl7p/VExhU4LXXXut7ftq0aX2PPSBAgMBAAtm5MXny5IGeLtV1Z2Cp2mWxBOpCIJUz0PlXF28niyBQOgFnYOlaZsEECFRJIJXzL+PwfWCV3hTKEGgggZTOwAZqm60Og4Bf4T4MyG5BgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAvUv0NS79U/9L9MKCRAYisCGDRsqK1as2PbSj3zkI5XmZr90YiiOXkMgdYHsVzV98F+pH3744ZWRI0cmsWVnYBJttAkChQukeAY6/wp/27gBgWQEnIHJtNJGCBDIKZDi+ZcR+D4w5xtBnECDCqR6BjZoO227IAED9IJglSVAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBcgn4Fe7l6pfVEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEBBAgboBcEqS4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLlEjBAL1e/rJYAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEChIwQC8IVlkCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKJeAAXq5+mW1BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFCQgAF6QbDKEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEC5BAzQy9UvqyVAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBggQM0AuCVZYAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEyiVggF6uflktAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQk0FxQXWUJECBAoA4FXvzli5Xrl15fWfTsokr2eMRuIyofH/fxypmfOLNy/qfPr+yx+x51uGpLIkCAwK4LOP923VAFAgTKK+AMLG/vrJwAgV0XcAbuuqEKBAiUU8D5V86+WTUBAvUh0NS79U99LMUqCBAgQKBIgUXPLKqcfdfZlV9u/OVOb/Ob43+zct/Z91V+48O/sdPnXSRAgEBZBZx/Ze2cdRMgUA0BZ2A1FNUgQKCsAs7AsnbOugkQ2FUB59+uCno9AQKNLmCA3ujvAPsnQKAhBB5f/Xjl+JuPr7yz+Z3K6JbRlT//zJ9X2ia3Vd7d8m7ljp/fUfnHZf+4zeGQfQ6pPHruo9syDQFjkwQIJC/g/Eu+xTZIgMAgAs7AQXA8RYBA8gLOwORbbIMECAwg4PwbAMZlAgQI5BDYrWPrnxx5UQIECBAoocBZ3z+r8uzrz1aaP9RceeDLD1RmHz67cuBeB1Y+9uGPVU77zdMqe+6+Z+VHv/hRZc07ayojm0dWfmvyb5Vwl5ZMgACBHQWcfzuauEKAQOMIOAMbp9d2SoDAjgLOwB1NXCFAoDEEnH+N0We7JECgWIEPFVtedQIECBCotcCjLz9a6X6he9syzjnqnMpxBx63w5IuPf7SyqH7HLrt+nU/ua6y+f8275BxgQABAmUTcP6VrWPWS4BANQWcgdXUVIsAgbIJOAPL1jHrJUCgWgLOv2pJqkOAQKMLGKA3+jvA/gkQSF7gB0/9oG+Pc46c0/e4/4MPNX2o8uUjvrzt0roN6/oG7v0zHhMgQKBsAs6/snXMegkQqKaAM7CammoRIFA2AWdg2TpmvQQIVEvA+VctSXUIEGh0AQP0Rn8H2D8BAskLPPjig9v2mP2a9mP2O2bA/f7WpF/92vaHXnxowJwnCBAgUBYB519ZOmWdBAgUIeAMLEJVTQIEyiLgDCxLp6yTAIFqCzj/qi2qHgECjSpggN6onbdvAgQaRuDJNU9u2+vHx3182/8H+kAbP2SfQ/qe+uA1fRc8IECAQAkFPjjLnH8lbJ4lEyCwywLOwF0mVIAAgRILOANL3DxLJ0BglwScf7vE58UECBDoEzBA76PwgAABAukJbNiyobLmnTXbNnbA2AMG3eCHR324kn1KPfvT82bPoFlPEiBAoN4FnH/13iHrI0CgSAFnYJG6ahMgUO8CzsB675D1ESBQlIDzryhZdQkQaEQBA/RG7Lo9EyDQMALrN67v2+voltF9jwd6sGfL+wP0tza9NVDEdQIECJRCwPlXijZZJAECBQk4AwuCVZYAgVIIOANL0SaLJECgAAHnXwGoShIg0LACBugN23obJ0CgEQSy//L0gz8tu7V88HDAf47YbcS2597d/O6AGU8QIECgDALOvzJ0yRoJEChKwBlYlKy6BAiUQcAZWIYuWSMBAkUIOP+KUFWTAIFGFTBAb9TO2zcBAg0hMLJ5ZN8+N/3fpr7HAz3Y+H8btz01avdRA0VcJ0CAQCkEnH+laJNFEiBQkIAzsCBYZQkQKIWAM7AUbbJIAgQKEHD+FYCqJAECDStggN6wrbdxAgQaQWDMiDF924z5texvb3p7Wz7m1733FfaAAAECdSjg/KvDplgSAQLDJuAMHDZqNyJAoA4FnIF12BRLIkBgWAScf8PC7CYECDSIgAF6gzTaNgkQaEyB7L883WePfbZt/qU3XxoUYd276ypvb35/gH7g2AMHzXqSAAEC9S7g/Kv3DlkfAQJFCjgDi9RVmwCBehdwBtZ7h6yPAIGiBJx/RcmqS4BAIwoYoDdi1+2ZAIGGEjh0n0O37fe515+rbHlvy4B7f2rNU33PffCavgseECBAoIQCH5xlzr8SNs+SCRDYZQFn4C4TKkCAQIkFnIElbp6lEyCwSwLOv13i82ICBAj0CRig91F4QIAAgTQFPnPQZ7ZtLPt0+WOvPDbgJpesXNL33AkHndD32AMCBAiUVcD5V9bOWTcBAtUQcAZWQ1ENAgTKKuAMLGvnrJsAgV0VcP7tqqDXEyBA4H0BA3TvBAIECCQuMPOQmX07vOVnt/Q97v/gvd73Krc/fvu2S3uP3LvSNrmt/9MeEyBAoJQCzr9Sts2iCRCokoAzsEqQyhAgUEoBZ2Ap22bRBAhUQcD5VwVEJQgQILBVwADd24AAAQKJC0zbf1rlxINO3LbLm5bfVHmk55Eddrzg4QWVJ9c8ue36RdMvquy+2+47ZFwgQIBA2QScf2XrmPUSIFBNAWdgNTXVIkCgbALOwLJ1zHoJEKiWgPOvWpLqECDQ6AJNvVv/NDqC/RMgQCB1geWrlldOuPmEyrtb3q2MbhldufIzV1baPtZWeXfzu5U7fn5H5TvLvrONYMr4KZWfnvvTypgRY1InsT8CBBpEwPnXII22TQIEdirgDNwpi4sECDSIgDOwQRptmwQI7CDg/NuBxAUCBAjkFjBAz03mBQQIECinwL1P31v5vbt/r/Lmxjd3uoFseL5o9qLKx8d9fKfPu0iAAIGyCjj/yto56yZAoBoCzsBqKKpBgEBZBZyBZe2cdRMgsKsCzr9dFfR6AgQaXcAAvdHfAfZPgEBDCax8Y2Xl20u/XVn07KLKS2++VGnZrWXbwHzW1FmVC6ZdUNlj9z0aysNmCRBoHAHnX+P02k4JENhRwBm4o4krBAg0joAzsHF6bacECGwv4Pzb3sNXBAgQyCNggJ5HS5YAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEkhX4ULI7szECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJBDwAA9B5YoAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECKQrYICebm/tjAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgRyCBig58ASJUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIF0BQzQ0+2tnREgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBADgED9BxYogQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQroABerq9tTMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQyCFggJ4DS5QAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE0hUwQE+3t3ZGgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAjkEDNBzYIkSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQLoCBujp9tbOCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCCHgAF6DixRAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEEhXwAA93d7aGQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjkEDBAz4ElSoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLpChigp9tbOyNAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBHAIG6DmwRAkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgXQED9HR7a2cECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkEPAAD0HligBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIpCtggJ5ub+2MAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBHIIGKDnwBIlQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgXQFDNDT7a2dESBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAOAQP0HFiiBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJCugAF6ur21MwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBDIIWCAngNLlAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTSFTBAT7e3dkaAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECOQQM0HNgiRIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAugIG6On21s4IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIIeAAXoOLFECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQSFfAAD3d3toZAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECOQQMEDPgSVKgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAukKGKCn21s7I0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEcAgboObBECRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCBdAQP0dHtrZwQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQQ8AAPQeWKAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAikK2CAnm5v7YwAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEcggYoOfAEiVAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBdAUM0NPtrZ0RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQA4BA/QcWKIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkK6AAXq6vbUzAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEMghYICeA0uUAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBNIVMEBPt7d2RoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQI5BAzQc2CJEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEC6Agbo6fbWzggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgh4ABeg4sUQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBIV8AAPd3e2hkBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI5BAwQM+BJUqAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC6QoYoKfbWzsjQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgRwCBug5sEQJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIF0BA/R0e2tnBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJBDwAA9B5YoAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECKQrYICebm/tjAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgRyCBig58ASJUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIF0BQzQ0+2tnREgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBADgED9BxYogQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQroABerq9tTMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQyCFggJ4DS5QAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE0hUwQE+3t3ZGgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAjkEDNBzYIkSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQLoCBujp9tbOCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCCHgAF6DixRAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEEhXwAA93d7aGQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjkEDBAz4ElSoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLpChigp9tbOyNAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBHAIG6DmwRAkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgXQED9HR7a2cECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkEPAAD0HligBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIpCtggJ5ub+2MAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBHIIGKDnwBIlQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgXQFDNDT7a2dESBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAOAQP0HFiiBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJCugAF6ur21MwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBDIIWCAngNLlAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTSFTBAT7e3dkaAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECOQQM0HNgiRIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAugIG6On21s4IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIIeAAXoOLFECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQSFfAAD3d3toZAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECOQQMEDPgSVKgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAukKGKCn21s7I0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEcAgboObBECRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCBdAQP0dHtrZwQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQQ8AAPQeWKAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAikK2CAnm5v7YwAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEcggYoOfAEiVAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBdAUM0NPtrZ0RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQA4BA/QcWKIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkK6AAXq6vbUzAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEMghYICeA0uUAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBNIVMEBPt7d2RoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQI5BAzQc2CJEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEC6Agbo6fbWzggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgh4ABeg4sUQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBIV8AAPd3e2hkBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI5BAwQM+BJUqAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC6QoYoKfbWzsjQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgRwCBug5sEQJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIF0BA/R0e2tnBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJBDwAA9B5YoAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECKQrYICebm/tjAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgRyCBig58ASJUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIF0BQzQ0+2tnREgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBADgED9BxYogQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQroABerq9tTMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQyCFggJ4DS5QAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE0hUwQE+3t3ZGgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAjkEDNBzYIkSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQLoCBujp9tbOCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCCHgAF6DixRAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEEhXwAA93d7aGQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjkEDBAz4ElSoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLpChigp9tbOyNAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBHAIG6DmwRAkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgXQED9HR7a2cECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkEPAAD0HligBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIpCtggJ5ub+2MAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBHIIGKDnwBIlQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgXQFmtPdmp0RILBhw4bKihUrtkF85CMfqTQ3+1feu4IAgR0FtmzZUnnttde2PXH44YdXRo4cuWOohFecgSVsmiUTqIFAimeg868GbyS3JFBSAWdgSRtn2QQI7LJAiudfhuL7wF1+ayhAoCEEUj0DG6J5NjlsAqZpw0btRgSGXyAbnk+bNm34b+yOBAiUVuA///M/K5/+9KdLu/7+C3cG9tfwmACBGIFUzkDnX0y3ZQgQ+HUBZ+Cvi/iaAIFGEUjl/Mv65fvARnnX2ieB6gmkdAZWT0UlApWKX+HuXUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBLYK+AS6twGBhAWyX9v+wZ/svySbOHHiB1/6JwECBPoEVq1a1ffbKvqfG32Bkj7ovxdnYEmbaNkEhkEgxTPQ+TcMbxy3IJCIgDMwkUbaBgECuQVSPP8yBN8H5n4reAGBhhRI9QxsyGbadGECBuiF0SpMoPYC/f8/z7Ph+QEHHFD7RVkBAQJ1LdD/3KjrhUYsrv9enIERYCIECFT6nxtl5ui/D+dfmTtp7QSGV6D/2TG8d67u3frvwxlYXVvVCKQq0P/cKPse++/FGVj2blo/geER6H9uDM8d3YVAOQT8Cvdy9MkqCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKBgAQP0goGVJ0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFyCBigl6NPVkmAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBQsYoBcMrDwBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIlEPAAL0cfbJKAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEChYwAC9YGDlCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAcAgbo5eiTVRIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAwQIG6AUDK0+AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC5RAwQC9Hn6ySAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoWMEAvGFh5AgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECiHgAF6OfpklQQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQsIABesHAyhMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAOQQM0MvRJ6skQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgYIFDNALBlaeAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMohYIBejj5ZJQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgULGCAXjCw8gQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQDgED9HL0ySoJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoGABA/SCgZUnQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgXIIGKCXo09WSYAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIFCxigFwysPAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAiUQ8AAvRx9skoCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKFjAAL1gYOUJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBwCBujl6JNVEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEDBAgboBQMrT4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLlEDBAL0efrJIAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEChYwQC8YWHkCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKIeAAXo5+mSVBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFCwgAF6wcDKEyBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEA5BAzQy9EnqyRAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBggUM0AsGVp4AAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEyiFggF6OPlklAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBQsYIBeMLDyBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFAOAQP0cvTJKgkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgYAED9IKBlSdAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBcggYoJejT1ZJgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgULNBdcX3kCBAgQIECAAAECBAgQIEBgFwQWLlwYfPXMmTODmTyBuXPnBuOjRo0KZrLAvHnzgrlx48YFMwIECBAYSGDTpk0DPbXd9a6uru2+HuiLJUuWDPRU3/VHHnmk7/FgD4477rjBns713OzZs4P5ww47LJgRIECAAAECBAgQGFzAJ9AH9/EsAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECDSIgAF6gzTaNgkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEBgcAED9MF9PEuAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECDSJggN4gjbZNAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEBhcwAB9cB/PEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgECDCBigN0ijbZMAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEBhcwQB/cx7MECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAg0CACBugN0mjbJECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHBBQzQB/fxLAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAg0iEBzg+zTNgkQINAwAh0dHVXda7XrVXVxihEgQODXBK6++upfu7LzL6+88sqdPzGEq6ecckrUq2666aZgbt999w1mBAgQSEdg6dKlUZv5m7/5m2CuqakpmMkTuPnmm4Px2Hvec889wVr/8i//EsxkgRkzZkTlhAgQyC+wefPm4IvWrFkTzGSB8ePHB3MtLS3BTBZ45513grkvfvGLwUwWuP/++6Nyvb29wVzsGbhkyZJgrdjAP//zPwejd911VzCTBY488sio3O677x6VEyJAgAABAgQIpCTgE+gpddNeCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQGDIAgboQ6bzQgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBIScAAPaVu2gsBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIDFnAAH3IdF5IgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAikJGKCn1E17IUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEhCxigD5nOCwkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgJQED9JS6aS8ECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgMGQBA/Qh03khAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECKQk0JzSZuyFAAEC9SjQ3d0dXFZMJivS2dkZrFXtwJIlS4Il29vbg5lqB1pbW6tdUj0CBOpcYMWKFcEV/v3f/30wkwWampqicjGhRYsWxcQqhx12WDDX1tYWzGSBo48+Opj7/d///WAmC+y3335ROSECBPIJLF++PPiCSy+9NJjJAg8//HAwN3bs2GAmC5x00klRubvvvjsqFxN66aWXgrE777wzmMkCM2bMiMoJESDwK4FNmzb96otBHn39618f5Nn3n5o/f34wkwUuv/zyYO7KK68MZrLA3/3d3wVz999/fzCTJ7DPPvsE4+PGjQtmskCM/8qVK6Nqvfzyy8Hc9OnTg5kssHjx4qhc7PenUcWECBAgQIAAAQIlEfAJ9JI0yjIJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoFgBA/RifVUnQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgZIIGKCXpFGWSYAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLFChigF+urOgECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAiURMAAvSSNskwCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKFbAAL1YX9UJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoCQCBuglaZRlEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgECxAgboxfqqToAAAQIECBAgQIAAAQIECBAgQIAAAYub1KcAAEAASURBVAIECBAgQIAAAQIlETBAL0mjLJMAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEihVoLra86gQIEEhXoK2tLWpz3d3dUbnhDrW3t0fdcsmSJcFcZ2dnMJMFqmnR2toavGdXV1cwI0CAQLECPT09wRv85Cc/CWaywEUXXRTMvfrqq8FMrQJr164N3vrOO+8MZrJATK6lpSWq1sUXXxyVEyJA4H2BN954I4riK1/5SjC3YsWKYCYLnHDCCcFczLmQFZkwYUKwVhb43Oc+F8z9+Mc/DmZiA9WsFXtPOQKNIrBy5cqorc6fPz8qFxO67bbbgrHYnw+XLl0arLXvvvsGM1lg1qxZUbnzzz8/mDv00EODmSywfv36YO68884LZrLAsmXLgrlnnnkmmMkCMX9PZbl77rkn+8egf4444ohBn/ckAQL1IbBu3bqohWzYsCEqFxPac889g7GxY8cGMwIECBAYbgGfQB9ucfcjQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgboUMECvy7ZYFAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgMt4AB+nCLux8BAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI1KWAAXpdtsWiCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQGC4BQzQh1vc/QgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgLgUM0OuyLRZFgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAsMtYIA+3OLuR4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQJ1KWCAXpdtsSgCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQGG4BA/ThFnc/AgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEKhLgea6XJVFESBAoMYCbW1twRV0d3cHM9UOtLa2Bkt2dXUFM7UKdHR0BG/d2dkZzGSBGP+YPma12tvbs38E/8T4B4sIECiBwPr164Or/Ku/+qtgJgvceuutwdzatWuDmSzQ29sbzDU1NQUzjRK49tpro7Z6yimnROWmTJkSlRMikLrA3Llzo7b4xBNPBHPTp08PZrLAwoULg7lx48YFM3kCd955ZzB+xBFHBDNZYOXKlcHcG2+8EcxkgVdffTWYmzBhQjAjQCAFgU2bNkVt44YbbojKVTO0evXqYLmYTLDI/w/cddddUdHjjz8+KlfN0JgxY4Llvvvd7wYzWeDxxx8P5mJ/bu3p6QnWygLXX399MHfTTTcFMwIEqi2wZcuWqJL33ntvVG7BggXB3OzZs4OZLHD++edH5WJCK1asCMb++q//OpjJAj/+8Y+jcrE/o8cU23///YOxSZMmBTNZ4Itf/GIwN2fOnGAmC+y1115ROSECBBpXwCfQG7f3dk6AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC/QQM0PtheEiAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECjStggN64vbdzAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEOgnYIDeD8NDAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGhcAQP0xu29nRMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAPwED9H4YHhIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBA4woYoDdu7+2cAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBPoJGKD3w/CQAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBBpXoLlxt27nBAg0okBbW1vUtru7u6Ny1Qr19vZWq1Rd1+no6Aiur7W1NZjJAp2dncFcbB9jc11dXcF7xq4/WEiAQAEC69evj6oa8z5evnx5VK1qhmpxVv73f/93cAv7779/MJMFZs6cGcw9/PDDwUwW2LhxYzDX09MTzGSBv/3bv43KXX/99VE5IQJlFnjnnXeCy3/++eeDmdhAzHmb1Ro3blxsyWHNxZ7LMblVq1ZFrX3+/PnB3IIFC4IZAQIpCKxduzZqG7F/10cVq0HowgsvDN71U5/6VDCTQuCII44IbuPmm28OZrLAnDlzonL33XdfMPf6668HM1mgXv8+i1q8UN0JxLw3s0WfccYZUWuP+fcrqtDWUMz3lPPmzYsqF/Pv9Jo1a6JqNTU1ReVOPvnkYG7q1KnBTBaI6dNzzz0XVeuSSy4J5q655ppgJgucffbZUbmrrroqmNttt92CGQECBMon4BPo5euZFRMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAAQIG6AWgKkmAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAEC5RMwQC9fz6yYAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAoQMEAvAFVJAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECifgAF6+XpmxQQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQgIABegGoShIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBA+QQM0MvXMysmQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgQIEDNALQFWSAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMonYIBevp5ZMQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUINBcQE0lCRAgMOwCHR0dUffs7u6OysWEWltbY2KVrq6uqJzQ+wKxrjG5tra2KNbY90VMvd7e3qh7ChGohcDMmTOjbrt8+fJgrqmpKZipRWDs2LFRt504cWJU7vTTTw/mnnzyyWAmCyxevDiYO/7444OZLLB06dKoXExo2bJlMTEZAg0hcM011wT3uWLFimAmNnDYYYfFRoc9F3O2vfLKK1Hr2nvvvYO5t956K5jJAtdee20wN2bMmGAmC8T+DBFVTIgAgdwCF110UdRrrr766mCupaUlmGmUQMz3r5nFvHnzokgef/zxYO6yyy4LZrLAd77znWCuudn/XB1EaoDAiy++GNzl2WefHcxkgQMPPDAq98gjjwRz7733XjCTBWLWtnDhwqhae+21VzB33nnnBTNZIPbcPeSQQ6LqxYTmz58fjN19993BTBY444wzgrmXXnopmMkC3/zmN6NyRx99dDA3a9asYEaAAIHyCfgEevl6ZsUECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUICAAXoBqEoSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQPkEDNDL1zMrJkCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIECBAzQC0BVkgABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTKJ2CAXr6eWTEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFCBggF4AqpIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUD4BA/Ty9cyKCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAAAQP0AlCVJECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHyCTSXb8lWTIAAgR0FOjs7d7y4C1fa29uDr+7o6AhmBGor0NXVFbWApqamqFxMqK2tLSZWiV1bVDEhApECzz33XGSyvLGJEydGLf7kk0+Oyl133XXB3DPPPBPMZIEpU6ZE5YY7tHbt2qhbxuTGjx8fVUuIQL0K3H333VVb2uc///lgrZkzZwYztQpMnz49eOv7778/mMkCU6dODeYuueSSYCYL3HHHHcHc9773vWAmC1x22WVRudGjR0flhAgMt8A//MM/DPcto+934IEHBrMXX3xxMJMFRowYEZUTqp3ArbfeGnXzG264IZhrbvY/VweRShx47733olb/9a9/PZh7++23g5ks8NnPfjYqN3LkyGBu2bJlwUwWWLhwYTC31157BTNZIMYi9jyNumENQqecckrUXS+66KJg7sYbbwxmssDGjRujcldddVUwN2PGjGAmC0yYMCEqJ0SAQH0I+AR6ffTBKggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgxgIG6DVugNsTIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQH0IGKDXRx+sggABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgRqLGCAXuMGuD0BAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI1IeAAXp99MEqCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKDGAgboNW6A2xMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAfQgYoNdHH6yCAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBGosYIBe4wa4PQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjUh4ABen30wSoIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoMYCzTW+v9sTIEAgKNDd3R3MVDvQ0dFR7ZLq1bFAe3t71Oo6OzuDudj3a0yutbU1eD8BApnAvHnzoiBWr14dlRvu0NKlS6Nu+dRTTwVz9913XzCTBa677rqoXExo4sSJMbGozLnnnhuVizWLKfbMM8/ExCrPPvtsMDd+/PhgRoBAPQv09PRUbXmzZs0K1ho9enQwU8+Bk046qWrL+9KXvhRV65577gnmnnzyyWAmC3zrW9+KyvnZIIpJqAYC69atq8Fd4245Z86cYPCggw4KZgSKE7jwwgujip9zzjlROSECMQL/9m//FhOr3H777cHcwQcfHMxkgWuvvTYqFxOK/XkzptbYsWNjYpXzzjsvKlfmUEtLS9TyY3r52GOPRdV66KGHonIrVqwI5hYvXhzMZIHZs2dH5YQIEKgPgQ/VxzKsggABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI1FbAAL22/u5OgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAnUiYIBeJ42wDAIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBCorYABem393Z0AAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE6kTAAL1OGmEZBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFBbAQP02vq7OwECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjUiYABep00wjIIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoLYCBui19Xd3AgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEKgTgeY6WYdlECBAYECBtra2AZ/L+0R7e3vel8g3gEBHR0fULjs7O6NyMaHu7u5grLW1NZgRSF/ghRdeCG7yxhtvDGaywObNm6Nyvb29wdykSZOCmSxwww03BHOf+tSngpksEJM76aSTomotWLAgKvfKK68Ec2PGjAlmYgNz5syJisb8fdbT0xNVKza0fv362KgcAQIEcgt84QtfiHrNmWeeGczdcsstwUwW+P73vx+Vi/1eMaqYEIFIgbfeeiuYjP0eMFgoR+Cggw6KSn/lK1+JygnVTmDPPfes3c3duWEFZs2aVbW9jx8/PqrW5MmTo3IxoT/4gz+IiVX222+/YO7II48MZrLAqFGjonJC7wvcdtttURQnn3xyVO7pp58O5ubNmxfMZIHZs2dH5YQIEKgPAZ9Ar48+WAUBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI1FjAAL3GDXB7AgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEKgPAQP0+uiDVRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAjQUM0GvcALcnQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgfoQMECvjz5YBQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjUWMAAvcYNcHsCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqA8BA/T66INVECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgECNBQzQa9wAtydAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACB+hAwQK+PPlgFAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECNRYoLnG93d7AgQIECBQGoH29vbgWjs7O4OZLLBkyZKonBCBVatWBRFeffXVYCYLNDU1ReUmTZoUzF1xxRXBTBY49dRTo3LVCu23337VKrWtzsSJE6tar1rFYnoZk8nWE5tbvXp1tZavDoG6Fejt7Q2uLSaTFdl3332DtQTyC5xwwgnBF91yyy3BTBaI/fszqpgQgSoLxJw1mzZtqvJdw+Vivk/MqkyePDlcTKKmAt/4xjei7h/zXowqJERgq0DsuRXzM8rJJ5887KYf/ehHo+45d+7cqJxQ9QU+9rGPRRX97Gc/G5V7+umng7lnnnkmmBEgQKB8Aj6BXr6eWTEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIFCBggF4AqpIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUD4BA/Ty9cyKCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKAAAQP0AlCVJECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHyCRigl69nVkyAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBQgYoBeAqiQBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIlE/AAL18PbNiAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEChAwAC9AFQlCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKB8Agbo5euZFRMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAAQLNBdRUkgABAnUrsGTJkrpdm4XVv0Bra2twkZ2dncFMFuju7o7KCRGohcCFF14YvO15550XzAgUJ3D00UcHi/f09AQzeQIHH3xwnrgsgboSePDBB6PWs379+mCuqakpmMkCp556alROKJ9ArH9M1TfeeCMmVnnggQeCuZNOOimYESCQR2DEiBHBeMzPJ1mRav4c/Itf/CK4rizwxBNPBHOf/OQngxmB2gtU89yt/W6soCwCvb29waUee+yxwYwAgYEEYt5j2WtjcwPdx3UCBMor4BPo5e2dlRMgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAFQUM0KuIqRQBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIlFfAAL28vbNyAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEKiigAF6FTGVIkCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHyChigl7d3Vk6AAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECVRQwQK8iplIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUF4BA/Ty9s7KCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKCKAgboVcRUigABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTKK9Bc3qVbOQECjSLQ2toa3Gp3d3cwI0BgVwU6Ozt3tUTf67u6uvoee0BgMIHHHntssKc916ACy5Yta9Cd2zaBoQmceOKJUS8cO3ZsMLdmzZpgJgusXLkymJs0aVIwI1CcwJ577hlVfOrUqVE5IQLVFGhpaQmWu+KKK4KZLLBkyZKoXEzo5ZdfjolVTj311GDuu9/9bjCTBaZNmxbMxXgFiyQSePvtt6N2snHjxqhcTOjMM8+MiVX0KYop6dCIESOi9rdp06Zg7rrrrgtmskDM/66Y5ZqbjUoyh0b5M3ny5KitNjU1ReWECBBIT8An0NPrqR0RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwBAEDNCHgOYlBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJCegAF6ej21IwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAYgoAB+hDQvIQAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE0hMwQE+vp3ZEgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAkMQMEAfApqXECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEB6Agbo6fXUjggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEBgCAIG6ENA8xICBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQSE/AAD29ntoRAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECAxBoHkIr/ESAgQIlFagu7u7tGu38OIE2traoorHvH9aW1ujasXmoooJJS1wzDHHBPfX29sbzOQJVLtennunnN24cWNwe7fcckswkwV6enqCudg+HnvsscFaWSA2F1VMiMAwCzz44INRd3zzzTejcjGhxYsXB2Nz584NZgSKE2hpaYkq/tGPfjQqJ0RguAU++clPDvcto+/30ksvBbMzZswIZrLAN7/5zWDuT//0T4OZFAJvvfVWcBt/+Id/GMxkgaeeeioqF/Pz8q233hpVa/fdd4/KCaUrcPXVV0dt7pJLLgnm7rvvvmAmC/zRH/1RVO6aa64J5saMGRPMCJRDYOrUqVVb6N577121WgoRIFA/Aj6BXj+9sBICBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqKGAAXoN8d2aAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBOpHwAC9fnphJQQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQQwED9BriuzUBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQI1I+AAXr99MJKCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKCGAgboNcR3awIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBCoHwED9PrphZUQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQA0FDNBriO/WBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIFA/As31sxQrIUCAwM4F2tvbd/5Ev6vd3d39vtr1h01NTcEivb29wYxAcQIxPe/s7IxaQEytqEJbQ11dXbFROQJVE4g5s/LcrNr18tw75eyiRYuC2/va174WzGSBavbotNNOi7qnEIEyC5x44olRyx87dmwwt2bNmmBGoDiBZ599tmrFp0yZUrVaChGohcC4ceOibnvOOedE5W666aaoXLVCRx11VFSp6dOnR+UaIXTPPfcEt3nHHXcEM3kCMX+Hjhw5Mk9J2QYWOOuss6J2/z//8z/B3A033BDMZIHYs+2RRx4J1rv44ouDmSzwO7/zO8HcfvvtF8wI5BfYsGFD1IsuvfTSqFxM6Ic//GFMTIYAgZIJ+AR6yRpmuQQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQjIABejGuqhIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAyQQM0EvWMMslQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgWIEDNCLcVWVAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBEomYIBesoZZLgECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUI2CAXoyrqgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQMgED9JI1zHIJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoBgBA/RiXFUlQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgZIJGKCXrGGWS4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLFCDQXU1ZVAgQIVE+gtbU1WCwmkxXp7u4O1ooNNDU1RUVj1tbe3l61WlGFqhyKdY3JLVmyJGp1MbWiCuUIxfYpR0lRAgQSEHjooYeidvHVr341Klet0B577BFVqq2tLSonRKARBPbdd9/gNl977bVgJgt8+9vfDubOOuusYCYLjBo1KipX5tA//dM/RS3/6quvjsrFhE4//fSYmAyBuhUYMWJE1Nquv/76qNzzzz8fzFXz57Arr7wyeL8sMGPGjKhcmUP//u//HrX8P/7jP47KxYRivwe8/PLLY8rJEIgSmDBhQlRuwYIFwVzs2fCtb30rWCsLLF26NJg799xzg5ksMHr06GDuhBNOCGayQExu2rRpUbWOPfbYqNzYsWOjctUKbdy4MarUvffeG8zddtttwUwWePnll6Nyl112WTB3zDHHBDMCBAiUT8An0MvXMysmQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgQIEDNALQFWSAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMonYIBevp5ZMQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgUIGCAXgCqkgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBQPgED9PL1zIoJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoAABA/QCUJUkQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgfIJGKCXr2dWTIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIFCBigF4CqJAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAiUT6C5fEu2YgIECOwo0NXVtePFnVxpa2vbydUdL3V3d+94cYhXYmrFZIZ4+4Z+WXt7e1X339HRUdV6ihGIEZg4cWIwFpPJiqxatSpYKwtce+21wdwpp5wSzGSBKVOmROXKHPrLv/zLqOWvXbs2Klet0OWXXx5V6thjj43KCRFoBIGbb745uM3Yf2eeeOKJYK1vfOMbwUwW+Iu/+ItgrqWlJZipVeDnP/958Nax32c1NTUFax111FHBTBa44IILonJCBMouMGrUqKgtxJxJp59+elSt1atXB3PnnntuMJMFYr6HevLJJ6NqzZ07Nyp3yCGHBHN33313MJMF1q1bF8xdccUVwUwW+OUvfxmViwn92Z/9WUysEvv+iSomRCBSoLk5PLY444wzoqqddtppUbmf/exnwdydd94ZzGSBxYsXB3PLly8PZrLAD3/4w6hcTOiAAw6IiVVGjx4dlatWaPPmzVGlnn/++WAudo/f+973grWywOc///monBABAukJ+AR6ej21IwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAYgoAB+hDQvIQAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE0hMwQE+vp3ZEgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAkMQMEAfApqXECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEB6Agbo6fXUjggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEBgCAIG6ENA8xICBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQSE/AAD29ntoRAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECAxBwAB9CGheQoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQLpCRigp9dTOyJAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBIQg0D+E1XkKAAIHSCnR1dUWtvbu7O5hra2sLZgS2F2hvb9/+wk6+am1t3cnVHS/F5nZ8pSsEyiUwefLk4IKvueaaYCYLfOlLX4rK9fT0BHNnn312MJMF7r///mBu/PjxwUxsYOPGjVHR//3f/43KnX/++cFczN8ZWZGmpqZgrdjAD37wg2D0C1/4QjAjQIDA9gJTp07d/sJOvorJZC/7r//6r528evtLV1111fYXBvjq6aefHuCZX12ePXv2r74Ypkd33XVX1J1+9KMfBXOrV68OZrLApEmTgrmvfe1rwUwWaGlpicoJEWgUgenTpwe3euONNwYzWWDmzJnB3BtvvBHMZIGvfvWrUbmY0O233x4Tq+y1117B3AsvvBDM1CJw4YUXRt3Wz9RRTEIJCMT+fT9t2rTgbmMywSL/P7B27dqo6Pr164O5mJ8PsyIrV64M1soCP/3pT6NyMaGDDz44GDvssMOCmSwQ8334cccdF1Ur5pyPKiREgECyAj6BnmxrbYwAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE8ggYoOfRkiVAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBZAUM0JNtrY0RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQB4BA/Q8WrIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkKyAAXqyrbUxAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEMgjYICeR0uWAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBJIVMEBPtrU2RoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQJ5BAzQ82jJEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgECyAgboybbWxggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgj0BznrAsAQIEGkWgtbU1uNXe3t5gptqBjo6OapcM1qvFPYOLEiBAYDuB448/fruvB/pin332Geip7a6vWbNmu6939sWyZct2dnmHa8ccc8wO1379wvTp03/90pC/fv3116Ne+8ADD0TlYkJNTU0xsUpM7rjjjouq9du//dtROSECBPIJjB49OviC//iP/whmssDnPve5YO7RRx8NZrLAv/7rvwZzMZlgkX6BmO91Y861fiUHfThhwoRBn//gyc7Ozg8eDvjPL3/5ywM+5wkCBHZN4BOf+ERUgd/93d8N5hYuXBjMVDuwbt26qJIx31NW8wyMWtTW0AUXXBCMzp8/P5jJAi0tLVE5IQIEihEYP358VOGY3J/8yZ9E1RIiQIAAgXgBn0CPt5IkQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgYQFDNATbq6tESBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEC8gAF6vJUkAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECCQsYICecHNtjQABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTiBQzQ460kCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCBhAQP0hJtrawQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECAQL2CAHm8lSYAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQOD/tXd3r1aXWRzAH/X4rlOWdTFpY3Uom0F0iHHoRcKuC7wp6IUgopuIuugi6sp/QKguCnoTgiCIcYKwm14oimnCsomCmimGMZ1xIMnJyqNpOe4d7nYcj7/fs/f+6X7W73Nu5tfeaz/nWZ91WEXfPEOAAIHAAgL0wMPVGgECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjUF5ioX6qSAAECBM60wObNm8/0FXx/AgTGUGDFihW1brV27dpada+99lqtujpFu3fvriyrU9M55NixY5VnzZo1q7LmTBWsWbOm8ltv3bq1sqZTsHDhwlp1iggQGL3AWWedVevQ5557rrLu8ccfr6zpFGzbtq2ybteuXZU1oy6o+/eVBx54oPJbb9iwobKmU3DBBRfUqlNEgEAzApdcckmtg1944YXKuscee6yyplPw6quvVtbVvdfixYsrzzoTBZs2bar1bdetW1dZNzHhX/dWIikgQIAAAQIECFQI+BPoFUDeJkCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIF2CAjQ2zFnXRIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAhYAAvQLI2wQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECDQDgEBejvmrEsCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqBAQoFcAeZsAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE2iEgQG/HnHVJgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAhUCAvQKIG8TIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQDsEBOjtmLMuCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKBCQIBeAeRtAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEGiHwEQ72tQlAQIECBAgQIDAtm3baiFcd911lXU7d+6srGlLwfXXX1+r1aeeeqqy7vzzz6+sUUCAQBkCk5OTlRfdsmVLZU2noG5drcMUESBA4DQITExU/yvHe++9t9ZN6tbVOkwRAQIECBAgQIAAgRoC/gR6DSQlBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIBBfQIAef8Y6JECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEaAgL0GkhKCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCC+gAA9/ox1SIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQI1BAToNZCUECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEB8AQF6/BnrkAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgRqCAjQayApIUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIH4AgL0+DPWIQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjUEJioUaOEAAECBAgQIEAggMDSpUtrdbFjx45adYoIECBAgAABAgQIECBAgAABAgQIECAQTcCfQI82Uf0QIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwEACAvSB2HyIAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBKIJCNCjTVQ/BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIDCQgAB9IDYfIkCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFoAgL0aBPVDwECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgMJCBAH4jNhwgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgmoAAPdpE9UOAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECAwkI0Adi8yECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQiCYgQI82Uf0QIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwEACAvSB2HyIAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBKIJCNCjTVQ/BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIDCQgAB9IDYfIkCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFoAgL0aBPVDwECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgMJCBAH4jNhwgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgmoAAPdpE9UOAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECAwkI0Adi8yECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQiCYgQI82Uf0QIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwEACAvSB2HyIAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBKIJCNCjTVQ/BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIDCQgAB9IDYfIkCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIFoAgL0aBPVDwECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgMJCBAH4jNhwgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgmoAAPdpE9UOAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECAwkI0Adi8yECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQiCYgQI82Uf0QIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwEACAvSB2HyIAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBKIJTERrSD8ECPwscPTo0d5f7N27t/fsgQABAv0C/fuhf2/015T43N9Lf48l9uLOBAg0J9C/H/r3RnPfsfmT+/vo76/57+w7ECBQmkD/jujfHaX10X/f/j76++uv8UyAAIH+/dC/N0qX6e+lv8fS+3J/AgRGK9C/H/r3xmi/i9MIlC0gQC97fm5P4JQCX375Ze/99evX9549ECBAYCaBzt5YtWrVTG8X9bodWNS4XJbAWAhE2YH231j8OLkEgeIE7MDiRubCBAiMSCDK/utw+OfAEf1QOIZAiwQi7cAWjU2rp0HAr3A/Dci+BQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAiMv8CsY8e/xv+abkiAwCAChw4dSh999FH3o+edd16amPBLJwZx9BkC0QU6v6rpxH+lvmbNmrRgwYIQLduBIcaoCQKNC0TcgfZf4z82vgGBMAJ2YJhRaoQAgUyBiPuvQ+CfAzN/EJQTaKlA1B3Y0nFquyEBAXpDsI4lQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbIE/Ar3subltgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECDQkIAAvSFYxxIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAWQIC9LLm5bYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAg0JCAAL0hWMcSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQFkCAvSy5uW2BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQINCQgAC9IVjHEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEBZAgL0subltgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECDQkIAAvSFYxxIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAWQIC9LLm5bYECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAg0JDAREPnOpYAAQIExlDgi6+/SI+++2ja/tn21HmeP2d+mjxnMt30u5vS3X+4Oy2au2gMb+1KBAgQGF7A/hve0AkECJQrYAeWOzs3J0BgeAE7cHhDJxAgUKaA/Vfm3NyaAIHxEJh17PjXeFzFLQgQIECgSYHt/9iebt12a/r68Ncn/TaXnXtZevnWl9PFyy4+6fteJECAQKkC9l+pk3NvAgRGIWAHjkLRGQQIlCpgB5Y6OfcmQGBYAftvWEGfJ0Cg7QIC9Lb/BOifAIFWCHz43w/TVc9clQ4eOZiWzFuSHrzmwbRx1cY0dXQqPf/x8+nJnU92HVYvX5123LWjW9MKGE0SIBBewP4LP2INEiBwCgE78BQ43iJAILyAHRh+xBokQGAGAftvBhgvEyBAIENgzubjXxn1SgkQIECgQIGb/3Rz+uyrz9LE7In0+u2vp1vW3JJWnrUyXbTsonTDZTekxXMXp1f++Urad3BfWjCxIF276toCu3RlAgQITBew/6abeIUAgfYI2IHtmbVOCRCYLmAHTjfxCgEC7RCw/9oxZ10SINCswOxmj3c6AQIECJxpgR3/3pHe+Ncb3Wvc+fs705Urr5x2pfuvuj9dvvzy7usP//XhdOSHI9NqvECAAIHSBOy/0ibmvgQIjFLADhylprMIEChNwA4sbWLuS4DAqATsv1FJOocAgbYLCNDb/hOgfwIEwgu8+OmLvR7vWHdH77n/Yfas2en2tbd3X9p/aH8vcO+v8UyAAIHSBOy/0ibmvgQIjFLADhylprMIEChNwA4sbWLuS4DAqATsv1FJOocAgbYLCNDb/hOgfwIEwgu89cVb3R47v6b9il9fMWO/1/7m51/b/vYXb89Y5w0CBAiUImD/lTIp9yRAoAkBO7AJVWcSIFCKgB1YyqTckwCBUQvYf6MWdR4BAm0VEKC3dfL6JkCgNQKf7Puk2+vkOZPd/w/0mRpfvXx1760Tn+m94IEAAQIFCpzYZfZfgcNzZQIEhhawA4cmdAABAgUL2IEFD8/VCRAYSsD+G4rPhwkQINATEKD3KDwQIEAgnsCho4fSvoP7uo2t+NWKUza4bOGy1PlT6p2v3Qd2n7LWmwQIEBh3Aftv3CfkfgQINClgBzap62wCBMZdwA4c9wm5HwECTQnYf03JOpcAgTYKCNDbOHU9EyDQGoFvDn/T63XJvCW955keFs/7KUD/9vtvZyrxOgECBIoQsP+KGJNLEiDQkIAd2BCsYwkQKELADixiTC5JgEADAvZfA6iOJECgtQIC9NaOXuMECLRBoPNfnp74mjdn3onHGf93/pz53femjkzNWOMNAgQIlCBg/5UwJXckQKApATuwKVnnEiBQgoAdWMKU3JEAgSYE7L8mVJ1JgEBbBQTobZ28vgkQaIXAgokFvT6//+H73vNMD4d/ONx9a+HchTOVeJ0AAQJFCNh/RYzJJQkQaEjADmwI1rEECBQhYAcWMSaXJECgAQH7rwFURxIg0FoBAXprR69xAgTaILB0/tJem3V+Lft333/Xra/z6957B3sgQIDAGArYf2M4FFciQOC0CdiBp43aNyJAYAwF7MAxHIorESBwWgTsv9PC7JsQINASAQF6SwatTQIE2inQ+S9Ply9a3m1+z4E9p0TYP7U/fXfkpwB95a9WnrLWmwQIEBh3Aftv3CfkfgQINClgBzap62wCBMZdwA4c9wm5HwECTQnYf03JOpcAgTYKCNDbOHU9EyDQKoHLl1/e7ffzrz5PR388OmPvn+77tPfeic/0XvBAgACBAgVO7DL7r8DhuTIBAkML2IFDEzqAAIGCBezAgofn6gQIDCVg/w3F58MECBDoCQjQexQeCBAgEFPgmguv6TbW+dPl7//n/RmbfHPXm733rr7w6t6zBwIECJQqYP+VOjn3JkBgFAJ24CgUnUGAQKkCdmCpk3NvAgSGFbD/hhX0eQIECPwkIED3k0CAAIHgAptWb+p1uPVvW3vP/Q8/HvsxPfvhs92Xzl5wdtq4amP/254JECBQpID9V+TYXJoAgREJ2IEjgnQMAQJFCtiBRY7NpQkQGIGA/TcCREcQIEDguIAA3Y8BAQIEggusv2B92nDhhm6XT3/wdHpn9zvTOt7yly3pk32fdF+/74/3pblz5k6r8QIBAgRKE7D/SpuY+xIgMEoBO3CUms4iQKA0ATuwtIm5LwECoxKw/0Yl6RwCBNouMOvY8a+2I+ifAAEC0QU+2PtBuvqZq9PU0am0ZN6S9NA1D6WNF21MU0em0vMfP5+e2PlEl+DScy9N7931Xlo6f2l0Ev0RINASAfuvJYPWJgECJxWwA0/K4kUCBFoiYAe2ZNDaJEBgmoD9N43ECwQIEMgWEKBnk/kAAQIEyhR46e8vpdv+fFs6cPjASRvohOfbb9meJs+ZPOn7XiRAgECpAvZfqZNzbwIERiFgB45C0RkECJQqYAeWOjn3JkBgWAH7b1hBnydAoO0CAvS2/wTonwCBVgns+t+u9Mi7j6Ttn21Pew7sSfPmzOsG5jf+9sZ0z/p70qK5i1rloVkCBNojYP+1Z9Y6JUBguoAdON3EKwQItEfADmzPrHVKgMAvBey/X3r4KwIECOQICNBztNQSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQFiB2WE70xgBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEMgQEKBnYCklQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbgCAvS4s9UZAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECGQICNAzsJQSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQFwBAXrc2eqMAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBDIEBOgZWEoJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIK6AAD3ubHVGgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAhkCAvQMLKUECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgEFdAgB53tjojQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgQwBAXoGllICBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQiCsgQI87W50RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQIaAAD0DSykBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIxBUQoMedrc4IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIENAgJ6BpZQAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE4goI0OPOVmcECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkCEgQM/AUkqAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECcQUE6HFnqzMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQyBAQoGdgKSVAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBuAIC9Liz1RkBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIZAgI0DOwlBIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAXAEBetzZ6owAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEMgQE6BlYSgkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgroAAPe5sdUaAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECGQIC9AwspQQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECAQV0CAHne2OiNAgAABAgQIECBAgAABAgQIECBAgACC3CviAAAOEElEQVQBAgQIECBAgACBDAEBegaWUgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBCIKyBAjztbnREgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAhoAAPQNLKQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjEFRCgx52tzggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgQ0CAnoGllAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTiCgjQ485WZwQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQISBAz8BSSoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQJxBQTocWerMwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBDIEBCgZ2ApJUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIG4AgL0uLPVGQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAhkCAjQM7CUEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEBcAQF63NnqjAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQyBAToGVhKCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCCugAA97mx1RoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIZAgL0DCylBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIBBXQIAed7Y6I0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEMAQF6BpZSAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEIgrIECPO1udESBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgECGgAA9A0spAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECMQVEKDHna3OCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCBDQICegaWUAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBOIKCNDjzlZnBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJAhIEDPwFJKgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAnEFBOhxZ6szAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEMgQEKBnYCklQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbgCAvS4s9UZAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECGQICNAzsJQSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQFwBAXrc2eqMAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBDIEBOgZWEoJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIK6AAD3ubHVGgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAhkCAvQMLKUECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgEFdAgB53tjojQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgQwBAXoGllICBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQiCsgQI87W50RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQIaAAD0DSykBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIxBUQoMedrc4IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIENAgJ6BpZQAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE4goI0OPOVmcECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkCEgQM/AUkqAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECcQUE6HFnqzMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQyBAQoGdgKSVAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBuAIC9Liz1RkBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIZAgI0DOwlBIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAXAEBetzZ6owAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEMgQE6BlYSgkQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgroAAPe5sdUaAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECGQIC9AwspQQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECAQV0CAHne2OiNAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBDAEBegaWUgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBCIKyBAjztbnREgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAhoAAPQNLKQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjEFRCgx52tzggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAgQ0CAnoGllAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTiCgjQ485WZwQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQISBAz8BSSoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQJxBQTocWerMwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBDIEBCgZ2ApJUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIG4AgL0uLPVGQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAhkCAjQM7CUEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEBcAQF63NnqjAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQyBAToGVhKCRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCCugAA97mx1RoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIZAgL0DCylBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIBBXQIAed7Y6I0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEMAQF6BpZSAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEIgrIECPO1udESBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgECGgAA9A0spAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECMQVEKDHna3OCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCBDQICegaWUAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBOIKCNDjzlZnBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJAhIEDPwFJKgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAnEFBOhxZ6szAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEMgQEKBnYCklQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbgCAvS4s9UZAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECGQICNAzsJQSIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQFwBAXrc2eqMAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBDIEBOgZWEoJECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIK6AAD3ubHVGgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAhkCAvQMLKUECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgEFdAgB53tjojQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgQwBAXoGllICBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQiCsgQI87W50RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQIaAAD0DSykBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIxBUQoMedrc4IECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAIENAgJ6BpZQAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE4goI0OPOVmcECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkCEgQM/AUkqAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECcQUE6HFnqzMCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQyBAQoGdgKSVAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBuAIC9Liz1RkBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIZAgI0DOwlBIgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIBAXAEBetzZ6owAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEMgT+D8hhBDDvJbKTAAAAAElFTkSuQmCC" width="1000"></p><h1 id="Task-3-Discriminator"><a href="#Task-3-Discriminator" class="headerlink" title="Task 3: Discriminator"></a>Task 3: Discriminator</h1><p><img src="artist_critic.png" alt="Artist and Critic"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">size = <span class="number">28</span></span><br><span class="line">noise_dim = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">discriminator = Sequential([</span><br><span class="line">    Conv2D(<span class="number">64</span>, <span class="number">3</span>, strides=<span class="number">2</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">128</span>, <span class="number">5</span>, strides=<span class="number">2</span>),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">256</span>, <span class="number">5</span>, strides=<span class="number">2</span>),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Flatten(),</span><br><span class="line">    Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">opt = tf.keras.optimizers.Adam(lr=<span class="number">2e-4</span>, beta_1=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">discriminator.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=opt, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">discriminator.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d (Conv2D)              (None, 13, 13, 64)        640       _________________________________________________________________leaky_re_lu (LeakyReLU)      (None, 13, 13, 64)        0         _________________________________________________________________batch_normalization (BatchNo (None, 13, 13, 64)        256       _________________________________________________________________conv2d_1 (Conv2D)            (None, 5, 5, 128)         204928    _________________________________________________________________leaky_re_lu_1 (LeakyReLU)    (None, 5, 5, 128)         0         _________________________________________________________________batch_normalization_1 (Batch (None, 5, 5, 128)         512       _________________________________________________________________conv2d_2 (Conv2D)            (None, 1, 1, 256)         819456    _________________________________________________________________leaky_re_lu_2 (LeakyReLU)    (None, 1, 1, 256)         0         _________________________________________________________________batch_normalization_2 (Batch (None, 1, 1, 256)         1024      _________________________________________________________________flatten (Flatten)            (None, 256)               0         _________________________________________________________________dense (Dense)                (None, 1)                 257       =================================================================Total params: 1,027,073Trainable params: 1,026,177Non-trainable params: 896_________________________________________________________________</code></pre><h1 id="Task-4-Generator"><a href="#Task-4-Generator" class="headerlink" title="Task 4: Generator"></a>Task 4: Generator</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">generator = Sequential([</span><br><span class="line">    Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>, input_shape=(noise_dim,)),</span><br><span class="line">    Reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>)),</span><br><span class="line">    </span><br><span class="line">    Conv2DTranspose(<span class="number">256</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    Conv2DTranspose(<span class="number">128</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2DTranspose(<span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    Conv2DTranspose(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2DTranspose(<span class="number">1</span>, <span class="number">4</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line"></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">generator.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential_1&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_1 (Dense)              (None, 256)               512       _________________________________________________________________reshape (Reshape)            (None, 1, 1, 256)         0         _________________________________________________________________conv2d_transpose (Conv2DTran (None, 5, 5, 256)         1638656   _________________________________________________________________batch_normalization_3 (Batch (None, 5, 5, 256)         1024      _________________________________________________________________conv2d_transpose_1 (Conv2DTr (None, 9, 9, 128)         819328    _________________________________________________________________batch_normalization_4 (Batch (None, 9, 9, 128)         512       _________________________________________________________________conv2d_transpose_2 (Conv2DTr (None, 21, 21, 64)        204864    _________________________________________________________________batch_normalization_5 (Batch (None, 21, 21, 64)        256       _________________________________________________________________conv2d_transpose_3 (Conv2DTr (None, 25, 25, 32)        51232     _________________________________________________________________batch_normalization_6 (Batch (None, 25, 25, 32)        128       _________________________________________________________________conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         513       =================================================================Total params: 2,717,025Trainable params: 2,716,065Non-trainable params: 960_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">noise = np.random.randn(<span class="number">1</span>, noise_dim)</span><br><span class="line">gen_image = generator.predict(noise)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(np.reshape(gen_image, (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">'binary'</span>);</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,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" width="640"></p><h1 id="Task-5-Generative-Adversarial-Network-GAN"><a href="#Task-5-Generative-Adversarial-Network-GAN" class="headerlink" title="Task 5: Generative Adversarial Network (GAN)"></a>Task 5: Generative Adversarial Network (GAN)</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input_layer = tf.keras.layers.Input(shape=(noise_dim,))</span><br><span class="line">gen_out = generator(input_layer)</span><br><span class="line">disc_out = discriminator(gen_out)</span><br><span class="line"></span><br><span class="line">gan = Model(</span><br><span class="line">    input_layer,</span><br><span class="line">    disc_out</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">discriminator.trainable = <span class="keyword">False</span></span><br><span class="line">gan.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=opt, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">gan.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         [(None, 1)]               0         _________________________________________________________________sequential_1 (Sequential)    (None, 28, 28, 1)         2717025   _________________________________________________________________sequential (Sequential)      (None, 1)                 1027073   =================================================================Total params: 3,744,098Trainable params: 2,716,065Non-trainable params: 1,028,033_________________________________________________________________</code></pre><h1 id="Tasks-6-and-7-Training-the-GAN"><a href="#Tasks-6-and-7-Training-the-GAN" class="headerlink" title="Tasks 6 and 7: Training the GAN"></a>Tasks 6 and 7: Training the GAN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">steps_per_epoch = int(<span class="number">2</span> * x.shape[<span class="number">0</span>]/batch_size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Steps per epoch='</span>, steps_per_epoch)</span><br><span class="line"></span><br><span class="line">dp = tfutils.plotting.DynamicPlot(plt, <span class="number">5</span>, <span class="number">5</span>, (<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">0</span>, epochs):</span><br><span class="line">    </span><br><span class="line">    dp.start_of_epoch(e)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">0</span>, steps_per_epoch):</span><br><span class="line">        true_examples = x[int(batch_size/<span class="number">2</span>)*step: int(batch_size/<span class="number">2</span>)*(step + <span class="number">1</span>)]</span><br><span class="line">        true_examples = np.reshape(true_examples, (true_examples.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        noise = np.random.randn(int(batch_size/<span class="number">2</span>), noise_dim)</span><br><span class="line">        generated_examples = generator.predict(noise)</span><br><span class="line"></span><br><span class="line">        x_batch = np.concatenate([generated_examples, true_examples], axis=<span class="number">0</span>)</span><br><span class="line">        y_batch = np.array([<span class="number">0</span>] * int(batch_size/<span class="number">2</span>) + [<span class="number">1</span>] * int(batch_size/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        indices = np.random.choice(range(batch_size), batch_size, replace=<span class="keyword">False</span>)</span><br><span class="line">        x_batch = x_batch[indices]</span><br><span class="line">        y_batch = y_batch[indices]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train the discriminator</span></span><br><span class="line">        discriminator.trainable = <span class="keyword">True</span></span><br><span class="line">        discriminator.train_on_batch(x_batch, y_batch)</span><br><span class="line">        discriminator.trainable = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># train the generator</span></span><br><span class="line">        loss, _ = gan.train_on_batch(noise, np.ones((int(batch_size/<span class="number">2</span>), <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        _, acc = discriminator.evaluate(x_batch, y_batch, verbose=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    noise = np.random.randn(<span class="number">1</span>, noise_dim)</span><br><span class="line">    generated_example = generator.predict(noise)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    dp.end_of_epoch(np.reshape(generated_example, (<span class="number">28</span>, <span class="number">28</span>)), <span class="string">'binary'</span>,</span><br><span class="line">                   <span class="string">'DiscAcc:&#123;:.2f&#125;'</span>.format(acc), <span class="string">'GANLoss:&#123;:.2f&#125;'</span>.format(loss))</span><br></pre></td></tr></table></figure><pre><code>Steps per epoch= 107&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,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" width="800"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Understanding-Deepfakes-with-Keras&quot;&gt;&lt;a href=&quot;#Understanding-Deepfakes-with-Keras&quot; class=&quot;headerlink&quot; title=&quot;Understanding Deepfakes 
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Auto-Complete</title>
    <link href="https://zhangruochi.com/Auto-Complete/2020/07/19/"/>
    <id>https://zhangruochi.com/Auto-Complete/2020/07/19/</id>
    <published>2020-07-19T09:45:54.000Z</published>
    <updated>2020-07-19T09:46:20.535Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Language-Models-Auto-Complete"><a href="#Language-Models-Auto-Complete" class="headerlink" title="Language Models: Auto-Complete"></a>Language Models: Auto-Complete</h1><p>In this assignment, you will build an auto-complete system.  Auto-complete system is something you may see every day</p><ul><li>When you google something, you often have suggestions to help you complete your search. </li><li>When you are writing an email, you get suggestions telling you possible endings to your sentence.  </li></ul><p>By the end of this assignment, you will develop a prototype of such a system.</p><p><img src="stanford.png" style="width:700px;height:300px;"></p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1 Load and Preprocess Data</a></li><li><a href="#1.1">1.1: Load the data</a></li><li><a href="#1.2">1.2 Pre-process the data</a><ul><li><a href="#ex-01">Exercise 01</a></li><li><a href="#ex-02">Exercise 02</a></li><li><a href="#ex-03">Exercise 03</a></li><li><a href="#ex-04">Exercise 04</a></li><li><a href="#ex-05">Exercise 05</a></li><li><a href="#ex-06">Exercise 06</a></li><li><a href="#ex-07">Exercise 07</a></li></ul></li><li><a href="#2">2 Develop n-gram based language models</a><ul><li><a href="#ex-08">Exercise 08</a></li><li><a href="#ex-09">Exercise 09</a>    </li></ul></li><li><a href="#3">3 Perplexity</a><ul><li><a href="#ex-10">Exercise 10</a></li></ul></li><li><a href="#4">4 Build an auto-complete system</a><ul><li><a href="#ex-11">Exercise 11</a></li></ul></li></ul><p>A key building block for an auto-complete system is a language model.<br>A language model assigns the probability to a sequence of words, in a way that more â€œlikelyâ€ sequences receive higher scores.  For example, </p><blockquote><p>â€œI have a penâ€<br>is expected to have a higher probability than<br>â€œI am a penâ€<br>since the first one seems to be a more natural sentence in the real world.</p></blockquote><p>You can take advantage of this probability calculation to develop an auto-complete system.<br>Suppose the user typed </p><blockquote><p>â€œI eat scrambledâ€<br>Then you can find a word <code>x</code>  such that â€œI eat scrambled xâ€ receives the highest probability.  If x = â€œeggsâ€, the sentence would be<br>â€œI eat scrambled eggsâ€</p></blockquote><p>While a variety of language models have been developed, this assignment uses <strong>N-grams</strong>, a simple but powerful method for language modeling.</p><ul><li>N-grams are also used in machine translation and speech recognition. </li></ul><p>Here are the steps of this assignment:</p><ol><li>Load and preprocess data<ul><li>Load and tokenize data.</li><li>Split the sentences into train and test sets.</li><li>Replace words with a low frequency by an unknown marker <code>&lt;unk&gt;</code>.</li></ul></li><li>Develop N-gram based language models<ul><li>Compute the count of n-grams from a given data set.</li><li>Estimate the conditional probability of a next word with k-smoothing.</li></ul></li><li>Evaluate the N-gram models by computing the perplexity score.</li><li>Use your own model to suggest an upcoming word given your sentence. </li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.data.path.append(<span class="string">'.'</span>)</span><br></pre></td></tr></table></figure><p><a name="1"></a></p><h2 id="Part-1-Load-and-Preprocess-Data"><a href="#Part-1-Load-and-Preprocess-Data" class="headerlink" title="Part 1: Load and Preprocess Data"></a>Part 1: Load and Preprocess Data</h2><p><a name="1.1"></a></p><h3 id="Part-1-1-Load-the-data"><a href="#Part-1-1-Load-the-data" class="headerlink" title="Part 1.1: Load the data"></a>Part 1.1: Load the data</h3><p>You will use twitter data.<br>Load the data and view the first few sentences by running the next cell.</p><p>Notice that data is a long string that contains many many tweets.<br>Observe that there is a line break â€œ\nâ€ between tweets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"en_US.twitter.txt"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line">print(<span class="string">"Data type:"</span>, type(data))</span><br><span class="line">print(<span class="string">"Number of letters:"</span>, len(data))</span><br><span class="line">print(<span class="string">"First 300 letters of the data"</span>)</span><br><span class="line">print(<span class="string">"-------"</span>)</span><br><span class="line">display(data[<span class="number">0</span>:<span class="number">300</span>])</span><br><span class="line">print(<span class="string">"-------"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Last 300 letters of the data"</span>)</span><br><span class="line">print(<span class="string">"-------"</span>)</span><br><span class="line">display(data[<span class="number">-300</span>:])</span><br><span class="line">print(<span class="string">"-------"</span>)</span><br></pre></td></tr></table></figure><pre><code>Data type: &lt;class &#39;str&#39;&gt;Number of letters: 3335477First 300 letters of the data-------&quot;How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\nWhen you meet someone special... you&#39;ll know. Your heart will beat more rapidly and you&#39;ll smile for no reason.\nthey&#39;ve decided its more fun if I don&#39;t.\nSo Tired D; Played Lazer Tag &amp; Ran A &quot;-------Last 300 letters of the data-------&quot;ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\nColombia is with an &#39;o&#39;...â€œ: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Familyâ€\n#GutsiestMovesYouCanMake Giving a cat a bath.\nCoffee after 5 was a TERRIBLE idea.\n&quot;-------</code></pre><p><a name="1.2"></a></p><h3 id="Part-1-2-Pre-process-the-data"><a href="#Part-1-2-Pre-process-the-data" class="headerlink" title="Part 1.2 Pre-process the data"></a>Part 1.2 Pre-process the data</h3><p>Preprocess this data with the following steps:</p><ol><li>Split data into sentences using â€œ\nâ€ as the delimiter.</li><li>Split each sentence into tokens. Note that in this assignment we use â€œtokenâ€ and â€œwordsâ€ interchangeably.</li><li>Assign sentences into train or test sets.</li><li>Find tokens that appear at least N times in the training data.</li><li>Replace tokens that appear less than N times by <code>&lt;unk&gt;</code></li></ol><p>Note: we omit validation data in this exercise.</p><ul><li>In real applications, we should hold a part of data as a validation set and use it to tune our training.</li><li>We skip this process for simplicity.</li></ul><p><a name="ex-01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p>Split data into sentences.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Use <a href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split" target="_blank" rel="noopener">str.split</a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: split_to_sentences ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_to_sentences</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Split data by linebreak "\n"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: str</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A list of sentences</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    sentences = data.split(<span class="string">"\n"</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Additional clearning (This part is already implemented)</span></span><br><span class="line">    <span class="comment"># - Remove leading and trailing spaces from each sentence</span></span><br><span class="line">    <span class="comment"># - Drop sentences if they are empty strings.</span></span><br><span class="line">    sentences = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">    sentences = [s <span class="keyword">for</span> s <span class="keyword">in</span> sentences <span class="keyword">if</span> len(s) &gt; <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sentences</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">x = <span class="string">"""</span></span><br><span class="line"><span class="string">I have a pen.\nI have an apple. \nAh\nApple pen.\n</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">split_to_sentences(x)</span><br></pre></td></tr></table></figure><pre><code>I have a pen.I have an apple. AhApple pen.[&#39;I have a pen.&#39;, &#39;I have an apple.&#39;, &#39;Ah&#39;, &#39;Apple pen.&#39;]</code></pre><p>Expected answer:<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']</span><br></pre></td></tr></table></figure></p><p><a name="ex-02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p>The next step is to tokenize sentences (split a sentence into a list of words). </p><ul><li>Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.</li><li>Append each tokenized list of words into a list of tokenized sentences.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Use <a href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.lower" target="_blank" rel="noopener">str.lower</a> to convert strings to lowercase. </li>    <li>Please use <a href="https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize" target="_blank" rel="noopener">nltk.word_tokenize</a> to split sentences into tokens.</li>    <li>If you used str.split insteaad of nltk.word_tokenize, there are additional edge cases to handle, such as the punctuation (comma, period) that follows a word.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: tokenize_sentences ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_sentences</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Tokenize sentences into tokens (words)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sentences: List of strings</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of tokens</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the list of lists of tokenized sentences</span></span><br><span class="line">    tokenized_sentences = []</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert to lowercase letters</span></span><br><span class="line">        sentence = sentence.lower()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert into a list of words</span></span><br><span class="line">        tokenized = nltk.word_tokenize(sentence)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the list of words to the list of lists</span></span><br><span class="line">        tokenized_sentences.append(tokenized)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenized_sentences</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [<span class="string">"Sky is blue."</span>, <span class="string">"Leaves are green."</span>, <span class="string">"Roses are red."</span>]</span><br><span class="line">tokenize_sentences(sentences)</span><br></pre></td></tr></table></figure><pre><code>[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;], [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;, &#39;.&#39;], [&#39;roses&#39;, &#39;are&#39;, &#39;red&#39;, &#39;.&#39;]]</code></pre><h3 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[['sky', 'is', 'blue', '.'],</span><br><span class="line"> ['leaves', 'are', 'green', '.'],</span><br><span class="line"> ['roses', 'are', 'red', '.']]</span><br></pre></td></tr></table></figure><p><a name="ex-03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p>Use the two functions that you have just implemented to get the tokenized data.</p><ul><li>split the data into sentences</li><li>tokenize those sentences</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: get_tokenized_data ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tokenized_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Make a list of tokenized sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: String</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of tokens</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the sentences by splitting up the data</span></span><br><span class="line">    sentences = split_to_sentences(data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the list of lists of tokens by tokenizing the sentences</span></span><br><span class="line">    tokenized_sentences = tokenize_sentences(sentences)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenized_sentences</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your function</span></span><br><span class="line">x = <span class="string">"Sky is blue.\nLeaves are green\nRoses are red."</span></span><br><span class="line">get_tokenized_data(x)</span><br></pre></td></tr></table></figure><pre><code>[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;], [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;], [&#39;roses&#39;, &#39;are&#39;, &#39;red&#39;, &#39;.&#39;]]</code></pre><h5 id="Expected-outcome"><a href="#Expected-outcome" class="headerlink" title="Expected outcome"></a>Expected outcome</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[['sky', 'is', 'blue', '.'],</span><br><span class="line"> ['leaves', 'are', 'green'],</span><br><span class="line"> ['roses', 'are', 'red', '.']]</span><br></pre></td></tr></table></figure><h3 id="Split-into-train-and-test-sets"><a href="#Split-into-train-and-test-sets" class="headerlink" title="Split into train and test sets"></a>Split into train and test sets</h3><p>Now run the cell below to split data into training and test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenized_data = get_tokenized_data(data)</span><br><span class="line">random.seed(<span class="number">87</span>)</span><br><span class="line">random.shuffle(tokenized_data)</span><br><span class="line"></span><br><span class="line">train_size = int(len(tokenized_data) * <span class="number">0.8</span>)</span><br><span class="line">train_data = tokenized_data[<span class="number">0</span>:train_size]</span><br><span class="line">test_data = tokenized_data[train_size:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"&#123;&#125; data are split into &#123;&#125; train and &#123;&#125; test set"</span>.format(</span><br><span class="line">    len(tokenized_data), len(train_data), len(test_data)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"First training sample:"</span>)</span><br><span class="line">print(train_data[<span class="number">0</span>])</span><br><span class="line">      </span><br><span class="line">print(<span class="string">"First test sample"</span>)</span><br><span class="line">print(test_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>47961 data are split into 38368 train and 9593 test setFirst training sample:[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;, &#39;team&#39;, &#39;local&#39;, &#39;company&#39;, &#39;and&#39;, &#39;quality&#39;, &#39;production&#39;]First test sample[&#39;that&#39;, &#39;picture&#39;, &#39;i&#39;, &#39;just&#39;, &#39;seen&#39;, &#39;whoa&#39;, &#39;dere&#39;, &#39;!&#39;, &#39;!&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;]</code></pre><h5 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">47961</span> data are split into <span class="number">38368</span> train <span class="keyword">and</span> <span class="number">9593</span> test <span class="built_in">set</span></span><br><span class="line">First training sample:</span><br><span class="line">['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']</span><br><span class="line">First test sample</span><br><span class="line">['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;']</span><br></pre></td></tr></table></figure><p><a name="ex-04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p>You wonâ€™t use all the tokens (words) appearing in the data for training.  Instead, you will use the more frequently used words.  </p><ul><li>You will focus on the words that appear at least N times in the data.</li><li>First count how many times each word appears in the data.</li></ul><p>You will need a double for-loop, one for sentences and the other for tokens within a sentence.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>If you decide to import and use defaultdict, remember to cast the dictionary back to a regular 'dict' before returning it. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: count_words ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_words</span><span class="params">(tokenized_sentences)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count the number of word appearence in the tokenized sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of strings</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict that maps word (str) to the frequency (int)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    word_counts = &#123;&#125;</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokenized_sentences: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each token in the sentence</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the token is not in the dictionary yet, set the count to 1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> token <span class="keyword">in</span> word_counts: <span class="comment"># complete this line</span></span><br><span class="line">                word_counts[token] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the token is already in the dictionary, increment the count by 1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                word_counts[token] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> word_counts</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tokenized_sentences = [[<span class="string">'sky'</span>, <span class="string">'is'</span>, <span class="string">'blue'</span>, <span class="string">'.'</span>],</span><br><span class="line">                       [<span class="string">'leaves'</span>, <span class="string">'are'</span>, <span class="string">'green'</span>, <span class="string">'.'</span>],</span><br><span class="line">                       [<span class="string">'roses'</span>, <span class="string">'are'</span>, <span class="string">'red'</span>, <span class="string">'.'</span>]]</span><br><span class="line">count_words(tokenized_sentences)</span><br></pre></td></tr></table></figure><pre><code>{&#39;sky&#39;: 1, &#39;is&#39;: 1, &#39;blue&#39;: 1, &#39;.&#39;: 3, &#39;leaves&#39;: 1, &#39;are&#39;: 2, &#39;green&#39;: 1, &#39;roses&#39;: 1, &#39;red&#39;: 1}</code></pre><h5 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output"></a>Expected output</h5><p>Note that the order may differ.</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;'sky': 1,</span><br><span class="line"> 'is': 1,</span><br><span class="line"> 'blue': 1,</span><br><span class="line"> <span class="string">'.'</span>: <span class="number">3</span>,</span><br><span class="line"> 'leaves': 1,</span><br><span class="line"> 'are': 2,</span><br><span class="line"> 'green': 1,</span><br><span class="line"> 'roses': 1,</span><br><span class="line"> 'red': 1&#125;</span><br></pre></td></tr></table></figure><h3 id="Handling-â€˜Out-of-Vocabularyâ€™-words"><a href="#Handling-â€˜Out-of-Vocabularyâ€™-words" class="headerlink" title="Handling â€˜Out of Vocabularyâ€™ words"></a>Handling â€˜Out of Vocabularyâ€™ words</h3><p>If your model is performing autocomplete, but encounters a word that it never saw during training, it wonâ€™t have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. </p><ul><li>This â€˜newâ€™ word is called an â€˜unknown wordâ€™, or <b>out of vocabulary (OOV)</b> words.</li><li>The percentage of unknown words in the test set is called the <b> OOV </b> rate. </li></ul><p>To handle unknown words during prediction, use a special token to represent all unknown words â€˜unkâ€™. </p><ul><li>Modify the training data so that it has some â€˜unknownâ€™ words to train on.</li><li>Words to convert into â€œunknownâ€ words are those that do not occur very frequently in the training set.</li><li>Create a list of the most frequent words in the training set, called the <b> closed vocabulary </b>. </li><li>Convert all the other words that are not part of the closed vocabulary to the token â€˜unkâ€™. </li></ul><p><a name="ex-05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>You will now create a function that takes in a text document and a threshold â€˜count_thresholdâ€™.</p><ul><li>Any word whose count is greater than or equal to the threshold â€˜count_thresholdâ€™ is kept in the closed vocabulary.</li><li>used that you want to keep, returns the document containing only the word closed vocabulary and the word unk. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: get_words_with_nplus_frequency ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_words_with_nplus_frequency</span><span class="params">(tokenized_sentences, count_threshold)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Find the words that appear N times or more</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of sentences</span></span><br><span class="line"><span class="string">        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of words that appear N times or more</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize an empty list to contain the words that</span></span><br><span class="line">    <span class="comment"># appear at least 'minimum_freq' times.</span></span><br><span class="line">    closed_vocab = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the word couts of the tokenized sentences</span></span><br><span class="line">    <span class="comment"># Use the function that you defined earlier to count the words</span></span><br><span class="line">    word_counts = count_words(tokenized_sentences)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># for each word and its count</span></span><br><span class="line">    <span class="keyword">for</span> word, cnt <span class="keyword">in</span> word_counts.items(): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check that the word's count</span></span><br><span class="line">        <span class="comment"># is at least as great as the minimum count</span></span><br><span class="line">        <span class="keyword">if</span> cnt &gt;= count_threshold:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the word to the list</span></span><br><span class="line">            closed_vocab.append(word)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> closed_vocab</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tokenized_sentences = [[<span class="string">'sky'</span>, <span class="string">'is'</span>, <span class="string">'blue'</span>, <span class="string">'.'</span>],</span><br><span class="line">                       [<span class="string">'leaves'</span>, <span class="string">'are'</span>, <span class="string">'green'</span>, <span class="string">'.'</span>],</span><br><span class="line">                       [<span class="string">'roses'</span>, <span class="string">'are'</span>, <span class="string">'red'</span>, <span class="string">'.'</span>]]</span><br><span class="line">tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">f"Closed vocabulary:"</span>)</span><br><span class="line">print(tmp_closed_vocab)</span><br></pre></td></tr></table></figure><pre><code>Closed vocabulary:[&#39;.&#39;, &#39;are&#39;]</code></pre><h5 id="Expected-output-3"><a href="#Expected-output-3" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Closed vocabulary:</span><br><span class="line">['.', 'are']</span><br></pre></td></tr></table></figure><p><a name="ex-06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p>The words that appear â€˜count_thresholdâ€™ times or more are in the â€˜closed vocabulary. </p><ul><li>All other words are regarded as â€˜unknownâ€™.</li><li>Replace words not in the closed vocabulary with the token â€œ<unk\>â€œ.</unk\></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: replace_oov_words_by_unk ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_oov_words_by_unk</span><span class="params">(tokenized_sentences, vocabulary, unknown_token=<span class="string">"&lt;unk&gt;"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Replace words not in the given vocabulary with '&lt;unk&gt;' token.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of strings</span></span><br><span class="line"><span class="string">        vocabulary: List of strings that we will use</span></span><br><span class="line"><span class="string">        unknown_token: A string representing unknown (out-of-vocabulary) words</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of strings, with words not in the vocabulary replaced</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Place vocabulary into a set for faster search</span></span><br><span class="line">    vocabulary = set(vocabulary)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a list that will hold the sentences</span></span><br><span class="line">    <span class="comment"># after less frequent words are replaced by the unknown token</span></span><br><span class="line">    replaced_tokenized_sentences = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokenized_sentences:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize the list that will contain</span></span><br><span class="line">        <span class="comment"># a single sentence with "unknown_token" replacements</span></span><br><span class="line">        replaced_sentence = []</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># for each token in the sentence</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the token is in the closed vocabulary</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> vocabulary: <span class="comment"># complete this line</span></span><br><span class="line">                <span class="comment"># If so, append the word to the replaced_sentence</span></span><br><span class="line">                replaced_sentence.append(token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># otherwise, append the unknown token instead</span></span><br><span class="line">                replaced_sentence.append(unknown_token)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Append the list of tokens to the list of lists</span></span><br><span class="line">        replaced_tokenized_sentences.append(replaced_sentence)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> replaced_tokenized_sentences</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenized_sentences = [[<span class="string">"dogs"</span>, <span class="string">"run"</span>], [<span class="string">"cats"</span>, <span class="string">"sleep"</span>]]</span><br><span class="line">vocabulary = [<span class="string">"dogs"</span>, <span class="string">"sleep"</span>]</span><br><span class="line">tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)</span><br><span class="line">print(<span class="string">f"Original sentence:"</span>)</span><br><span class="line">print(tokenized_sentences)</span><br><span class="line">print(<span class="string">f"tokenized_sentences with less frequent words converted to '&lt;unk&gt;':"</span>)</span><br><span class="line">print(tmp_replaced_tokenized_sentences)</span><br></pre></td></tr></table></figure><pre><code>Original sentence:[[&#39;dogs&#39;, &#39;run&#39;], [&#39;cats&#39;, &#39;sleep&#39;]]tokenized_sentences with less frequent words converted to &#39;&lt;unk&gt;&#39;:[[&#39;dogs&#39;, &#39;&lt;unk&gt;&#39;], [&#39;&lt;unk&gt;&#39;, &#39;sleep&#39;]]</code></pre><h3 id="Expected-answer"><a href="#Expected-answer" class="headerlink" title="Expected answer"></a>Expected answer</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original sentence:</span><br><span class="line">[['dogs', 'run'], ['cats', 'sleep']]</span><br><span class="line">tokenized_sentences with less frequent words converted to '&lt;unk&gt;':</span><br><span class="line">[['dogs', '&lt;unk&gt;'], ['&lt;unk&gt;', 'sleep']]</span><br></pre></td></tr></table></figure><p><a name="ex-07"></a></p><h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Now we are ready to process our data by combining the functions that you just implemented.</p><ol><li>Find tokens that appear at least count_threshold times in the training data.</li><li>Replace tokens that appear less than count_threshold times by â€œ<unk\>â€œ both for training and test data.</unk\></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: preprocess_data ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span><span class="params">(train_data, test_data, count_threshold)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Preprocess data, i.e.,</span></span><br><span class="line"><span class="string">        - Find tokens that appear at least N times in the training data.</span></span><br><span class="line"><span class="string">        - Replace tokens that appear less than N times by "&lt;unk&gt;" both for training and test data.        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        train_data, test_data: List of lists of strings.</span></span><br><span class="line"><span class="string">        count_threshold: Words whose count is less than this are </span></span><br><span class="line"><span class="string">                      treated as unknown.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tuple of</span></span><br><span class="line"><span class="string">        - training data with low frequent words replaced by "&lt;unk&gt;"</span></span><br><span class="line"><span class="string">        - test data with low frequent words replaced by "&lt;unk&gt;"</span></span><br><span class="line"><span class="string">        - vocabulary of words that appear n times or more in the training data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the closed vocabulary using the train data</span></span><br><span class="line">    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For the train data, replace less common words with "&lt;unk&gt;"</span></span><br><span class="line">    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token=<span class="string">"&lt;unk&gt;"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For the test data, replace less common words with "&lt;unk&gt;"</span></span><br><span class="line">    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token=<span class="string">"&lt;unk&gt;"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> train_data_replaced, test_data_replaced, vocabulary</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tmp_train = [[<span class="string">'sky'</span>, <span class="string">'is'</span>, <span class="string">'blue'</span>, <span class="string">'.'</span>],</span><br><span class="line">     [<span class="string">'leaves'</span>, <span class="string">'are'</span>, <span class="string">'green'</span>]]</span><br><span class="line">tmp_test = [[<span class="string">'roses'</span>, <span class="string">'are'</span>, <span class="string">'red'</span>, <span class="string">'.'</span>]]</span><br><span class="line"></span><br><span class="line">tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, </span><br><span class="line">                                                           tmp_test, </span><br><span class="line">                                                           count_threshold = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"tmp_train_repl"</span>)</span><br><span class="line">print(tmp_train_repl)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"tmp_test_repl"</span>)</span><br><span class="line">print(tmp_test_repl)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"tmp_vocab"</span>)</span><br><span class="line">print(tmp_vocab)</span><br></pre></td></tr></table></figure><pre><code>tmp_train_repl[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;], [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;]]tmp_test_repl[[&#39;&lt;unk&gt;&#39;, &#39;are&#39;, &#39;&lt;unk&gt;&#39;, &#39;.&#39;]]tmp_vocab[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;, &#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;]</code></pre><h5 id="Expected-outcome-1"><a href="#Expected-outcome-1" class="headerlink" title="Expected outcome"></a>Expected outcome</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_train_repl</span><br><span class="line">[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]</span><br><span class="line"></span><br><span class="line">tmp_test_repl</span><br><span class="line">[['&lt;unk&gt;', 'are', '&lt;unk&gt;', '.']]</span><br><span class="line"></span><br><span class="line">tmp_vocab</span><br><span class="line">['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']</span><br></pre></td></tr></table></figure><h3 id="Preprocess-the-train-and-test-data"><a href="#Preprocess-the-train-and-test-data" class="headerlink" title="Preprocess the train and test data"></a>Preprocess the train and test data</h3><p>Run the cell below to complete the preprocessing both for training and test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">minimum_freq = <span class="number">2</span></span><br><span class="line">train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, </span><br><span class="line">                                                                        test_data, </span><br><span class="line">                                                                        minimum_freq)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"First preprocessed training sample:"</span>)</span><br><span class="line">print(train_data_processed[<span class="number">0</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"First preprocessed test sample:"</span>)</span><br><span class="line">print(test_data_processed[<span class="number">0</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"First 10 vocabulary:"</span>)</span><br><span class="line">print(vocabulary[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Size of vocabulary:"</span>, len(vocabulary))</span><br></pre></td></tr></table></figure><pre><code>First preprocessed training sample:[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;, &#39;team&#39;, &#39;local&#39;, &#39;company&#39;, &#39;and&#39;, &#39;quality&#39;, &#39;production&#39;]First preprocessed test sample:[&#39;that&#39;, &#39;picture&#39;, &#39;i&#39;, &#39;just&#39;, &#39;seen&#39;, &#39;whoa&#39;, &#39;dere&#39;, &#39;!&#39;, &#39;!&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;]First 10 vocabulary:[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;]Size of vocabulary: 14821</code></pre><h5 id="Expected-output-4"><a href="#Expected-output-4" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">First preprocessed training sample:</span><br><span class="line">['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']</span><br><span class="line"></span><br><span class="line">First preprocessed test sample:</span><br><span class="line">['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;']</span><br><span class="line"></span><br><span class="line">First <span class="number">10</span> vocabulary:</span><br><span class="line">['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']</span><br><span class="line"></span><br><span class="line">Size of vocabulary: <span class="number">14821</span></span><br></pre></td></tr></table></figure><p>You are done with the preprocessing section of the assignment.<br>Objects <code>train_data_processed</code>, <code>test_data_processed</code>, and <code>vocabulary</code> will be used in the rest of the exercises.</p><p><a name="2"></a></p><h2 id="Part-2-Develop-n-gram-based-language-models"><a href="#Part-2-Develop-n-gram-based-language-models" class="headerlink" title="Part 2: Develop n-gram based language models"></a>Part 2: Develop n-gram based language models</h2><p>In this section, you will develop the n-grams language model.</p><ul><li>Assume the probability of the next word depends only on the previous n-gram.</li><li>The previous n-gram is the series of the previous â€˜nâ€™ words.</li></ul><p>The conditional probability for the word at position â€˜tâ€™ in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \cdots w_{t-n}$ is:</p><script type="math/tex; mode=display">P(w_t | w_{t-1}\dots w_{t-n}) \tag{1}</script><p>You can estimate this probability  by counting the occurrences of these series of words in the training data.</p><ul><li>The probability can be estimated as a ratio, where</li><li>The numerator is the number of times word â€˜tâ€™ appears after words t-1 through t-n appear in the training data.</li><li>The denominator is the number of times word t-1 through t-n appears in the training data.</li></ul><script type="math/tex; mode=display">\hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2}</script><ul><li>The function $C(\cdots)$ denotes the number of occurence of the given sequence. </li><li>$\hat{P}$ means the estimation of $P$. </li><li>Notice that denominator of the equation (2) is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$.</li></ul><p>Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.</p><p>The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).</p><p><a name="ex-08"></a></p><h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p>Next, you will implement a function that computes the counts of n-grams for an arbitrary number $n$.</p><p>When computing the counts for n-grams, prepare the sentence beforehand by prepending $n-1$ starting markers â€œ<s\>â€œ to indicate the beginning of the sentence.  </s\></p><ul><li>For example, in the bi-gram model (N=2), a sequence with two start tokens â€œ<s\><s\>â€œ should predict the first word of a sentence.</s\></s\></li><li>So, if the sentence is â€œI like foodâ€, modify it to be â€œ<s\><s\> I like foodâ€.</s\></s\></li><li>Also prepare the sentence for counting by appending an end token â€œ<e\>â€œ so that the model can predict when to finish a sentence.</e\></li></ul><p>Technical note: In this implementation, you will store the counts as a dictionary.</p><ul><li>The key of each key-value pair in the dictionary is a <strong>tuple</strong> of n words (and not a list)</li><li>The value in the key-value pair is the number of occurrences.  </li><li>The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created).  A tuple is â€œimmutableâ€, so it cannot be altered after it is first created.  This makes a tuple suitable as a data type for the key in a dictionary.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> To prepend or append, you can create lists and concatenate them using the + operator </li>    <li> To create a list of a repeated value, you can follow this syntax: <code>['a'] * 3</code> to get <code>['a','a','a']</code> </li>    <li>To set the range for index 'i', think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token).  So the index positions are <code>[0,1,2,3,4]</code>.  The largest index 'i' where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram. </li>    <li>Remember that the <code>range()</code> function excludes the value that is used for the maximum of the range.  <code> range(3) </code> produces (0,1,2) but excludes 3. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED FUNCTION: count_n_grams ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_n_grams</span><span class="params">(data, n, start_token=<span class="string">'&lt;s&gt;'</span>, end_token = <span class="string">'&lt;e&gt;'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count all n-grams in the data</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: List of lists of words</span></span><br><span class="line"><span class="string">        n: number of words in a sequence</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A dictionary that maps a tuple of n-words to its frequency</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dictionary of n-grams and their counts</span></span><br><span class="line">    n_grams = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence in the data</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> data: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># prepend start token n times, and  append &lt;e&gt; one time</span></span><br><span class="line">        sentence = [start_token] * n + sentence + [end_token]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert list to tuple</span></span><br><span class="line">        <span class="comment"># So that the sequence of words can be used as</span></span><br><span class="line">        <span class="comment"># a key in the dictionary</span></span><br><span class="line">        sentence = tuple(sentence)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use 'i' to indicate the start of the n-gram</span></span><br><span class="line">        <span class="comment"># from index 0</span></span><br><span class="line">        <span class="comment"># to the last index where the end of the n-gram</span></span><br><span class="line">        <span class="comment"># is within the sentence.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sentence)-n+<span class="number">1</span>): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Get the n-gram from i to i+n</span></span><br><span class="line">            n_gram = sentence[i:i+n]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the n-gram is in the dictionary</span></span><br><span class="line">            <span class="keyword">if</span> n_gram <span class="keyword">in</span> n_grams: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Increment the count for this n-gram</span></span><br><span class="line">                n_grams[n_gram] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Initialize this n-gram count to 1</span></span><br><span class="line">                n_grams[n_gram] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> n_grams</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line"><span class="comment"># CODE REVIEW COMMENT: Outcome does not match expected outcome</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">print(<span class="string">"Uni-gram:"</span>)</span><br><span class="line">print(count_n_grams(sentences, <span class="number">1</span>))</span><br><span class="line">print(<span class="string">"Bi-gram:"</span>)</span><br><span class="line">print(count_n_grams(sentences, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><pre><code>Uni-gram:{(&#39;&lt;s&gt;&#39;,): 2, (&#39;i&#39;,): 1, (&#39;like&#39;,): 2, (&#39;a&#39;,): 2, (&#39;cat&#39;,): 2, (&#39;&lt;e&gt;&#39;,): 2, (&#39;this&#39;,): 1, (&#39;dog&#39;,): 1, (&#39;is&#39;,): 1}Bi-gram:{(&#39;&lt;s&gt;&#39;, &#39;&lt;s&gt;&#39;): 2, (&#39;&lt;s&gt;&#39;, &#39;i&#39;): 1, (&#39;i&#39;, &#39;like&#39;): 1, (&#39;like&#39;, &#39;a&#39;): 2, (&#39;a&#39;, &#39;cat&#39;): 2, (&#39;cat&#39;, &#39;&lt;e&gt;&#39;): 2, (&#39;&lt;s&gt;&#39;, &#39;this&#39;): 1, (&#39;this&#39;, &#39;dog&#39;): 1, (&#39;dog&#39;, &#39;is&#39;): 1, (&#39;is&#39;, &#39;like&#39;): 1}</code></pre><p>Expected outcome:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Uni-gram:</span><br><span class="line">&#123;('&lt;s&gt;',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('&lt;e&gt;',): 2, ('this',): 1, ('dog',): 1, ('is',): 1&#125;</span><br><span class="line">Bi-gram:</span><br><span class="line">&#123;('&lt;s&gt;', '&lt;s&gt;'): 2, ('&lt;s&gt;', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '&lt;e&gt;'): 2, ('&lt;s&gt;', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1&#125;</span><br></pre></td></tr></table></figure><p><a name="ex-09"></a></p><h3 id="Exercise-09"><a href="#Exercise-09" class="headerlink" title="Exercise 09"></a>Exercise 09</h3><p>Next, estimate the probability of a word given the prior â€˜nâ€™ words using the n-gram counts.</p><script type="math/tex; mode=display">\hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2}</script><p>This formula doesnâ€™t work when a count of an n-gram is zero..</p><ul><li>Suppose we encounter an n-gram that did not occur in the training data.  </li><li>Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).</li></ul><p>A way to handle zero counts is to add k-smoothing.  </p><ul><li>K-smoothing adds a positive constant $k$ to each numerator and $k \times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.</li></ul><script type="math/tex; mode=display">\hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n) + k}{C(w_{t-1}\dots w_{t-n}) + k|V|} \tag{3}</script><p>For n-grams that have a zero count, the equation (3) becomes $\frac{1}{|V|}$.</p><ul><li>This means that any n-gram with zero count has the same probability of $\frac{1}{|V|}$.</li></ul><p>Define a function that computes the probability estimate (3) from n-gram counts and a constant $k$.</p><ul><li>The function takes in a dictionary â€˜n_gram_countsâ€™, where the key is the n-gram and the value is the count of that n-gram.</li><li>The function also takes another dictionary n_plus1_gram_counts, which youâ€™ll use to find the count for the previous n-gram plus the current word.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>To define a tuple containing a single value, add a comma after that value.  For example: <code>('apple',)</code> is a tuple containing a single string 'apple' </li>    <li>To concatenate two tuples, use the '+' operator</li>    <li><a href> words </a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED FUNCTION: estimate_probabilityy ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_probability</span><span class="params">(word, previous_n_gram, </span></span></span><br><span class="line"><span class="function"><span class="params">                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        word: next word</span></span><br><span class="line"><span class="string">        previous_n_gram: A sequence of words of length n</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of n-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary_size: number of words in the vocabulary</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A probability</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># convert list to tuple to use it as a dictionary key</span></span><br><span class="line">    previous_n_gram = tuple(previous_n_gram)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the denominator</span></span><br><span class="line">    <span class="comment"># If the previous n-gram exists in the dictionary of n-gram counts,</span></span><br><span class="line">    <span class="comment"># Get its count.  Otherwise set the count to zero</span></span><br><span class="line">    <span class="comment"># Use the dictionary that has counts for n-grams</span></span><br><span class="line">    previous_n_gram_count = n_gram_counts.get(previous_n_gram, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Calculate the denominator using the count of the previous n gram</span></span><br><span class="line">    <span class="comment"># and apply k-smoothing</span></span><br><span class="line">    denominator = previous_n_gram_count + k * vocabulary_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define n plus 1 gram as the previous n-gram plus the current word as a tuple</span></span><br><span class="line">    n_plus1_gram = n_gram_counts.get(previous_n_gram, <span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Set the count to the count in the dictionary,</span></span><br><span class="line">    <span class="comment"># otherwise 0 if not in the dictionary</span></span><br><span class="line">    <span class="comment"># use the dictionary that has counts for the n-gram plus current word</span></span><br><span class="line">    n_plus1_gram_count = n_plus1_gram_counts.get(previous_n_gram + (word, ) ,<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Define the numerator use the count of the n-gram plus current word,</span></span><br><span class="line">    <span class="comment"># and apply smoothing</span></span><br><span class="line">    numerator = n_plus1_gram_count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the probability as the numerator divided by denominator</span></span><br><span class="line">    probability = numerator / denominator</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> probability</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">tmp_prob = estimate_probability(<span class="string">"cat"</span>, <span class="string">"a"</span>, unigram_counts, bigram_counts, len(unique_words), k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The estimated probability of word 'cat' given the previous n-gram 'a' is: <span class="subst">&#123;tmp_prob:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>The estimated probability of word &#39;cat&#39; given the previous n-gram &#39;a&#39; is: 0.3333</code></pre><h5 id="Expected-output-5"><a href="#Expected-output-5" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333</span><br></pre></td></tr></table></figure><h3 id="Estimate-probabilities-for-all-words"><a href="#Estimate-probabilities-for-all-words" class="headerlink" title="Estimate probabilities for all words"></a>Estimate probabilities for all words</h3><p>The function defined below loops over all words in vocabulary to calculate probabilities for all possible words.</p><ul><li>This function is provided for you.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_probabilities</span><span class="params">(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Estimate the probabilities of next words using the n-gram counts with k-smoothing</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        previous_n_gram: A sequence of words of length n</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary: List of words</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A dictionary mapping from next words to the probability.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert list to tuple to use it as a dictionary key</span></span><br><span class="line">    previous_n_gram = tuple(previous_n_gram)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span></span><br><span class="line">    <span class="comment"># &lt;s&gt; is not needed since it should not appear as the next word</span></span><br><span class="line">    vocabulary = vocabulary + [<span class="string">"&lt;e&gt;"</span>, <span class="string">"&lt;unk&gt;"</span>]</span><br><span class="line">    vocabulary_size = len(vocabulary)</span><br><span class="line">    </span><br><span class="line">    probabilities = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocabulary:</span><br><span class="line">        probability = estimate_probability(word, previous_n_gram, </span><br><span class="line">                                           n_gram_counts, n_plus1_gram_counts, </span><br><span class="line">                                           vocabulary_size, k=k)</span><br><span class="line">        probabilities[word] = probability</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> probabilities</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">estimate_probabilities(<span class="string">"a"</span>, unigram_counts, bigram_counts, unique_words, k=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>{&#39;dog&#39;: 0.09090909090909091, &#39;like&#39;: 0.09090909090909091, &#39;cat&#39;: 0.2727272727272727, &#39;i&#39;: 0.09090909090909091, &#39;is&#39;: 0.09090909090909091, &#39;this&#39;: 0.09090909090909091, &#39;a&#39;: 0.09090909090909091, &#39;&lt;e&gt;&#39;: 0.09090909090909091, &#39;&lt;unk&gt;&#39;: 0.09090909090909091}</code></pre><h5 id="Expected-output-6"><a href="#Expected-output-6" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;'cat': 0.2727272727272727,</span><br><span class="line"> <span class="string">'i'</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> 'this': 0.09090909090909091,</span><br><span class="line"> <span class="string">'a'</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> 'is': 0.09090909090909091,</span><br><span class="line"> 'like': 0.09090909090909091,</span><br><span class="line"> 'dog': 0.09090909090909091,</span><br><span class="line"> '&lt;e&gt;': 0.09090909090909091,</span><br><span class="line"> '&lt;unk&gt;': 0.09090909090909091&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Additional test</span></span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">estimate_probabilities([<span class="string">"&lt;s&gt;"</span>, <span class="string">"&lt;s&gt;"</span>], bigram_counts, trigram_counts, unique_words, k=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>{&#39;dog&#39;: 0.09090909090909091, &#39;like&#39;: 0.09090909090909091, &#39;cat&#39;: 0.09090909090909091, &#39;i&#39;: 0.18181818181818182, &#39;is&#39;: 0.09090909090909091, &#39;this&#39;: 0.18181818181818182, &#39;a&#39;: 0.09090909090909091, &#39;&lt;e&gt;&#39;: 0.09090909090909091, &#39;&lt;unk&gt;&#39;: 0.09090909090909091}</code></pre><h5 id="Expected-output-7"><a href="#Expected-output-7" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;'cat': 0.09090909090909091,</span><br><span class="line"> <span class="string">'i'</span>: <span class="number">0.18181818181818182</span>,</span><br><span class="line"> 'this': 0.18181818181818182,</span><br><span class="line"> <span class="string">'a'</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> 'is': 0.09090909090909091,</span><br><span class="line"> 'like': 0.09090909090909091,</span><br><span class="line"> 'dog': 0.09090909090909091,</span><br><span class="line"> '&lt;e&gt;': 0.09090909090909091,</span><br><span class="line"> '&lt;unk&gt;': 0.09090909090909091&#125;</span><br></pre></td></tr></table></figure><h3 id="Count-and-probability-matrices"><a href="#Count-and-probability-matrices" class="headerlink" title="Count and probability matrices"></a>Count and probability matrices</h3><p>As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.  </p><ul><li>It can be more intuitive to present them as count or probability matrices.</li><li>The functions defined in the next cells return count or probability matrices.</li><li>This function is provided for you.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_count_matrix</span><span class="params">(n_plus1_gram_counts, vocabulary)</span>:</span></span><br><span class="line">    <span class="comment"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span></span><br><span class="line">    <span class="comment"># &lt;s&gt; is omitted since it should not appear as the next word</span></span><br><span class="line">    vocabulary = vocabulary + [<span class="string">"&lt;e&gt;"</span>, <span class="string">"&lt;unk&gt;"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># obtain unique n-grams</span></span><br><span class="line">    n_grams = []</span><br><span class="line">    <span class="keyword">for</span> n_plus1_gram <span class="keyword">in</span> n_plus1_gram_counts.keys():</span><br><span class="line">        n_gram = n_plus1_gram[<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">        n_grams.append(n_gram)</span><br><span class="line">    n_grams = list(set(n_grams))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># mapping from n-gram to row</span></span><br><span class="line">    row_index = &#123;n_gram:i <span class="keyword">for</span> i, n_gram <span class="keyword">in</span> enumerate(n_grams)&#125;</span><br><span class="line">    <span class="comment"># mapping from next word to column</span></span><br><span class="line">    col_index = &#123;word:j <span class="keyword">for</span> j, word <span class="keyword">in</span> enumerate(vocabulary)&#125;</span><br><span class="line">    </span><br><span class="line">    nrow = len(n_grams)</span><br><span class="line">    ncol = len(vocabulary)</span><br><span class="line">    count_matrix = np.zeros((nrow, ncol))</span><br><span class="line">    <span class="keyword">for</span> n_plus1_gram, count <span class="keyword">in</span> n_plus1_gram_counts.items():</span><br><span class="line">        n_gram = n_plus1_gram[<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">        word = n_plus1_gram[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocabulary:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        i = row_index[n_gram]</span><br><span class="line">        j = col_index[word]</span><br><span class="line">        count_matrix[i, j] = count</span><br><span class="line">    </span><br><span class="line">    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)</span><br><span class="line">    <span class="keyword">return</span> count_matrix</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">                 [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'bigram counts'</span>)</span><br><span class="line">display(make_count_matrix(bigram_counts, unique_words))</span><br></pre></td></tr></table></figure><pre><code>bigram counts</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>dog</th>      <th>like</th>      <th>cat</th>      <th>i</th>      <th>is</th>      <th>this</th>      <th>a</th>      <th>&lt;e&gt;</th>      <th>&lt;unk&gt;</th>    </tr>  </thead>  <tbody>    <tr>      <th>(dog,)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(like,)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(&lt;s&gt;,)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(i,)</th>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(is,)</th>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(this,)</th>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(cat,)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(a,)</th>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>  </tbody></table></div><h5 id="Expected-output-8"><a href="#Expected-output-8" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bigram counts</span><br><span class="line">          cat    i   <span class="keyword">this</span>   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;</span><br><span class="line">(&lt;s&gt;,)    <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(a,)      <span class="number">2.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(<span class="keyword">this</span>,)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(like,)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">2.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(dog,)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(cat,)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">2.0</span>    <span class="number">0.0</span></span><br><span class="line">(is,)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(i,)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show trigram counts</span></span><br><span class="line">print(<span class="string">'\ntrigram counts'</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">display(make_count_matrix(trigram_counts, unique_words))</span><br></pre></td></tr></table></figure><pre><code>trigram counts</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>dog</th>      <th>like</th>      <th>cat</th>      <th>i</th>      <th>is</th>      <th>this</th>      <th>a</th>      <th>&lt;e&gt;</th>      <th>&lt;unk&gt;</th>    </tr>  </thead>  <tbody>    <tr>      <th>(&lt;s&gt;, &lt;s&gt;)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(&lt;s&gt;, i)</th>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(is, like)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(i, like)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(&lt;s&gt;, this)</th>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(a, cat)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(like, a)</th>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(dog, is)</th>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(this, dog)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>  </tbody></table></div><h5 id="Expected-output-9"><a href="#Expected-output-9" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trigram counts</span><br><span class="line">              cat    i   <span class="keyword">this</span>   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;</span><br><span class="line">(dog, is)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(<span class="keyword">this</span>, dog)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(a, cat)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">2.0</span>    <span class="number">0.0</span></span><br><span class="line">(like, a)     <span class="number">2.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(is, like)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, i)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(i, like)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, &lt;s&gt;)    <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, <span class="keyword">this</span>)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br></pre></td></tr></table></figure><p>The following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form.</p><ul><li>This function is provided for you.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_probability_matrix</span><span class="params">(n_plus1_gram_counts, vocabulary, k)</span>:</span></span><br><span class="line">    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)</span><br><span class="line">    count_matrix += k</span><br><span class="line">    prob_matrix = count_matrix.div(count_matrix.sum(axis=<span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> prob_matrix</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">                 [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">print(<span class="string">"bigram probabilities"</span>)</span><br><span class="line">display(make_probability_matrix(bigram_counts, unique_words, k=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>bigram probabilities</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>dog</th>      <th>like</th>      <th>cat</th>      <th>i</th>      <th>is</th>      <th>this</th>      <th>a</th>      <th>&lt;e&gt;</th>      <th>&lt;unk&gt;</th>    </tr>  </thead>  <tbody>    <tr>      <th>(dog,)</th>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(like,)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>    <tr>      <th>(&lt;s&gt;,)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.181818</td>      <td>0.090909</td>      <td>0.181818</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>    <tr>      <th>(i,)</th>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(is,)</th>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(this,)</th>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(cat,)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>    </tr>    <tr>      <th>(a,)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"trigram probabilities"</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">display(make_probability_matrix(trigram_counts, unique_words, k=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>trigram probabilities</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>dog</th>      <th>like</th>      <th>cat</th>      <th>i</th>      <th>is</th>      <th>this</th>      <th>a</th>      <th>&lt;e&gt;</th>      <th>&lt;unk&gt;</th>    </tr>  </thead>  <tbody>    <tr>      <th>(&lt;s&gt;, &lt;s&gt;)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.181818</td>      <td>0.090909</td>      <td>0.181818</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>    <tr>      <th>(&lt;s&gt;, i)</th>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(is, like)</th>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(i, like)</th>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(&lt;s&gt;, this)</th>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(a, cat)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>    </tr>    <tr>      <th>(like, a)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>    <tr>      <th>(dog, is)</th>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(this, dog)</th>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>  </tbody></table></div><p>Confirm that you obtain the same results as for the <code>estimate_probabilities</code> function that you implemented.</p><p><a name="3"></a></p><h2 id="Part-3-Perplexity"><a href="#Part-3-Perplexity" class="headerlink" title="Part 3: Perplexity"></a>Part 3: Perplexity</h2><p>In this section, you will generate the perplexity score to evaluate your model on the test set. </p><ul><li>You will also use back-off when needed. </li><li>Perplexity is used as an evaluation metric of your language model. </li><li>To calculate the  the perplexity score of the test set on an n-gram model, use: </li></ul><script type="math/tex; mode=display">PP(W) =\sqrt[N]{ \prod_{t=n+1}^N \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4}</script><ul><li>where $N$ is the length of the sentence.</li><li>$n$ is the number of words in the n-gram (e.g. 2 for a bigram).</li><li>In math, the numbering starts at one and not zero.</li></ul><p>In code, array indexing starts at zero, so the code will use ranges for $t$ according to this formula:</p><script type="math/tex; mode=display">PP(W) =\sqrt[N]{ \prod_{t=n}^{N-1} \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4.1}</script><p>The higher the probabilities are, the lower the perplexity will be. </p><ul><li>The more the n-grams tell us about the sentence, the lower the perplexity score will be. </li></ul><p><a name="ex-10"></a></p><h3 id="Exercise-10"><a href="#Exercise-10" class="headerlink" title="Exercise 10"></a>Exercise 10</h3><p>Compute the perplexity score given an N-gram count matrix and a sentence. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember that <code>range(2,4)</code> produces the integers [2, 3] (and excludes 4).</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: calculate_perplexity</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_perplexity</span><span class="params">(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate perplexity for a list of sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sentence: List of strings</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary_size: number of unique words in the vocabulary</span></span><br><span class="line"><span class="string">        k: Positive smoothing constant</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Perplexity score</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># length of previous words</span></span><br><span class="line">    n = len(list(n_gram_counts.keys())[<span class="number">0</span>]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># prepend &lt;s&gt; and append &lt;e&gt;</span></span><br><span class="line">    sentence = [<span class="string">"&lt;s&gt;"</span>] * n + sentence + [<span class="string">"&lt;e&gt;"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Cast the sentence from a list to a tuple</span></span><br><span class="line">    sentence = tuple(sentence)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># length of sentence (after adding &lt;s&gt; and &lt;e&gt; tokens)</span></span><br><span class="line">    N = len(sentence)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The variable p will hold the product</span></span><br><span class="line">    <span class="comment"># that is calculated inside the n-root</span></span><br><span class="line">    <span class="comment"># Update this in the code below</span></span><br><span class="line">    product_pi = <span class="number">1.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Index t ranges from n to N - 1, inclusive on both ends</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(n, N): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the n-gram preceding the word at position t</span></span><br><span class="line">        n_gram = sentence[t-n:t]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the word at position t</span></span><br><span class="line">        word = sentence[t]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Estimate the probability of the word given the n-gram</span></span><br><span class="line">        <span class="comment"># using the n-gram counts, n-plus1-gram counts,</span></span><br><span class="line">        <span class="comment"># vocabulary size, and smoothing constant</span></span><br><span class="line">        probability = estimate_probability(word, n_gram, </span><br><span class="line">                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the product of the probabilities</span></span><br><span class="line">        <span class="comment"># This 'product_pi' is a cumulative product </span></span><br><span class="line">        <span class="comment"># of the (1/P) factors that are calculated in the loop</span></span><br><span class="line">        product_pi *= (<span class="number">1</span>/probability)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take the Nth root of the product</span></span><br><span class="line">    perplexity = product_pi ** (<span class="number">1</span>/N)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> perplexity</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line"></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">                 [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perplexity_train1 = calculate_perplexity(sentences[<span class="number">0</span>],</span><br><span class="line">                                         unigram_counts, bigram_counts,</span><br><span class="line">                                         len(unique_words), k=<span class="number">1.0</span>)</span><br><span class="line">print(<span class="string">f"Perplexity for first train sample: <span class="subst">&#123;perplexity_train1:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">test_sentence = [<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'dog'</span>]</span><br><span class="line">perplexity_test = calculate_perplexity(test_sentence,</span><br><span class="line">                                       unigram_counts, bigram_counts,</span><br><span class="line">                                       len(unique_words), k=<span class="number">1.0</span>)</span><br><span class="line">print(<span class="string">f"Perplexity for test sample: <span class="subst">&#123;perplexity_test:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Perplexity for first train sample: 2.8040Perplexity for test sample: 3.9654</code></pre><h3 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Perplexity <span class="keyword">for</span> first train sample: <span class="number">2.8040</span></span><br><span class="line">Perplexity <span class="keyword">for</span> test sample: <span class="number">3.9654</span></span><br></pre></td></tr></table></figure><p><b> Note: </b> If your sentence is really long, there will be underflow when multiplying many fractions.</p><ul><li>To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.</li></ul><p><a name="4"></a></p><h2 id="Part-4-Build-an-auto-complete-system"><a href="#Part-4-Build-an-auto-complete-system" class="headerlink" title="Part 4: Build an auto-complete system"></a>Part 4: Build an auto-complete system</h2><p>In this section, you will combine the language models developed so far to implement an auto-complete system. </p><p><a name="ex-11"></a></p><h3 id="Exercise-11"><a href="#Exercise-11" class="headerlink" title="Exercise 11"></a>Exercise 11</h3><p>Compute probabilities for all possible next words and suggest the most likely one.</p><ul><li>This function also take an optional argument <code>start_with</code>, which specifies the first few letters of the next words.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li><code>estimate_probabilities</code> returns a dictionary where the key is a word and the value is the word's probability.</li>    <li> Use <code>str1.startswith(str2)</code> to determine if a string starts with the letters of another string.  For example, <code>'learning'.startswith('lea')</code> returns True, whereas <code>'learning'.startswith('ear')</code> returns False. There are two additional parameters in <code>str.startswith()</code>, but you can use the default values for those parameters in this case.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: suggest_a_word</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">suggest_a_word</span><span class="params">(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>, start_with=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Get suggestion for the next word</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        previous_tokens: The sentence you input where each token is a word. Must have length &gt; n </span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary: List of words</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">        start_with: If not None, specifies the first few letters of the next word</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A tuple of </span></span><br><span class="line"><span class="string">          - string of the most likely next word</span></span><br><span class="line"><span class="string">          - corresponding probability</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># length of previous words</span></span><br><span class="line">    n = len(list(n_gram_counts.keys())[<span class="number">0</span>]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># From the words that the user already typed</span></span><br><span class="line">    <span class="comment"># get the most recent 'n' words as the previous n-gram</span></span><br><span class="line">    previous_n_gram = previous_tokens[-n:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Estimate the probabilities that each word in the vocabulary</span></span><br><span class="line">    <span class="comment"># is the next word,</span></span><br><span class="line">    <span class="comment"># given the previous n-gram, the dictionary of n-gram counts,</span></span><br><span class="line">    <span class="comment"># the dictionary of n plus 1 gram counts, and the smoothing constant</span></span><br><span class="line">    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize suggested word to None</span></span><br><span class="line">    <span class="comment"># This will be set to the word with highest probability</span></span><br><span class="line">    suggestion = <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the highest word probability to 0</span></span><br><span class="line">    <span class="comment"># this will be set to the highest probability </span></span><br><span class="line">    <span class="comment"># of all words to be suggested</span></span><br><span class="line">    max_prob = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For each word and its probability in the probabilities dictionary:</span></span><br><span class="line">    <span class="keyword">for</span> word, prob <span class="keyword">in</span> probabilities.items(): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the optional start_with string is set</span></span><br><span class="line">        <span class="keyword">if</span> start_with: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the beginning of word does not match with the letters in 'start_with'</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> word.startswith(start_with): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># if they don't match, skip this word (move onto the next word)</span></span><br><span class="line">                 <span class="keyword">continue</span><span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if this word's probability</span></span><br><span class="line">        <span class="comment"># is greater than the current maximum probability</span></span><br><span class="line">        <span class="keyword">if</span> prob &gt; max_prob: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If so, save this word as the best suggestion (so far)</span></span><br><span class="line">            suggestion = word</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the new maximum probability</span></span><br><span class="line">            max_prob = prob</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> suggestion, max_prob</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">previous_tokens = [<span class="string">"i"</span>, <span class="string">"like"</span>]</span><br><span class="line">tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=<span class="number">1.0</span>)</span><br><span class="line">print(<span class="string">f"The previous words are 'i like',\n\tand the suggested word is `<span class="subst">&#123;tmp_suggest1[<span class="number">0</span>]&#125;</span>` with a probability of <span class="subst">&#123;tmp_suggest1[<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print()</span><br><span class="line"><span class="comment"># test your code when setting the starts_with</span></span><br><span class="line">tmp_starts_with = <span class="string">'c'</span></span><br><span class="line">tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=<span class="number">1.0</span>, start_with=tmp_starts_with)</span><br><span class="line">print(<span class="string">f"The previous words are 'i like', the suggestion must start with `<span class="subst">&#123;tmp_starts_with&#125;</span>`\n\tand the suggested word is `<span class="subst">&#123;tmp_suggest2[<span class="number">0</span>]&#125;</span>` with a probability of <span class="subst">&#123;tmp_suggest2[<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>The previous words are &#39;i like&#39;,    and the suggested word is `a` with a probability of 0.2727The previous words are &#39;i like&#39;, the suggestion must start with `c`    and the suggested word is `cat` with a probability of 0.0909</code></pre><h3 id="Expected-output-10"><a href="#Expected-output-10" class="headerlink" title="Expected output"></a>Expected output</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The previous words are 'i like',</span><br><span class="line">    <span class="keyword">and</span> the suggested word is `a` with a probability of <span class="number">0.2727</span></span><br><span class="line"></span><br><span class="line">The previous words are 'i like', the suggestion must start with `c`</span><br><span class="line">    <span class="keyword">and</span> the suggested word is `cat` with a probability of <span class="number">0.0909</span></span><br></pre></td></tr></table></figure><h3 id="Get-multiple-suggestions"><a href="#Get-multiple-suggestions" class="headerlink" title="Get multiple suggestions"></a>Get multiple suggestions</h3><p>The function defined below loop over varioud n-gram models to get multiple suggestions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_suggestions</span><span class="params">(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>, start_with=None)</span>:</span></span><br><span class="line">    model_counts = len(n_gram_counts_list)</span><br><span class="line">    suggestions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(model_counts<span class="number">-1</span>):</span><br><span class="line">        n_gram_counts = n_gram_counts_list[i]</span><br><span class="line">        n_plus1_gram_counts = n_gram_counts_list[i+<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        suggestion = suggest_a_word(previous_tokens, n_gram_counts,</span><br><span class="line">                                    n_plus1_gram_counts, vocabulary,</span><br><span class="line">                                    k=k, start_with=start_with)</span><br><span class="line">        suggestions.append(suggestion)</span><br><span class="line">    <span class="keyword">return</span> suggestions</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">quadgram_counts = count_n_grams(sentences, <span class="number">4</span>)</span><br><span class="line">qintgram_counts = count_n_grams(sentences, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]</span><br><span class="line">previous_tokens = [<span class="string">"i"</span>, <span class="string">"like"</span>]</span><br><span class="line">tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are 'i like', the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest3)</span><br></pre></td></tr></table></figure><pre><code>The previous words are &#39;i like&#39;, the suggestions are:[(&#39;a&#39;, 0.2727272727272727), (&#39;a&#39;, 0.2), (&#39;dog&#39;, 0.1111111111111111), (&#39;dog&#39;, 0.1111111111111111)]</code></pre><h3 id="Suggest-multiple-words-using-n-grams-of-varying-length"><a href="#Suggest-multiple-words-using-n-grams-of-varying-length" class="headerlink" title="Suggest multiple words using n-grams of varying length"></a>Suggest multiple words using n-grams of varying length</h3><p>Congratulations!  You have developed all building blocks for implementing your own auto-complete systems.</p><p>Letâ€™s see this with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-gramsâ€¦6-grams).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_gram_counts_list = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    print(<span class="string">"Computing n-gram counts with n ="</span>, n, <span class="string">"..."</span>)</span><br><span class="line">    n_model_counts = count_n_grams(train_data_processed, n)</span><br><span class="line">    n_gram_counts_list.append(n_model_counts)</span><br></pre></td></tr></table></figure><pre><code>Computing n-gram counts with n = 1 ...Computing n-gram counts with n = 2 ...Computing n-gram counts with n = 3 ...Computing n-gram counts with n = 4 ...Computing n-gram counts with n = 5 ...</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"i"</span>, <span class="string">"am"</span>, <span class="string">"to"</span>]</span><br><span class="line">tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest4)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;i&#39;, &#39;am&#39;, &#39;to&#39;], the suggestions are:[(&#39;be&#39;, 0.027665685098338604), (&#39;have&#39;, 0.00013487086115044844), (&#39;have&#39;, 0.00013490725126475548), (&#39;i&#39;, 6.746272684341901e-05)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"i"</span>, <span class="string">"want"</span>, <span class="string">"to"</span>, <span class="string">"go"</span>]</span><br><span class="line">tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest5)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;i&#39;, &#39;want&#39;, &#39;to&#39;, &#39;go&#39;], the suggestions are:[(&#39;to&#39;, 0.014051961029228078), (&#39;to&#39;, 0.004697942168993581), (&#39;to&#39;, 0.0009424436216762033), (&#39;to&#39;, 0.0004044489383215369)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"hey"</span>, <span class="string">"how"</span>, <span class="string">"are"</span>]</span><br><span class="line">tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest6)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;], the suggestions are:[(&#39;you&#39;, 0.023426812585499317), (&#39;you&#39;, 0.003559435862995299), (&#39;you&#39;, 0.00013491635186184566), (&#39;i&#39;, 6.746272684341901e-05)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"hey"</span>, <span class="string">"how"</span>, <span class="string">"are"</span>, <span class="string">"you"</span>]</span><br><span class="line">tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest7)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;], the suggestions are:[(&quot;&#39;re&quot;, 0.023973994311255586), (&#39;?&#39;, 0.002888465830762161), (&#39;?&#39;, 0.0016134453781512605), (&#39;&lt;e&gt;&#39;, 0.00013491635186184566)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"hey"</span>, <span class="string">"how"</span>, <span class="string">"are"</span>, <span class="string">"you"</span>]</span><br><span class="line">tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>, start_with=<span class="string">"d"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest8)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;], the suggestions are:[(&#39;do&#39;, 0.009020723283218204), (&#39;doing&#39;, 0.0016411737674785006), (&#39;doing&#39;, 0.00047058823529411766), (&#39;dvd&#39;, 6.745817593092283e-05)]</code></pre><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>Youâ€™ve completed this assignment by building an autocomplete model using an n-gram language model!  </p><p>Please continue onto the fourth and final week of this course!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Language-Models-Auto-Complete&quot;&gt;&lt;a href=&quot;#Language-Models-Auto-Complete&quot; class=&quot;headerlink&quot; title=&quot;Language Models: Auto-Complete&quot;&gt;&lt;/
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Auto Correct</title>
    <link href="https://zhangruochi.com/Auto-Correct/2020/07/19/"/>
    <id>https://zhangruochi.com/Auto-Correct/2020/07/19/</id>
    <published>2020-07-19T08:27:04.000Z</published>
    <updated>2020-07-19T08:27:50.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-1-Auto-Correct"><a href="#Assignment-1-Auto-Correct" class="headerlink" title="Assignment 1: Auto Correct"></a>Assignment 1: Auto Correct</h1><p>Welcome to the first assignment of Course 2. This assignment will give you a chance to brush up on your python and probability skills. In doing so, you will implement an auto-correct system that is very effective and useful.</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">0. Overview</a><ul><li><a href="#0-1">0.1 Edit Distance</a></li></ul></li><li><a href="#1">1. Data Preprocessing</a><ul><li><a href="#ex-1">1.1 Exercise 1</a></li><li><a href="#ex-2">1.2 Exercise 2</a></li><li><a href="#ex-3">1.3 Exercise 3</a></li></ul></li><li><a href="#2">2. String Manipulation</a><ul><li><a href="#ex-4">2.1 Exercise 4</a></li><li><a href="#ex-5">2.2 Exercise 5</a></li><li><a href="#ex-6">2.3 Exercise 6</a></li><li><a href="#ex-7">2.4 Exercise 7</a></li></ul></li><li><a href="#3">3. Combining the edits</a><ul><li><a href="#ex-8">3.1 Exercise 8</a></li><li><a href="#ex-9">3.2 Exercise 9</a></li><li><a href="#ex-10">3.3 Exercise 10</a></li></ul></li><li><a href="#4">4. Minimum Edit Distance</a><ul><li><a href="#ex-11">4.1 Exercise 11</a></li></ul></li><li><a href="#5">5. Backtrace (Optional)</a></li></ul><p><a name="0"></a></p><h2 id="0-Overview"><a href="#0-Overview" class="headerlink" title="0. Overview"></a>0. Overview</h2><p>You use autocorrect every day on your cell phone and computer. In this assignment, you will explore what really goes on behind the scenes. Of course, the model you are about to implement is not identical to the one used in your phone, but it is still quite good. </p><p>By completing this assignment you will learn how to: </p><ul><li>Get a word count given a corpus</li><li>Get a word probability in the corpus </li><li>Manipulate strings </li><li>Filter strings </li><li>Implement Minimum edit distance to compare strings and to help find the optimal path for the edits. </li><li>Understand how dynamic programming works</li></ul><p>Similar systems are used everywhere. </p><ul><li>For example, if you type in the word <strong>â€œI am lerninggâ€</strong>, chances are very high that you meant to write <strong>â€œlearningâ€</strong>, as shown in <strong>Figure 1</strong>. </li></ul><div style="width:image width px; font-size:100%; text-align:center;"><img src="auto-correct.png" alt="alternate text" width="width" height="height" style="width:300px;height:250px;"> Figure 1 </div><p><a name="0-1"></a></p><h4 id="0-1-Edit-Distance"><a href="#0-1-Edit-Distance" class="headerlink" title="0.1 Edit Distance"></a>0.1 Edit Distance</h4><p>In this assignment, you will implement models that correct words that are 1 and 2 edit distances away. </p><ul><li>We say two words are n edit distance away from each other when we need n edits to change one word into another. </li></ul><p>An edit could consist of one of the following options: </p><ul><li>Delete (remove a letter): â€˜hatâ€™ =&gt; â€˜at, ha, htâ€™</li><li>Switch (swap 2 adjacent letters): â€˜etaâ€™ =&gt; â€˜eat, tea,â€¦â€™</li><li>Replace (change 1 letter to another): â€˜jatâ€™ =&gt; â€˜hat, rat, cat, mat, â€¦â€™</li><li>Insert (add a letter): â€˜teâ€™ =&gt; â€˜the, ten, ate, â€¦â€™</li></ul><p>You will be using the four methods above to implement an Auto-correct. </p><ul><li>To do so, you will need to compute probabilities that a certain word is correct given an input. </li></ul><p>This auto-correct you are about to implement was first created by <a href="https://en.wikipedia.org/wiki/Peter_Norvig" target="_blank" rel="noopener">Peter Norvig</a> in 2007. </p><ul><li>His <a href="https://norvig.com/spell-correct.html" target="_blank" rel="noopener">original article</a> may be a useful reference for this assignment.</li></ul><p>The goal of our spell check model is to compute the following probability:</p><script type="math/tex; mode=display">P(c|w) = \frac{P(w|c)\times P(c)}{P(w)} \tag{Eqn-1}</script><p>The equation above is <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="noopener">Bayes Rule</a>. </p><ul><li>Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word $w$, given that it is correct $P(w|c)$, multiplied by the probability of being correct in general $P(C)$ divided by the probability of that word $w$ appearing $P(w)$ in general.</li><li>To compute equation 1, you will first import a data set and then create all the probabilities that you need using that data set. </li></ul><p><a name="1"></a></p><h1 id="Part-1-Data-Preprocessing"><a href="#Part-1-Data-Preprocessing" class="headerlink" title="Part 1: Data Preprocessing"></a>Part 1: Data Preprocessing</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><p>As in any other machine learning task, the first thing you have to do is process your data set. </p><ul><li>Many courses load in pre-processed data for you. </li><li>However, in the real world, when you build these NLP systems, you load the datasets and process them.</li><li>So letâ€™s get some real world practice in pre-processing the data!</li></ul><p>Your first task is to read in a file called <strong>â€˜shakespeare.txtâ€™</strong> which is found in your file directory. To look at this file you can go to <code>File ==&gt; Open</code>. </p><p><a name="ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><p>Implement the function <code>process_data</code> which </p><p>1) Reads in a corpus (text file)</p><p>2) Changes everything to lowercase</p><p>3) Returns a list of words. </p><h4 id="Options-and-Hints"><a href="#Options-and-Hints" class="headerlink" title="Options and Hints"></a>Options and Hints</h4><ul><li>If you would like more of a real-life practice, donâ€™t open the â€˜Hintsâ€™ below (yet) and try searching the web to derive your answer.</li><li>If you want a little help, click on the green â€œGeneral Hintsâ€ section by clicking on it with your mouse.</li><li>If you get stuck or are not getting the expected results, click on the green â€˜Detailed Hintsâ€™ section to get hints for each step that youâ€™ll take to complete this function.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>General Hints</b></font></summary></p><p>General Hints to get started<ul>    <li>Python <a href="https://docs.python.org/3/tutorial/inputoutput.html" target="_blank" rel="noopener">input and output<a></a></a></li>    <li>Python <a href="https://docs.python.org/3/library/re.html" target="_blank" rel="noopener">'re' documentation </a> </li></ul></p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Detailed Hints</b></font></summary></p><p>     Detailed hints if you're stuck<ul>    <li>Use 'with' syntax to read a file</li>    <li>Decide whether to use 'read()' or 'readline().  What's the difference?</li>    <li>Choose whether to use either str.lower() or str.lowercase().  What is the difference?</li>    <li>Use re.findall(pattern, string)</li>    <li>Look for the "Raw String Notation" section in the Python 're' documentation to understand the difference between r'\W', r'\W' and '\\W'. </li>    <li>For the pattern, decide between using '\s', '\w', '\s+' or '\w+'.  What do you think are the differences?</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: process_data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_data</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        A file_name which is found in your current directory. You just have to read it in. </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        words: a list containing all the words in the corpus (text file you read) in lower case. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words = [] <span class="comment"># return this variable correctly</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> open(file_name, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        content = f.read().lower()</span><br><span class="line">    </span><br><span class="line">    words = re.findall(<span class="string">'\w+'</span>,content)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> words</span><br></pre></td></tr></table></figure><p>Note, in the following cell, â€˜wordsâ€™ is converted to a python <code>set</code>. This eliminates any duplicate entries.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">word_l = process_data(<span class="string">'shakespeare.txt'</span>)</span><br><span class="line">vocab = set(word_l)  <span class="comment"># this will be your new vocabulary</span></span><br><span class="line">print(<span class="string">f"The first ten words in the text are: \n<span class="subst">&#123;word_l[<span class="number">0</span>:<span class="number">10</span>]&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;len(vocab)&#125;</span> unique words in the vocabulary."</span>)</span><br></pre></td></tr></table></figure><pre><code>The first ten words in the text are: [&#39;o&#39;, &#39;for&#39;, &#39;a&#39;, &#39;muse&#39;, &#39;of&#39;, &#39;fire&#39;, &#39;that&#39;, &#39;would&#39;, &#39;ascend&#39;, &#39;the&#39;]There are 6116 unique words in the vocabulary.</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The first ten words <span class="keyword">in</span> the text are: </span><br><span class="line">[<span class="string">'o'</span>, <span class="string">'for'</span>, <span class="string">'a'</span>, <span class="string">'muse'</span>, <span class="string">'of'</span>, <span class="string">'fire'</span>, <span class="string">'that'</span>, <span class="string">'would'</span>, <span class="string">'ascend'</span>, <span class="string">'the'</span>]</span><br><span class="line">There are <span class="number">6116</span> unique words <span class="keyword">in</span> the vocabulary.</span><br></pre></td></tr></table></figure><p><a name="ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><p>Implement a <code>get_count</code> function that returns a dictionary</p><ul><li>The dictionaryâ€™s keys are words</li><li>The value for each word is the number of times that word appears in the corpus. </li></ul><p>For example, given the following sentence: <strong>â€œI am happy because I am learningâ€</strong>, your dictionary should return the following: </p><table style="width:20%">  <tr>    <td> <b>Key </b>  </td>    <td> <b>Value </b> </td>   </tr>  <tr>    <td> I  </td>    <td> 2</td>   </tr>  <tr>    <td>am</td>    <td>2</td>   </tr>  <tr>    <td>happy</td>    <td>1</td>   </tr>   <tr>    <td>because</td>    <td>1</td>   </tr>   <tr>    <td>learning</td>    <td>1</td>   </tr></table><p><strong>Instructions</strong>:<br>Implement a <code>get_count</code> which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.  </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Try implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions</li>    <li>You can also use defaultdict instead of a regualr dictionary, along with the for loop</li>    <li>Otherwise, to skip using a for loop, you can use Python's <a href="https://docs.python.org/3.7/library/collections.html#collections.Counter" target="_blank" rel="noopener"> Counter class</a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_count</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_count</span><span class="params">(word_l)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word_l: a set of words representing the corpus. </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    word_count_dict = &#123;&#125;  <span class="comment"># fill this with word counts</span></span><br><span class="line">    <span class="comment">### START CODE HERE </span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_l:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_count_dict:</span><br><span class="line">            word_count_dict[word] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            word_count_dict[word] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> word_count_dict</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">word_count_dict = get_count(word_l)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;len(word_count_dict)&#125;</span> key values pairs"</span>)</span><br><span class="line">print(<span class="string">f"The count for the word 'thee' is <span class="subst">&#123;word_count_dict.get(<span class="string">'thee'</span>,<span class="number">0</span>)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>There are 6116 key values pairsThe count for the word &#39;thee&#39; is 240</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">There are <span class="number">6116</span> key values pairs</span><br><span class="line">The count <span class="keyword">for</span> the word <span class="string">'thee'</span> <span class="keyword">is</span> <span class="number">240</span></span><br></pre></td></tr></table></figure><p><a name="ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><p>Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.</p><script type="math/tex; mode=display">P(w_i) = \frac{C(w_i)}{M} \tag{Eqn-2}</script><p>where </p><p>$C(w_i)$ is the total number of times $w_i$ appears in the corpus.</p><p>$M$ is the total number of words in the corpus.</p><p>For example, the probability of the word â€˜amâ€™ in the sentence <strong>â€˜I am happy because I am learningâ€™</strong> is:</p><script type="math/tex; mode=display">P(am) = \frac{C(w_i)}{M} = \frac {2}{7} \tag{Eqn-3}.</script><p><strong>Instructions:</strong> Implement <code>get_probs</code> function which gives you the probability<br>that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p>General advice<ul>    <li> Use dictionary.values() </li>    <li> Use sum() </li>    <li> The cardinality (number of words in the corpus should be equal to len(word_l).  You will calculate this same number, but using the word count dictionary.</li></ul>If you're using a for loop:<ul>    <li> Use dictionary.keys() </li></ul>If you're using a dictionary comprehension:<ul>    <li>Use dictionary.items() </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_probs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probs</span><span class="params">(word_count_dict)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        probs: A dictionary where keys are the words and the values are the probability that a word will occur. </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    probs = &#123;&#125;  <span class="comment"># return this variable correctly</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    M = np.sum(list(word_count_dict.values()))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word, C <span class="keyword">in</span> word_count_dict.items():</span><br><span class="line">        probs[word] =  float(C) / M</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> probs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">probs = get_probs(word_count_dict)</span><br><span class="line">print(<span class="string">f"Length of probs is <span class="subst">&#123;len(probs)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"P('thee') is <span class="subst">&#123;probs[<span class="string">'thee'</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Length of probs is 6116P(&#39;thee&#39;) is 0.0045</code></pre><h4 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Length of probs <span class="keyword">is</span> <span class="number">6116</span></span><br><span class="line">P(<span class="string">'thee'</span>) <span class="keyword">is</span> <span class="number">0.0045</span></span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h1 id="Part-2-String-Manipulations"><a href="#Part-2-String-Manipulations" class="headerlink" title="Part 2: String Manipulations"></a>Part 2: String Manipulations</h1><p>Now, that you have computed $P(w_i)$ for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, you will implement four functions: </p><ul><li><code>delete_letter</code>: given a word, it returns all the possible strings that have <strong>one character removed</strong>. </li><li><code>switch_letter</code>: given a word, it returns all the possible strings that have <strong>two adjacent letters switched</strong>.</li><li><code>replace_letter</code>: given a word, it returns all the possible strings that have <strong>one character replaced by another different letter</strong>.</li><li><code>insert_letter</code>: given a word, it returns all the possible strings that have an <strong>additional character inserted</strong>. </li></ul><h4 id="List-comprehensions"><a href="#List-comprehensions" class="headerlink" title="List comprehensions"></a>List comprehensions</h4><p>String and list manipulation in python will often make use of a python feature called  <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions" target="_blank" rel="noopener">list comprehensions</a>. The routines below will be described as using list comprehensions, but if you would rather implement them in another way, you are free to do so as long as the result is the same. Further, the following section will provide detailed instructions on how to use list comprehensions and how to implement the desired functions. If you are a python expert, feel free to skip the python hints and move to implementing the routines directly.</p><p>Python List Comprehensions embed a looping structure inside of a list declaration, collapsing many lines of code into a single line. If you are not familiar with them, they seem slightly out of order relative to for loops. </p><div style="width:image width px; font-size:100%; text-align:center;"><img src="GenericListComp3.PNG" alt="alternate text" width="width" height="height" style="width:800px;height:400px;"> Figure 2 </div><p>The diagram above shows that the components of a list comprehension are the same components you would find in a typical for loop that appends to a list, but in a different order. With that in mind, weâ€™ll continue the specifics of this assignment. We will be very descriptive for the first function, <code>deletes()</code>, and less so in later functions as you become familiar with list comprehensions.</p><p><a name="ex-4"></a></p><h3 id="Exercise-4"><a href="#Exercise-4" class="headerlink" title="Exercise 4"></a>Exercise 4</h3><p><strong>Instructions for delete_letter():</strong> Implement a <code>delete_letter()</code> function that, given a word, returns a list of strings with one character deleted. </p><p>For example, given the word <strong>nice</strong>, it would return the set: {â€˜iceâ€™, â€˜nceâ€™, â€˜nicâ€™, â€˜nieâ€™}. </p><p><strong>Step 1:</strong> Create a list of â€˜splitsâ€™. This is all the ways you can split a word into Left and Right: For example,<br>â€˜nice is split into : <code>[(&#39;&#39;, &#39;nice&#39;), (&#39;n&#39;, &#39;ice&#39;), (&#39;ni&#39;, &#39;ce&#39;), (&#39;nic&#39;, &#39;e&#39;), (&#39;nice&#39;, &#39;&#39;)]</code><br>This is common to all four functions (delete, replace, switch, insert).</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="Splits1.PNG" alt="alternate text" width="width" height="height" style="width:650px;height:200px;"> Figure 3 </div><p><strong>Step 2:</strong> This is specific to <code>delete_letter</code>. Here, we are generating all words that result from deleting one character.<br>This can be done in a single line with a list comprehension. You can makes use of this type of syntax:<br><code>[f(a,b) for a, b in splits if condition]</code>  </p><p>For our â€˜niceâ€™ example you get:<br>[â€˜iceâ€™, â€˜nceâ€™, â€˜nieâ€™, â€˜nicâ€™]</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="ListComp2.PNG" alt="alternate text" width="width" height="height" style="width:550px;height:300px;"> Figure 4 </div><h4 id="Levels-of-assistance"><a href="#Levels-of-assistance" class="headerlink" title="Levels of assistance"></a>Levels of assistance</h4><p>Try this exercise with these levels of assistance.  </p><ul><li>We hope that this will make it both a meaningful experience but also not a frustrating experience. </li><li><p>Start with level 1, then move onto level 2, and 3 as needed.</p><ul><li>Level 1. Try to think this through and implement this yourself.</li><li>Level 2. Click on the â€œLevel 2 Hintsâ€ section for some hints to get started.</li><li>Level 3. If you would prefer more guidance, please click on the â€œLevel 3 Hintsâ€ cell for step by step instructions.</li></ul></li><li><p>If you are still stuck, look at the images in the â€œlist comprehensionsâ€ section above.</p></li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Level 2 Hints</b></font></summary></p><p><ul>    <li><a href> Use array slicing like my_string[0:2] </a> </li>    <li><a href> Use list comprehensions or for loops </a> </li></ul></p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Level 3 Hints</b></font></summary></p><p><ul>    <li>splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.</li>    <li>Do this in a loop or list comprehension, so that you have a list of tuples.    </li><li> For example, "cake" can get split into "ca" and "ke". They're stored in a tuple ("ca","ke"), and the tuple is appended to a list.  We'll refer to these as L and R, so the tuple is (L,R)</li>    <li>When choosing the range for your loop, if you input the word "cans" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) </li>    <li>deletes: Go through the list of tuples and combine the two strings together. You can use the + operator to combine two strings</li>    <li>When combining the tuples, make sure that you leave out a middle character.</li>    <li>Use array slicing to leave out the first character of the right substring.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: deletes</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete_letter</span><span class="params">(word, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the string/word for which you will generate all possible words </span></span><br><span class="line"><span class="string">                in the vocabulary which have 1 missing character</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        delete_l: a list of all possible strings obtained by deleting 1 character from word</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    delete_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    delete_l = [L + R[<span class="number">1</span>:] <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> R]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">f"input word <span class="subst">&#123;word&#125;</span>, \nsplit_l = <span class="subst">&#123;split_l&#125;</span>, \ndelete_l = <span class="subst">&#123;delete_l&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> delete_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">delete_word_l = delete_letter(word=<span class="string">"cans"</span>,</span><br><span class="line">                        verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>input word cans, split_l = [(&#39;&#39;, &#39;cans&#39;), (&#39;c&#39;, &#39;ans&#39;), (&#39;ca&#39;, &#39;ns&#39;), (&#39;can&#39;, &#39;s&#39;), (&#39;cans&#39;, &#39;&#39;)], delete_l = [&#39;ans&#39;, &#39;cns&#39;, &#39;cas&#39;, &#39;can&#39;]</code></pre><h4 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input word cans, </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'cans'</span>), (<span class="string">'c'</span>, <span class="string">'ans'</span>), (<span class="string">'ca'</span>, <span class="string">'ns'</span>), (<span class="string">'can'</span>, <span class="string">'s'</span>)], </span><br><span class="line">delete_l = [<span class="string">'ans'</span>, <span class="string">'cns'</span>, <span class="string">'cas'</span>, <span class="string">'can'</span>]</span><br></pre></td></tr></table></figure><h4 id="Note-1"><a href="#Note-1" class="headerlink" title="Note 1"></a>Note 1</h4><p>You might get a slightly different result with split_l.  </p><ul><li>Notice how it has the extra tuple <code>(&#39;cans&#39;, &#39;&#39;)</code>.</li><li>This will be fine as long as you have checked the size of the right-side substring in tuple (L,R).</li><li>Can you explain why this will give you the same result for the list of deletion strings (delete_l)?</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input word cans, </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'cans'</span>), (<span class="string">'c'</span>, <span class="string">'ans'</span>), (<span class="string">'ca'</span>, <span class="string">'ns'</span>), (<span class="string">'can'</span>, <span class="string">'s'</span>), (<span class="string">'cans'</span>, <span class="string">''</span>)], </span><br><span class="line">delete_l = [<span class="string">'ans'</span>, <span class="string">'cns'</span>, <span class="string">'cas'</span>, <span class="string">'can'</span>]</span><br></pre></td></tr></table></figure><h4 id="Note-2"><a href="#Note-2" class="headerlink" title="Note 2"></a>Note 2</h4><p>If you end up getting the same word as your input word, like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input word cans, </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'cans'</span>), (<span class="string">'c'</span>, <span class="string">'ans'</span>), (<span class="string">'ca'</span>, <span class="string">'ns'</span>), (<span class="string">'can'</span>, <span class="string">'s'</span>), (<span class="string">'cans'</span>, <span class="string">''</span>)], </span><br><span class="line">delete_l = [<span class="string">'ans'</span>, <span class="string">'cns'</span>, <span class="string">'cas'</span>, <span class="string">'can'</span>, <span class="string">'cans'</span>]</span><br></pre></td></tr></table></figure><ul><li>Check how you set the <code>range</code>.</li><li>See if you check the length of the string on the right-side of the split.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line">print(<span class="string">f"Number of outputs of delete_letter('at') is <span class="subst">&#123;len(delete_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of outputs of delete_letter(&#39;at&#39;) is 2</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of outputs of delete_letter('at') is 2</span><br></pre></td></tr></table></figure><p><a name="ex-5"></a></p><h3 id="Exercise-5"><a href="#Exercise-5" class="headerlink" title="Exercise 5"></a>Exercise 5</h3><p><strong>Instructions for switch_letter()</strong>: Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters <strong>that are adjacent to each other</strong>. </p><ul><li>For example, given the word â€˜etaâ€™, it returns {â€˜eatâ€™, â€˜teaâ€™}, but does not return â€˜ateâ€™.</li></ul><p><strong>Step 1:</strong> is the same as in delete_letter()<br><strong>Step 2:</strong> A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:<br><code>[f(L,R) for L, R in splits if condition]</code>  where â€˜conditionâ€™ will test the length of R in a given iteration. See below.</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="Switches1.PNG" alt="alternate text" width="width" height="height" style="width:600px;height:200px;"> Figure 5 </div>      <h4 id="Levels-of-difficulty"><a href="#Levels-of-difficulty" class="headerlink" title="Levels of difficulty"></a>Levels of difficulty</h4><p>Try this exercise with these levels of difficulty.  </p><ul><li>Level 1. Try to think this through and implement this yourself.</li><li>Level 2. Click on the â€œLevel 2 Hintsâ€ section for some hints to get started.</li><li>Level 3. If you would prefer more guidance, please click on the â€œLevel 3 Hintsâ€ cell for step by step instructions.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Level 2 Hints</b></font></summary></p><p><ul>    <li><a href> Use array slicing like my_string[0:2] </a> </li>    <li><a href> Use list comprehensions or for loops </a> </li>    <li>To do a switch, think of the whole word as divided into 4 distinct parts.  Write out 'cupcakes' on a piece of paper and see how you can split it into ('cupc', 'k', 'a', 'es')</li></ul></p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Level 3 Hints</b></font></summary></p><p><ul>    <li>splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.</li>    <li>Splitting is the same as for delete_letter</li>    <li>To perform the switch, go through the list of tuples and combine four strings together. You can use the + operator to combine strings</li>    <li>The four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.</li>    <li>Unlike delete_letter, you will want to check that your right substring is at least a minimum length.  To see why, review the previous hint bullet point (directly before this one).</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: switches</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">switch_letter</span><span class="params">(word, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: input string</span></span><br><span class="line"><span class="string">     Output:</span></span><br><span class="line"><span class="string">        switches: a list of all possible strings with one adjacent charater switched</span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    </span><br><span class="line">    switch_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(L,R)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> L[:<span class="number">-1</span>] + R[<span class="number">0</span>] + L[<span class="number">-1</span>] + R[<span class="number">1</span>:]</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    switch_l = [f(L,R) <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> L <span class="keyword">and</span> R]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">f"Input word = <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \nswitch_l = <span class="subst">&#123;switch_l&#125;</span>"</span>) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> switch_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">switch_word_l = switch_letter(word=<span class="string">"eta"</span>,</span><br><span class="line">                         verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Input word = eta split_l = [(&#39;&#39;, &#39;eta&#39;), (&#39;e&#39;, &#39;ta&#39;), (&#39;et&#39;, &#39;a&#39;), (&#39;eta&#39;, &#39;&#39;)] switch_l = [&#39;tea&#39;, &#39;eat&#39;]</code></pre><h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = eta </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'eta'</span>), (<span class="string">'e'</span>, <span class="string">'ta'</span>), (<span class="string">'et'</span>, <span class="string">'a'</span>)] </span><br><span class="line">switch_l = [<span class="string">'tea'</span>, <span class="string">'eat'</span>]</span><br></pre></td></tr></table></figure><h4 id="Note-1-1"><a href="#Note-1-1" class="headerlink" title="Note 1"></a>Note 1</h4><p>You may get this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = eta </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'eta'</span>), (<span class="string">'e'</span>, <span class="string">'ta'</span>), (<span class="string">'et'</span>, <span class="string">'a'</span>), (<span class="string">'eta'</span>, <span class="string">''</span>)] </span><br><span class="line">switch_l = [<span class="string">'tea'</span>, <span class="string">'eat'</span>]</span><br></pre></td></tr></table></figure></p><ul><li>Notice how it has the extra tuple <code>(&#39;eta&#39;, &#39;&#39;)</code>.</li><li>This is also correct.</li><li>Can you think of why this is the case?</li></ul><h4 id="Note-2-1"><a href="#Note-2-1" class="headerlink" title="Note 2"></a>Note 2</h4><p>If you get an error<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IndexError: string index out of range</span><br></pre></td></tr></table></figure></p><ul><li>Please see if you have checked the length of the strings when switching characters.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line">print(<span class="string">f"Number of outputs of switch_letter('at') is <span class="subst">&#123;len(switch_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of outputs of switch_letter(&#39;at&#39;) is 1</code></pre><h4 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of outputs of switch_letter('at') is 1</span><br></pre></td></tr></table></figure><p><a name="ex-6"></a></p><h3 id="Exercise-6"><a href="#Exercise-6" class="headerlink" title="Exercise 6"></a>Exercise 6</h3><p><strong>Instructions for replace_letter()</strong>: Now implement a function that takes in a word and returns a list of strings with one <strong>replaced letter</strong> from the original word. </p><p><strong>Step 1:</strong> is the same as in <code>delete_letter()</code></p><p><strong>Step 2:</strong> A list comprehension or for loop which form strings by replacing letters.  This can be of the form:<br><code>[f(a,b,c) for a, b in splits if condition for c in string]</code>   Note the use of the second for loop.<br>It is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of â€˜earâ€™ with â€˜eâ€™ will return â€˜earâ€™.</p><p><strong>Step 3:</strong> Remove the original input letter from the output.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>To remove a word from a list, first store its contents inside a set()</li>    <li>Use set.discard('the_word') to remove a word in a set (if the word does not exist in the set, then it will not throw a KeyError.  Using set.remove('the_word') throws a KeyError if the word does not exist in the set. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: replaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_letter</span><span class="params">(word, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        replaces: a list of all possible strings where we replaced one letter from the original word. </span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    </span><br><span class="line">    letters = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">    replace_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    replace_set = [L + C + R[<span class="number">1</span>:] <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> R <span class="keyword">for</span> C <span class="keyword">in</span> letters <span class="keyword">if</span> C <span class="keyword">is</span> <span class="keyword">not</span> R[<span class="number">0</span>]]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># turn the set back into a list and sort it, for easier viewing</span></span><br><span class="line">    replace_l = sorted(list(replace_set))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">f"Input word = <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \nreplace_l <span class="subst">&#123;replace_l&#125;</span>"</span>)   </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> replace_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">replace_l = replace_letter(word=<span class="string">'can'</span>,</span><br><span class="line">                              verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Input word = can split_l = [(&#39;&#39;, &#39;can&#39;), (&#39;c&#39;, &#39;an&#39;), (&#39;ca&#39;, &#39;n&#39;), (&#39;can&#39;, &#39;&#39;)] replace_l [&#39;aan&#39;, &#39;ban&#39;, &#39;caa&#39;, &#39;cab&#39;, &#39;cac&#39;, &#39;cad&#39;, &#39;cae&#39;, &#39;caf&#39;, &#39;cag&#39;, &#39;cah&#39;, &#39;cai&#39;, &#39;caj&#39;, &#39;cak&#39;, &#39;cal&#39;, &#39;cam&#39;, &#39;cao&#39;, &#39;cap&#39;, &#39;caq&#39;, &#39;car&#39;, &#39;cas&#39;, &#39;cat&#39;, &#39;cau&#39;, &#39;cav&#39;, &#39;caw&#39;, &#39;cax&#39;, &#39;cay&#39;, &#39;caz&#39;, &#39;cbn&#39;, &#39;ccn&#39;, &#39;cdn&#39;, &#39;cen&#39;, &#39;cfn&#39;, &#39;cgn&#39;, &#39;chn&#39;, &#39;cin&#39;, &#39;cjn&#39;, &#39;ckn&#39;, &#39;cln&#39;, &#39;cmn&#39;, &#39;cnn&#39;, &#39;con&#39;, &#39;cpn&#39;, &#39;cqn&#39;, &#39;crn&#39;, &#39;csn&#39;, &#39;ctn&#39;, &#39;cun&#39;, &#39;cvn&#39;, &#39;cwn&#39;, &#39;cxn&#39;, &#39;cyn&#39;, &#39;czn&#39;, &#39;dan&#39;, &#39;ean&#39;, &#39;fan&#39;, &#39;gan&#39;, &#39;han&#39;, &#39;ian&#39;, &#39;jan&#39;, &#39;kan&#39;, &#39;lan&#39;, &#39;man&#39;, &#39;nan&#39;, &#39;oan&#39;, &#39;pan&#39;, &#39;qan&#39;, &#39;ran&#39;, &#39;san&#39;, &#39;tan&#39;, &#39;uan&#39;, &#39;van&#39;, &#39;wan&#39;, &#39;xan&#39;, &#39;yan&#39;, &#39;zan&#39;]</code></pre><h4 id="Expected-Output-4"><a href="#Expected-Output-4" class="headerlink" title="Expected Output**:"></a>Expected Output**:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'can'</span>), (<span class="string">'c'</span>, <span class="string">'an'</span>), (<span class="string">'ca'</span>, <span class="string">'n'</span>)] </span><br><span class="line">replace_l [<span class="string">'aan'</span>, <span class="string">'ban'</span>, <span class="string">'caa'</span>, <span class="string">'cab'</span>, <span class="string">'cac'</span>, <span class="string">'cad'</span>, <span class="string">'cae'</span>, <span class="string">'caf'</span>, <span class="string">'cag'</span>, <span class="string">'cah'</span>, <span class="string">'cai'</span>, <span class="string">'caj'</span>, <span class="string">'cak'</span>, <span class="string">'cal'</span>, <span class="string">'cam'</span>, <span class="string">'cao'</span>, <span class="string">'cap'</span>, <span class="string">'caq'</span>, <span class="string">'car'</span>, <span class="string">'cas'</span>, <span class="string">'cat'</span>, <span class="string">'cau'</span>, <span class="string">'cav'</span>, <span class="string">'caw'</span>, <span class="string">'cax'</span>, <span class="string">'cay'</span>, <span class="string">'caz'</span>, <span class="string">'cbn'</span>, <span class="string">'ccn'</span>, <span class="string">'cdn'</span>, <span class="string">'cen'</span>, <span class="string">'cfn'</span>, <span class="string">'cgn'</span>, <span class="string">'chn'</span>, <span class="string">'cin'</span>, <span class="string">'cjn'</span>, <span class="string">'ckn'</span>, <span class="string">'cln'</span>, <span class="string">'cmn'</span>, <span class="string">'cnn'</span>, <span class="string">'con'</span>, <span class="string">'cpn'</span>, <span class="string">'cqn'</span>, <span class="string">'crn'</span>, <span class="string">'csn'</span>, <span class="string">'ctn'</span>, <span class="string">'cun'</span>, <span class="string">'cvn'</span>, <span class="string">'cwn'</span>, <span class="string">'cxn'</span>, <span class="string">'cyn'</span>, <span class="string">'czn'</span>, <span class="string">'dan'</span>, <span class="string">'ean'</span>, <span class="string">'fan'</span>, <span class="string">'gan'</span>, <span class="string">'han'</span>, <span class="string">'ian'</span>, <span class="string">'jan'</span>, <span class="string">'kan'</span>, <span class="string">'lan'</span>, <span class="string">'man'</span>, <span class="string">'nan'</span>, <span class="string">'oan'</span>, <span class="string">'pan'</span>, <span class="string">'qan'</span>, <span class="string">'ran'</span>, <span class="string">'san'</span>, <span class="string">'tan'</span>, <span class="string">'uan'</span>, <span class="string">'van'</span>, <span class="string">'wan'</span>, <span class="string">'xan'</span>, <span class="string">'yan'</span>, <span class="string">'zan'</span>]</span><br></pre></td></tr></table></figure><ul><li>Note how the input word â€˜canâ€™ should not be one of the output words.</li></ul><h4 id="Note-1-2"><a href="#Note-1-2" class="headerlink" title="Note 1"></a>Note 1</h4><p>If you get something like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'can'</span>), (<span class="string">'c'</span>, <span class="string">'an'</span>), (<span class="string">'ca'</span>, <span class="string">'n'</span>), (<span class="string">'can'</span>, <span class="string">''</span>)] </span><br><span class="line">replace_l [<span class="string">'aan'</span>, <span class="string">'ban'</span>, <span class="string">'caa'</span>, <span class="string">'cab'</span>, <span class="string">'cac'</span>, <span class="string">'cad'</span>, <span class="string">'cae'</span>, <span class="string">'caf'</span>, <span class="string">'cag'</span>, <span class="string">'cah'</span>, <span class="string">'cai'</span>, <span class="string">'caj'</span>, <span class="string">'cak'</span>, <span class="string">'cal'</span>, <span class="string">'cam'</span>, <span class="string">'cao'</span>, <span class="string">'cap'</span>, <span class="string">'caq'</span>, <span class="string">'car'</span>, <span class="string">'cas'</span>, <span class="string">'cat'</span>, <span class="string">'cau'</span>, <span class="string">'cav'</span>, <span class="string">'caw'</span>, <span class="string">'cax'</span>, <span class="string">'cay'</span>, <span class="string">'caz'</span>, <span class="string">'cbn'</span>, <span class="string">'ccn'</span>, <span class="string">'cdn'</span>, <span class="string">'cen'</span>, <span class="string">'cfn'</span>, <span class="string">'cgn'</span>, <span class="string">'chn'</span>, <span class="string">'cin'</span>, <span class="string">'cjn'</span>, <span class="string">'ckn'</span>, <span class="string">'cln'</span>, <span class="string">'cmn'</span>, <span class="string">'cnn'</span>, <span class="string">'con'</span>, <span class="string">'cpn'</span>, <span class="string">'cqn'</span>, <span class="string">'crn'</span>, <span class="string">'csn'</span>, <span class="string">'ctn'</span>, <span class="string">'cun'</span>, <span class="string">'cvn'</span>, <span class="string">'cwn'</span>, <span class="string">'cxn'</span>, <span class="string">'cyn'</span>, <span class="string">'czn'</span>, <span class="string">'dan'</span>, <span class="string">'ean'</span>, <span class="string">'fan'</span>, <span class="string">'gan'</span>, <span class="string">'han'</span>, <span class="string">'ian'</span>, <span class="string">'jan'</span>, <span class="string">'kan'</span>, <span class="string">'lan'</span>, <span class="string">'man'</span>, <span class="string">'nan'</span>, <span class="string">'oan'</span>, <span class="string">'pan'</span>, <span class="string">'qan'</span>, <span class="string">'ran'</span>, <span class="string">'san'</span>, <span class="string">'tan'</span>, <span class="string">'uan'</span>, <span class="string">'van'</span>, <span class="string">'wan'</span>, <span class="string">'xan'</span>, <span class="string">'yan'</span>, <span class="string">'zan'</span>]</span><br></pre></td></tr></table></figure><ul><li>Notice how split_l has an extra tuple <code>(&#39;can&#39;, &#39;&#39;)</code>, but the output is still the same, so this is okay.</li></ul><h4 id="Note-2-2"><a href="#Note-2-2" class="headerlink" title="Note 2"></a>Note 2</h4><p>If you get something like this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'can'</span>), (<span class="string">'c'</span>, <span class="string">'an'</span>), (<span class="string">'ca'</span>, <span class="string">'n'</span>), (<span class="string">'can'</span>, <span class="string">''</span>)] </span><br><span class="line">replace_l [<span class="string">'aan'</span>, <span class="string">'ban'</span>, <span class="string">'caa'</span>, <span class="string">'cab'</span>, <span class="string">'cac'</span>, <span class="string">'cad'</span>, <span class="string">'cae'</span>, <span class="string">'caf'</span>, <span class="string">'cag'</span>, <span class="string">'cah'</span>, <span class="string">'cai'</span>, <span class="string">'caj'</span>, <span class="string">'cak'</span>, <span class="string">'cal'</span>, <span class="string">'cam'</span>, <span class="string">'cana'</span>, <span class="string">'canb'</span>, <span class="string">'canc'</span>, <span class="string">'cand'</span>, <span class="string">'cane'</span>, <span class="string">'canf'</span>, <span class="string">'cang'</span>, <span class="string">'canh'</span>, <span class="string">'cani'</span>, <span class="string">'canj'</span>, <span class="string">'cank'</span>, <span class="string">'canl'</span>, <span class="string">'canm'</span>, <span class="string">'cann'</span>, <span class="string">'cano'</span>, <span class="string">'canp'</span>, <span class="string">'canq'</span>, <span class="string">'canr'</span>, <span class="string">'cans'</span>, <span class="string">'cant'</span>, <span class="string">'canu'</span>, <span class="string">'canv'</span>, <span class="string">'canw'</span>, <span class="string">'canx'</span>, <span class="string">'cany'</span>, <span class="string">'canz'</span>, <span class="string">'cao'</span>, <span class="string">'cap'</span>, <span class="string">'caq'</span>, <span class="string">'car'</span>, <span class="string">'cas'</span>, <span class="string">'cat'</span>, <span class="string">'cau'</span>, <span class="string">'cav'</span>, <span class="string">'caw'</span>, <span class="string">'cax'</span>, <span class="string">'cay'</span>, <span class="string">'caz'</span>, <span class="string">'cbn'</span>, <span class="string">'ccn'</span>, <span class="string">'cdn'</span>, <span class="string">'cen'</span>, <span class="string">'cfn'</span>, <span class="string">'cgn'</span>, <span class="string">'chn'</span>, <span class="string">'cin'</span>, <span class="string">'cjn'</span>, <span class="string">'ckn'</span>, <span class="string">'cln'</span>, <span class="string">'cmn'</span>, <span class="string">'cnn'</span>, <span class="string">'con'</span>, <span class="string">'cpn'</span>, <span class="string">'cqn'</span>, <span class="string">'crn'</span>, <span class="string">'csn'</span>, <span class="string">'ctn'</span>, <span class="string">'cun'</span>, <span class="string">'cvn'</span>, <span class="string">'cwn'</span>, <span class="string">'cxn'</span>, <span class="string">'cyn'</span>, <span class="string">'czn'</span>, <span class="string">'dan'</span>, <span class="string">'ean'</span>, <span class="string">'fan'</span>, <span class="string">'gan'</span>, <span class="string">'han'</span>, <span class="string">'ian'</span>, <span class="string">'jan'</span>, <span class="string">'kan'</span>, <span class="string">'lan'</span>, <span class="string">'man'</span>, <span class="string">'nan'</span>, <span class="string">'oan'</span>, <span class="string">'pan'</span>, <span class="string">'qan'</span>, <span class="string">'ran'</span>, <span class="string">'san'</span>, <span class="string">'tan'</span>, <span class="string">'uan'</span>, <span class="string">'van'</span>, <span class="string">'wan'</span>, <span class="string">'xan'</span>, <span class="string">'yan'</span>, <span class="string">'zan'</span>]</span><br></pre></td></tr></table></figure></p><ul><li>Notice how there are strings that are 1 letter longer than the original word, such as <code>cana</code>.</li><li>Please check for the case when there is an empty string <code>&#39;&#39;</code>, and if so, do not use that empty string when setting replace_l.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line">print(<span class="string">f"Number of outputs of switch_letter('at') is <span class="subst">&#123;len(switch_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of outputs of switch_letter(&#39;at&#39;) is 1</code></pre><h4 id="Expected-output-3"><a href="#Expected-output-3" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of outputs of switch_letter('at') is 1</span><br></pre></td></tr></table></figure><p><a name="ex-7"></a></p><h3 id="Exercise-7"><a href="#Exercise-7" class="headerlink" title="Exercise 7"></a>Exercise 7</h3><p><strong>Instructions for insert_letter()</strong>: Now implement a function that takes in a word and returns a list with a letter inserted at every offset.</p><p><strong>Step 1:</strong> is the same as in <code>delete_letter()</code></p><p><strong>Step 2:</strong> This can be a list comprehension of the form:<br><code>[f(a,b,c) for a, b in splits if condition for c in string]</code>   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: inserts</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_letter</span><span class="params">(word, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        inserts: a set of all possible strings with one new letter inserted at every offset</span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    letters = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">    insert_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    insert_l = [L + C + R <span class="keyword">for</span> L, R <span class="keyword">in</span> split_l <span class="keyword">for</span> C <span class="keyword">in</span> letters]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">f"Input word <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \ninsert_l = <span class="subst">&#123;insert_l&#125;</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> insert_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert_l = insert_letter(<span class="string">'at'</span>, <span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">f"Number of strings output by insert_letter('at') is <span class="subst">&#123;len(insert_l)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Input word at split_l = [(&#39;&#39;, &#39;at&#39;), (&#39;a&#39;, &#39;t&#39;), (&#39;at&#39;, &#39;&#39;)] insert_l = [&#39;aat&#39;, &#39;bat&#39;, &#39;cat&#39;, &#39;dat&#39;, &#39;eat&#39;, &#39;fat&#39;, &#39;gat&#39;, &#39;hat&#39;, &#39;iat&#39;, &#39;jat&#39;, &#39;kat&#39;, &#39;lat&#39;, &#39;mat&#39;, &#39;nat&#39;, &#39;oat&#39;, &#39;pat&#39;, &#39;qat&#39;, &#39;rat&#39;, &#39;sat&#39;, &#39;tat&#39;, &#39;uat&#39;, &#39;vat&#39;, &#39;wat&#39;, &#39;xat&#39;, &#39;yat&#39;, &#39;zat&#39;, &#39;aat&#39;, &#39;abt&#39;, &#39;act&#39;, &#39;adt&#39;, &#39;aet&#39;, &#39;aft&#39;, &#39;agt&#39;, &#39;aht&#39;, &#39;ait&#39;, &#39;ajt&#39;, &#39;akt&#39;, &#39;alt&#39;, &#39;amt&#39;, &#39;ant&#39;, &#39;aot&#39;, &#39;apt&#39;, &#39;aqt&#39;, &#39;art&#39;, &#39;ast&#39;, &#39;att&#39;, &#39;aut&#39;, &#39;avt&#39;, &#39;awt&#39;, &#39;axt&#39;, &#39;ayt&#39;, &#39;azt&#39;, &#39;ata&#39;, &#39;atb&#39;, &#39;atc&#39;, &#39;atd&#39;, &#39;ate&#39;, &#39;atf&#39;, &#39;atg&#39;, &#39;ath&#39;, &#39;ati&#39;, &#39;atj&#39;, &#39;atk&#39;, &#39;atl&#39;, &#39;atm&#39;, &#39;atn&#39;, &#39;ato&#39;, &#39;atp&#39;, &#39;atq&#39;, &#39;atr&#39;, &#39;ats&#39;, &#39;att&#39;, &#39;atu&#39;, &#39;atv&#39;, &#39;atw&#39;, &#39;atx&#39;, &#39;aty&#39;, &#39;atz&#39;]Number of strings output by insert_letter(&#39;at&#39;) is 78</code></pre><h4 id="Expected-output-4"><a href="#Expected-output-4" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'at'</span>), (<span class="string">'a'</span>, <span class="string">'t'</span>), (<span class="string">'at'</span>, <span class="string">''</span>)] </span><br><span class="line">insert_l = [<span class="string">'aat'</span>, <span class="string">'bat'</span>, <span class="string">'cat'</span>, <span class="string">'dat'</span>, <span class="string">'eat'</span>, <span class="string">'fat'</span>, <span class="string">'gat'</span>, <span class="string">'hat'</span>, <span class="string">'iat'</span>, <span class="string">'jat'</span>, <span class="string">'kat'</span>, <span class="string">'lat'</span>, <span class="string">'mat'</span>, <span class="string">'nat'</span>, <span class="string">'oat'</span>, <span class="string">'pat'</span>, <span class="string">'qat'</span>, <span class="string">'rat'</span>, <span class="string">'sat'</span>, <span class="string">'tat'</span>, <span class="string">'uat'</span>, <span class="string">'vat'</span>, <span class="string">'wat'</span>, <span class="string">'xat'</span>, <span class="string">'yat'</span>, <span class="string">'zat'</span>, <span class="string">'aat'</span>, <span class="string">'abt'</span>, <span class="string">'act'</span>, <span class="string">'adt'</span>, <span class="string">'aet'</span>, <span class="string">'aft'</span>, <span class="string">'agt'</span>, <span class="string">'aht'</span>, <span class="string">'ait'</span>, <span class="string">'ajt'</span>, <span class="string">'akt'</span>, <span class="string">'alt'</span>, <span class="string">'amt'</span>, <span class="string">'ant'</span>, <span class="string">'aot'</span>, <span class="string">'apt'</span>, <span class="string">'aqt'</span>, <span class="string">'art'</span>, <span class="string">'ast'</span>, <span class="string">'att'</span>, <span class="string">'aut'</span>, <span class="string">'avt'</span>, <span class="string">'awt'</span>, <span class="string">'axt'</span>, <span class="string">'ayt'</span>, <span class="string">'azt'</span>, <span class="string">'ata'</span>, <span class="string">'atb'</span>, <span class="string">'atc'</span>, <span class="string">'atd'</span>, <span class="string">'ate'</span>, <span class="string">'atf'</span>, <span class="string">'atg'</span>, <span class="string">'ath'</span>, <span class="string">'ati'</span>, <span class="string">'atj'</span>, <span class="string">'atk'</span>, <span class="string">'atl'</span>, <span class="string">'atm'</span>, <span class="string">'atn'</span>, <span class="string">'ato'</span>, <span class="string">'atp'</span>, <span class="string">'atq'</span>, <span class="string">'atr'</span>, <span class="string">'ats'</span>, <span class="string">'att'</span>, <span class="string">'atu'</span>, <span class="string">'atv'</span>, <span class="string">'atw'</span>, <span class="string">'atx'</span>, <span class="string">'aty'</span>, <span class="string">'atz'</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">'at'</span>) <span class="keyword">is</span> <span class="number">78</span></span><br></pre></td></tr></table></figure><h4 id="Note-1-3"><a href="#Note-1-3" class="headerlink" title="Note 1"></a>Note 1</h4><p>If you get a split_l like this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'at'</span>), (<span class="string">'a'</span>, <span class="string">'t'</span>)] </span><br><span class="line">insert_l = [<span class="string">'aat'</span>, <span class="string">'bat'</span>, <span class="string">'cat'</span>, <span class="string">'dat'</span>, <span class="string">'eat'</span>, <span class="string">'fat'</span>, <span class="string">'gat'</span>, <span class="string">'hat'</span>, <span class="string">'iat'</span>, <span class="string">'jat'</span>, <span class="string">'kat'</span>, <span class="string">'lat'</span>, <span class="string">'mat'</span>, <span class="string">'nat'</span>, <span class="string">'oat'</span>, <span class="string">'pat'</span>, <span class="string">'qat'</span>, <span class="string">'rat'</span>, <span class="string">'sat'</span>, <span class="string">'tat'</span>, <span class="string">'uat'</span>, <span class="string">'vat'</span>, <span class="string">'wat'</span>, <span class="string">'xat'</span>, <span class="string">'yat'</span>, <span class="string">'zat'</span>, <span class="string">'aat'</span>, <span class="string">'abt'</span>, <span class="string">'act'</span>, <span class="string">'adt'</span>, <span class="string">'aet'</span>, <span class="string">'aft'</span>, <span class="string">'agt'</span>, <span class="string">'aht'</span>, <span class="string">'ait'</span>, <span class="string">'ajt'</span>, <span class="string">'akt'</span>, <span class="string">'alt'</span>, <span class="string">'amt'</span>, <span class="string">'ant'</span>, <span class="string">'aot'</span>, <span class="string">'apt'</span>, <span class="string">'aqt'</span>, <span class="string">'art'</span>, <span class="string">'ast'</span>, <span class="string">'att'</span>, <span class="string">'aut'</span>, <span class="string">'avt'</span>, <span class="string">'awt'</span>, <span class="string">'axt'</span>, <span class="string">'ayt'</span>, <span class="string">'azt'</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">'at'</span>) <span class="keyword">is</span> <span class="number">52</span></span><br></pre></td></tr></table></figure></p><ul><li>Notice that split_l is missing the extra tuple (â€˜atâ€™, â€˜â€™).  For insertion, we actually <strong>WANT</strong> this tuple.</li><li>The function is not creating all the desired output strings.</li><li>Check the range that you use for the for loop.</li></ul><h4 id="Note-2-3"><a href="#Note-2-3" class="headerlink" title="Note 2"></a>Note 2</h4><p>If you see this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'at'</span>), (<span class="string">'a'</span>, <span class="string">'t'</span>), (<span class="string">'at'</span>, <span class="string">''</span>)] </span><br><span class="line">insert_l = [<span class="string">'aat'</span>, <span class="string">'bat'</span>, <span class="string">'cat'</span>, <span class="string">'dat'</span>, <span class="string">'eat'</span>, <span class="string">'fat'</span>, <span class="string">'gat'</span>, <span class="string">'hat'</span>, <span class="string">'iat'</span>, <span class="string">'jat'</span>, <span class="string">'kat'</span>, <span class="string">'lat'</span>, <span class="string">'mat'</span>, <span class="string">'nat'</span>, <span class="string">'oat'</span>, <span class="string">'pat'</span>, <span class="string">'qat'</span>, <span class="string">'rat'</span>, <span class="string">'sat'</span>, <span class="string">'tat'</span>, <span class="string">'uat'</span>, <span class="string">'vat'</span>, <span class="string">'wat'</span>, <span class="string">'xat'</span>, <span class="string">'yat'</span>, <span class="string">'zat'</span>, <span class="string">'aat'</span>, <span class="string">'abt'</span>, <span class="string">'act'</span>, <span class="string">'adt'</span>, <span class="string">'aet'</span>, <span class="string">'aft'</span>, <span class="string">'agt'</span>, <span class="string">'aht'</span>, <span class="string">'ait'</span>, <span class="string">'ajt'</span>, <span class="string">'akt'</span>, <span class="string">'alt'</span>, <span class="string">'amt'</span>, <span class="string">'ant'</span>, <span class="string">'aot'</span>, <span class="string">'apt'</span>, <span class="string">'aqt'</span>, <span class="string">'art'</span>, <span class="string">'ast'</span>, <span class="string">'att'</span>, <span class="string">'aut'</span>, <span class="string">'avt'</span>, <span class="string">'awt'</span>, <span class="string">'axt'</span>, <span class="string">'ayt'</span>, <span class="string">'azt'</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">'at'</span>) <span class="keyword">is</span> <span class="number">52</span></span><br></pre></td></tr></table></figure></p><ul><li>Even though you may have fixed the split_l so that it contains the tuple <code>(&#39;at&#39;, &#39;&#39;)</code>, notice that youâ€™re still missing some output strings.<ul><li>Notice that itâ€™s missing strings such as â€˜ataâ€™, â€˜atbâ€™, â€˜atcâ€™ all the way to â€˜atzâ€™.</li></ul></li><li>To fix this, make sure that when you set insert_l, you allow the use of the empty string <code>&#39;&#39;</code>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line">print(<span class="string">f"Number of outputs of insert_letter('at') is <span class="subst">&#123;len(insert_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of outputs of insert_letter(&#39;at&#39;) is 78</code></pre><h4 id="Expected-output-5"><a href="#Expected-output-5" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of outputs of insert_letter('at') is 78</span><br></pre></td></tr></table></figure><p><a name="3"></a></p><h1 id="Part-3-Combining-the-edits"><a href="#Part-3-Combining-the-edits" class="headerlink" title="Part 3: Combining the edits"></a>Part 3: Combining the edits</h1><p>Now that you have implemented the string manipulations, you will create two functions that, given a string, will return all the possible single and double edits on that string. These will be <code>edit_one_letter()</code> and <code>edit_two_letters()</code>.</p><p><a name="3-1"></a></p><h2 id="3-1-Edit-one-letter"><a href="#3-1-Edit-one-letter" class="headerlink" title="3.1 Edit one letter"></a>3.1 Edit one letter</h2><p><a name="ex-8"></a></p><h3 id="Exercise-8"><a href="#Exercise-8" class="headerlink" title="Exercise 8"></a>Exercise 8</h3><p><strong>Instructions</strong>: Implement the <code>edit_one_letter</code> function to get all the possible edits that are one edit away from a word. The edits  consist of the replace, insert, delete, and optionally the switch operation. You should use the previous functions you have already implemented to complete this function. The â€˜switchâ€™ function  is a less common edit function, so its use will be selected by an â€œallow_switchesâ€ input argument.</p><p>Note that those functions return <em>lists</em> while this function should return a <em>python set</em>. Utilizing a set eliminates any duplicate entries.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Each of the functions returns a list.  You can combine lists using the `+` operator. </li>    <li> To get unique strings (avoid duplicates), you can use the set() function. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: edit_one_letter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edit_one_letter</span><span class="params">(word, allow_switches = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the string/word for which we will generate all possible wordsthat are one edit away.</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    edit_one_set = set()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    edit_one_set = edit_one_set | set(delete_letter(word)) | set(insert_letter(word)) | set(replace_letter(word))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> allow_switches:</span><br><span class="line">        edit_one_set |= set(switch_letter(word))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> edit_one_set</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_word = <span class="string">"at"</span></span><br><span class="line">tmp_edit_one_set = edit_one_letter(tmp_word)</span><br><span class="line"><span class="comment"># turn this into a list to sort it, in order to view it</span></span><br><span class="line">tmp_edit_one_l = sorted(list(tmp_edit_one_set))</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"input word <span class="subst">&#123;tmp_word&#125;</span> \nedit_one_l \n<span class="subst">&#123;tmp_edit_one_l&#125;</span>\n"</span>)</span><br><span class="line">print(<span class="string">f"The type of the returned object should be a set <span class="subst">&#123;type(tmp_edit_one_set)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Number of outputs from edit_one_letter('at') is <span class="subst">&#123;len(edit_one_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>input word at edit_one_l [&#39;a&#39;, &#39;aa&#39;, &#39;aat&#39;, &#39;ab&#39;, &#39;abt&#39;, &#39;ac&#39;, &#39;act&#39;, &#39;ad&#39;, &#39;adt&#39;, &#39;ae&#39;, &#39;aet&#39;, &#39;af&#39;, &#39;aft&#39;, &#39;ag&#39;, &#39;agt&#39;, &#39;ah&#39;, &#39;aht&#39;, &#39;ai&#39;, &#39;ait&#39;, &#39;aj&#39;, &#39;ajt&#39;, &#39;ak&#39;, &#39;akt&#39;, &#39;al&#39;, &#39;alt&#39;, &#39;am&#39;, &#39;amt&#39;, &#39;an&#39;, &#39;ant&#39;, &#39;ao&#39;, &#39;aot&#39;, &#39;ap&#39;, &#39;apt&#39;, &#39;aq&#39;, &#39;aqt&#39;, &#39;ar&#39;, &#39;art&#39;, &#39;as&#39;, &#39;ast&#39;, &#39;ata&#39;, &#39;atb&#39;, &#39;atc&#39;, &#39;atd&#39;, &#39;ate&#39;, &#39;atf&#39;, &#39;atg&#39;, &#39;ath&#39;, &#39;ati&#39;, &#39;atj&#39;, &#39;atk&#39;, &#39;atl&#39;, &#39;atm&#39;, &#39;atn&#39;, &#39;ato&#39;, &#39;atp&#39;, &#39;atq&#39;, &#39;atr&#39;, &#39;ats&#39;, &#39;att&#39;, &#39;atu&#39;, &#39;atv&#39;, &#39;atw&#39;, &#39;atx&#39;, &#39;aty&#39;, &#39;atz&#39;, &#39;au&#39;, &#39;aut&#39;, &#39;av&#39;, &#39;avt&#39;, &#39;aw&#39;, &#39;awt&#39;, &#39;ax&#39;, &#39;axt&#39;, &#39;ay&#39;, &#39;ayt&#39;, &#39;az&#39;, &#39;azt&#39;, &#39;bat&#39;, &#39;bt&#39;, &#39;cat&#39;, &#39;ct&#39;, &#39;dat&#39;, &#39;dt&#39;, &#39;eat&#39;, &#39;et&#39;, &#39;fat&#39;, &#39;ft&#39;, &#39;gat&#39;, &#39;gt&#39;, &#39;hat&#39;, &#39;ht&#39;, &#39;iat&#39;, &#39;it&#39;, &#39;jat&#39;, &#39;jt&#39;, &#39;kat&#39;, &#39;kt&#39;, &#39;lat&#39;, &#39;lt&#39;, &#39;mat&#39;, &#39;mt&#39;, &#39;nat&#39;, &#39;nt&#39;, &#39;oat&#39;, &#39;ot&#39;, &#39;pat&#39;, &#39;pt&#39;, &#39;qat&#39;, &#39;qt&#39;, &#39;rat&#39;, &#39;rt&#39;, &#39;sat&#39;, &#39;st&#39;, &#39;t&#39;, &#39;ta&#39;, &#39;tat&#39;, &#39;tt&#39;, &#39;uat&#39;, &#39;ut&#39;, &#39;vat&#39;, &#39;vt&#39;, &#39;wat&#39;, &#39;wt&#39;, &#39;xat&#39;, &#39;xt&#39;, &#39;yat&#39;, &#39;yt&#39;, &#39;zat&#39;, &#39;zt&#39;]The type of the returned object should be a set &lt;class &#39;set&#39;&gt;Number of outputs from edit_one_letter(&#39;at&#39;) is 129</code></pre><h4 id="Expected-Output-5"><a href="#Expected-Output-5" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input word at </span><br><span class="line">edit_one_l </span><br><span class="line">['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']</span><br><span class="line"></span><br><span class="line">The type of the returned object should be a <span class="built_in">set</span> &lt;class '<span class="built_in">set</span>'&gt;</span><br><span class="line">Number of outputs from edit_one_letter('at') is 129</span><br></pre></td></tr></table></figure><p><a name="3-2"></a></p><h2 id="Part-3-2-Edit-two-letters"><a href="#Part-3-2-Edit-two-letters" class="headerlink" title="Part 3.2 Edit two letters"></a>Part 3.2 Edit two letters</h2><p><a name="ex-9"></a></p><h3 id="Exercise-9"><a href="#Exercise-9" class="headerlink" title="Exercise 9"></a>Exercise 9</h3><p>Now you can generalize this to implement to get two edits on a word. To do so, you would have to get all the possible edits on a single word and then for each modified word, you would have to modify it again. </p><p><strong>Instructions</strong>: Implement the <code>edit_two_letters</code> function that returns a set of words that are two edits away. Note that creating additional edits based on the <code>edit_one_letter</code> function may â€˜restoreâ€™ some one_edits to zero or one edits. That is allowed here. This accounted for in get_corrections.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>You will likely want to take the union of two sets.</li>    <li>You can either use set.union() or use the '|' (or operator) to union two sets</li>    <li>See the documentation <a href="https://docs.python.org/2/library/sets.html" target="_blank" rel="noopener"> Python sets </a> for examples of using operators or functions of the Python set.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: edit_two_letters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edit_two_letters</span><span class="params">(word, allow_switches = True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        edit_two_set: a set of strings with all possible two edits</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    edit_two_set = set()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    one_letter =  edit_one_letter(word,allow_switches)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> one_letter:</span><br><span class="line">        edit_two_set |= edit_one_letter(word,allow_switches)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> edit_two_set</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tmp_edit_two_set = edit_two_letters(<span class="string">"a"</span>)</span><br><span class="line">tmp_edit_two_l = sorted(list(tmp_edit_two_set))</span><br><span class="line">print(<span class="string">f"Number of strings with edit distance of two: <span class="subst">&#123;len(tmp_edit_two_l)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"First 10 strings <span class="subst">&#123;tmp_edit_two_l[:<span class="number">10</span>]&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Last 10 strings <span class="subst">&#123;tmp_edit_two_l[<span class="number">-10</span>:]&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"The data type of the returned object should be a set <span class="subst">&#123;type(tmp_edit_two_set)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Number of strings that are 2 edit distances from 'at' is <span class="subst">&#123;len(edit_two_letters(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of strings with edit distance of two: 2654First 10 strings [&#39;&#39;, &#39;a&#39;, &#39;aa&#39;, &#39;aaa&#39;, &#39;aab&#39;, &#39;aac&#39;, &#39;aad&#39;, &#39;aae&#39;, &#39;aaf&#39;, &#39;aag&#39;]Last 10 strings [&#39;zv&#39;, &#39;zva&#39;, &#39;zw&#39;, &#39;zwa&#39;, &#39;zx&#39;, &#39;zxa&#39;, &#39;zy&#39;, &#39;zya&#39;, &#39;zz&#39;, &#39;zza&#39;]The data type of the returned object should be a set &lt;class &#39;set&#39;&gt;Number of strings that are 2 edit distances from &#39;at&#39; is 7154</code></pre><h4 id="Expected-Output-6"><a href="#Expected-Output-6" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Number of strings with edit distance of two: <span class="number">2654</span></span><br><span class="line">First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']</span><br><span class="line">Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']</span><br><span class="line">The data type of the returned object should be a <span class="built_in">set</span> &lt;class '<span class="built_in">set</span>'&gt;</span><br><span class="line">Number of strings that are 2 edit distances from 'at' is 7154</span><br></pre></td></tr></table></figure><p><a name="3-3"></a></p><h2 id="Part-3-3-suggest-spelling-suggestions"><a href="#Part-3-3-suggest-spelling-suggestions" class="headerlink" title="Part 3-3: suggest spelling suggestions"></a>Part 3-3: suggest spelling suggestions</h2><p>Now you will use your <code>edit_two_letters</code> function to get a set of all the possible 2 edits on your word. You will then use those strings to get the most probable word you meant to type aka your typing suggestion.</p><p><a name="ex-10"></a></p><h3 id="Exercise-10"><a href="#Exercise-10" class="headerlink" title="Exercise 10"></a>Exercise 10</h3><p><strong>Instructions</strong>: Implement <code>get_corrections</code>, which returns a list of zero to n possible suggestion tuples of the form (word, probability_of_word). </p><p><strong>Step 1:</strong> Generate suggestions for a supplied word: Youâ€™ll use the edit functions you have developed. The â€˜suggestion algorithmâ€™ should follow this logic: </p><ul><li>If the word is in the vocabulary, suggest the word. </li><li>Otherwise, if there are suggestions from <code>edit_one_letter</code> that are in the vocabulary, use those. </li><li>Otherwise, if there are suggestions from <code>edit_two_letters</code> that are in the vocabulary, use those. </li><li>Otherwise, suggest the input word.*  </li><li>The idea is that words generated from fewer edits are more likely than words with more edits.</li></ul><p>Note: </p><ul><li>Edits of one or two letters may â€˜restoreâ€™ strings to either zero or one edit. This algorithm accounts for this by preferentially selecting lower distance edits first.</li></ul><h4 id="Short-circuit"><a href="#Short-circuit" class="headerlink" title="Short circuit"></a>Short circuit</h4><p>In Python, logical operations such as <code>and</code> and <code>or</code> have two useful properties. They can operate on lists and they have <a href="https://docs.python.org/3/library/stdtypes.html" target="_blank" rel="noopener">â€˜short-circuitâ€™ behavior</a>. Try these:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example of logical operation on lists or sets</span></span><br><span class="line">print( [] <span class="keyword">and</span> [<span class="string">"a"</span>,<span class="string">"b"</span>] )</span><br><span class="line">print( [] <span class="keyword">or</span> [<span class="string">"a"</span>,<span class="string">"b"</span>] )</span><br><span class="line"><span class="comment">#example of Short circuit behavior</span></span><br><span class="line">val1 =  [<span class="string">"Most"</span>,<span class="string">"Likely"</span>] <span class="keyword">or</span> [<span class="string">"Less"</span>,<span class="string">"so"</span>] <span class="keyword">or</span> [<span class="string">"least"</span>,<span class="string">"of"</span>,<span class="string">"all"</span>]  <span class="comment"># selects first, does not evalute remainder</span></span><br><span class="line">print(val1)</span><br><span class="line">val2 =  [] <span class="keyword">or</span> [] <span class="keyword">or</span> [<span class="string">"least"</span>,<span class="string">"of"</span>,<span class="string">"all"</span>] <span class="comment"># continues evaluation until there is a non-empty list</span></span><br><span class="line">print(val2)</span><br></pre></td></tr></table></figure><pre><code>[][&#39;a&#39;, &#39;b&#39;][&#39;Most&#39;, &#39;Likely&#39;][&#39;least&#39;, &#39;of&#39;, &#39;all&#39;]</code></pre><p>The logical <code>or</code> could be used to implement the suggestion algorithm very compactly. Alternately, if/then constructs could be used.</p><p><strong>Step 2</strong>: Create a â€˜best_wordsâ€™ dictionary where the â€˜keyâ€™ is a suggestion and the â€˜valueâ€™ is the probability of that word in your vocabulary. If the word is not in the vocabulary, assign it a probability of 0.</p><p><strong>Step 3</strong>: Select the n best suggestions. There may be fewer than n.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>edit_one_letter and edit_two_letters return *python sets*. </li>    <li> Sets have a handy <a href="https://docs.python.org/2/library/sets.html" target="_blank" rel="noopener"> set.intersection </a> feature</li>    <li>To find the keys that have the highest values in a dictionary, you can use the Counter dictionary to create a Counter object from a regular dictionary.  Then you can use Counter.most_common(n) to get the n most common keys.    </li>    <li>To find the intersection of two sets, you can use set.intersection or the & operator.</li>    <li>If you are not as familiar with short circuit syntax (as shown above), feel free to use if else statements instead.</li>    <li>To use an if statement to check of a set is empty, use 'if not x:' syntax </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_corrections</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_corrections</span><span class="params">(word, probs, vocab, n=<span class="number">2</span>, verbose = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        word: a user entered string to check for suggestions</span></span><br><span class="line"><span class="string">        probs: a dictionary that maps each word to its probability in the corpus</span></span><br><span class="line"><span class="string">        vocab: a set containing all the vocabulary</span></span><br><span class="line"><span class="string">        n: number of possible word corrections you want returned in the dictionary</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        n_best: a list of tuples with the most probable n corrected words and their probabilities.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    suggestions = []</span><br><span class="line">    n_best = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">        suggestions = [(word, probs[word])]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        suggestions = [(_, probs[_]) <span class="keyword">for</span> _ <span class="keyword">in</span> edit_one_letter(word) <span class="keyword">if</span> _ <span class="keyword">in</span> vocab] <span class="keyword">or</span>  \</span><br><span class="line">                    [(_, probs[_]) <span class="keyword">for</span> _ <span class="keyword">in</span> edit_two_letter(word) <span class="keyword">if</span> _ <span class="keyword">in</span> vocab] <span class="keyword">or</span> \</span><br><span class="line">                    [(word,<span class="number">0</span>)]</span><br><span class="line">    </span><br><span class="line">    n_best = sorted(suggestions, key = <span class="keyword">lambda</span> x : x[<span class="number">-1</span>], reverse = <span class="keyword">True</span>)[:n]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">"suggestions = "</span>, suggestions)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> n_best</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test your implementation - feel free to try other words in my word</span></span><br><span class="line">my_word = <span class="string">'dys'</span> </span><br><span class="line">tmp_corrections = get_corrections(my_word, probs, vocab, <span class="number">2</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, word_prob <span class="keyword">in</span> enumerate(tmp_corrections):</span><br><span class="line">    print(<span class="string">f"word <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;word_prob[<span class="number">0</span>]&#125;</span>, probability <span class="subst">&#123;word_prob[<span class="number">1</span>]:<span class="number">.6</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CODE REVIEW COMMENT: using "tmp_corrections" insteads of "cors". "cors" is not defined</span></span><br><span class="line">print(<span class="string">f"data type of corrections <span class="subst">&#123;type(tmp_corrections)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>suggestions =  [(&#39;dye&#39;, 1.865184466743761e-05), (&#39;days&#39;, 0.0004103405826836274)]word 0: days, probability 0.000410word 1: dye, probability 0.000019data type of corrections &lt;class &#39;list&#39;&gt;</code></pre><h4 id="Expected-Output-7"><a href="#Expected-Output-7" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word <span class="number">0</span>: days, probability <span class="number">0.000410</span></span><br><span class="line">word <span class="number">1</span>: dye, probability <span class="number">0.000019</span></span><br><span class="line">data type of corrections &lt;<span class="class"><span class="keyword">class</span> '<span class="title">list</span>'&gt;</span></span><br></pre></td></tr></table></figure><p><a name="4"></a></p><h1 id="Part-4-Minimum-Edit-distance"><a href="#Part-4-Minimum-Edit-distance" class="headerlink" title="Part 4: Minimum Edit distance"></a>Part 4: Minimum Edit distance</h1><p>Now that you have implemented your auto-correct, how do you evaluate the similarity between two strings? For example: â€˜wahtâ€™ and â€˜whatâ€™</p><p>Also how do you efficiently find the shortest path to go from the word, â€˜wahtâ€™ to the word â€˜whatâ€™?</p><p>You will implement a dynamic programming system that will tell you the minimum number of edits required to convert a string into another string.</p><p><a name="4-1"></a></p><h3 id="Part-4-1-Dynamic-Programming"><a href="#Part-4-1-Dynamic-Programming" class="headerlink" title="Part 4.1 Dynamic Programming"></a>Part 4.1 Dynamic Programming</h3><p>Dynamic Programming breaks a problem down into subproblems which can be combined to form the final solution. Here, given a string source[0..i] and a string target[0..j], we will compute all the combinations of substrings[i, j] and calculate their edit distance. To do this efficiently, we will use a table to maintain the previously computed substrings and use those to calculate larger substrings.</p><p>You have to create a matrix and update each element in the matrix as follows:  </p><script type="math/tex; mode=display">\text{Initialization}</script><p>\begin{align}<br>D[0,0] &amp;= 0 \\<br>D[i,0] &amp;= D[i-1,0] + del_cost(source[i]) \tag{4}\\<br>D[0,j] &amp;= D[0,j-1] + ins_cost(target[j]) \\<br>\end{align}</p><script type="math/tex; mode=display">\text{Per Cell Operations}</script><p>\begin{align}<br> \\<br>D[i,j] =min<br>\begin{cases}<br>D[i-1,j] + del_cost\\<br>D[i,j-1] + ins_cost\\<br>D[i-1,j-1] + \left\{\begin{matrix}<br>rep_cost; &amp; if src[i]\neq tar[j]\\<br>0 ; &amp; if src[i]=tar[j]<br>\end{matrix}\right.<br>\end{cases}<br>\tag{5}<br>\end{align}</p><p>So converting the source word <strong>play</strong> to the target word <strong>stay</strong>, using an input cost of one, a delete cost of 1, and replace cost of 2 would give you the following table:</p><table style="width:20%">  <tr>    <td> <b> </b>  </td>    <td> <b># </b>  </td>    <td> <b>s </b>  </td>    <td> <b>t </b> </td>     <td> <b>a </b> </td>     <td> <b>y </b> </td>   </tr>   <tr>    <td> <b>  #  </b></td>    <td> 0</td>     <td> 1</td>     <td> 2</td>     <td> 3</td>     <td> 4</td>   </tr>  <tr>    <td> <b>  p  </b></td>    <td> 1</td>  <td> 2</td>     <td> 3</td>     <td> 4</td>    <td> 5</td>  </tr>  <tr>    <td> <b> l </b></td>    <td>2</td>     <td>3</td>     <td>4</td>     <td>5</td>     <td>6</td>  </tr>  <tr>    <td> <b> a </b></td>    <td>3</td>      <td>4</td>      <td>5</td>      <td>4</td>     <td>5</td>   </tr>   <tr>    <td> <b> y </b></td>    <td>4</td>       <td>5</td>      <td>6</td>      <td>5</td>     <td>4</td>   </tr></table><p>The operations used in this algorithm are â€˜insertâ€™, â€˜deleteâ€™, and â€˜replaceâ€™. These correspond to the functions that you defined earlier: insert_letter(), delete_letter() and replace_letter(). switch_letter() is not used here.</p><p>The diagram below describes how to initialize the table. Each entry in D[i,j] represents the minimum cost of converting string source[0:i] to string target[0:j]. The first column is initialized to represent the cumulative cost of deleting the source characters to convert string â€œEERâ€ to â€œâ€. The first row is initialized to represent the cumulative cost of inserting the target characters to convert from â€œâ€ to â€œNEARâ€.</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="EditDistInit4.PNG" alt="alternate text" width="width" height="height" style="width:1000px;height:400px;"> Figure 6 Initializing Distance Matrix</div>     <p>Filling in the remainder of the table utilizes the â€˜Per Cell Operationsâ€™ in the equation (5) above. Note, the diagram below includes in the table some of the 3 sub-calculations shown in light grey. Only â€˜minâ€™ of those operations is stored in the table in the <code>min_edit_distance()</code> function.</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="EditDistFill2.PNG" alt="alternate text" width="width" height="height" style="width:800px;height:400px;"> Figure 7 Filling Distance Matrix</div>     <p>Note that the formula for $D[i,j]$ shown in the image is equivalent to:</p><p>\begin{align}<br> \\<br>D[i,j] =min<br>\begin{cases}<br>D[i-1,j] + del_cost\\<br>D[i,j-1] + ins_cost\\<br>D[i-1,j-1] + \left\{\begin{matrix}<br>rep_cost; &amp; if src[i]\neq tar[j]\\<br>0 ; &amp; if src[i]=tar[j]<br>\end{matrix}\right.<br>\end{cases}<br>\tag{5}<br>\end{align}</p><p>The variable <code>sub_cost</code> (for substitution cost) is the same as <code>rep_cost</code>; replacement cost.  We will stick with the term â€œreplaceâ€ whenever possible.</p><p>Below are some examples of cells where replacement is used. This also shows the minimum path from the lower right final position where â€œEERâ€ has been replaced by â€œNEARâ€ back to the start. This provides a starting point for the optional â€˜backtraceâ€™ algorithm below.</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="EditDistExample1.PNG" alt="alternate text" width="width" height="height" style="width:1200px;height:400px;"> Figure 8 Examples Distance Matrix</div>    <p><a name="ex-11"></a></p><h3 id="Exercise-11"><a href="#Exercise-11" class="headerlink" title="Exercise 11"></a>Exercise 11</h3><p>Again, the word â€œsubstitutionâ€ appears in the figure, but think of this as â€œreplacementâ€.</p><p><strong>Instructions</strong>: Implement the function below to get the minimum amount of edits required given a source string and a target string. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>The range(start, stop, step) function excludes 'stop' from its output</li>    <li><a href> words </a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: min_edit_distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min_edit_distance</span><span class="params">(source, target, ins_cost = <span class="number">1</span>, del_cost = <span class="number">1</span>, rep_cost = <span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        source: a string corresponding to the string you are starting with</span></span><br><span class="line"><span class="string">        target: a string corresponding to the string you want to end with</span></span><br><span class="line"><span class="string">        ins_cost: an integer setting the insert cost</span></span><br><span class="line"><span class="string">        del_cost: an integer setting the delete cost</span></span><br><span class="line"><span class="string">        rep_cost: an integer setting the replace cost</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances</span></span><br><span class="line"><span class="string">        med: the minimum edit distance (med) required to convert the source string to the target</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># use deletion and insert cost as  1</span></span><br><span class="line">    m = len(source) </span><br><span class="line">    n = len(target) </span><br><span class="line">    <span class="comment">#initialize cost matrix with zeros and dimensions (m+1,n+1) </span></span><br><span class="line">    D = np.zeros((m+<span class="number">1</span>, n+<span class="number">1</span>), dtype=int) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fill in column 0, from row 1 to row m, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">0</span>,m+<span class="number">1</span>): <span class="comment"># Replace None with the proper range</span></span><br><span class="line">        D[row,<span class="number">0</span>] = row</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Fill in row 0, for all columns from 1 to n, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(<span class="number">0</span>,n+<span class="number">1</span>): <span class="comment"># Replace None with the proper range</span></span><br><span class="line">        D[<span class="number">0</span>,col] = col</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Loop through row 1 to row m, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">1</span>,m+<span class="number">1</span>): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop through column 1 to column n, both inclusive</span></span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> range(<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Intialize r_cost to the 'replace' cost that is passed into this function</span></span><br><span class="line">            r_cost = rep_cost</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check to see if source character at the previous row</span></span><br><span class="line">            <span class="comment"># matches the target character at the previous column, </span></span><br><span class="line">            <span class="keyword">if</span> source[row<span class="number">-1</span>] == target[col<span class="number">-1</span>]:</span><br><span class="line">                <span class="comment"># Update the replacement cost to 0 if source and target are the same</span></span><br><span class="line">                r_cost = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Update the cost at row, col based on previous entries in the cost matrix</span></span><br><span class="line">            <span class="comment"># Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)</span></span><br><span class="line">            D[row,col] = min([ D[row<span class="number">-1</span>,col] + del_cost, D[row, col<span class="number">-1</span>] + ins_cost , D[row<span class="number">-1</span>, col<span class="number">-1</span>] + r_cost])</span><br><span class="line">          </span><br><span class="line">    <span class="comment"># Set the minimum edit distance with the cost found at row m, column n</span></span><br><span class="line">    med = D[m,n]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> D, med</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line"><span class="comment"># testing your implementation </span></span><br><span class="line">source =  <span class="string">'play'</span></span><br><span class="line">target = <span class="string">'stay'</span></span><br><span class="line">matrix, min_edits = min_edit_distance(source, target)</span><br><span class="line">print(<span class="string">"minimum edits: "</span>,min_edits, <span class="string">"\n"</span>)</span><br><span class="line">idx = list(<span class="string">'#'</span> + source)</span><br><span class="line">cols = list(<span class="string">'#'</span> + target)</span><br><span class="line">df = pd.DataFrame(matrix, index=idx, columns= cols)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><pre><code>minimum edits:  4    #  s  t  a  y#  0  1  2  3  4p  1  2  3  4  5l  2  3  4  5  6a  3  4  5  4  5y  4  5  6  5  4</code></pre><p><strong>Expected Results:</strong>  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   <span class="meta">#  s  t  a  y</span></span><br><span class="line">#  <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">p  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span></span><br><span class="line">l  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span></span><br><span class="line">a  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">4</span>  <span class="number">5</span></span><br><span class="line">y  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">5</span>  <span class="number">4</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line"><span class="comment"># testing your implementation </span></span><br><span class="line">source =  <span class="string">'eer'</span></span><br><span class="line">target = <span class="string">'near'</span></span><br><span class="line">matrix, min_edits = min_edit_distance(source, target)</span><br><span class="line">print(<span class="string">"minimum edits: "</span>,min_edits, <span class="string">"\n"</span>)</span><br><span class="line">idx = list(source)</span><br><span class="line">idx.insert(<span class="number">0</span>, <span class="string">'#'</span>)</span><br><span class="line">cols = list(target)</span><br><span class="line">cols.insert(<span class="number">0</span>, <span class="string">'#'</span>)</span><br><span class="line">df = pd.DataFrame(matrix, index=idx, columns= cols)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><pre><code>minimum edits:  3    #  n  e  a  r#  0  1  2  3  4e  1  2  1  2  3e  2  3  2  3  4r  3  4  3  4  3</code></pre><p><strong>Expected Results</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">minimum edits:  <span class="number">3</span> </span><br><span class="line"></span><br><span class="line">   <span class="meta">#  n  e  a  r</span></span><br><span class="line">#  <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">e  <span class="number">1</span>  <span class="number">2</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span></span><br><span class="line">e  <span class="number">2</span>  <span class="number">3</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">r  <span class="number">3</span>  <span class="number">4</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">3</span></span><br></pre></td></tr></table></figure></p><p>We can now test several of our routines at once:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source = <span class="string">"eer"</span></span><br><span class="line">targets = edit_one_letter(source,allow_switches = <span class="keyword">False</span>)  <span class="comment">#disable switches since min_edit_distance does not include them</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> targets:</span><br><span class="line">    _, min_edits = min_edit_distance(source, t,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)  <span class="comment"># set ins, del, sub costs all to one</span></span><br><span class="line">    <span class="keyword">if</span> min_edits != <span class="number">1</span>: print(source, t, min_edits)</span><br></pre></td></tr></table></figure><p><strong>Expected Results:</strong>  (empty)</p><p>The â€˜replace()â€™ routine utilizes all letters a-z one of which returns the original word.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source = <span class="string">"eer"</span></span><br><span class="line">targets = edit_two_letters(source,allow_switches = <span class="keyword">False</span>) <span class="comment">#disable switches since min_edit_distance does not include them</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> targets:</span><br><span class="line">    _, min_edits = min_edit_distance(source, t,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)  <span class="comment"># set ins, del, sub costs all to one</span></span><br><span class="line">    <span class="keyword">if</span> min_edits != <span class="number">2</span> <span class="keyword">and</span> min_edits != <span class="number">1</span>: print(source, t, min_edits)</span><br></pre></td></tr></table></figure><pre><code>eer eer 0</code></pre><p><strong>Expected Results:</strong>  eer eer 0<br>We have to allow single edits here because some two_edits will restore a single edit.</p><h1 id="Submission"><a href="#Submission" class="headerlink" title="Submission"></a>Submission</h1><p>Make sure you submit your assignment before you modify anything below</p><p><a name="5"></a></p><h1 id="Part-5-Optional-Backtrace"><a href="#Part-5-Optional-Backtrace" class="headerlink" title="Part 5: Optional - Backtrace"></a>Part 5: Optional - Backtrace</h1><p>Once you have computed your matrix using minimum edit distance, how would find the shortest path from the top left corner to the bottom right corner? </p><p>Note that you could use backtrace algorithm.  Try to find the shortest path given the matrix that your <code>min_edit_distance</code> function returned.</p><p>You can use these <a href="https://web.stanford.edu/class/cs124/lec/med.pdf" target="_blank" rel="noopener">lecture slides on minimum edit distance</a> by Dan Jurafsky to learn about the algorithm for backtrace.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Experiment with back trace - insert your code here</span></span><br></pre></td></tr></table></figure><h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul><li>Dan Jurafsky - Speech and Language Processing - Textbook</li><li>This auto-correct explanation was first done by Peter Norvig in 2007 </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-1-Auto-Correct&quot;&gt;&lt;a href=&quot;#Assignment-1-Auto-Correct&quot; class=&quot;headerlink&quot; title=&quot;Assignment 1: Auto Correct&quot;&gt;&lt;/a&gt;Assignment
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Parts-of-Speech Tagging</title>
    <link href="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/"/>
    <id>https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/</id>
    <published>2020-07-18T17:39:21.000Z</published>
    <updated>2020-07-18T17:50:42.411Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Parts-of-Speech-Tagging-POS"><a href="#Assignment-2-Parts-of-Speech-Tagging-POS" class="headerlink" title="Assignment 2: Parts-of-Speech Tagging (POS)"></a>Assignment 2: Parts-of-Speech Tagging (POS)</h1><p>Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjectiveâ€¦) to each word in an input text.  Tagging is difficult because some words can represent more than one part of speech at different times. They are  <strong>Ambiguous</strong>. Letâ€™s look at the following example: </p><ul><li>The whole team played <strong>well</strong>. [adverb]</li><li>You are doing <strong>well</strong> for yourself. [adjective]</li><li><strong>Well</strong>, this assignment took me forever to complete. [interjection]</li><li>The <strong>well</strong> is dry. [noun]</li><li>Tears were beginning to <strong>well</strong> in her eyes. [verb]</li></ul><p>Distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will: </p><ul><li>Learn how parts-of-speech tagging works</li><li>Compute the transition matrix A in a Hidden Markov Model</li><li>Compute the transition matrix B in a Hidden Markov Model</li><li>Compute the Viterbi algorithm </li><li>Compute the accuracy of your own model </li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">0 Data Sources</a></li><li><a href="#1">1 POS Tagging</a><ul><li><a href="#1.1">1.1 Training</a><ul><li><a href="#ex-01">Exercise 01</a></li></ul></li><li><a href="#1.2">1.2 Testing</a><ul><li><a href="#ex-02">Exercise 02</a></li></ul></li></ul></li><li><a href="#2">2 Hidden Markov Models</a><ul><li><a href="#2.1">2.1 Generating Matrices</a><ul><li><a href="#ex-03">Exercise 03</a></li><li><a href="#ex-04">Exercise 04</a></li></ul></li></ul></li><li><a href="#3">3 Viterbi Algorithm</a><ul><li><a href="#3.1">3.1 Initialization</a><ul><li><a href="#ex-05">Exercise 05</a></li></ul></li><li><a href="#3.2">3.2 Viterbi Forward</a><ul><li><a href="#ex-06">Exercise 06</a></li></ul></li><li><a href="#3.3">3.3 Viterbi Backward</a><ul><li><a href="#ex-07">Exercise 07</a></li></ul></li></ul></li><li><a href="#4">4 Predicting on a data set</a><ul><li><a href="#ex-08">Exercise 08</a></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Importing packages and loading in the data set </span></span><br><span class="line"><span class="keyword">from</span> utils_pos <span class="keyword">import</span> get_word_tag, preprocess  </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><p><a name="0"></a></p><h2 id="Part-0-Data-Sources"><a href="#Part-0-Data-Sources" class="headerlink" title="Part 0: Data Sources"></a>Part 0: Data Sources</h2><p>This assignment will use two tagged data sets collected from the <strong>Wall Street Journal (WSJ)</strong>. </p><p><a href="http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html" target="_blank" rel="noopener">Here</a> is an example â€˜tag-setâ€™ or Part of Speech designation describing the two or three letter tag and their meaning. </p><ul><li>One data set (<strong>WSJ-2_21.pos</strong>) will be used for <strong>training</strong>.</li><li>The other (<strong>WSJ-24.pos</strong>) for <strong>testing</strong>. </li><li>The tagged training data has been preprocessed to form a vocabulary (<strong>hmm_vocab.txt</strong>). </li><li>The words in the vocabulary are words from the training set that were used two or more times. </li><li>The vocabulary is augmented with a set of â€˜unknown word tokensâ€™, described below. </li></ul><p>The training set will be used to create the emission, transmission and tag counts. </p><p>The test set (WSJ-24.pos) is read in to create <code>y</code>. </p><ul><li>This contains both the test text and the true tag. </li><li>The test set has also been preprocessed to remove the tags to form <strong>test_words.txt</strong>. </li><li>This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in <strong>utils_pos.py</strong>. </li><li>This forms the list <code>prep</code>, the preprocessed text used to test our  POS taggers.</li></ul><p>A POS tagger will necessarily encounter words that are not in its datasets. </p><ul><li>To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. </li><li>For example, the suffix â€˜izeâ€™ is a hint that the word is a verb, as in â€˜final-izeâ€™ or â€˜character-izeâ€™. </li><li>A set of unknown-tokens, such as â€˜â€”unk-verbâ€”â€˜ or â€˜â€”unk-nounâ€”â€˜ will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.</li></ul><p><img src="DataSources1.png"></p><p>Implementation note: </p><ul><li>For python 3.6 and beyond, dictionaries retain the insertion order. </li><li>Furthermore, their hash-based lookup makes them suitable for rapid membership tests. <ul><li>If _di_ is a dictionary, <code>key in di</code> will return <code>True</code> if _di_ has a key _key_, else <code>False</code>. </li></ul></li></ul><p>The dictionary <code>vocab</code> will utilize these features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the training corpus</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"WSJ_02-21.pos"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    training_corpus = f.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"A few items of the training corpus list"</span>)</span><br><span class="line">print(training_corpus[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>A few items of the training corpus list[&#39;In\tIN\n&#39;, &#39;an\tDT\n&#39;, &#39;Oct.\tNNP\n&#39;, &#39;19\tCD\n&#39;, &#39;review\tNN\n&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read the vocabulary data, split by each line of text, and save the list</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"hmm_vocab.txt"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    voc_l = f.read().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"A few items of the vocabulary list"</span>)</span><br><span class="line">print(voc_l[<span class="number">0</span>:<span class="number">50</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"A few items at the end of the vocabulary list"</span>)</span><br><span class="line">print(voc_l[<span class="number">-50</span>:])</span><br></pre></td></tr></table></figure><pre><code>A few items of the vocabulary list[&#39;!&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;, &#39;&amp;&#39;, &quot;&#39;&quot;, &quot;&#39;&#39;&quot;, &quot;&#39;40s&quot;, &quot;&#39;60s&quot;, &quot;&#39;70s&quot;, &quot;&#39;80s&quot;, &quot;&#39;86&quot;, &quot;&#39;90s&quot;, &quot;&#39;N&quot;, &quot;&#39;S&quot;, &quot;&#39;d&quot;, &quot;&#39;em&quot;, &quot;&#39;ll&quot;, &quot;&#39;m&quot;, &quot;&#39;n&#39;&quot;, &quot;&#39;re&quot;, &quot;&#39;s&quot;, &quot;&#39;til&quot;, &quot;&#39;ve&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;-&#39;, &#39;--&#39;, &#39;--n--&#39;, &#39;--unk--&#39;, &#39;--unk_adj--&#39;, &#39;--unk_adv--&#39;, &#39;--unk_digit--&#39;, &#39;--unk_noun--&#39;, &#39;--unk_punct--&#39;, &#39;--unk_upper--&#39;, &#39;--unk_verb--&#39;, &#39;.&#39;, &#39;...&#39;, &#39;0.01&#39;, &#39;0.0108&#39;, &#39;0.02&#39;, &#39;0.03&#39;, &#39;0.05&#39;, &#39;0.1&#39;, &#39;0.10&#39;, &#39;0.12&#39;, &#39;0.13&#39;, &#39;0.15&#39;]A few items at the end of the vocabulary list[&#39;yards&#39;, &#39;yardstick&#39;, &#39;year&#39;, &#39;year-ago&#39;, &#39;year-before&#39;, &#39;year-earlier&#39;, &#39;year-end&#39;, &#39;year-on-year&#39;, &#39;year-round&#39;, &#39;year-to-date&#39;, &#39;year-to-year&#39;, &#39;yearlong&#39;, &#39;yearly&#39;, &#39;years&#39;, &#39;yeast&#39;, &#39;yelled&#39;, &#39;yelling&#39;, &#39;yellow&#39;, &#39;yen&#39;, &#39;yes&#39;, &#39;yesterday&#39;, &#39;yet&#39;, &#39;yield&#39;, &#39;yielded&#39;, &#39;yielding&#39;, &#39;yields&#39;, &#39;you&#39;, &#39;young&#39;, &#39;younger&#39;, &#39;youngest&#39;, &#39;youngsters&#39;, &#39;your&#39;, &#39;yourself&#39;, &#39;youth&#39;, &#39;youthful&#39;, &#39;yuppie&#39;, &#39;yuppies&#39;, &#39;zero&#39;, &#39;zero-coupon&#39;, &#39;zeroing&#39;, &#39;zeros&#39;, &#39;zinc&#39;, &#39;zip&#39;, &#39;zombie&#39;, &#39;zone&#39;, &#39;zones&#39;, &#39;zoning&#39;, &#39;{&#39;, &#39;}&#39;, &#39;&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocab: dictionary that has the index of the corresponding words</span></span><br><span class="line">vocab = &#123;&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the index of the corresponding words. </span></span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sorted(voc_l)): </span><br><span class="line">    vocab[word] = i       </span><br><span class="line">    </span><br><span class="line">print(<span class="string">"Vocabulary dictionary, key is the word, value is a unique integer"</span>)</span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> vocab.items():</span><br><span class="line">    print(<span class="string">f"<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;v&#125;</span>"</span>)</span><br><span class="line">    cnt += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> cnt &gt; <span class="number">20</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>Vocabulary dictionary, key is the word, value is a unique integer:0!:1#:2$:3%:4&amp;:5&#39;:6&#39;&#39;:7&#39;40s:8&#39;60s:9&#39;70s:10&#39;80s:11&#39;86:12&#39;90s:13&#39;N:14&#39;S:15&#39;d:16&#39;em:17&#39;ll:18&#39;m:19&#39;n&#39;:20</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the test corpus</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"WSJ_24.pos"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    y = f.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"A sample of the test corpus"</span>)</span><br><span class="line">print(y[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>A sample of the test corpus[&#39;The\tDT\n&#39;, &#39;economy\tNN\n&#39;, &quot;&#39;s\tPOS\n&quot;, &#39;temperature\tNN\n&#39;, &#39;will\tMD\n&#39;, &#39;be\tVB\n&#39;, &#39;taken\tVBN\n&#39;, &#39;from\tIN\n&#39;, &#39;several\tJJ\n&#39;, &#39;vantage\tNN\n&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#corpus without tags, preprocessed</span></span><br><span class="line">_, prep = preprocess(vocab, <span class="string">"test.words"</span>)     </span><br><span class="line"></span><br><span class="line">print(<span class="string">'The length of the preprocessed test corpus: '</span>, len(prep))</span><br><span class="line">print(<span class="string">'This is a sample of the test_corpus: '</span>)</span><br><span class="line">print(prep[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>The length of the preprocessed test corpus:  34199This is a sample of the test_corpus: [&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;, &#39;from&#39;, &#39;several&#39;, &#39;--unk--&#39;]</code></pre><p><a name="1"></a></p><h1 id="Part-1-Parts-of-speech-tagging"><a href="#Part-1-Parts-of-speech-tagging" class="headerlink" title="Part 1: Parts-of-speech tagging"></a>Part 1: Parts-of-speech tagging</h1><p><a name="1.1"></a></p><h2 id="Part-1-1-Training"><a href="#Part-1-1-Training" class="headerlink" title="Part 1.1 - Training"></a>Part 1.1 - Training</h2><p>You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. </p><p>In this section, you will find the words that are not ambiguous. </p><ul><li>For example, the word <code>is</code> is a verb and it is not ambiguous. </li><li>In the <code>WSJ</code> corpus, $86$% of the token are unambiguous (meaning they have only one tag) </li><li>About $14\%$ are ambiguous (meaning that they have more than one tag)</li></ul><p><img src="pos.png" style="width:400px;height:250px;"></p><p>Before you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables. </p><h4 id="Transition-counts"><a href="#Transition-counts" class="headerlink" title="Transition counts"></a>Transition counts</h4><ul><li>The first dictionary is the <code>transition_counts</code> dictionary which computes the number of times each tag happened next to another tag. </li></ul><p>This dictionary will be used to compute: </p><script type="math/tex; mode=display">P(t_i |t_{i-1}) \tag{1}</script><p>This is the probability of a tag at position $i$ given the tag at position $i-1$.</p><p>In order for you to compute equation 1, you will create a <code>transition_counts</code> dictionary where </p><ul><li>The keys are <code>(prev_tag, tag)</code></li><li>The values are the number of times those two tags appeared in that order. </li></ul><h4 id="Emission-counts"><a href="#Emission-counts" class="headerlink" title="Emission counts"></a>Emission counts</h4><p>The second dictionary you will compute is the <code>emission_counts</code> dictionary. This dictionary will be used to compute:</p><script type="math/tex; mode=display">P(w_i|t_i)\tag{2}</script><p>In other words, you will use it to compute the probability of a word given its tag. </p><p>In order for you to compute equation 2, you will create an <code>emission_counts</code> dictionary where </p><ul><li>The keys are <code>(tag, word)</code> </li><li>The values are the number of times that pair showed up in your training set. </li></ul><h4 id="Tag-counts"><a href="#Tag-counts" class="headerlink" title="Tag counts"></a>Tag counts</h4><p>The last dictionary you will compute is the <code>tag_counts</code> dictionary. </p><ul><li>The key is the tag </li><li>The value is the number of times each tag appeared.</li></ul><p><a name="ex-01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Write a program that takes in the <code>training_corpus</code> and returns the three dictionaries mentioned above <code>transition_counts</code>, <code>emission_counts</code>, and <code>tag_counts</code>. </p><ul><li><code>emission_counts</code>: maps (tag, word) to the number of times it happened. </li><li><code>transition_counts</code>: maps (prev_tag, tag) to the number of times it has appeared. </li><li><code>tag_counts</code>: maps (tag) to the number of times it has occured. </li></ul><p>Implementation note: This routine utilises <em>defaultdict</em>, which is a subclass of <em>dict</em>. </p><ul><li>A standard Python dictionary throws a <em>KeyError</em> if you try to access an item with a key that is not currently in the dictionary. </li><li>In contrast, the <em>defaultdict</em> will create an item of the type of the argument, in this case an integer with the default value of 0. </li><li>See <a href="https://docs.python.org/3.3/library/collections.html#defaultdict-objects" target="_blank" rel="noopener">defaultdict</a>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_dictionaries</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dictionaries</span><span class="params">(training_corpus, vocab)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        training_corpus: a corpus where each line has a word followed by its tag.</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary where the keys are the tags and the values are the counts</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the dictionaries using defaultdict</span></span><br><span class="line">    emission_counts = defaultdict(int)</span><br><span class="line">    transition_counts = defaultdict(int)</span><br><span class="line">    tag_counts = defaultdict(int)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "prev_tag" (previous tag) with the start state, denoted by '--s--'</span></span><br><span class="line">    prev_tag = <span class="string">'--s--'</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use 'i' to track the line number in the corpus</span></span><br><span class="line">    i = <span class="number">0</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Each item in the training corpus contains a word and its POS tag</span></span><br><span class="line">    <span class="comment"># Go through each word and its tag in the training corpus</span></span><br><span class="line">    <span class="keyword">for</span> word_tag <span class="keyword">in</span> training_corpus:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the word_tag count</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Every 50,000 words, print the word count</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"word count = <span class="subst">&#123;i&#125;</span>"</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># get the word and tag using the get_word_tag helper function (imported from utils_pos.py)</span></span><br><span class="line">        word, tag = get_word_tag(word_tag,vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the transition count for the previous word and tag</span></span><br><span class="line">        transition_counts[(prev_tag, tag)] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the emission count for the tag and word</span></span><br><span class="line">        emission_counts[(tag, word)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Increment the tag count</span></span><br><span class="line">        tag_counts[tag] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set the previous tag to this tag (for the next iteration of the loop)</span></span><br><span class="line">        prev_tag = tag</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> emission_counts, transition_counts, tag_counts</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)</span><br></pre></td></tr></table></figure><pre><code>word count = 50000word count = 100000word count = 150000word count = 200000word count = 250000word count = 300000word count = 350000word count = 400000word count = 450000word count = 500000word count = 550000word count = 600000word count = 650000word count = 700000word count = 750000word count = 800000word count = 850000word count = 900000word count = 950000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get all the POS states</span></span><br><span class="line">states = sorted(tag_counts.keys())</span><br><span class="line">print(<span class="string">f"Number of POS tags (number of 'states'): <span class="subst">&#123;len(states)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">"View these POS tags (states)"</span>)</span><br><span class="line">print(states)</span><br></pre></td></tr></table></figure><pre><code>Number of POS tags (number of &#39;states&#39;): 46View these POS tags (states)[&#39;#&#39;, &#39;$&#39;, &quot;&#39;&#39;&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;--s--&#39;, &#39;.&#39;, &#39;:&#39;, &#39;CC&#39;, &#39;CD&#39;, &#39;DT&#39;, &#39;EX&#39;, &#39;FW&#39;, &#39;IN&#39;, &#39;JJ&#39;, &#39;JJR&#39;, &#39;JJS&#39;, &#39;LS&#39;, &#39;MD&#39;, &#39;NN&#39;, &#39;NNP&#39;, &#39;NNPS&#39;, &#39;NNS&#39;, &#39;PDT&#39;, &#39;POS&#39;, &#39;PRP&#39;, &#39;PRP$&#39;, &#39;RB&#39;, &#39;RBR&#39;, &#39;RBS&#39;, &#39;RP&#39;, &#39;SYM&#39;, &#39;TO&#39;, &#39;UH&#39;, &#39;VB&#39;, &#39;VBD&#39;, &#39;VBG&#39;, &#39;VBN&#39;, &#39;VBP&#39;, &#39;VBZ&#39;, &#39;WDT&#39;, &#39;WP&#39;, &#39;WP$&#39;, &#39;WRB&#39;, &#39;``&#39;]</code></pre><h5 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Number of POS tags (number of 'states'46</span><br><span class="line">View these states</span><br><span class="line">['#', '$', "''", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']</span><br></pre></td></tr></table></figure><p>The â€˜statesâ€™ are the Parts-of-speech designations found in the training data. They will also be referred to as â€˜tagsâ€™ or POS in this assignment. </p><ul><li>â€œNNâ€ is noun, singular, </li><li>â€˜NNSâ€™ is noun, plural. </li><li>In addition, there are helpful tags like â€˜â€”sâ€”â€˜ which indicate a start of a sentence.</li><li>You can get a more complete description at <a href="https://www.clips.uantwerpen.be/pages/mbsp-tags" target="_blank" rel="noopener">Penn Treebank II tag set</a>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"transition examples: "</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> list(transition_counts.items())[:<span class="number">3</span>]:</span><br><span class="line">    print(ex)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"emission examples: "</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> list(emission_counts.items())[<span class="number">200</span>:<span class="number">203</span>]:</span><br><span class="line">    <span class="keyword">print</span> (ex)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"ambiguous word example: "</span>)</span><br><span class="line"><span class="keyword">for</span> tup,cnt <span class="keyword">in</span> emission_counts.items():</span><br><span class="line">    <span class="keyword">if</span> tup[<span class="number">1</span>] == <span class="string">'back'</span>: <span class="keyword">print</span> (tup, cnt)</span><br></pre></td></tr></table></figure><pre><code>transition examples: ((&#39;--s--&#39;, &#39;IN&#39;), 5050)((&#39;IN&#39;, &#39;DT&#39;), 32364)((&#39;DT&#39;, &#39;NNP&#39;), 9044)emission examples: ((&#39;DT&#39;, &#39;any&#39;), 721)((&#39;NN&#39;, &#39;decrease&#39;), 7)((&#39;NN&#39;, &#39;insider-trading&#39;), 5)ambiguous word example: (&#39;RB&#39;, &#39;back&#39;) 304(&#39;VB&#39;, &#39;back&#39;) 20(&#39;RP&#39;, &#39;back&#39;) 84(&#39;JJ&#39;, &#39;back&#39;) 25(&#39;NN&#39;, &#39;back&#39;) 29(&#39;VBP&#39;, &#39;back&#39;) 4</code></pre><h5 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">transition examples: </span><br><span class="line">(('--s--', 'IN'), 5050)</span><br><span class="line">(('IN', 'DT'), 32364)</span><br><span class="line">(('DT', 'NNP'), 9044)</span><br><span class="line"></span><br><span class="line">emission examples: </span><br><span class="line">(('DT', 'any'), 721)</span><br><span class="line">(('NN', 'decrease'), 7)</span><br><span class="line">(('NN', 'insider-trading'), 5)</span><br><span class="line"></span><br><span class="line">ambiguous word example: </span><br><span class="line">('RB', 'back') 304</span><br><span class="line">('VB', 'back') 20</span><br><span class="line">('RP', 'back') 84</span><br><span class="line">('JJ', 'back') 25</span><br><span class="line">('NN', 'back') 29</span><br><span class="line">('VBP', 'back') 4</span><br></pre></td></tr></table></figure><p><a name="1.2"></a></p><h3 id="Part-1-2-Testing"><a href="#Part-1-2-Testing" class="headerlink" title="Part 1.2 - Testing"></a>Part 1.2 - Testing</h3><p>Now you will test the accuracy of your parts-of-speech tagger using your <code>emission_counts</code> dictionary. </p><ul><li>Given your preprocessed test corpus <code>prep</code>, you will assign a parts-of-speech tag to every word in that corpus. </li><li>Using the original tagged test corpus <code>y</code>, you will then compute what percent of the tags you got correct. </li></ul><p><a name="ex-02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement <code>predict_pos</code> that computes the accuracy of your model. </p><ul><li>This is a warm up exercise. </li><li>To assign a part of speech to a word, assign the most frequent POS for that word in the training set. </li><li>Then evaluate how well this approach works.  Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same.  If so, the prediction was correct!</li><li>Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: predict_pos</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_pos</span><span class="params">(prep, y, emission_counts, vocab, states)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.</span></span><br><span class="line"><span class="string">        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">        states: a sorted list of all possible tags for this assignment</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: Number of times you classified a word correctly</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the number of correct predictions to zero</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the (tag, word) tuples, stored as a set</span></span><br><span class="line">    all_words = set(emission_counts.keys())  <span class="comment"># (tag, word)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of (word, POS) tuples in the corpus 'y'</span></span><br><span class="line">    total = len(y)</span><br><span class="line">    <span class="keyword">for</span> word, y_tup <span class="keyword">in</span> zip(prep, y): </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the (word, POS) string into a list of two items</span></span><br><span class="line">        y_tup_l = y_tup.split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Verify that y_tup contain both word and POS</span></span><br><span class="line">        <span class="keyword">if</span> len(y_tup_l) == <span class="number">2</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the true POS label for this word</span></span><br><span class="line">            true_label = y_tup_l[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># If the y_tup didn't contain word and POS, go to next word</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">        count_final = <span class="number">0</span></span><br><span class="line">        pos_final = <span class="string">''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the word is in the vocabulary...</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> states:</span><br><span class="line"></span><br><span class="line">            <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># define the key as the tuple containing the POS and word</span></span><br><span class="line">                key = (pos, word)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if the (pos, word) key exists in the emission_counts dictionary</span></span><br><span class="line">                <span class="keyword">if</span> key <span class="keyword">in</span> emission_counts: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># get the emission count of the (pos,word) tuple </span></span><br><span class="line">                    count = emission_counts[key]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># keep track of the POS with the largest count</span></span><br><span class="line">                    <span class="keyword">if</span> count &gt; count_final: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final count (largest count)</span></span><br><span class="line">                        count_final = count</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final POS</span></span><br><span class="line">                        pos_final = pos</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the final POS (with the largest count) matches the true POS:</span></span><br><span class="line">            <span class="keyword">if</span> pos_final == true_label: <span class="comment"># complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Update the number of correct predictions</span></span><br><span class="line">                num_correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    accuracy = num_correct / total</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)</span><br><span class="line">print(<span class="string">f"Accuracy of prediction using predict_pos is <span class="subst">&#123;accuracy_predict_pos:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Accuracy of prediction using predict_pos is 0.8889</code></pre><h5 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of prediction <span class="keyword">using</span> predict_pos is <span class="number">0.8889</span></span><br></pre></td></tr></table></figure><p>88.9% is really good for this warm up exercise. With hidden markov models, you should be able to get <strong>95% accuracy.</strong></p><p><a name="2"></a></p><h1 id="Part-2-Hidden-Markov-Models-for-POS"><a href="#Part-2-Hidden-Markov-Models-for-POS" class="headerlink" title="Part 2: Hidden Markov Models for POS"></a>Part 2: Hidden Markov Models for POS</h1><p>Now you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder</p><ul><li>The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization. </li><li>In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. </li><li>By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in Part 1.</li></ul><p>The Markov Model contains a number of states and the probability of transition between those states. </p><ul><li>In this case, the states are the parts-of-speech. </li><li>A Markov Model utilizes a transition matrix, <code>A</code>. </li><li>A Hidden Markov Model adds an observation or emission matrix <code>B</code> which describes the probability of a visible observation when we are in a particular state. </li><li>In this case, the emissions are the words in the corpus</li><li>The state, which is hidden, is the POS tag of that word.</li></ul><p><a name="2.1"></a></p><h2 id="Part-2-1-Generating-Matrices"><a href="#Part-2-1-Generating-Matrices" class="headerlink" title="Part 2.1 Generating Matrices"></a>Part 2.1 Generating Matrices</h2><h3 id="Creating-the-â€˜Aâ€™-transition-probabilities-matrix"><a href="#Creating-the-â€˜Aâ€™-transition-probabilities-matrix" class="headerlink" title="Creating the â€˜Aâ€™ transition probabilities matrix"></a>Creating the â€˜Aâ€™ transition probabilities matrix</h3><p>Now that you have your <code>emission_counts</code>, <code>transition_counts</code>, and <code>tag_counts</code>, you will start implementing the Hidden Markov Model. </p><p>This will allow you to quickly construct the </p><ul><li><code>A</code> transition probabilities matrix.</li><li>and the <code>B</code> emission probabilities matrix. </li></ul><p>You will also use some smoothing when computing these matrices. </p><p>Here is an example of what the <code>A</code> transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):</p><div class="table-container"><table><thead><tr><th><strong>A</strong></th><th>â€¦</th><th>RBS</th><th>RP</th><th>SYM</th><th>TO</th><th>UH</th><th>â€¦</th></tr></thead><tbody><tr><td><strong>RBS</strong></td><td>â€¦</td><td>2.217069e-06</td><td>2.217069e-06</td><td>2.217069e-06</td><td>0.008870</td><td>2.217069e-06</td><td>â€¦</td></tr><tr><td><strong>RP</strong></td><td>â€¦</td><td>3.756509e-07</td><td>7.516775e-04</td><td>3.756509e-07</td><td>0.051089</td><td>3.756509e-07</td><td>â€¦</td></tr><tr><td><strong>SYM</strong></td><td>â€¦</td><td>1.722772e-05</td><td>1.722772e-05</td><td>1.722772e-05</td><td>0.000017</td><td>1.722772e-05</td><td>â€¦</td></tr><tr><td><strong>TO</strong></td><td>â€¦</td><td>4.477336e-05</td><td>4.472863e-08</td><td>4.472863e-08</td><td>0.000090</td><td>4.477336e-05</td><td>â€¦</td></tr><tr><td><strong>UH</strong></td><td>â€¦</td><td>1.030439e-05</td><td>1.030439e-05</td><td>1.030439e-05</td><td>0.061837</td><td>3.092348e-02</td><td>â€¦</td></tr><tr><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td></tr></tbody></table></div><p>Note that the matrix above was computed with smoothing. </p><p>Each cell gives you the probability to go from one part of speech to another. </p><ul><li>In other words, there is a 4.47e-8 chance of going from parts-of-speech <code>TO</code> to <code>RP</code>. </li><li>The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.</li></ul><p>The smoothing was done as follows: </p><script type="math/tex; mode=display">P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_{i}) + \alpha }{C(t_{i-1}) +\alpha * N}\tag{3}</script><ul><li>$N$ is the total number of tags</li><li>$C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in <code>transition_counts</code> dictionary.</li><li>$C(t_{i-1})$ is the count of the previous POS in the <code>tag_counts</code> dictionary.</li><li>$\alpha$ is a smoothing parameter.</li></ul><p><a name="ex-03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the <code>create_transition_matrix</code> below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix <code>A</code>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_transition_matrix</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_transition_matrix</span><span class="params">(alpha, tag_counts, transition_counts)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: number used for smoothing</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        transition_counts: transition count for the previous word and tag</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        A: matrix of dimension (num_tags,num_tags)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get a sorted list of unique POS tags</span></span><br><span class="line">    all_tags = sorted(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Count the number of unique POS tags</span></span><br><span class="line">    num_tags = len(all_tags)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the transition matrix 'A'</span></span><br><span class="line">    A = np.zeros((num_tags,num_tags))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the unique transition tuples (previous POS, current POS)</span></span><br><span class="line">    trans_keys = set(transition_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Return instances of 'None' with your code) ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row of the transition matrix A</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column of the transition matrix A</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_tags):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the count of the (prev POS, current POS) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># Define the tuple (prev POS, current POS)</span></span><br><span class="line">            <span class="comment"># Get the tag at position i and tag at position j (from the all_tags list)</span></span><br><span class="line">            key = (all_tags[i],all_tags[j])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check if the (prev POS, current POS) tuple </span></span><br><span class="line">            <span class="comment"># exists in the transition counts dictionaory</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> trans_keys: <span class="comment">#complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get count from the transition_counts dictionary </span></span><br><span class="line">                <span class="comment"># for the (prev POS, current POS) tuple</span></span><br><span class="line">                count = transition_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the previous tag (index position i) from tag_counts</span></span><br><span class="line">            count_prev_tag = tag_counts[all_tags[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Apply smoothing using count of the tuple, alpha, </span></span><br><span class="line">            <span class="comment"># count of previous tag, alpha, and number of total tags</span></span><br><span class="line">            A[i,j] = (count + alpha ) / ( count_prev_tag + alpha * num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line">A = create_transition_matrix(alpha, tag_counts, transition_counts)</span><br><span class="line"><span class="comment"># Testing your function</span></span><br><span class="line">print(<span class="string">f"A at row 0, col 0: <span class="subst">&#123;A[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"A at row 3, col 1: <span class="subst">&#123;A[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"View a subset of transition matrix A"</span>)</span><br><span class="line">A_sub = pd.DataFrame(A[<span class="number">30</span>:<span class="number">35</span>,<span class="number">30</span>:<span class="number">35</span>], index=states[<span class="number">30</span>:<span class="number">35</span>], columns = states[<span class="number">30</span>:<span class="number">35</span>] )</span><br><span class="line">print(A_sub)</span><br></pre></td></tr></table></figure><pre><code>A at row 0, col 0: 0.000007040A at row 3, col 1: 0.1691View a subset of transition matrix A              RBS            RP           SYM        TO            UHRBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02</code></pre><h5 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A at row <span class="number">0</span>, col <span class="number">0</span>: <span class="number">0.000007040</span></span><br><span class="line">A at row <span class="number">3</span>, col <span class="number">1</span>: <span class="number">0.1691</span></span><br><span class="line">View a subset of transition matrix A</span><br><span class="line">              RBS            RP           SYM        TO            UH</span><br><span class="line">RBS  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">0.008870</span>  <span class="number">2.217069e-06</span></span><br><span class="line">RP   <span class="number">3.756509e-07</span>  <span class="number">7.516775e-04</span>  <span class="number">3.756509e-07</span>  <span class="number">0.051089</span>  <span class="number">3.756509e-07</span></span><br><span class="line">SYM  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">0.000017</span>  <span class="number">1.722772e-05</span></span><br><span class="line">TO   <span class="number">4.477336e-05</span>  <span class="number">4.472863e-08</span>  <span class="number">4.472863e-08</span>  <span class="number">0.000090</span>  <span class="number">4.477336e-05</span></span><br><span class="line">UH   <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">0.061837</span>  <span class="number">3.092348e-02</span></span><br></pre></td></tr></table></figure><h3 id="Create-the-â€˜Bâ€™-emission-probabilities-matrix"><a href="#Create-the-â€˜Bâ€™-emission-probabilities-matrix" class="headerlink" title="Create the â€˜Bâ€™ emission probabilities matrix"></a>Create the â€˜Bâ€™ emission probabilities matrix</h3><p>Now you will create the <code>B</code> transition matrix which computes the emission probability. </p><p>You will use smoothing as defined below: </p><script type="math/tex; mode=display">P(w_i | t_i) = \frac{C(t_i, word_i)+ \alpha}{C(t_{i}) +\alpha * N}\tag{4}</script><ul><li>$C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in <code>emission_counts</code> dictionary).</li><li>$C(t_i)$ is the number of times $tag_i$ was in the training data (stored in <code>tag_counts</code> dictionary).</li><li>$N$ is the number of words in the vocabulary</li><li>$\alpha$ is a smoothing parameter. </li></ul><p>The matrix <code>B</code> is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. </p><p>Here is an example of the matrix, only a subset of tags and words are shown: </p><p style="text-align: center;"> <b>B Emissions Probability Matrix (subset)</b>  </p><div class="table-container"><table><thead><tr><th><strong>B</strong></th><th>â€¦</th><th>725</th><th>adroitly</th><th>engineers</th><th>promoted</th><th>synergy</th><th>â€¦</th></tr></thead><tbody><tr><td><strong>CD</strong></td><td>â€¦</td><td><strong>8.201296e-05</strong></td><td>2.732854e-08</td><td>2.732854e-08</td><td>2.732854e-08</td><td>2.732854e-08</td><td>â€¦</td></tr><tr><td><strong>NN</strong></td><td>â€¦</td><td>7.521128e-09</td><td>7.521128e-09</td><td>7.521128e-09</td><td>7.521128e-09</td><td><strong>2.257091e-05</strong></td><td>â€¦</td></tr><tr><td><strong>NNS</strong></td><td>â€¦</td><td>1.670013e-08</td><td>1.670013e-08</td><td><strong>4.676203e-04</strong></td><td>1.670013e-08</td><td>1.670013e-08</td><td>â€¦</td></tr><tr><td><strong>VB</strong></td><td>â€¦</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>â€¦</td></tr><tr><td><strong>RB</strong></td><td>â€¦</td><td>3.226454e-08</td><td><strong>6.456135e-05</strong></td><td>3.226454e-08</td><td>3.226454e-08</td><td>3.226454e-08</td><td>â€¦</td></tr><tr><td><strong>RP</strong></td><td>â€¦</td><td>3.723317e-07</td><td>3.723317e-07</td><td>3.723317e-07</td><td><strong>3.723317e-07</strong></td><td>3.723317e-07</td><td>â€¦</td></tr><tr><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td><td>â€¦</td></tr></tbody></table></div><p><a name="ex-04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement the <code>create_emission_matrix</code> below that computes the <code>B</code> emission probabilities matrix. Your function takes in $\alpha$, the smoothing parameter, <code>tag_counts</code>, which is a dictionary mapping each tag to its respective count, the <code>emission_counts</code> dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix <code>B</code>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_emission_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_emission_matrix</span><span class="params">(alpha, tag_counts, emission_counts, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: tuning parameter used in smoothing </span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        B: a matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get the number of POS tag</span></span><br><span class="line">    num_tags = len(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a list of all POS tags</span></span><br><span class="line">    all_tags = sorted(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the total number of unique words in the vocabulary</span></span><br><span class="line">    num_words = len(vocab)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the emission matrix B with places for</span></span><br><span class="line">    <span class="comment"># tags in the rows and words in the columns</span></span><br><span class="line">    B = np.zeros((num_tags, num_words))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a set of all (POS, word) tuples </span></span><br><span class="line">    <span class="comment"># from the keys of the emission_counts dictionary</span></span><br><span class="line">    emis_keys = set(list(emission_counts.keys()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row (POS tags)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column (words)</span></span><br><span class="line">        <span class="keyword">for</span> j,word <span class="keyword">in</span> enumerate(vocab): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the emission count for the (POS tag, word) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># Define the (POS tag, word) tuple for this row and column</span></span><br><span class="line">            key =  (all_tags[i], word)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the (POS tag, word) tuple exists as a key in emission counts</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> emis_keys: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">                <span class="comment"># Get the count of (POS tag, word) from the emission_counts d</span></span><br><span class="line">                count = emission_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the POS tag</span></span><br><span class="line">            count_tag = tag_counts[all_tags[i]]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Apply smoothing and store the smoothed value </span></span><br><span class="line">            <span class="comment"># into the emission matrix B for this row and column</span></span><br><span class="line">            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> B</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># creating your emission probability matrix. this takes a few minutes to run. </span></span><br><span class="line">B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"View Matrix position at row 0, column 0: <span class="subst">&#123;B[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"View Matrix position at row 3, column 1: <span class="subst">&#123;B[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Try viewing emissions for a few words in a sample dataframe</span></span><br><span class="line">cidx  = [<span class="string">'725'</span>,<span class="string">'adroitly'</span>,<span class="string">'engineers'</span>, <span class="string">'promoted'</span>, <span class="string">'synergy'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the integer ID for each word</span></span><br><span class="line">cols = [vocab[a] <span class="keyword">for</span> a <span class="keyword">in</span> cidx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose POS tags to show in a sample dataframe</span></span><br><span class="line">rvals =[<span class="string">'CD'</span>,<span class="string">'NN'</span>,<span class="string">'NNS'</span>, <span class="string">'VB'</span>,<span class="string">'RB'</span>,<span class="string">'RP'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># For each POS tag, get the row number from the 'states' list</span></span><br><span class="line">rows = [states.index(a) <span class="keyword">for</span> a <span class="keyword">in</span> rvals]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the emissions for the sample of words, and the sample of POS tags</span></span><br><span class="line">B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )</span><br><span class="line">print(B_sub)</span><br></pre></td></tr></table></figure><pre><code>View Matrix position at row 0, column 0: 0.000006032View Matrix position at row 3, column 1: 0.000000720              725      adroitly     engineers      promoted       synergyCD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07</code></pre><h5 id="Expected-Output-4"><a href="#Expected-Output-4" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">View Matrix position at row <span class="number">0</span>, column <span class="number">0</span>: <span class="number">0.000006032</span></span><br><span class="line">View Matrix position at row <span class="number">3</span>, column <span class="number">1</span>: <span class="number">0.000000720</span></span><br><span class="line">              <span class="number">725</span>      adroitly     engineers      promoted       synergy</span><br><span class="line">CD   <span class="number">8.201296e-05</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span></span><br><span class="line">NN   <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">2.257091e-05</span></span><br><span class="line">NNS  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span>  <span class="number">4.676203e-04</span>  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span></span><br><span class="line">VB   <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span></span><br><span class="line">RB   <span class="number">3.226454e-08</span>  <span class="number">6.456135e-05</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span></span><br><span class="line">RP   <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span></span><br></pre></td></tr></table></figure><p><a name="3"></a></p><h1 id="Part-3-Viterbi-Algorithm-and-Dynamic-Programming"><a href="#Part-3-Viterbi-Algorithm-and-Dynamic-Programming" class="headerlink" title="Part 3: Viterbi Algorithm and Dynamic Programming"></a>Part 3: Viterbi Algorithm and Dynamic Programming</h1><p>In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, <code>A</code> and <code>B</code> to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. </p><ul><li><strong>Initialization</strong> - In this part you initialize the <code>best_paths</code> and <code>best_probabilities</code> matrices that you will be populating in <code>feed_forward</code>.</li><li><strong>Feed forward</strong> - At each step, you calculate the probability of each path happening and the best paths up to that point. </li><li><strong>Feed backward</strong>: This allows you to find the best path with the highest probabilities. </li></ul><p><a name="3.1"></a></p><h2 id="Part-3-1-Initialization"><a href="#Part-3-1-Initialization" class="headerlink" title="Part 3.1:  Initialization"></a>Part 3.1:  Initialization</h2><p>You will start by initializing two matrices of the same dimension. </p><ul><li><p>best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.</p></li><li><p>best_paths: A matrix that helps you trace through the best possible path in the corpus. </p></li></ul><p><a name="ex-05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions</strong>:<br>Write a program below that initializes the <code>best_probs</code> and the <code>best_paths</code> matrix. </p><p>Both matrices will be initialized to zero except for column zero of <code>best_probs</code>.  </p><ul><li>Column zero of <code>best_probs</code> is initialized with the assumption that the first word of the corpus was preceded by a start token (â€œâ€”sâ€”â€œ). </li><li>This allows you to reference the <strong>A</strong> matrix for the transition probability</li></ul><p>Here is how to initialize column 0 of <code>best_probs</code>:</p><ul><li>The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\textrm{best_probs}[s_{idx}, i]$.</li><li>This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\mathbf{B}[i, vocab[corpus[0]]]$.</li><li>Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). </li><li><strong>vocab</strong> is a dictionary that returns the unique integer that refers to that particular word.</li></ul><p>Conceptually, it looks like this:<br>$\textrm{best_probs}[s_{idx}, i] = \mathbf{A}[s_{idx}, i] \times \mathbf{B}[i, corpus[0] ]$</p><p>In order to avoid multiplying and storing small values on the computer, weâ€™ll take the log of the product, which becomes the sum of two logs:</p><p>$best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$</p><p>Also, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set $best_probs[i,0] = float(â€˜-infâ€™)$ when $A[s_{idx}, i] == 0$</p><p>So the implementation to initialize $best_probs$ looks like this:</p><p>$ if A[s_{idx}, i] &lt;&gt; 0 : best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])$</p><p>$ if A[s_{idx}, i] == 0 : best_probs[i,0] = float(â€˜-infâ€™)$</p><p>Please use <a href="https://docs.python.org/3/library/math.html" target="_blank" rel="noopener">math.log</a> to compute the natural logarithm.</p><p>The example below shows the initialization assuming the corpus starts with the phrase â€œLoss tracks upwardâ€.</p><p><img src="Initialize4.png"></p><p>Represent infinity and negative infinity like this:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">float('inf')</span><br><span class="line">float('-inf')</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: initialize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(states, tag_counts, A, B, corpus, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        states: a list of all possible parts-of-speech</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        A: Transition Matrix of dimension (num_tags, num_tags)</span></span><br><span class="line"><span class="string">        B: Emission Matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">        corpus: a sequence of words whose POS is to be identified in a list </span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        best_probs: matrix of dimension (num_tags, len(corpus)) of floats</span></span><br><span class="line"><span class="string">        best_paths: matrix of dimension (num_tags, len(corpus)) of integers</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the total number of unique POS tags</span></span><br><span class="line">    num_tags = len(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_probs matrix </span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as the columns</span></span><br><span class="line">    best_probs = np.zeros((num_tags, len(corpus)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_paths matrix</span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as columns</span></span><br><span class="line">    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the start token</span></span><br><span class="line">    s_idx = states.index(<span class="string">"--s--"</span>)</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each of the POS tags</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags) : <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Handle the special case when the transition from start token to POS tag i is zero</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] == <span class="number">0</span>: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag 'i', column 0, to negative infinity</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = float(<span class="string">"-inf"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># For all other cases when transition from start token to POS tag i is non-zero:</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag 'i', column 0</span></span><br><span class="line">            <span class="comment"># Check the formula in the instructions above</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = math.log(A[s_idx,i])  +  math.log(B[i,vocab[corpus[<span class="number">0</span>]]])</span><br><span class="line">            </span><br><span class="line">                         </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the function</span></span><br><span class="line">print(<span class="string">f"best_probs[0,0]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>"</span>) </span><br><span class="line">print(<span class="string">f"best_paths[2,3]: <span class="subst">&#123;best_paths[<span class="number">2</span>,<span class="number">3</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>best_probs[0,0]: -22.6098best_paths[2,3]: 0.0000</code></pre><h5 id="Expected-Output-5"><a href="#Expected-Output-5" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">0</span>]: <span class="number">-22.6098</span></span><br><span class="line">best_paths[<span class="number">2</span>,<span class="number">3</span>]: <span class="number">0.0000</span></span><br></pre></td></tr></table></figure><p><a name="3.2"></a></p><h2 id="Part-3-2-Viterbi-Forward"><a href="#Part-3-2-Viterbi-Forward" class="headerlink" title="Part 3.2 Viterbi Forward"></a>Part 3.2 Viterbi Forward</h2><p>In this part of the assignment, you will implement the <code>viterbi_forward</code> segment. In other words, you will populate your <code>best_probs</code> and <code>best_paths</code> matrices.</p><ul><li>Walk forward through the corpus.</li><li>For each word, compute a probability for each possible tag. </li><li>Unlike the previous algorithm <code>predict_pos</code> (the â€˜warm-upâ€™ exercise), this will include the path up to that (word,tag) combination. </li></ul><p>Here is an example with a three-word corpus â€œLoss tracks upwardâ€:</p><ul><li>Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. </li><li>In the diagram below, the first word â€œLossâ€ is already initialized. </li><li>The algorithm will compute a probability for each of the potential tags in the second and future words. </li></ul><p>Compute the probability that the tag of the second work (â€˜tracksâ€™) is a verb, 3rd person singular present (VBZ).  </p><ul><li>In the <code>best_probs</code> matrix, go to the column of the second word (â€˜tracksâ€™), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.</li><li>Examine each of the paths from the tags of the first word (â€˜Lossâ€™) and choose the most likely path.  </li><li>An example of the calculation for <strong>one</strong> of those paths is the path from (â€˜Lossâ€™, NN) to (â€˜tracksâ€™, VBZ).</li><li>The log of the probability of the path up to and including the first word â€˜Lossâ€™ having POS tag NN is $-14.32$.  The <code>best_probs</code> matrix contains this value -14.32 in the column for â€˜Lossâ€™ and row for â€˜NNâ€™.</li><li>Find the probability that NN transitions to VBZ.  To find this probability, go to the <code>A</code> transition matrix, and go to the row for â€˜NNâ€™ and the column for â€˜VBZâ€™.  The value is $4.37e-02$, which is circled in the diagram, so add $-14.32 + log(4.37e-02)$. </li><li>Find the log of the probability that the tag VBS would â€˜emitâ€™ the word â€˜tracksâ€™.  To find this, look at the â€˜Bâ€™ emission matrix in row â€˜VBZâ€™ and the column for the word â€˜tracksâ€™.  The value $4.61e-04$ is circled in the diagram below.  So add $-14.32 + log(4.37e-02) + log(4.61e-04)$.</li><li>The sum of $-14.32 + log(4.37e-02) + log(4.61e-04)$ is $-25.13$. Store $-25.13$ in the <code>best_probs</code> matrix at row â€˜VBZâ€™ and column â€˜tracksâ€™ (as seen in the cell that is highlighted in light orange in the diagram).</li><li>All other paths in best_probs are calculated.  Notice that $-25.13$ is greater than all of the other values in column â€˜tracksâ€™ of matrix <code>best_probs</code>, and so the most likely path to â€˜VBZâ€™ is from â€˜NNâ€™.  â€˜NNâ€™ is in row 20 of the <code>best_probs</code> matrix, so $20$ is the most likely path.</li><li>Store the most likely path $20$ in the <code>best_paths</code> table.  This is highlighted in light orange in the diagram below.</li></ul><p>The formula to compute the probability and path for the $i^{th}$ word in the $corpus$, the prior word $i-1$ in the corpus, current POS tag $j$, and previous POS tag $k$ is:</p><p>$\mathrm{prob} = \mathbf{best_prob}_{k, i-1} + \mathrm{log}(\mathbf{A}_{k, j}) + \mathrm{log}(\mathbf{B}_{j, vocab(corpus_{i})})$</p><p>where $corpus_{i}$ is the word in the corpus at index $i$, and $vocab$ is the dictionary that gets the unique integer that represents a given word.</p><p>$\mathrm{path} = k$</p><p>where $k$ is the integer representing the previous POS tag.</p><p><a name="ex-06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p>Instructions: Implement the <code>viterbi_forward</code> algorithm and store the best_path and best_prob for every possible tag for each word in the matrices <code>best_probs</code> and <code>best_tags</code> using the pseudo code below.</p><p>`for each word in the corpus</p><pre><code>for each POS tag type that this word may be    for POS tag type that the previous word could be        compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.        retain the highest probability computed for the current word        set best_probs to this highest probability        set best_paths to the index &#39;k&#39;, representing the POS tag of the previous word which produced the highest probability `</code></pre><p>Please use <a href="https://docs.python.org/3/library/math.html" target="_blank" rel="noopener">math.log</a> to compute the natural logarithm.</p><p><img src="Forward4.png"></p><h2 id><a href="#" class="headerlink" title></a><details></details></h2><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember that when accessing emission matrix B, the column index is the unique integer ID associated with the word.  It can be accessed by using the 'vocab' dictionary, where the key is the word, and the value is the unique integer ID for that word.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_forward</span><span class="params">(A, B, test_corpus, best_probs, best_paths, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        A, B: The transiton and emission matrices respectively</span></span><br><span class="line"><span class="string">        test_corpus: a list containing a preprocessed corpus</span></span><br><span class="line"><span class="string">        best_probs: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        best_probs: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the number of unique POS tags (which is the num of rows in best_probs)</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through every word in the corpus starting from word 1</span></span><br><span class="line">    <span class="comment"># Recall that word 0 was initialized in `initialize()`</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(test_corpus)): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print number of words processed, every 5000 words</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">5000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Words processed: &#123;:&gt;8&#125;"</span>.format(i))</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###</span></span><br><span class="line">        <span class="comment"># For each unique POS tag that the current word can be</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_prob for word i to negative infinity</span></span><br><span class="line">            best_prob_i = float(<span class="string">"-inf"</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_path for current word i to None</span></span><br><span class="line">            best_path_i = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># For each POS tag that the previous word can be:</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Calculate the probability = </span></span><br><span class="line">                <span class="comment"># best probs of POS tag k, previous word i-1 + </span></span><br><span class="line">                <span class="comment"># log(prob of transition from POS k to POS j) + </span></span><br><span class="line">                <span class="comment"># log(prob that emission of POS j is word i)</span></span><br><span class="line">                prob = best_probs[k, i<span class="number">-1</span>] + math.log(A[k,j]) + math.log(B[j, vocab[test_corpus[i]]])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if this path's probability is greater than</span></span><br><span class="line">                <span class="comment"># the best probability up to and before this point</span></span><br><span class="line">                <span class="keyword">if</span> prob &gt; best_prob_i: <span class="comment"># complete this line</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Keep track of the best probability</span></span><br><span class="line">                    best_prob_i = prob</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># keep track of the POS tag of the previous word</span></span><br><span class="line">                    <span class="comment"># that is part of the best path.  </span></span><br><span class="line">                    <span class="comment"># Save the index (integer) associated with </span></span><br><span class="line">                    <span class="comment"># that previous word's POS tag</span></span><br><span class="line">                    best_path_i = k</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Save the best probability for the </span></span><br><span class="line">            <span class="comment"># given current word's POS tag</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus</span></span><br><span class="line">            best_probs[j,i] = best_prob_i</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the unique integer ID of the previous POS tag</span></span><br><span class="line">            <span class="comment"># into best_paths matrix, for the POS tag of the current word</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus.</span></span><br><span class="line">            best_paths[j,i] = best_path_i</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure><p>Run the <code>viterbi_forward</code> function to fill in the <code>best_probs</code> and <code>best_paths</code> matrices.</p><p><strong>Note</strong> that this will take a few minutes to run.  There are about 30,000 words to process.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this will take a few minutes to run =&gt; processes ~ 30,000 words</span></span><br><span class="line">best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)</span><br></pre></td></tr></table></figure><pre><code>Words processed:     5000Words processed:    10000Words processed:    15000Words processed:    20000Words processed:    25000Words processed:    30000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test this function </span></span><br><span class="line">print(<span class="string">f"best_probs[0,1]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>) </span><br><span class="line">print(<span class="string">f"best_probs[0,4]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">4</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>best_probs[0,1]: -24.7822best_probs[0,4]: -49.5601</code></pre><h5 id="Expected-Output-6"><a href="#Expected-Output-6" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">1</span>]: <span class="number">-24.7822</span></span><br><span class="line">best_probs[<span class="number">0</span>,<span class="number">4</span>]: <span class="number">-49.5601</span></span><br></pre></td></tr></table></figure><p><a name="3.3"></a></p><h2 id="Part-3-3-Viterbi-backward"><a href="#Part-3-3-Viterbi-backward" class="headerlink" title="Part 3.3 Viterbi backward"></a>Part 3.3 Viterbi backward</h2><p>Now you will implement the Viterbi backward algorithm.</p><ul><li>The Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the <code>best_paths</code> and the <code>best_probs</code> matrices.</li></ul><p>The example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: â€œLoss tracks upwardâ€.</p><p>POS tag for â€˜upwardâ€™ is <code>RB</code></p><ul><li>Select the the most likely POS tag for the last word in the corpus, â€˜upwardâ€™ in the <code>best_prob</code> table.</li><li>Look for the row in the column for â€˜upwardâ€™ that has the largest probability.</li><li>Notice that in row 28 of <code>best_probs</code>, the estimated probability is -34.99, which is larger than the other values in the column.  So the most likely POS tag for â€˜upwardâ€™ is <code>RB</code> an adverb, at row 28 of <code>best_prob</code>. </li><li>The variable <code>z</code> is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus.  In array z, at position 2, store the value 28 to indicate that the word â€˜upwardâ€™ (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is <code>RB</code>).</li><li>The variable <code>pred</code> contains the POS tags in string form.  So <code>pred</code> at index 2 stores the string <code>RB</code>.</li></ul><p>POS tag for â€˜tracksâ€™ is <code>VBZ</code></p><ul><li>The next step is to go backward one word in the corpus (â€˜tracksâ€™).  Since the most likely POS tag for â€˜upwardâ€™ is <code>RB</code>, which is uniquely identified by integer ID 28, go to the <code>best_paths</code> matrix in column 2, row 28.  The value stored in <code>best_paths</code>, column 2, row 28 indicates the unique ID of the POS tag of the previous word.  In this case, the value stored here is 40, which is the unique ID for POS tag <code>VBZ</code> (verb, 3rd person singular present).</li><li>So the previous word at index 1 of the corpus (â€˜tracksâ€™), most likely has the POS tag with unique ID 40, which is <code>VBZ</code>.</li><li>In array <code>z</code>, store the value 40 at position 1, and for array <code>pred</code>, store the string <code>VBZ</code> to indicate that the word â€˜tracksâ€™ most likely has POS tag <code>VBZ</code>.</li></ul><p>POS tag for â€˜Lossâ€™ is <code>NN</code></p><ul><li>In <code>best_paths</code> at column 1, the unique ID stored at row 40 is 20.  20 is the unique ID for POS tag <code>NN</code>.</li><li>In array <code>z</code> at position 0, store 20.  In array <code>pred</code> at position 0, store <code>NN</code>.</li></ul><p><img src="Backwards5.png"></p><p><a name="ex-07"></a></p><h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Implement the <code>viterbi_backward</code> algorithm, which returns a list of predicted POS tags for each word in the corpus.</p><ul><li>Note that the numbering of the index positions starts at 0 and not 1. </li><li><code>m</code> is the number of words in the corpus.  <ul><li>So the indexing into the corpus goes from <code>0</code> to <code>m - 1</code>.</li><li>Also, the columns in <code>best_probs</code> and <code>best_paths</code> are indexed from <code>0</code> to <code>m - 1</code></li></ul></li></ul><p><strong>In Step 1:</strong><br>Loop through all the rows (POS tags) in the last entry of <code>best_probs</code> and find the row (POS tag) with the maximum value.<br>Convert the unique integer ID to a tag (a string representation) using the dictionary <code>states</code>.  </p><p>Referring to the three-word corpus described above:</p><ul><li><code>z[2] = 28</code>: For the word â€˜upwardâ€™ at position 2 in the corpus, the POS tag ID is 28.  Store 28 in <code>z</code> at position 2.</li><li>states(28) is â€˜RBâ€™: The POS tag ID 28 refers to the POS tag â€˜RBâ€™.</li><li><code>pred[2] = &#39;RB&#39;</code>: In array <code>pred</code>, store the POS tag for the word â€˜upwardâ€™.</li></ul><p><strong>In Step 2:</strong>  </p><ul><li>Starting at the last column of best_paths, use <code>best_probs</code> to find the most likely POS tag for the last word in the corpus.</li><li>Then use <code>best_paths</code> to find the most likely POS tag for the previous word. </li><li>Update the POS tag for each word in <code>z</code> and in <code>preds</code>.</li></ul><p>Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.<br><code>z[1] = best_paths[z[2],2]</code>  </p><p>The small test following the routine prints the last few words of the corpus and their states to aid in debug.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_backward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_backward</span><span class="params">(best_probs, best_paths, corpus, states)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function returns the best path.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the number of words in the corpus</span></span><br><span class="line">    <span class="comment"># which is also the number of columns in best_probs, best_paths</span></span><br><span class="line">    m = best_paths.shape[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize array z, same length as the corpus</span></span><br><span class="line">    z = [<span class="keyword">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of unique POS tags</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the best probability for the last word</span></span><br><span class="line">    best_prob_for_last_word = float(<span class="string">'-inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize pred array, same length as corpus</span></span><br><span class="line">    pred = [<span class="keyword">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment">## Step 1 ##</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each POS tag for the last word (last column of best_probs)</span></span><br><span class="line">    <span class="comment"># in order to find the row (POS tag integer ID) </span></span><br><span class="line">    <span class="comment"># with highest probability for the last word</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the probability of POS tag at row k </span></span><br><span class="line">        <span class="comment"># is better than the previosly best probability for the last word:</span></span><br><span class="line">        <span class="keyword">if</span> best_probs[k,<span class="number">-1</span>] &gt; best_prob_for_last_word: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Store the new best probability for the lsat word</span></span><br><span class="line">            best_prob_for_last_word = best_probs[k,<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># Store the unique integer ID of the POS tag</span></span><br><span class="line">            <span class="comment"># which is also the row number in best_probs</span></span><br><span class="line">            z[m - <span class="number">1</span>] = k</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># Convert the last word's predicted POS tag</span></span><br><span class="line">    <span class="comment"># from its unique integer ID into the string representation</span></span><br><span class="line">    <span class="comment"># using the 'states' dictionary</span></span><br><span class="line">    <span class="comment"># store this in the 'pred' array for the last word</span></span><br><span class="line">    pred[m - <span class="number">1</span>] = states[k]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2 ##</span></span><br><span class="line">    <span class="comment"># Find the best POS tags by walking backward through the best_paths</span></span><br><span class="line">    <span class="comment"># From the last word in the corpus to the 0th word in the corpus</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the unique integer ID of</span></span><br><span class="line">        <span class="comment"># the POS tag for the word at position 'i' in the corpus</span></span><br><span class="line">        pos_tag_for_word_i = best_paths[z[i], i]</span><br><span class="line"><span class="comment">#         print(z[i])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># In best_paths, go to the row representing the POS tag of word i</span></span><br><span class="line">        <span class="comment"># and the column representing the word's position in the corpus</span></span><br><span class="line">        <span class="comment"># to retrieve the predicted POS for the word at position i-1 in the corpus</span></span><br><span class="line">        z[i - <span class="number">1</span>] = pos_tag_for_word_i</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the previous word's POS tag in string form</span></span><br><span class="line">        <span class="comment"># Use the 'states' dictionary, </span></span><br><span class="line">        <span class="comment"># where the key is the unique integer ID of the POS tag,</span></span><br><span class="line">        <span class="comment"># and the value is the string representation of that POS tag</span></span><br><span class="line">        pred[i - <span class="number">1</span>] = states[pos_tag_for_word_i]        </span><br><span class="line">     <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run and test your function</span></span><br><span class="line">pred = viterbi_backward(best_probs, best_paths, prep, states)</span><br><span class="line">m=len(pred)</span><br><span class="line">print(<span class="string">'The prediction for pred[-7:m-1] is: \n'</span>, prep[<span class="number">-7</span>:m<span class="number">-1</span>], <span class="string">"\n"</span>, pred[<span class="number">-7</span>:m<span class="number">-1</span>], <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">'The prediction for pred[0:8] is: \n'</span>, pred[<span class="number">0</span>:<span class="number">7</span>], <span class="string">"\n"</span>, prep[<span class="number">0</span>:<span class="number">7</span>])</span><br></pre></td></tr></table></figure><pre><code>The prediction for pred[-7:m-1] is:  [&#39;see&#39;, &#39;them&#39;, &#39;here&#39;, &#39;with&#39;, &#39;us&#39;, &#39;.&#39;]  [&#39;VB&#39;, &#39;PRP&#39;, &#39;RB&#39;, &#39;IN&#39;, &#39;PRP&#39;, &#39;.&#39;] The prediction for pred[0:8] is:  [&#39;DT&#39;, &#39;NN&#39;, &#39;POS&#39;, &#39;NN&#39;, &#39;MD&#39;, &#39;VB&#39;, &#39;VBN&#39;]  [&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;]</code></pre><p><strong>Expected Output:</strong>   </p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The prediction <span class="keyword">for</span> prep[<span class="number">-7</span>:m<span class="number">-1</span>] is:  </span><br><span class="line"> ['see', 'them', 'here', 'with', 'us', '.']  </span><br><span class="line"> ['VB', 'PRP', 'RB', 'IN', 'PRP', '.']   </span><br><span class="line">The prediction <span class="keyword">for</span> pred[<span class="number">0</span>:<span class="number">8</span>] is:    </span><br><span class="line"> ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN']   </span><br><span class="line"> ['The', 'economy', "'s", 'temperature', 'will', 'be', 'taken']</span><br></pre></td></tr></table></figure><p>Now you just have to compare the predicted labels to the true labels to evaluate your model on the accuracy metric!</p><p><a name="4"></a></p><h1 id="Part-4-Predicting-on-a-data-set"><a href="#Part-4-Predicting-on-a-data-set" class="headerlink" title="Part 4: Predicting on a data set"></a>Part 4: Predicting on a data set</h1><p>Compute the accuracy of your prediction by comparing it with the true <code>y</code> labels. </p><ul><li><code>pred</code> is a list of predicted POS tags corresponding to the words of the <code>test_corpus</code>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The third word is:'</span>, prep[<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'Your prediction is:'</span>, pred[<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'Your corresponding label y is: '</span>, y[<span class="number">3</span>])</span><br></pre></td></tr></table></figure><pre><code>The third word is: temperatureYour prediction is: NNYour corresponding label y is:  temperature NN</code></pre><p><a name="ex-08"></a></p><h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p>Implement a function to compute the accuracy of the viterbi algorithmâ€™s POS tag predictions.</p><ul><li>To split y into the word and its tag you can use <code>y.split()</code>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        pred: a list of the predicted parts-of-speech </span></span><br><span class="line"><span class="string">        y: a list of lines where each word is separated by a '\t' (i.e. word \t tag)</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zip together the prediction and the labels</span></span><br><span class="line">    <span class="keyword">for</span> prediction, y <span class="keyword">in</span> zip(pred, y):</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># Split the label into the word and the POS tag</span></span><br><span class="line">        word_tag_tuple = y.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check that there is actually a word and a tag</span></span><br><span class="line">        <span class="comment"># no more and no less than 2 items</span></span><br><span class="line">        <span class="keyword">if</span> len(word_tag_tuple) &lt; <span class="number">2</span>: <span class="comment"># complete this line</span></span><br><span class="line">            <span class="keyword">continue</span> </span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># store the word and tag separately</span></span><br><span class="line">        word, tag = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> word_tag_tuple]</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(tag, prediction)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if the POS tag label matches the prediction</span></span><br><span class="line">        <span class="keyword">if</span> tag == prediction: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># count the number of times that the prediction</span></span><br><span class="line">            <span class="comment"># and label match</span></span><br><span class="line">            num_correct += <span class="number">1.0</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># keep track of the total number of examples (that have valid labels)</span></span><br><span class="line">        total += <span class="number">1.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> num_correct/total</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Accuracy of the Viterbi algorithm is <span class="subst">&#123;compute_accuracy(pred, y):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Accuracy of the Viterbi algorithm is 0.9531</code></pre><h5 id="Expected-Output-7"><a href="#Expected-Output-7" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of the Viterbi algorithm is <span class="number">0.9531</span></span><br></pre></td></tr></table></figure><p>Congratulations you were able to classify the parts-of-speech with 95% accuracy. </p><h3 id="Key-Points-and-overview"><a href="#Key-Points-and-overview" class="headerlink" title="Key Points and overview"></a>Key Points and overview</h3><p>In this assignment you learned about parts-of-speech tagging. </p><ul><li>In this assignment, you predicted POS tags by walking forward through a corpus and knowing the previous word.</li><li>There are other implementations that use bidirectional POS tagging.</li><li>Bidirectional POS tagging requires knowing the previous word and the next word in the corpus when predicting the current wordâ€™s POS tag.</li><li>Bidirectional POS tagging would tell you more about the POS instead of just knowing the previous word. </li><li>Since you have learned to implement the unidirectional approach, you have the foundation to implement other POS taggers used in industry.</li></ul><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul><li><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">â€œSpeech and Language Processingâ€, Dan Jurafsky and James H. Martin</a></li><li>We would like to thank Melanie Tosik for her help and inspiration</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Parts-of-Speech-Tagging-POS&quot;&gt;&lt;a href=&quot;#Assignment-2-Parts-of-Speech-Tagging-POS&quot; class=&quot;headerlink&quot; title=&quot;Assignment 2
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>ML-Interview-Computer-Vision</title>
    <link href="https://zhangruochi.com/ML-Interview-Computer-Vision/2020/05/29/"/>
    <id>https://zhangruochi.com/ML-Interview-Computer-Vision/2020/05/29/</id>
    <published>2020-05-28T23:41:05.000Z</published>
    <updated>2020-05-29T04:43:09.902Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>ä¸ºä»€ä¹ˆè¾“å…¥ç½‘ç»œå‰è¦å¯¹å›¾åƒåšå½’ä¸€åŒ–ï¼Ÿ</p><blockquote><ol><li>æŠŠä¸åŒçš„å›¾ç‰‡æ˜ å°„åˆ°åŒä¸€åæ ‡ç³»ï¼Œä½¿å…¶å…·æœ‰ç›¸åŒçš„å°ºåº¦åŠç›¸ä¼¼çš„ç‰¹å¾åˆ†å¸ƒã€‚</li><li>ä¸€å®šç¨‹åº¦ä¸Šæ¶ˆé™¤äº†è¿‡åº¦æ›å…‰ï¼Œè´¨é‡ä¸ä½³æˆ–è€…å™ªå£°ç­‰å„ç§åŽŸå› å¯¹æ¨¡åž‹æƒå€¼æ›´æ–°çš„å½±å“ã€‚</li><li>åŠ å¿«gradientæ›´æ–°çš„æ”¶æ•›é€Ÿåº¦ã€‚</li></ol></blockquote></li><li><p>æƒé‡åˆå§‹åŒ–æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ</p><blockquote><ol><li>Small random numbers (gaussian with zero mean and 1e-2 standard deviation):  Works okay for small networks, but problems with deeper networks.<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%"></center></li><li><strong>Xavier</strong>: åŸºæœ¬æ€æƒ³æ˜¯ä¿æŒè¾“å…¥å’Œè¾“å‡ºçš„æ–¹å·®ä¸€è‡´ï¼Œè¿™æ ·å°±é¿å…äº†æ‰€æœ‰è¾“å‡ºå€¼éƒ½è¶‹å‘äºŽ0. åˆå§‹åŒ–æ–¹å·®ä¸º:  <code>std = sqrt(node_in)</code>. å‚è€ƒ <a href="https://zhuanlan.zhihu.com/p/27919794" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27919794</a></li><li><strong>Kaiming</strong>: åœ¨ReLUç½‘ç»œä¸­ï¼Œå‡å®šæ¯ä¸€å±‚æœ‰ä¸€åŠçš„ç¥žç»å…ƒè¢«æ¿€æ´»ï¼Œå¦ä¸€åŠä¸º0ï¼Œæ‰€ä»¥ï¼Œè¦ä¿æŒvarianceä¸å˜ï¼Œåªéœ€è¦åœ¨Xavierçš„åŸºç¡€ä¸Šå†é™¤ä»¥2. åˆå§‹åŒ–æ–¹å·®ä¸º: <code>std = sqrt(node_in / 2)</code></li></ol></blockquote></li><li><p>è¯´è¯´ FCN çš„åŸºæœ¬æ€æƒ³.</p><blockquote><p>FCNå¯¹å›¾åƒè¿›è¡Œåƒç´ çº§çš„åˆ†ç±»ï¼Œä»Žè€Œè§£å†³äº†è¯­ä¹‰çº§åˆ«çš„å›¾åƒåˆ†å‰²é—®é¢˜ã€‚ä¸Žç»å…¸çš„CNNåœ¨å·ç§¯å±‚ä½¿ç”¨å…¨è¿žæŽ¥å±‚å¾—åˆ°å›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡è¿›è¡Œåˆ†ç±»ä¸åŒï¼ŒFCNå¯ä»¥æŽ¥å—ä»»æ„å°ºå¯¸çš„è¾“å…¥å›¾åƒï¼Œé‡‡ç”¨åå·ç§¯å±‚å¯¹æœ€åŽä¸€ä¸ªå·åŸºå±‚çš„ç‰¹å¾å›¾ï¼ˆfeature mapï¼‰è¿›è¡Œä¸Šé‡‡æ ·ï¼Œä½¿å®ƒæ¢å¤åˆ°è¾“å…¥å›¾åƒç›¸åŒçš„å°ºå¯¸ï¼Œä»Žè€Œå¯ä»¥å¯¹æ¯ä¸€ä¸ªåƒç´ éƒ½äº§ç”Ÿä¸€ä¸ªé¢„æµ‹ï¼ŒåŒæ—¶ä¿ç•™äº†åŽŸå§‹è¾“å…¥å›¾åƒä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œæœ€åŽåœ¨ä¸Šé‡‡æ ·çš„ç‰¹å¾å›¾è¿›è¡Œåƒç´ çš„åˆ†ç±»ã€‚</p></blockquote></li><li><p>ä»€ä¹ˆæ˜¯è½¬ç½®å·ç§¯?</p><blockquote><p>äº‹å®žä¸Šï¼Œå·ç§¯è¿ç®—è¿˜å¯ä»¥é€šè¿‡çŸ©é˜µä¹˜æ³•æ¥å®žçŽ°.å‡è®¾æˆ‘ä»¬å®šä¹‰é«˜å’Œå®½åˆ†åˆ«ä¸º4çš„è¾“å…¥Xï¼Œä»¥åŠé«˜å’Œå®½åˆ†åˆ«ä¸º3çš„å·ç§¯æ ¸K, å·ç§¯è¿ç®—è¾“å‡ºé«˜å’Œå®½åˆ†åˆ«ä¸º2.</p><script type="math/tex; mode=display">\frac{h(w) - k + 2p}{ s } + 1</script><p>æˆ‘ä»¬å°†å·ç§¯æ ¸Kæ”¹å†™æˆå«æœ‰å¤§é‡é›¶å…ƒç´ çš„ç¨€ç–çŸ©é˜µWï¼Œå³æƒé‡çŸ©é˜µã€‚æƒé‡çŸ©é˜µçš„å½¢çŠ¶ä¸º(4, 16)ï¼Œ å…¶ä¸­çš„éžé›¶å…ƒç´ æ¥è‡ªå·ç§¯æ ¸Kä¸­çš„å…ƒç´ ã€‚å°†è¾“å…¥Xé€è¡Œè¿žç»“ï¼Œå¾—åˆ°â»“åº¦ä¸º16çš„å‘é‡ã€‚ç„¶åŽå°†Wä¸Žå‘é‡åŒ–çš„XåšçŸ©é˜µä¹˜æ³•ï¼Œå¾—åˆ°â»“åº¦ä¸º4çš„å‘é‡ã€‚å¯¹å…¶å˜å½¢åŽï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°å’Œä¸Šé¢å·ç§¯è¿ç®—ç›¸åŒçš„ç»“ æžœã€‚å¯â»…ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªä¾‹å­ä¸­ä½¿ç”¨çŸ©é˜µä¹˜æ³•å®žçŽ°äº†å·ç§¯è¿ç®—ã€‚<br>çŽ°åœ¨æˆ‘ä»¬ä»Ž<strong>çŸ©é˜µä¹˜æ³•</strong>çš„â»†åº¦æ¥æè¿°å·ç§¯è¿ç®—ã€‚è®¾è¾“å…¥å‘é‡ä¸º$x$ï¼Œæƒé‡çŸ©é˜µä¸º$W$ï¼Œå·ç§¯çš„å‰å‘è®¡ç®— å‡½æ•°çš„å®žçŽ°å¯ä»¥çœ‹ä½œå°†å‡½æ•°è¾“å…¥ä¹˜ä»¥æƒé‡çŸ©é˜µï¼Œå¹¶è¾“å‡ºå‘é‡$y=Wx$.ä»¬çŸ¥é“ï¼Œåå‘ä¼ æ’­éœ€è¦ä¾æ®é“¾å¼æ³•åˆ™ã€‚ç”±äºŽ$\triangledown_x y = W^T$ï¼Œå·ç§¯çš„åå‘ä¼ æ’­å‡½æ•°çš„å®žçŽ°å¯ä»¥çœ‹ä½œå°†å‡½æ•°è¾“å…¥ä¹˜ä»¥è½¬ç½®åŽçš„æƒé‡çŸ©é˜µ$W^T$ã€‚è€Œè½¬ç½®å·ç§¯å±‚æ­£å¥½äº¤æ¢äº†å·ç§¯å±‚çš„å‰å‘è®¡ç®—å‡½æ•°ä¸Žåå‘ä¼ æ’­å‡½æ•°:è½¬ç½®å·ç§¯å±‚çš„è¿™ä¸¤ä¸ªå‡½æ•°å¯ä»¥çœ‹ä½œå°†å‡½æ•°è¾“å…¥å‘é‡åˆ†åˆ«ä¹˜ä»¥$W^T$ å’Œ $W$.<br>ä¸éš¾æƒ³è±¡ï¼Œè½¬ç½®å·ç§¯å±‚å¯ä»¥ç”¨æ¥äº¤æ¢å·ç§¯å±‚è¾“å…¥å’Œè¾“å‡ºçš„å½¢çŠ¶ã€‚è®©æˆ‘ä»¬ç»§ç»­ç”¨çŸ©é˜µä¹˜æ³•æè¿°å·ç§¯ã€‚è®¾æƒé‡çŸ©é˜µæ˜¯å½¢çŠ¶ä¸º4 Ã— 16çš„çŸ©é˜µï¼Œå¯¹äºŽâ»“åº¦ä¸º16çš„è¾“å…¥å‘é‡ï¼Œå·ç§¯å‰å‘è®¡ç®—è¾“å‡ºâ»“åº¦ä¸º4çš„å‘é‡ã€‚å‡å¦‚è¾“å…¥å‘é‡çš„â»“åº¦ä¸º4ï¼Œè½¬ç½®æƒé‡çŸ©é˜µçš„å½¢çŠ¶ä¸º16 Ã— 4ï¼Œé‚£ä¹ˆè½¬ç½®å·ç§¯å±‚å°†è¾“å‡ºâ»“åº¦ä¸º16çš„å‘é‡ã€‚åœ¨æ¨¡åž‹è®¾è®¡ä¸­ï¼Œè½¬ç½®å·ç§¯å±‚å¸¸ç”¨äºŽå°†è¾ƒå°çš„ç‰¹å¾å›¾å˜æ¢ä¸ºæ›´å¤§çš„ç‰¹å¾å›¾ã€‚åœ¨å…¨å·ç§¯ç½‘ç»œä¸­ï¼Œå½“è¾“å…¥æ˜¯é«˜å’Œå®½è¾ƒå°çš„ç‰¹å¾å›¾æ—¶ï¼Œè½¬ç½®å·ç§¯å±‚å¯ä»¥ç”¨æ¥å°†é«˜å’Œå®½æ”¾å¤§åˆ°è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%"></center></blockquote></li><li><p>ä»€ä¹ˆæ˜¯ç©ºæ´žå·ç§¯ï¼ˆDilated convolutionï¼‰?</p><blockquote><ol><li>ä»Žkernelï¼ˆå·ç§¯æ ¸ï¼‰è§’åº¦ï¼šç›¸å½“äºŽåœ¨æ ‡å‡†æ¦‚å¿µçš„kernelï¼ˆå·ç§¯æ ¸ï¼‰ä¸­ï¼Œç›¸é‚»ç‚¹ä¹‹é—´æ·»åŠ rate-1ä¸ª0ï¼Œç„¶åŽä½¿ç”¨æ‰©å¼ åŽçš„kernelï¼ˆå·ç§¯æ ¸ï¼‰ä¸ŽåŽŸå›¾è¿›è¡Œå·ç§¯ã€‚å¦‚ä¸‹å›¾rate=2ï¼Œç›¸å½“äºŽæ ‡å‡†çš„3<em>3å·ç§¯æ ¸å˜ä¸º5</em>5å·ç§¯æ ¸ï¼Œæ¯ä¸€è¡Œä¸­é—´æ·»åŠ 2-1ä¸ª0</li><li>ä»ŽåŽŸå›¾è§’åº¦ï¼šä½¿ç”¨æ ‡å‡†æ¦‚å¿µçš„kernelï¼ˆå·ç§¯æ ¸ï¼‰åœ¨åŽŸå›¾ä¸­æ¯éš”rate-1è¿›è¡Œåƒç´ ç‚¹å·ç§¯é‡‡æ ·ã€‚å¦‚ä¸‹å›¾rate=2ï¼Œåœ¨åŽŸå›¾ä¸­æ¯éš”rate-1è¿›è¡Œå·ç§¯ã€‚<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%"></center></li></ol></blockquote></li><li><p>è§£é‡Šä¸‹ Unet çš„ Architecture.</p><blockquote><p>Unet ä½¿ç”¨ encoder å’Œ decoder çš„æž¶æž„ï¼Œåœ¨encoderä¸‹é‡‡æ ·4æ¬¡ï¼Œä¸€å…±ä¸‹é‡‡æ ·16å€ã€‚å¯¹ç§°åœ°ï¼Œå…¶decoderä¹Ÿç›¸åº”ä¸Šé‡‡æ ·4æ¬¡ï¼Œå°†encoderå¾—åˆ°çš„é«˜çº§è¯­ä¹‰ç‰¹å¾å›¾æ¢å¤åˆ°åŽŸå›¾ç‰‡çš„åˆ†è¾¨çŽ‡ã€‚ç›¸æ¯”äºŽFCNå’ŒDeeplabç­‰ï¼ŒUNetå…±è¿›è¡Œäº†4æ¬¡ä¸Šé‡‡æ ·ï¼Œå¹¶åœ¨åŒä¸€ä¸ªstageä½¿ç”¨äº†skip connectionï¼Œè€Œä¸æ˜¯ç›´æŽ¥åœ¨é«˜çº§è¯­ä¹‰ç‰¹å¾ä¸Šè¿›è¡Œç›‘ç£å’Œlossåä¼ ï¼Œè¿™æ ·å°±ä¿è¯äº†æœ€åŽæ¢å¤å‡ºæ¥çš„ç‰¹å¾å›¾èžåˆäº†æ›´å¤šçš„low-levelçš„featureï¼Œä¹Ÿä½¿å¾—ä¸åŒscaleçš„featureå¾—åˆ°äº†çš„èžåˆã€‚</p></blockquote></li><li><p>è§£é‡Šä¸‹ FPN ç½‘ç»œ.</p><blockquote><p>ä¸€ä¸ªè‡ªåº•å‘ä¸Šçš„çº¿è·¯ï¼Œä¸€ä¸ªè‡ªé¡¶å‘ä¸‹çš„çº¿è·¯ï¼Œæ¨ªå‘è¿žæŽ¥ï¼ˆlateral connectionï¼‰ã€‚å›¾ä¸­æ”¾å¤§çš„åŒºåŸŸå°±æ˜¯æ¨ªå‘è¿žæŽ¥ï¼Œè¿™é‡Œ1 * 1çš„å·ç§¯æ ¸çš„ä¸»è¦ä½œç”¨æ˜¯å‡å°‘channelçš„æ•°é‡ï¼Œä¹Ÿå°±æ˜¯å‡å°‘äº†feature mapçš„ä¸ªæ•°ï¼Œå¹¶ä¸æ”¹å˜feature mapçš„å°ºå¯¸å¤§å°ã€‚</p><ol><li><strong>è‡ªåº•å‘ä¸Š</strong>å…¶å®žå°±æ˜¯ç½‘ç»œçš„å‰å‘è¿‡ç¨‹ã€‚åœ¨å‰å‘è¿‡ç¨‹ä¸­ï¼Œfeature mapçš„å¤§å°åœ¨ç»è¿‡æŸäº›å±‚åŽä¼šæ”¹å˜ï¼Œè€Œåœ¨ç»è¿‡å…¶ä»–ä¸€äº›å±‚çš„æ—¶å€™ä¸ä¼šæ”¹å˜ï¼Œä½œè€…å°†ä¸æ”¹å˜feature mapå¤§å°çš„å±‚å½’ä¸ºä¸€ä¸ªstageï¼Œå› æ­¤æ¯æ¬¡æŠ½å–çš„ç‰¹å¾éƒ½æ˜¯æ¯ä¸ªstageçš„æœ€åŽä¸€ä¸ªå±‚è¾“å‡ºï¼Œè¿™æ ·å°±èƒ½æž„æˆç‰¹å¾é‡‘å­—å¡”ã€‚ </li><li><strong>è‡ªé¡¶å‘ä¸‹</strong>çš„è¿‡ç¨‹é‡‡ç”¨ä¸Šé‡‡æ ·ï¼ˆupsamplingï¼‰è¿›è¡Œï¼Œè€Œæ¨ªå‘è¿žæŽ¥åˆ™æ˜¯å°†ä¸Šé‡‡æ ·çš„ç»“æžœå’Œè‡ªåº•å‘ä¸Šç”Ÿæˆçš„ç›¸åŒå¤§å°çš„feature mapè¿›è¡Œèžåˆï¼ˆmergeï¼‰ã€‚åœ¨èžåˆä¹‹åŽè¿˜ä¼šå†é‡‡ç”¨3 * 3çš„å·ç§¯æ ¸å¯¹æ¯ä¸ªèžåˆç»“æžœè¿›è¡Œå·ç§¯ï¼Œç›®çš„æ˜¯æ¶ˆé™¤ä¸Šé‡‡æ ·çš„æ··å æ•ˆåº”ï¼ˆaliasing effectï¼‰ã€‚å¹¶å‡è®¾ç”Ÿæˆçš„feature mapç»“æžœæ˜¯P2ï¼ŒP3ï¼ŒP4ï¼ŒP5ï¼Œå’ŒåŽŸæ¥è‡ªåº•å‘ä¸Šçš„å·ç§¯ç»“æžœC2ï¼ŒC3ï¼ŒC4ï¼ŒC5ä¸€ä¸€å¯¹åº”ã€‚<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%"></center></li></ol></blockquote></li><li><p>ä»€ä¹ˆæ˜¯Anchorsï¼Ÿ</p><blockquote><p>å‚è€ƒ <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a></p></blockquote></li><li><p>è§£é‡Š ROI Pooling å’Œ ROI Align çš„åŒºåˆ«.</p><blockquote><p>å¯¹äºŽä¸€ä¸ªregion proposalï¼Œé¦–å…ˆä»ŽåŽŸå›¾ç»è¿‡å…¨å·ç§¯ç½‘ç»œåˆ°ç‰¹å¾å›¾ï¼Œå¾—åˆ°çš„å€™é€‰æ¡†ä½ç½®å¯èƒ½å­˜åœ¨æµ®ç‚¹æ•°ï¼Œè¿›è¡Œå–æ•´æ“ä½œä»Žè€Œå‡ºçŽ°ç¬¬ä¸€æ¬¡é‡åŒ–ï¼›å…¶æ¬¡ï¼Œåœ¨ROI Poolingæ±‚å–æ¯ä¸ªå°ç½‘æ ¼çš„ä½ç½®æ—¶ä¹ŸåŒæ ·å­˜åœ¨æµ®ç‚¹æ•°å–æ•´çš„æƒ…å†µã€‚è¿™ä¸¤æ¬¡é‡åŒ–çš„ç»“æžœéƒ½ä½¿å¾—å€™é€‰æ¡†çš„ä½ç½®ä¼šå‡ºçŽ°åå·®ï¼Œåœ¨è®ºæ–‡é‡Œï¼Œä½œè€…æŠŠå®ƒæ€»ç»“ä¸ºâ€œä¸åŒ¹é…é—®é¢˜ï¼ˆmisalignmentï¼‰<br>ã€‚ä¸ºäº†è§£å†³ROI Poolingçš„ä¸Šè¿°ç¼ºç‚¹ï¼ŒROI Alignæå‡ºæ”¹è¿›çš„æ–¹æ³•ã€‚ROI Alignçš„æ€è·¯æ˜¯ï¼šå–æ¶ˆé‡åŒ–æ“ä½œï¼Œä½¿ç”¨åŒçº¿æ€§å†…æ’çš„æ–¹æ³•èŽ·å¾—åæ ‡ä¸ºæµ®ç‚¹æ•°çš„åƒç´ ç‚¹ä¸Šçš„å›¾åƒæ•°å€¼,ä»Žè€Œå°†æ•´ä¸ªç‰¹å¾èšé›†è¿‡ç¨‹è½¬åŒ–ä¸ºä¸€ä¸ªè¿žç»­çš„æ“ä½œ</p></blockquote></li><li><p>è¯·è§£é‡Šä¸‹two stage object detection çš„å‘å±•è„‰ç»œã€‚</p><blockquote><p>å‚è€ƒ <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a><br>è®²è§£ R-CNN, Fast R-CNN, Faster RCNN, Mask R-CNN çš„å‘å±•è½¨è¿¹ã€‚</p></blockquote></li><li><p>è¯·è§£é‡Šä¸‹one stage object detection çš„å‘å±•è„‰ç»œã€‚</p><blockquote><p>å‚è€ƒ <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a><br>è®²è§£YOLO1,YOLO2,YOLO3,YOLO4</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      FCN, Unet, Mask R-CNN, YOLO
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Natural-Language-Processing</title>
    <link href="https://zhangruochi.com/ML-Interview-Natural-Language-Processing/2020/05/28/"/>
    <id>https://zhangruochi.com/ML-Interview-Natural-Language-Processing/2020/05/28/</id>
    <published>2020-05-28T07:14:07.000Z</published>
    <updated>2020-05-28T22:50:59.616Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><ol><li><p>ç®€è¿° Transformer æå‡ºçš„èƒŒæ™¯ã€‚</p><blockquote><ol><li>seq2seq å¤„ç†é•¿æœŸä¾èµ–ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li><li>seq2seq æ¨¡åž‹æž¶æž„çš„é¡ºåºç‰¹æ€§é˜»æ­¢äº†å¹¶è¡ŒåŒ–ã€‚</li></ol></blockquote></li><li><p>ä»€ä¹ˆæ˜¯self-attentionæœºåˆ¶ï¼Ÿ è¯·ä¸¾ä¾‹æ¥è¯´æ˜Žã€‚</p><blockquote><p>å‚è€ƒ <a href="https://mp.weixin.qq.com/s/8Kic83oCoiKzAKe-dvRUvw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/8Kic83oCoiKzAKe-dvRUvw</a></p><ol><li>è¾“å…¥X, é€šè¿‡3ä¸ªçº¿æ€§è½¬æ¢æŠŠXè½¬æ¢ä¸ºQ,K,V. æœ‰äº†K,Q,V ä¸‰ä¸ªç‰¹å¾å‘é‡ï¼Œå°±å¯ä»¥åšattention. </li><li>ç”¨æ¯ä¸ªå•è¯çš„ query å‘é‡ä¸Žå…¶è‡ªèº«åŠå…¶ä»–å•è¯çš„ key å‘é‡åšdot product. å¾—åˆ°æƒé‡çš„åˆ†å¸ƒè¡¨ç¤ºã€‚<br>å‡è®¾æœ‰ä¸¤ä¸ªå•è¯ï¼ŒThinking, Machines. é€šè¿‡åµŒå…¥å˜æ¢ä¼š$X_1$,$X_2$ä¸¤ä¸ªå‘é‡ã€‚åˆ†åˆ«ä¸Ž$Wq$, $W_k$,$W_v$ä¸‰ä¸ªçŸ©é˜µæƒ³åšç‚¹ä¹˜å¾—åˆ°ï¼Œ{q1,q2},{k1,k2},{v1,v2} 6ä¸ªå‘é‡ã€‚ ç„¶åŽ{q1,k1} åšç‚¹ä¹˜å¾—åˆ°å¾—åˆ†$score_1$, {q1,k2}åšç‚¹ä¹˜å¾—åˆ°$score_2$ã€‚å¯¹ä¸Šè¿° socre è¿›è¡Œè§„èŒƒåŒ–ï¼Œç„¶åŽ softmax å¾—åˆ°æƒé‡ $[w_1,w_2]$. </li><li>å¯¹ valuex å‘é‡æ±‚åŠ æƒå¹³å‡ã€‚ç”¨æƒé‡å‘é‡$[w_1,w_2]$ ä¹˜ä»¥$[v1,v2]$å€¼å¾—åˆ°ä¸€ä¸ªåŠ æƒåŽçš„å€¼. <script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{Q,K^T}{\sqrt{d_k}})V</script><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></li></ol></blockquote></li><li><p>encoder ä¸­çš„self-attention ä¸Ž decoder ä¸­ masked self-attention æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</p><blockquote><p>encoder ä¸­ï¼Œ$QK^T$ ä¼šç»„æˆä¸€ä¸ªword2wordçš„attention map. æ˜¯ä¸€ä¸ªæ–¹é˜µ. æ¯”å¦‚è¯´ä½ çš„è¾“å…¥æ˜¯ä¸€å¥è¯ â€œi have a dreamâ€ æ€»å…±4ä¸ªå•è¯ï¼Œ è¿™é‡Œå°±ä¼šå½¢æˆä¸€å¼ 4x4çš„æ³¨æ„åŠ›æœºåˆ¶çš„å›¾.</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center>è¿™é‡Œçš„maskedå°±æ˜¯è¦åœ¨åšlanguage modellingï¼ˆæˆ–è€…åƒç¿»è¯‘ï¼‰çš„æ—¶å€™ï¼Œä¸ç»™æ¨¡åž‹çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ã€‚æ­¤æ—¶çš„ attention map æ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µã€‚<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%"></center></blockquote></li><li><p>ä»€ä¹ˆæ˜¯ Multi-Head Attentionï¼Ÿ</p><blockquote><p>Multi-Head Attentionå°±æ˜¯æŠŠself-attention åš N æ¬¡ï¼Œç„¶åŽæŠŠ N ä¸ª heads concatenate åœ¨ä¸€èµ·ã€‚æœ€åŽå†åšä¸€ä¸ªçº¿æ€§å˜æ¢ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="50%" height="50%"></center></blockquote></li><li><p>Batch Norm å’Œ Layer Norm çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>BNå¹¶ä¸é€‚ç”¨äºŽRNNç­‰åŠ¨æ€ç½‘ç»œå’Œbatchsizeè¾ƒå°çš„æ—¶å€™æ•ˆæžœä¸å¥½ã€‚Layer Normalizationçš„æå‡ºæœ‰æ•ˆçš„è§£å†³BNçš„è¿™ä¸¤ä¸ªé—®é¢˜ã€‚LNå’ŒBNä¸åŒç‚¹æ˜¯å½’ä¸€åŒ–çš„ç»´åº¦æ˜¯äº’ç›¸åž‚ç›´çš„. Nè¡¨ç¤ºæ ·æœ¬è½´ï¼ŒC è¡¨ç¤ºé€šé“è½´ï¼ŒF æ˜¯æ¯ä¸ªé€šé“çš„ç‰¹å¾æ•°é‡ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="50%" height="50%"></center></blockquote></li><li><p>ä»€ä¹ˆæ˜¯transformer ä¸­çš„ Encoder-Decoder Attentionï¼Ÿ</p><blockquote><p>åœ¨decoderä¸­ï¼ŒTransformer blockæ¯”ç¼–ç å™¨ä¸­å¤šäº†ä¸ªencoder-cecoder attentionã€‚åœ¨encoder-decoder attentionä¸­ï¼ŒQ æ¥è‡ªäºŽ decoder çš„ä¸Šä¸€ä¸ªè¾“å‡ºï¼ŒK,V æ¥è‡ªäºŽ encoder çš„è¾“å‡º.</p></blockquote></li><li><p>ä»€ä¹ˆæ˜¯Positional Encodingï¼Ÿ</p><blockquote><p>å‚è€ƒ <a href="https://zhuanlan.zhihu.com/p/95079337" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/95079337</a><br>TransformeræŠ›å¼ƒäº†RNNï¼Œè€ŒRNNæœ€å¤§çš„ä¼˜ç‚¹å°±æ˜¯åœ¨æ—¶é—´åºåˆ—ä¸Šå¯¹æ•°æ®çš„æŠ½è±¡ï¼Œæ‰€ä»¥æ–‡ç« ä¸­ä½œè€…æå‡ºä¸¤ç§Positional Encodingçš„æ–¹æ³•ï¼Œå°†encodingåŽçš„æ•°æ®ä¸Žembeddingæ•°æ®æ±‚å’Œï¼ŒåŠ å…¥äº†ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚</p></blockquote></li><li><p>ç®€è¿° Transformer çš„æž¶æž„ã€‚</p><blockquote><p>Transformer åŒ…å« Encoder Part and Decoder Part. Encoder Part æ¯”è¾ƒé‡è¦çš„æ˜¯ Self-Attention. Decoder Part æ¯”è¾ƒé‡è¦çš„æ˜¯ Masked Multi-Head Attentionï¼ŒEncoder-Decoder Attention. åŒæ—¶ï¼Œencoder å’Œ decoder éƒ½åŒ…å« Feed Forward, Residuals ä»¥åŠ Layer Norm. </p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="80%" height="80%"></center></blockquote></li></ol><h2 id="ELMo-OpenAI-GPT-BERT"><a href="#ELMo-OpenAI-GPT-BERT" class="headerlink" title="ELMo,OpenAI GPT,BERT"></a>ELMo,OpenAI GPT,BERT</h2><blockquote><p>å‚è€ƒ <a href="https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/">https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/</a></p></blockquote><ol><li><p>è§£é‡Šä¸‹ ELMO çš„æ€æƒ³.</p><blockquote><p>ELMO çš„æ„æ€æ˜¯ embedding from language model. word2vec æœ€å¤§çš„é—®é¢˜æ˜¯åœ¨è®­ç»ƒå¥½åŽï¼Œè¯å‘é‡åœ¨ä»»ä½•contextçš„æƒ…å†µä¸‹éƒ½æ˜¯ä¸å˜çš„ã€‚è€Œå®žé™…ä¸Šæˆ‘ä»¬çŸ¥é“å•è¯çš„æ„æ€éšç€è¯­å¢ƒçš„å˜åŒ–è€Œå˜åŒ–ã€‚ELMO è®­ç»ƒäº†ä¸€ä¸ªåŒå‘çš„ LSTM, ç„¶åŽå°† hidden layer concate åœ¨ä¸€èµ·ã€‚å¯¹äºŽLå±‚çš„åŒå‘lstmè¯­è¨€æ¨¡åž‹ï¼Œæ¯ä¸ªå•è¯ä¸€å…±æœ‰2L+1ä¸ªè¡¨å¾ï¼ˆrepresentationsï¼‰.æœ€åŽæ ¹æ®å…·ä½“çš„ä»»åŠ¡ï¼Œå°†2L+1ä¸ªè¡¨å¾åŠ æƒå¹³å‡åœ¨ä¸€èµ·ã€‚</p></blockquote></li><li><p>è§£é‡Šä¸‹OpenAI Transformer.</p><blockquote><p>OpenAI Pre-training a Transformer Decoder for Language Modeling.</p></blockquote></li><li><p>Bert æ˜¯æ€Žæ ·å®žçŽ° mask çš„ï¼Ÿ</p><blockquote><ul><li>MLMï¼šå°†å®Œæ•´å¥å­ä¸­çš„éƒ¨åˆ†å­—maskï¼Œé¢„æµ‹è¯¥maskè¯</li><li>NSPï¼šä¸ºæ¯ä¸ªè®­ç»ƒå‰çš„ä¾‹å­é€‰æ‹©å¥å­ A å’Œ B æ—¶ï¼Œ50% çš„æƒ…å†µä¸‹ B æ˜¯çœŸçš„åœ¨ A åŽé¢çš„ä¸‹ä¸€ä¸ªå¥å­ï¼Œ 50% çš„æƒ…å†µä¸‹æ˜¯æ¥è‡ªè¯­æ–™åº“çš„éšæœºå¥å­ï¼Œè¿›è¡ŒäºŒåˆ†é¢„æµ‹æ˜¯å¦ä¸ºçœŸå®žä¸‹ä¸€å¥</li></ul></blockquote></li><li><p>åœ¨æ•°æ®ä¸­éšæœºmask15%çš„tokenï¼Œå…¶ä¸­80%è¢«æ¢ä½[mask]ï¼Œ10%ä¸å˜ã€10%éšæœºæ›¿æ¢å…¶ä»–å•è¯ï¼Œè¿™æ ·åšçš„åŽŸå› æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>Bertéšæœºmaskè¯­æ–™ä¸­15%çš„tokenï¼Œç„¶åŽé¢„æµ‹masked tokenï¼Œé‚£ä¹ˆmasked token ä½ç½®è¾“å‡ºçš„final hidden vectorså–‚ç»™softmaxç½‘ç»œå³å¯å¾—åˆ°maskedtokençš„é¢„æµ‹ç»“æžœã€‚è¿™æ ·æ“ä½œå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œfine-tuningçš„æ—¶å€™æ²¡æœ‰[MASK]tokenï¼Œå› æ­¤å­˜åœ¨pre-trainingå’Œfine-tuningä¹‹é—´çš„mismatchï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé‡‡ç”¨äº†ä¸‹é¢çš„ç­–ç•¥ï¼š</p><ol><li>80%çš„æ—¶é—´ä¸­ï¼šå°†é€‰ä¸­çš„è¯ç”¨[MASK]tokenæ¥ä»£æ›¿ï¼Œä¾‹å¦‚<br>my dog is hairy â†’ my dog is [MASK]</li><li>10%çš„æ—¶é—´ä¸­ï¼šå°†é€‰ä¸­çš„è¯ç”¨ä»»æ„çš„è¯æ¥è¿›è¡Œä»£æ›¿ï¼Œä¾‹å¦‚<br>my dog is hairy â†’ my dog is apple</li><li>10%çš„æ—¶é—´ä¸­ï¼šé€‰ä¸­çš„è¯ä¸å‘ç”Ÿå˜åŒ–ï¼Œä¾‹å¦‚<br>my dog is hairy â†’ my dog is hairy</li></ol></blockquote></li><li><p>ä¸ºä»€ä¹ˆBERTæœ‰3ä¸ªåµŒå…¥å±‚ï¼Œå®ƒä»¬éƒ½æ˜¯å¦‚ä½•å®žçŽ°çš„ï¼Ÿ</p><blockquote><ul><li>input_idæ˜¯è¯­ä¹‰è¡¨è¾¾ï¼Œå’Œä¼ ç»Ÿçš„w2vä¸€æ ·ï¼Œæ–¹æ³•ä¹Ÿä¸€æ ·çš„lookup</li><li>segment_idæ˜¯è¾…åŠ©BERTåŒºåˆ«å¥å­å¯¹ä¸­çš„ä¸¤ä¸ªå¥å­çš„å‘é‡è¡¨ç¤ºï¼Œä»Ž[1,embedding_size]é‡Œé¢lookup</li><li>position_idæ˜¯ä¸ºäº†èŽ·å–æ–‡æœ¬å¤©ç”Ÿçš„æœ‰åºä¿¡æ¯ï¼Œå¦åˆ™å°±å’Œä¼ ç»Ÿè¯è¢‹æ¨¡åž‹ä¸€æ ·äº†ï¼Œä»Ž[511,embedding_size]é‡Œé¢lookup</li></ul></blockquote></li><li><p>Bertçš„æŸå¤±å‡½æ•°ï¼Ÿ</p><blockquote><ol><li>MLM:åœ¨ encoder çš„è¾“å‡ºä¸Šæ·»åŠ ä¸€ä¸ªåˆ†ç±»å±‚,ç”¨åµŒå…¥çŸ©é˜µä¹˜ä»¥è¾“å‡ºå‘é‡ï¼Œå°†å…¶è½¬æ¢ä¸ºè¯æ±‡çš„ç»´åº¦,ç”¨ softmax è®¡ç®—maskä¸­æ¯ä¸ªå•è¯çš„æ¦‚çŽ‡</li><li>NSP:ç”¨ä¸€ä¸ªç®€å•çš„åˆ†ç±»å±‚å°† [CLS] æ ‡è®°çš„è¾“å‡ºå˜æ¢ä¸º 2Ã—1 å½¢çŠ¶çš„å‘é‡,ç”¨ softmax è®¡ç®— IsNextSequence çš„æ¦‚çŽ‡</li><li>MLM+NSPå³ä¸ºæœ€åŽçš„æŸå¤±</li></ol></blockquote></li><li><p>elmoã€GPTã€bertä¸‰è€…ä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</p><blockquote><ol><li>ç‰¹å¾æå–å™¨ï¼šelmoé‡‡ç”¨LSTMè¿›è¡Œæå–ï¼ŒGPTå’Œbertåˆ™é‡‡ç”¨Transformerè¿›è¡Œæå–ã€‚å¾ˆå¤šä»»åŠ¡è¡¨æ˜ŽTransformerç‰¹å¾æå–èƒ½åŠ›å¼ºäºŽLSTMï¼Œelmoé‡‡ç”¨1å±‚é™æ€å‘é‡+2å±‚LSTMï¼Œå¤šå±‚æå–èƒ½åŠ›æœ‰é™ï¼Œè€ŒGPTå’Œbertä¸­çš„Transformerå¯é‡‡ç”¨å¤šå±‚ï¼Œå¹¶è¡Œè®¡ç®—èƒ½åŠ›å¼ºã€‚</li><li>å•/åŒå‘è¯­è¨€æ¨¡åž‹ï¼šGPTé‡‡ç”¨å•å‘è¯­è¨€æ¨¡åž‹ï¼Œelmoå’Œberté‡‡ç”¨åŒå‘è¯­è¨€æ¨¡åž‹ã€‚ä½†æ˜¯elmoå®žé™…ä¸Šæ˜¯ä¸¤ä¸ªå•å‘è¯­è¨€æ¨¡åž‹ï¼ˆæ–¹å‘ç›¸åï¼‰çš„æ‹¼æŽ¥ï¼Œè¿™ç§èžåˆç‰¹å¾çš„èƒ½åŠ›æ¯”bertä¸€ä½“åŒ–èžåˆç‰¹å¾æ–¹å¼å¼±ã€‚</li><li>GPTå’Œbertéƒ½é‡‡ç”¨Transformerï¼ŒTransformeræ˜¯encoder-decoderç»“æž„ï¼ŒGPTçš„å•å‘è¯­è¨€æ¨¡åž‹é‡‡ç”¨decoderéƒ¨åˆ†ï¼Œdecoderçš„éƒ¨åˆ†è§åˆ°çš„éƒ½æ˜¯ä¸å®Œæ•´çš„å¥å­ï¼›bertçš„åŒå‘è¯­è¨€æ¨¡åž‹åˆ™é‡‡ç”¨encoderéƒ¨åˆ†ï¼Œé‡‡ç”¨äº†å®Œæ•´å¥å­</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      Natural language processing
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Ensemble</title>
    <link href="https://zhangruochi.com/ML-Interview-Ensemble/2020/05/28/"/>
    <id>https://zhangruochi.com/ML-Interview-Ensemble/2020/05/28/</id>
    <published>2020-05-28T02:07:46.000Z</published>
    <updated>2020-05-28T22:45:39.171Z</updated>
    
    <content type="html"><![CDATA[<h2 id="é›†æˆå­¦ä¹ "><a href="#é›†æˆå­¦ä¹ " class="headerlink" title="é›†æˆå­¦ä¹ "></a>é›†æˆå­¦ä¹ </h2><ol><li><p>é›†æˆå­¦ä¹ åˆ†å“ªå‡ ç§ï¼Œä»–ä»¬æœ‰ä½•å¼‚åŒï¼Ÿ</p><blockquote><ol><li>Boosting: é‡‡ç”¨ä¸²è¡Œçš„æ–¹å¼ï¼Œå„ä¸ªåŸºç¡€åˆ†ç±»å™¨ä¹‹é—´æœ‰ä¾èµ–ã€‚å®ƒçš„åŸºæœ¬æ€è·¯æ˜¯å°†åŸºåˆ†ç±»å™¨å±‚å±‚å åŠ ï¼Œæ¯ä¸€å±‚åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œå¯¹å‰ä¸€å±‚åŸºåˆ†ç±»å™¨åˆ†é”™çš„æ ·æœ¬ï¼Œç»™äºˆæ›´é«˜çš„æƒé‡ã€‚æµ‹è¯•æ—¶ï¼Œæ ¹æ®å„å±‚åˆ†ç±»å™¨çš„ç»“æžœçš„åŠ æƒå¾—åˆ°æœ€ç»ˆçš„ç»“æžœã€‚</li><li>Bagging: é‡‡ç”¨å¹¶è¡Œçš„æ–¹å¼ï¼Œå„ä¸ªåŸºåˆ†ç±»å™¨ä¹‹é—´æ— ä¾èµ–ã€‚å…¶ä¸­æ¯”è¾ƒè‘—åçš„ç®—æ³•ä¹‹ä¸€æ˜¯åŸºäºŽå†³ç­–æ ‘çš„éšæœºæ£®æž—ã€‚ä¸ºäº†è®©åŠåˆ†ç±»å™¨ä¹‹é—´äº’ç›¸ç‹¬ç«‹ï¼Œå°†è®­ç»ƒé›†åˆ†æˆè‹¥å¹²å­é›†ï¼ˆå½“è®­ç»ƒæ ·æœ¬è¾ƒå°‘æ—¶ï¼Œå­é›†ä¹‹é—´æœ‰é‡å ï¼‰ã€‚åœ¨æœ€ç»ˆçš„å†³ç­–é˜¶æ®µï¼Œæ¯ä¸ªä¸ªä½“å•ç‹¬åšå‡ºåˆ¤æ–­ï¼Œç„¶åŽé€šè¿‡æŠ•ç¥¨çš„æ–¹å¼åšå‡ºæœ€åŽçš„é›†ä½“å†³ç­–ã€‚</li></ol></blockquote></li><li><p>é›†æˆå­¦ä¹ æ˜¯å¦‚ä½•æé«˜åŸºåˆ†ç±»å™¨çš„æ€§èƒ½çš„ï¼Ÿ</p><blockquote><p>åŸºåˆ†ç±»å™¨çš„è¯¯å·®ï¼Œæ˜¯æ–¹å·®å’Œåå·®ä¸¤ç§é”™è¯¯ä¹‹å’Œã€‚åå·®æºäºŽunderfittingï¼Œæ–¹å·®æºäºŽoverfitting. Boosting æ–¹æ³•é€šè¿‡é€æ­¥èšç„¦äºŽåŸºåˆ†ç±»å™¨åˆ†é”™çš„æ ·æœ¬ï¼Œå‡å°‘é›†æˆåˆ†ç±»å™¨çš„åå·®ã€‚Baggingé€šè¿‡å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œå¤šæ¬¡é‡‡æ ·ï¼Œåˆ†åˆ«è®­ç»ƒå¤šä¸ªä¸åŒçš„æ¨¡åž‹ï¼Œç„¶åŽåšç»¼åˆï¼Œæ¥å‡å°‘åˆ†ç±»å™¨çš„æ–¹å·®ã€‚</p></blockquote></li><li><p>é›†æˆå­¦ä¹ çš„æœ‰å“ªäº›åŸºæœ¬æ­¥éª¤ï¼Ÿè¯·ä»¥ Adaboosting æ¥ä¸¾ä¾‹ã€‚</p><blockquote><ol><li>initialize equal weights for all samples<script type="math/tex; mode=display">\alpha_{i} = \frac{1}{N}</script></li><li>Repeat t = 1,â€¦,T<ul><li>learn $f_{t}(x)$ with data weights $\alpha_{i}$</li><li>compute weighted error<script type="math/tex; mode=display">weighted_{error_{t}} = \sum_{i=1}^{m}\alpha_{i}I(y_{i} \neq f_{t}(x_{i}))</script></li><li>compute coefficient <script type="math/tex; mode=display">\hat{w_{t}} = \frac{1}{2}\ln(\frac{1 - weighted_{error_{t}} }{weighted_{error_{t}}})</script><ul><li>$\hat{w_{t}}$ is higher when weighted_error is larger</li></ul></li><li>recomputed weights $\alpha_{i}$<script type="math/tex; mode=display">\alpha_{i} =      \begin{equation}    \left\{     \begin{array}{lr}      \alpha_{i}e^{-\hat{w_{t}}} \quad if \ f_t(x_i) = y_i & \\       \alpha_{i}e^{\hat{w_{t}}}  \quad if \ f_t(x_i) \neq y_i  &     \end{array}     \right.     \end{equation}</script></li><li>Normalize weights $\alpha_{i}$<ul><li>if $x_{i}$ often mistake, weight $\alpha_{i}$ gets very large</li><li>if $x_{i}$ often correct, weight $\alpha_{i}$ gets very small<script type="math/tex; mode=display">\alpha_{i} = \frac{\alpha_{i}}{\sum_{i}^{m}\alpha_{i}}</script></li></ul></li></ul></li><li>In the testing time, the final prediction is:<script type="math/tex; mode=display">\hat{y_{t}} = sign( \sum_1^T \hat{w_{t}} f_t(x) )</script></li></ol></blockquote></li><li><p>å¸¸ç”¨çš„åŸºåˆ†ç±»å™¨æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>å¸¸ç”¨å†³ç­–æ ‘ä½œä¸ºåŸºåˆ†ç±»å™¨ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ æ–¹é¢çš„åŽŸå› </p><ol><li>å†³ç­–æ ‘çš„è¡¨è¾¾èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡è°ƒèŠ‚æ ‘çš„å±‚æ•°æ¥æ–¹ä¾¿å®žçŽ°ã€‚</li><li>æ•°æ®æ ·æœ¬çš„æ‰°åŠ¨å¯¹äºŽå†³ç­–æ ‘çš„å½±å“è¾ƒå¤§ï¼Œå› æ­¤ä¸åŒå­æ ·æœ¬é›†åˆç”Ÿæˆçš„å†³ç­–æ ‘éšæœºæ€§è¾ƒå¤§ï¼Œè¿™æ ·â€ä¸ç¨³å®šçš„å­¦ä¹ å™¨â€æ›´é€‚åˆä½œä¸ºåŸºåˆ†ç±»å™¨ã€‚</li><li>å†³ç­–æ ‘åœ¨èŠ‚ç‚¹åˆ†è£‚æ—¶ï¼Œéšæœºåœ°é€‰æ‹©ä¸€ä¸ªç‰¹å¾å­é›†ï¼Œä»Žä¸­æ‰¾å‡ºæœ€ä¼˜åˆ†è£‚ç‰¹å¾ï¼Œå¾ˆå¥½åœ°å¼•å…¥äº†éšæœºæ€§ã€‚<br>ç¥žç»ç½‘ç»œæ¨¡åž‹ä¹Ÿé€‚åˆä½œä¸ºåŸºåˆ†ç±»å™¨ï¼Œå› ä¸ºç¥žç»ç½‘ç»œä¹Ÿæ˜¯æ¯”è¾ƒâ€ä¸ç¨³å®šçš„â€ã€‚è¿˜å¯ä»¥é€šè¿‡è°ƒæ•´ç¥žç»å…ƒçš„æ•°é‡ï¼Œè¿žæŽ¥æ–¹å¼ï¼Œç½‘ç»œå±‚æ•°ï¼Œåˆå§‹æƒé‡å¼•å…¥éšæœºæ€§ã€‚</li></ol></blockquote></li><li><p>åœ¨éšæœºæ£®æž—ä¸­ï¼Œå¯å¦ä½¿ç”¨çº¿æ€§åˆ†ç±»å™¨æˆ–è€…K-è¿‘é‚»ä½œä¸ºåŸºåˆ†ç±»å™¨ï¼Ÿ</p><blockquote><p>éšæœºæ£®æž—æ˜¯å±žäºŽ Baggingç±»çš„é›†æˆå­¦ä¹ ã€‚Baggingçš„ä¸»è¦å¥½å¤„æ˜¯é›†æˆåŽçš„åˆ†ç±»å™¨çš„æ–¹å·®ï¼Œæ¯”åŸºåˆ†ç±»å™¨çš„åå·®å°ã€‚Bagging æ‰€é‡‡ç”¨çš„åŸºåˆ†ç±»å™¨ï¼Œæœ€å¥½æ˜¯æœ¬èº«ä¸ç¨³å®šçš„åˆ†ç±»å™¨ï¼Œè¿™æ ·æ‰èƒ½èŽ·å¾—æ›´ä¼˜çš„æ€§èƒ½ã€‚çº¿æ€§åˆ†ç±»å™¨æˆ–è€…K-è¿‘é‚»éƒ½æ˜¯è¾ƒä¸ºç¨³å®šçš„åˆ†ç±»å™¨ï¼Œæœ¬èº«åå·®å°±ä¸å¤§ï¼Œæ‰€ä»¥ä¸é€‚åˆã€‚</p></blockquote></li><li><p>éšæœºæ£®æž—çš„éšæœºæ€§ä½“çŽ°åœ¨å“ªé‡Œï¼Ÿ</p><blockquote><ol><li>æ¯æ£µæ ‘çš„æ ·æœ¬æ˜¯éšæœºæŠ½æ ·å¾—åˆ°çš„</li><li>æ¯è¯¾æ•°ç”Ÿé•¿æ—¶åˆ†è£‚çš„å±žæ€§é›†åˆä¸åŒ</li></ol></blockquote></li><li><p>ä»€ä¹ˆæ˜¯bias ä»€ä¹ˆæ˜¯ variance?</p><blockquote><p>Bias æ˜¯ underfitting é€ æˆçš„ã€‚Biasæ˜¯æŒ‡ç”±æ‰€æœ‰é‡‡æ ·å¾—åˆ°çš„å¤§å°ä¸ºmçš„è®­ç»ƒæ•°æ®è®­ç»ƒå‡ºçš„æ‰€æœ‰æ¨¡åž‹çš„è¾“å‡ºçš„å¹³å‡å€¼å’ŒçœŸå®žæ¨¡åž‹è¾“å‡ºä¹‹é—´çš„åå·®ã€‚<br>Variance æ˜¯ overfitting é€ æˆçš„ã€‚Varianceæ˜¯ä¹‹ç”±æ‰€æœ‰é‡‡æ ·å¾—åˆ°çš„å¤§å°ä¸ºmçš„è®­ç»ƒæ•°æ®é›†è®­ç»ƒå‡ºçš„æ‰€æœ‰æ¨¡åž‹çš„è¾“å‡ºæ–¹å·®ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></blockquote></li><li><p>GBDTçš„åŸºæœ¬åŽŸç†æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>Gradient Boosting æ˜¯ Boosting ä¸­çš„ä¸€å¤§ç±»ç®—æ³•ï¼Œå…¶åŸºæœ¬æ€æƒ³æ˜¯æ ¹æ®å½“å‰æ¨¡åž‹æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦ä¿¡æ¯æ¥è®­ç»ƒæ–°åŠ å…¥çš„å¼±åˆ†ç±»å™¨ï¼Œç„¶åŽå°†è®­ç»ƒå¥½çš„çš„å¼±åˆ†ç±»å™¨ä»¥ç´¯åŠ çš„å½¢å¼ç»“åˆåˆ°çŽ°æœ‰æ¨¡åž‹ä¸­ã€‚</p></blockquote></li><li><p>GBDT å’Œ Adatboost çš„å…³ç³»å’ŒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>å’ŒAdaBoostä¸€æ ·ï¼ŒGradient Boostingä¹Ÿæ˜¯é‡å¤é€‰æ‹©ä¸€ä¸ªè¡¨çŽ°ä¸€èˆ¬çš„æ¨¡åž‹å¹¶ä¸”æ¯æ¬¡åŸºäºŽå…ˆå‰æ¨¡åž‹çš„è¡¨çŽ°è¿›è¡Œè°ƒæ•´ã€‚ä¸åŒçš„æ˜¯ï¼ŒAdaBoostæ˜¯é€šè¿‡æå‡é”™åˆ†æ•°æ®ç‚¹çš„æƒé‡æ¥å®šä½æ¨¡åž‹çš„ä¸è¶³è€ŒGradient Boostingæ˜¯é€šè¿‡ç®—æ¢¯åº¦ï¼ˆgradientï¼‰æ¥å®šä½æ¨¡åž‹çš„ä¸è¶³ã€‚å› æ­¤ç›¸æ¯”AdaBoost, Gradient Boostingå¯ä»¥ä½¿ç”¨æ›´å¤šç§ç±»çš„ç›®æ ‡å‡½æ•°,è€Œå½“ç›®æ ‡å‡½æ•°æ˜¯å‡æ–¹è¯¯å·®æ—¶ï¼Œè®¡ç®—æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦å€¼åœ¨å½“å‰æ¨¡åž‹çš„å€¼å³ä¸ºæ®‹å·®ã€‚å½“ç›®æ ‡å‡½æ•°ä¸æ˜¯ square loss æ—¶æ®‹å·®å¹¶ä¸ä¸€å®šç­‰äºŽè´Ÿæ¢¯åº¦ã€‚Adaboost æ˜¯ GBDT çš„ä¸€ä¸ªç‰¹ä¾‹ï¼ŒGBDT æ˜¯ Adaboostçš„æŽ¨å¹¿ã€‚</p><script type="math/tex; mode=display">\left\{ \begin{aligned}& L(y_i, F(x_i)) = \frac{1}{2} * (y_i - F(x_i))^2 \\& - \frac{\partial(y_i, F(x_i))}{\partial F(x_i)} = (y_i - F(x_i))\end{aligned}\right.</script></blockquote></li><li><p>GBDT ä¸ºä»€ä¹ˆè¦æ‹Ÿåˆä¸Šä¸€æ¬¡æ¨¡åž‹çš„è´Ÿæ¢¯åº¦ï¼Ÿ</p><blockquote><p>æˆ‘ä»¬è¦æ‹ŸåˆæŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦ï¼Œå¯ä»¥çœ‹åšæ‹Ÿåˆä¸€ä¸ªæ–¹å‘ä¸ºè´Ÿæ¢¯åº¦æ–¹å‘ï¼Œæ­¥é•¿ä¸ºå•ä½é•¿åº¦çš„å€¼ï¼Œæ‰€ä»¥æ‹Ÿåˆçš„è¿‡ç¨‹ç›¸å½“äºŽæˆ‘ä»¬æ²¿ç€è´Ÿæ¢¯åº¦æ–¹å‘èµ°äº†ä¸€ä¸ªæ­¥é•¿ï¼Œå…·ä½“èµ°å¤šå°‘æ­¥ï¼ˆå¤šå°‘æ­¥å¯ä»¥ç†è§£ä¸ºè®­ç»ƒå¤šå°‘ä¸ªå†³ç­–æ ‘æ¥æ‹Ÿåˆè¯¥åˆ†ç±»å™¨ï¼Œä½¿å¾—æŸå¤±å‡½æ•°æœ€ä½Žï¼‰è¾¾åˆ°ç»ˆæ­¢çš„æ¡ä»¶ï¼Œå³èµ°åˆ°æœ€ä¼˜ç‚¹çš„é™„è¿‘ã€‚</p></blockquote></li><li><p>æ¢¯åº¦æå‡å’Œæ¢¯åº¦ä¸‹é™çš„åŒºåˆ«å’Œè”ç³»ï¼Ÿ</p><blockquote><p>ä¸¤è€…éƒ½æ˜¯åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼Œåˆ©ç”¨æŸå¤±å‡½æ•°ç›¸å¯¹äºŽæ¨¡åž‹çš„è´Ÿæ¢¯åº¦æ–¹å‘ä¿¡æ¯æ¥å¯¹å½“å‰æ¨¡åž‹è¿›è¡Œæ›´æ–°ã€‚åœ¨æ¢¯åº¦ä¸‹é™ä¸­ï¼Œæ¨¡åž‹æ˜¯ä»¥å‚æ•°åŒ–å½¢å¼è¡¨ç¤ºï¼Œä»Žè€Œæ¨¡åž‹çš„æ›´æ–°ç­‰ä»·äºŽå‚æ•°çš„æ›´æ–°ã€‚åœ¨æ¢¯åº¦æå‡ä¸­ï¼Œæ¨¡åž‹å¹¶ä¸éœ€è¦è¿›è¡Œå‚æ•°åŒ–è¡¨ç¤ºï¼Œè€Œæ˜¯ç›´æŽ¥å®šä¹‰åœ¨å‡½æ•°ç©ºé—´ä¸­ã€‚</p><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center></blockquote></li><li><p>GBDTå’Œå±€é™æ€§æœ‰å“ªäº›ï¼Ÿ</p><blockquote><p>GBDT åœ¨é«˜çº¬åº¦ç¨€ç–æ•°æ®é›†ä¸Šï¼Œè¡¨çŽ°ä¸å¦‚æ”¯æŒå‘é‡æœºæˆ–è€…ç¥žç»ç½‘ç»œã€‚<br>è®­ç»ƒè¿‡ç¨‹éœ€è¦ä¸²è¡Œè®­ç»ƒã€‚</p></blockquote></li><li><p>XGBoost ä¸Ž GBDT çš„è”ç³»å’ŒåŒºåˆ«ï¼Ÿ</p><blockquote><ol><li>åŽŸå§‹çš„ GBDT ç®—æ³•åŸºäºŽæŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦æ¥æž„é€ ä¿¡è´·å†³ç­–æ ‘ï¼Œåªæ˜¯åœ¨å†³ç­–æ ‘æž„å»ºå®Œæˆæž„é€ æ–°çš„å†³ç­–æ ‘ï¼Œåªæ˜¯åœ¨å†³ç­–æ ‘æž„å»ºå®ŒæˆåŽè¿›è¡Œå‰ªæžã€‚è€Œ XGBooståœ¨å†³ç­–æ ‘æž„å»ºé˜¶æ®µå°±åŠ å…¥äº†æ­£åˆ™é¡¹ã€‚</li><li>ä¸åŒçš„å†³ç­–æ ‘ç®—æ³•é‡‡ç”¨ä¸åŒçš„å‡†åˆ™æ¥è¿›è¡Œæ ‘çš„æž„å»ºï¼Œæ¯”å¦‚ IC3 é‡‡ç”¨ä¿¡æ¯å¢žç›Šï¼ŒC4.5 ä¸ºäº†å…‹æœç‰¹å¾ä¸­å–å€¼è¾ƒå¤šçš„ç‰¹å¾è€Œé‡‡ç”¨ä¿¡æ¯å¢žç›Šæ¯”ï¼Œ CART é‡‡ç”¨åŸºå°¼ç³»æ•°å’Œå¹³æ–¹è¯¯å·®ã€‚XGBoost å°†é¢„æµ‹å€¼å¸¦å…¥åˆ°æŸå¤±å‡½æ•°ä¸­æ±‚å¾—æŸå¤±å‡½æ•°çš„å½“å‰æœ€å°å€¼ï¼Œç„¶åŽè®¡ç®—å‡ºåˆ†è£‚å‰åŽæŸå¤±å‡½æ•°çš„å·®å€¼ï¼Œåˆ©ç”¨æœ€å¤§åŒ–è¿™ä¸ªå·®å€¼æ¥ä½œä¸ºå‡†åˆ™å®Œæˆæ ‘çš„æž„å»ºã€‚<br>æ€»çš„æ¥è¯´ï¼Œä¸¤è€…çš„åŒºåˆ«å’Œè”ç³»å¯ä»¥æ€»ç»“ä¸º:<br>a. GBDTæ˜¯æœºå™¨å­¦ä¹ ç®—æ³•ï¼ŒXGBoost æ˜¯å…¶å·¥ç¨‹å®žçŽ°ã€‚<br>b. åœ¨ä½¿ç”¨ CART ä½œä¸ºåŸºåˆ†ç±»å™¨æ—¶ï¼ŒXGBoost æ˜¾å¼åŠ å…¥æ­£åˆ™é¡¹æ¥æŽ§åˆ¶æ¨¡åž‹çš„å¤æ‚åº¦ï¼Œæœ‰åˆ©äºŽé˜²æ­¢è¿‡æ‹Ÿåˆã€‚<br>c. GBDTåœ¨æ¨¡åž‹è®­ç»ƒæ—¶åªä½¿ç”¨äº†ä»£ä»·å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°ä¿¡æ¯ï¼ŒXGBoostå¯¹ä»£ä»·å‡½æ•°è¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå¯ä»¥åŒæ—¶ä½¿ç”¨ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°ã€‚<br>d. ä¼ ç»Ÿçš„ GBDT é‡‡ç”¨ CART ä½œä¸ºåŸºåˆ†ç±»å™¨ï¼ŒXGBoost æ”¯æŒå¤šç§åŸºåˆ†ç±»å™¨ã€‚<br>e. ä¼ ç»Ÿçš„ GBDT åœ¨æ¯è½®è¿­ä»£æ—¶ä½¿ç”¨å…¨éƒ¨çš„æ•°æ®ï¼ŒXGBoost åˆ™é‡‡ç”¨äº†ä¸Žéšæœºæ£®æž—ç›¸ä¼¼çš„ç­–ç•¥ï¼Œæ”¯æŒå¯¹æ•°æ®è¿›è¡Œé‡‡æ ·ã€‚<br>f. XGBoostèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å‡ºç¼ºå¤±å€¼çš„å¤„ç†ç­–ç•¥</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      é›†æˆå­¦ä¹ ,Bagging,Boosting, Bias,Variance
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Deep-Learning</title>
    <link href="https://zhangruochi.com/ML-Interview-Deep-Learning/2020/05/27/"/>
    <id>https://zhangruochi.com/ML-Interview-Deep-Learning/2020/05/27/</id>
    <published>2020-05-27T09:46:36.000Z</published>
    <updated>2020-05-28T23:09:19.497Z</updated>
    
    <content type="html"><![CDATA[<h2 id="æ¿€æ´»å‡½æ•°"><a href="#æ¿€æ´»å‡½æ•°" class="headerlink" title="æ¿€æ´»å‡½æ•°"></a>æ¿€æ´»å‡½æ•°</h2><ol><li><p>å†™å‡ºå¸¸ç”¨æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°</p><blockquote><p>Sigmod </p><script type="math/tex; mode=display">f(z) = \frac{1}{1+exp(-z)}</script><script type="math/tex; mode=display">f\prime(z) = f(z)(1 - f(z))</script><p>Tanh</p><script type="math/tex; mode=display">f(z) = tanh(z) = \frac{e^z - e^{-z}}{ e^z + e^{-z}}</script><script type="math/tex; mode=display">f\prime(z) = 1 - (f(z))^2</script><p>Relu</p><script type="math/tex; mode=display">f(z) = max(0,z)</script><script type="math/tex; mode=display">f\prime(z) = \left\{\begin{aligned}& 1, z > 0 \\& 0, z \leq 0\end{aligned}\right.</script></blockquote></li><li><p>ä¸ºä»€ä¹ˆ Sigmoid å’Œ Tanh æ¿€æ´»å‡½æ•°ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±çŽ°è±¡ï¼Ÿ</p><blockquote><p>Sigmoid å‡½æ•°å°†è¾“å…¥æ˜ å°„åˆ°åŒºé—´(0,1)ï¼Œå½“ z è¾ƒå¤§å’Œè¾ƒå°æ—¶ï¼Œf(z) è¶‹è¿‘äºŽ 1. æ­¤æ—¶çš„æ¢¯åº¦è¶‹è¿‘äºŽ0. Tanh å®žé™…ç›¸å½“äºŽ Sigmoid çš„å¹³ç§»ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></blockquote></li><li><p>ReLU ç³»åˆ—çš„æ¿€æ´»å‡½æ•°ç›¸å¯¹äºŽSigmoid å’Œ Tanh æ¿€æ´»å‡½æ•°çš„ä¼˜ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿå±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>ä¼˜ç‚¹: 1. è®¡ç®—ç®€ä¾¿ 2. æœ‰æ•ˆåœ°è§£å†³æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ 3. ReLU å•ä¾§æŠ‘åˆ¶æä¾›äº†ç½‘ç»œçš„ç¨€ç–è¡¨è¾¾èƒ½åŠ›<br>å±€é™æ€§: ç¥žç»å…ƒæ­»äº¡çš„é—®é¢˜ã€‚å› ä¸º$f(z) = max(0,z)$ å¯¼è‡´è´Ÿæ¢¯åº¦åœ¨ç»è¿‡è¯¥ ReLUå•å…ƒæ—¶è¢«ç½®ä¸º 0ï¼Œä¸”ä¹‹åŽä¹Ÿä¸è¢«ä»»ä½•æ•°æ®æ¿€æ´»ã€‚å®žé™…è®­ç»ƒæ—¶ï¼Œå¦‚æžœ learning rate è¿‡å¤§ï¼Œä¼šå¯¼è‡´ä¸€å®šæ¯”ä¾‹çš„ neuron ä¸å¯é€†æ­»äº¡ï¼Œä½¿å¾—æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¤±è´¥ã€‚Leaky ReLU å¯ä»¥æœ‰æ•ˆåœ°è§£å†³è¯¥é—®é¢˜ã€‚</p><script type="math/tex; mode=display">f(z) = \left\{\begin{aligned} & z, z > 0 \\& az, z \leq 0\end{aligned}\right.</script></blockquote></li><li><p>å¹³æ–¹è¯¯å·®æŸå¤±å‡½æ•°å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°åˆ†åˆ«é€‚ç”¨ä»€ä¹ˆåœºæ™¯ï¼Ÿ</p><blockquote><p>ä¸€èˆ¬æ¥è¯´ï¼Œå¹³æ–¹æŸå¤±å‡½æ•°é€‚åˆäºŽè¿žç»­è¾“å‡ºï¼Œå¹¶ä¸”æœ€åŽä¸€å±‚ä¸å« Sigmoid æˆ–è€… Softmax æ¿€æ´»å‡½æ•°çš„ç¥žç»ç½‘ç»œã€‚äº¤å‰ç†µæŸå¤±åˆ™æ›´é€‚åˆäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»åœºæ™¯ã€‚</p></blockquote></li></ol><h2 id="ç¥žç»ç½‘ç»œè®­ç»ƒæŠ€å·§"><a href="#ç¥žç»ç½‘ç»œè®­ç»ƒæŠ€å·§" class="headerlink" title="ç¥žç»ç½‘ç»œè®­ç»ƒæŠ€å·§"></a>ç¥žç»ç½‘ç»œè®­ç»ƒæŠ€å·§</h2><ol><li><p>ç¥žç»ç½‘è·¯è®­ç»ƒæ—¶æ˜¯å¦å¯ä»¥å°†å…¨éƒ¨å‚æ•°åˆå§‹åŒ–ä¸º0.</p><blockquote><p>åŒä¸€å±‚çš„ç¥žç»å…ƒéƒ½æ˜¯åŒæž„çš„ï¼Œä»–ä»¬æ‹¥æœ‰ç›¸åŒçš„è¾“å…¥ï¼Œå¦‚æžœå°†å‚æ•°å…¨éƒ¨åˆå§‹åŒ–ä¸ºç›¸åŒçš„å€¼ï¼Œé‚£ä¹ˆæ— è®º forward è¿˜æ˜¯ backward éƒ½ä¼šæ‹¥æœ‰å®Œå…¨ç›¸åŒçš„å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦éšæœºåœ°åˆå§‹åŒ–ç¥žç»ç½‘ç»œçš„å‚æ•°ï¼Œä»¥æ‰“ç ´è¿™ç§å¯¹ç§°æ€§ã€‚</p></blockquote></li><li><p>ä¸ºä»€ä¹ˆ Dropout å¯ä»¥æŠ‘åˆ¶è¿‡æ‹Ÿåˆï¼Œå®ƒçš„å·¥ä½œåŽŸç†æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>Dropoutä½œç”¨ä¸Žæ¯ä»½å°æ‰¹é‡è®­ç»ƒæ•°æ®ï¼Œç”±äºŽå…¶éšæœºä¸¢å¼ƒç¥žç»å…ƒçš„æœºåˆ¶ï¼Œç›¸å½“äºŽæ¯æ¬¡è¿­ä»£éƒ½åœ¨è®­ç»ƒä¸åŒç»“æž„çš„ç¥žç»ç½‘ç»œã€‚ç±»æ¯”äºŽBaggingæ–¹æ³•ï¼ŒDropoutå¯è¢«è®¤ä¸ºæ˜¯ä¸€ç§å®žç”¨çš„å¤§è§„æ¨¡ç¥žç»ç½‘ç»œçš„æ¨¡åž‹ç»§æ‰¿ç®—æ³•ã€‚å¯¹äºŽåŒ…å« N ä¸ªç¥žç»å…ƒç»“ç‚¹çš„ç½‘ç»œï¼Œåœ¨ Dropout çš„ä½œç”¨ä¸‹å¯çœ‹åšä¸º$2^N$ä¸ªæ¨¡åž‹çš„é›†æˆã€‚è¿™$2^N$ä¸ªæ¨¡åž‹å¯è®¤ä¸ºæ˜¯åŽŸå§‹ç½‘ç»œçš„å­ç½‘ç»œã€‚åº”ç”¨DropoutåŒ…æ‹¬è®­ç»ƒå’Œé¢„æµ‹ä¸¤ä¸ªé˜¶æ®µï¼Œåœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¯ä¸ªç¥žç»å…ƒéœ€è¦å¢žåŠ ä¸€ä¸ªæ¦‚çŽ‡ç³»æ•°.</p><script type="math/tex; mode=display">\left\{ \begin{aligned} & r_j^{(l)} \sim Bernoulli(p) \\& \tilde{y}^{(l)} = r^{(l)} * y^{(l)}\end{aligned}\right.</script><p>æµ‹è¯•é˜¶æ®µæ˜¯å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œæ¯ä¸ªç¥žç»å…ƒçš„å‚æ•°è¦é¢„å…ˆä¹˜ä»¥æ¦‚çŽ‡ç³»æ•°pï¼Œä»¥æ¢å¤åœ¨è®­ç»ƒæ—¶è¯¥ç¥žç»å…ƒåªæœ‰pçš„æ¦‚çŽ‡è¢«ç”¨äºŽæ•´ä¸ªç¥žç»ç½‘ç»œçš„å‰å‘ä¼ æ’­è®¡ç®—</p></blockquote></li><li><p>BatchNorm çš„åŸºæœ¬åŠ¨æœºä¸ŽåŽŸç†æ˜¯ä»€ä¹ˆï¼Ÿ åœ¨å·ç§¯ç½‘ç»œä¸­å¦‚ä½•ä½¿ç”¨?</p><blockquote><ol><li>ç¥žç»ç½‘ç»œè®­ç»ƒçš„æœ¬è´¨æ˜¯å­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œå› æ­¤æˆ‘ä»¬å¸¸å‡è®¾è®­ç»ƒæ•°æ®ä¸Žæµ‹è¯•æ•°æ®æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ã€‚å¦‚æžœåˆ†å¸ƒä¸åŒå°†å¤§å¤§é™ä½Žç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒå¼€å§‹å‰å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚éšç€ç½‘ç»œè®­ç»ƒçš„è¿›è¡Œï¼Œæ¯ä¸ªhidden layerçš„å‚æ•°å˜åŒ–ä½¿å¾—åŽä¸€å±‚çš„è¾“å…¥å‘ç”Ÿå˜åŒ–ï¼Œä»Žè€Œæ¯ä¸€æ‰¹è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒä¹Ÿéšä¹‹å‘ç”Ÿå˜åŒ–ï¼Œä½¿å¾—ç½‘ç»œåœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½éœ€è¦æ‹Ÿåˆä¸åŒçš„æ•°æ®åˆ†å¸ƒï¼Œå¢žå¤§è®­ç»ƒçš„å¤æ‚åº¦ä»¥åŠè¿‡æ‹Ÿåˆé£Žé™©ã€‚</li><li>BatchNorm æ˜¯é’ˆå¯¹æ¯ä¸€æ‰¹æ•°æ®ï¼Œåœ¨ç½‘ç»œçš„æ¯ä¸€å±‚è¾“å…¥ä¹‹å‰å¢žåŠ å½’ä¸€åŒ–å¤„ç†ï¼Œå°†æ‰€æœ‰batchæ•°æ®å¼ºåˆ¶ç»Ÿä¸€åœ¨ç»Ÿä¸€çš„æ•°æ®åˆ†å¸ƒä¸‹ã€‚<script type="math/tex; mode=display">\hat{x}^{k} = \frac{x^{(k) - E[x^{(k)}]}}{\sqrt{Var[x^{(k)}]}}</script>å…¶ä¸­x^{(k)}ä¸ºè¯¥å±‚ç¬¬ K ä¸ªç¥žç»å…ƒçš„åŽŸå§‹è¾“å…¥æ•°æ®ï¼Œ$E[x^{(k)}]$ä¸ºè¿™ä¸€ä¸ªbatchåœ¨ç¬¬kä¸ªç¥žç»å…ƒçš„å‡å€¼ï¼Œ$\sqrt{Var[x^{(k)}]}$ä¸ºè¿™ä¸€æ‰¹æ•°æ®åœ¨ç¬¬kä¸ªç¥žç»å…ƒçš„æ ‡å‡†å·®ã€‚</li><li>ä½†æ˜¯å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º1 è¿™ä¸ªé™åˆ¶å¤ªä¸¥æ ¼äº†ï¼Œé™ä½Žäº†ç¥žç»ç½‘ç»œçš„æ‹Ÿåˆèƒ½åŠ›ã€‚å› æ­¤åŠ å…¥äº†ä¸¤ä¸ªå¯å­¦ä¹ å‚æ•° $\beta$ å’Œ $\eta$<script type="math/tex; mode=display">y_i = \eta \hat{x}^{k} + \beta</script>åœ¨æµ‹è¯•é˜¶æ®µï¼Œæ²¡æœ‰batch mean å’Œ var. æˆ‘ä»¬ä½¿ç”¨è®­ç»ƒé˜¶æ®µçš„ running average.</li><li>BatchNorm usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.</li></ol></blockquote></li></ol><h2 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h2><ol><li><p>è¯´è¯´å·åŠæ“ä½œçš„æœ¬è´¨ã€‚</p><blockquote><ol><li>Sparse Interactionï¼ˆç¨€ç–äº¤äº’ï¼‰ï¼š å·ç§¯æ“ä½œä¸­ï¼Œæ¯ä¸ªè¾“å‡ºç¥žç»å…ƒä»…ä»…ä¸Žå‰ä¸€å±‚ç‰¹å®šå±€éƒ¨åŒºåŸŸçš„ç¥žç»å…ƒå­˜åœ¨è¿žæŽ¥æƒé‡ã€‚æ—¶é—´å¤æ‚åº¦å¾—åˆ°ä¼˜åŒ–ï¼Œè¿‡æ‹Ÿåˆçš„æƒ…å†µä¹Ÿå¾—åˆ°æ”¹å–„ã€‚<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center></li><li>Hierarchical feature representation   ï¼šé€šå¸¸æ¥è¯´ï¼Œå›¾åƒï¼Œæ–‡æœ¬ï¼Œè¯­éŸ³ç­‰çŽ°å®žä¸–ç•Œä¸­çš„æ•°æ®éƒ½æ˜¯å…·æœ‰å±€éƒ¨çš„ç‰¹å¾ç»“æž„ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆå­¦ä¹ å±€éƒ¨çš„ç‰¹å¾ï¼Œå†å°†å±€éƒ¨ç‰¹å¾ç»„åˆèµ·æ¥å½¢æˆæ›´åŠ å¤æ‚çš„å’ŒæŠ½è±¡çš„ç‰¹å¾ã€‚è¿™ä¸Žäººç±»è§†è§‰æ„ŸçŸ¥ç‰©ä½“çš„å…±é€šçš„ã€‚</li><li>Parameter Sharing ï¼ˆå‚æ•°å…±äº«ï¼‰ï¼šç»™å®šä¸€ä¸ª feature map, æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª filter åŽ»æ‰«è¿™ä¸ª feature map. filter é‡Œé¢çš„å‚æ•°å«æƒé‡ï¼Œè¿™å¼ å›¾é‡Œæ¯ä¸ªä½ç½®éƒ½æ˜¯è¢«åŒæ ·çš„ filter æ‰«æçš„ï¼Œæ‰€ä»¥æƒé‡æ˜¯ç›¸åŒçš„ã€‚å‚æ•°å…±äº«çš„ç‰©ç†æ„ä¹‰æ˜¯ä½¿å¾—å·ç§¯å±‚å…·æœ‰å¹³ç§»ä¸å˜æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨çŒ«çš„å›¾ç‰‡ä¸Šå…ˆè¿›è¡Œ convolutionï¼Œå†å¹³ç§»l åƒç´ è¾“å‡ºï¼Œä¸ŽçŽ°å°†å›¾ç‰‡å¹³ç§»l åƒç´ å†è¿›è¡Œå·ç§¯æ“ä½œçš„è¾“å‡ºç»“æžœæ˜¯ç›¸ç­‰çš„ã€‚</li></ol></blockquote></li><li><p>å¸¸ç”¨çš„æ± åŒ–æ“ä½œæœ‰å“ªäº›ï¼Ÿæ± åŒ–çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>Mean Pooling å’Œ Max pooling. æ± åŒ–æ“ä½œé™¤äº†èƒ½æ˜¾è‘—é™ä½Žå‚æ•°æ•°é‡ï¼Œè¿˜èƒ½å¤Ÿä¿æŒå¯¹å¹³ç§»ã€ä¼¸ç¼©ã€æ—‹è½¬æ“ä½œçš„ä¸å˜æ€§ã€‚Mean Pooling å¯¹èƒŒæ™¯çš„ä¿ç•™æ•ˆæžœè¾ƒå¥½ï¼ŒMax pooling å¯¹çº¹ç†çš„æå–æ•ˆæžœæ›´å¥½ã€‚<br>ç‰¹æ®Šçš„æ± åŒ–æ–¹å¼æœ‰ï¼ŒGlobal Average Poolingï¼ŒSpatial Pyramid Pooling(ç©ºé—´é‡‘å­—å¡”æ± åŒ–). Global Average Pooling å¯ä»¥å°† feature map è½¬æ¢åˆ°ç‰¹å®šçš„ç»´åº¦ã€‚SPP ä¸»è¦è€ƒè™‘å¤šå°ºåº¦ä¿¡æ¯ï¼Œä¾‹å¦‚è®¡ç®—1x1ã€2x2ã€4x4çš„æ± åŒ–å¹¶å°†ç»“æžœæ‹¼æŽ¥åœ¨ä¸€èµ·ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚è¿˜å¯ä»¥ä½¿å¾—æˆ‘ä»¬æž„å»ºçš„ç½‘ç»œèƒ½å¤Ÿè¾“å…¥ä»»æ„å¤§å°çš„å›¾ç‰‡ï¼Œè€Œä¸éœ€è¦æå‰ç»è¿‡è£å‰ªç¼©æ”¾ç­‰é¢„å¤„ç†æ“ä½œ</p></blockquote></li><li><p>CNN å¦‚ä½•ç”¨äºŽæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Ÿ</p><blockquote><p>å¯¹äºŽæ–‡æœ¬æ¥è¯´ï¼Œå±€éƒ¨ç‰¹å¾å°±æ˜¯ç”±è‹¥å¹²å•è¯ç»„æˆçš„æ»‘åŠ¨çª—å£ï¼Œç±»ä¼¼äºŽ N-Gram. CNN çš„ä½œç”¨å°±æ˜¯èƒ½å¤Ÿè‡ªåŠ¨åœ°å¯¹ N-gram ç‰¹å¾è¿›è¡Œç»„åˆå’Œç­›é€‰ï¼ŒèŽ·å¾—ä¸åŒæŠ½è±¡å±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚å¸¸ç”¨çš„åº”ç”¨å¦‚ char-based model, æŠŠæ¯ä¸ªchar çš„ vector concat åœ¨ä¸€èµ·ï¼Œç„¶åŽä½¿ç”¨ conv1dæå–ç‰¹æ®Šçš„pattern å’Œ semantic.</p></blockquote></li><li><p>ResNet çš„æ ¸å¿ƒç†è®ºæ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>ResNetæå‡ºçš„èƒŒæ™¯æ˜¯ç¼“è§£æ·±å±‚çš„ç¥žç»ç½‘ç»œä¸­æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚ç›´è§‚æ¥è®²ï¼Œä¸€ä¸ª L+1 å±‚çš„ç½‘ç»œä¸ä¼šæ¯” L å±‚çš„ç½‘ç»œæ•ˆæžœå·®ï¼Œå› ä¸ºæˆ‘ä»¬ç®€å•åœ°è®¾æœ€åŽä¸€å±‚ä¸ºä¸€ä¸ªæ’ç­‰æ˜ å°„å³å¯ã€‚ç„¶è€Œå®žé™…ä¸Šæ·±å±‚ç½‘ç»œåè€Œä¼šæœ‰æ›´å¤§çš„è®­ç»ƒè¯¯å·®ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šå½’ç»“äºŽæ·±åº¦ç¥žç»ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚<br>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¾“å…¥$x$ç»è¿‡ä¸¤ä¸ªç¥žç»ç½‘ç»œå˜æ¢å¾—åˆ°$F(x)$,åŒæ—¶ $x$ çŸ­æŽ¥åˆ°ä¸¤å±‚ä¹‹åŽï¼Œæœ€åŽè¿™ä¸ªåŒ…å«ä¸¤å±‚çš„ç¥žç»ç½‘ç»œçš„è¾“å‡ºä¸º $H(x) = F(x) + x$. è¿™æ ·ä¸€æ¥ï¼Œ$F(x)$è¢«è®¾è®¡ä¸ºåªéœ€è¦æ‹Ÿåˆxä¸Žç›®æ ‡è¾“å‡ºH(x)çš„æ®‹å·® $H(x) - x$. å¦‚æžœæŸä¸€å±‚çš„æ•ˆæžœè¶³å¤Ÿå¥½ï¼Œé‚£ä¹ˆå¤šåŠ å±‚ä¸ä¼šä½¿å¾—æ¨¡åž‹å˜å·®ï¼Œå› ä¸ºè¯¥å±‚çš„è¾“å‡ºçŸ­æŽ¥åˆ°äº†åŽé¢çš„å±‚ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center></blockquote></li><li><p>DenseNet çš„æ ¸å¿ƒç†è®ºæ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>æ—¢å°† $x_0$ åˆ° $l_1$ å±‚çš„æ‰€æœ‰è¾“å‡ºfeature map é€šè¿‡ Channel concatåœ¨ä¸€èµ·.ç”±äºŽåœ¨DenseNetä¸­éœ€è¦å¯¹ä¸åŒå±‚çš„feature mapè¿›è¡Œcatæ“ä½œ,æ‰€ä»¥éœ€è¦ä¸åŒå±‚çš„feature mapä¿æŒç›¸åŒçš„feature size,è¿™å°±é™åˆ¶äº†ç½‘ç»œä¸­Down samplingçš„å®žçŽ°.ä¸ºäº†ä½¿ç”¨Down sampling,ä½œè€…å°†DenseNetåˆ†ä¸ºå¤šä¸ªDenseblock. åœ¨åŒä¸€ä¸ªDenseblockä¸­è¦æ±‚feature sizeä¿æŒç›¸åŒå¤§å°,åœ¨ä¸åŒDenseblockä¹‹é—´è®¾ç½®transition layerså®žçŽ°Down sampling, åœ¨ä½œè€…çš„å®žéªŒä¸­transition layerç”±BN + Conv(1Ã—1) ï¼‹2Ã—2 average-poolingç»„æˆ.</p></blockquote></li></ol><h2 id="å¾ªçŽ¯ç¥žç»ç½‘ç»œ"><a href="#å¾ªçŽ¯ç¥žç»ç½‘ç»œ" class="headerlink" title="å¾ªçŽ¯ç¥žç»ç½‘ç»œ"></a>å¾ªçŽ¯ç¥žç»ç½‘ç»œ</h2><ol><li><p>å¤„ç†æ–‡æœ¬æ•°æ®æ—¶ï¼Œå¾ªçŽ¯ç¥žç»ç½‘ç»œä¸Žå‰é¦ˆç¥žç»ç½‘ç»œç›¸æ¯”æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ</p><blockquote><p>ä¸€ä¸ªé•¿åº¦ä¸ºTçš„åºåˆ—ç”¨RNNå»ºæ¨¡ï¼Œå±•å¼€ä¹‹åŽå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ª T å±‚çš„å‰é¦ˆç¥žç»ç½‘ç»œã€‚å…¶ä¸­ï¼Œç¬¬$t$å±‚çš„éšå«çŠ¶æ€$h_t$ ç¼–ç äº†åºåˆ—å‰$t$ä¸ªè¾“å…¥ä¿¡æ¯ï¼Œå¯ä»¥é€šè¿‡å½“å‰çš„è¾“å…¥$x_t$ å’Œä¸Šä¸€å±‚ç¥žç»ç½‘ç»œçš„çŠ¶æ€$h_{t-1}$è®¡ç®—å¾—åˆ°. $h_t$å’Œyçš„è®¡ç®—å…¬å¼ä¸º:</p><script type="math/tex; mode=display">\left\{ \begin{aligned}& net_t = Ux_t + Wh_{t-1} \\& h_t = f(net_t) \\& y = g(Vh_t)\end{aligned}\right.</script><p>å…¶ä¸­ï¼Œ$f$ å’Œ $g$ ä¸ºæ¿€æ´»å‡½æ•°ï¼ŒU ä¸ºè¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µï¼ŒW ä¸ºéšè—å±‚ä»Žä¸Šä¸€æ—¶åˆ»åˆ°ä¸€ä¸‹æ—¶åˆ»çŠ¶æ€è½¬ç§»çš„æƒé‡çŸ©é˜µã€‚åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œ$f$å¯ä»¥é€‰å–Tanhå‡½æ•°æˆ–è€…ReLUå‡½æ•°ï¼Œ$g$å¯ä»¥é‡‡ç”¨ softmax å‡½æ•°ã€‚ç›¸æ¯”äºŽCNN, RNN ç”±äºŽå…·å¤‡å¯¹åºåˆ—ä¿¡æ¯çš„åˆ»ç”»èƒ½åŠ›ï¼Œå¾€å¾€èƒ½å¤Ÿå¾—åˆ°æ›´å‡†ç¡®çš„ç»“æžœã€‚</p></blockquote></li><li><p>å¾ªçŽ¯ç¥žç»ç½‘ç»œä¸ºä»€ä¹ˆä¼šå‡ºçŽ°æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ï¼Ÿæœ‰å“ªäº›æ”¹è¿›æ–¹æ¡ˆï¼Ÿ</p><blockquote><p>RNN æ±‚è§£é‡‡ç”¨ BPTT(back propagation through time) ç®—æ³•å®žçŽ°ï¼Œå®žé™…ä¸Šæ˜¯ back propagation ç®—æ³•çš„å˜ç§ã€‚ä½¿ç”¨ BPTTç®—æ³•å­¦ä¹ çš„RNN å¹¶ä¸èƒ½æ•æ‰é•¿è·ç¦»çš„ä¾èµ–å…³ç³»ï¼Œè¿™ç§çŽ°è±¡ä¸»è¦æºäºŽç¥žç»ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±ã€‚å› ä¸ºRNN çš„æ¢¯åº¦å¯ä»¥å†™æˆè¿žä¹˜çš„å½¢å¼ã€‚è¯¦ç»†å¯å‚è€ƒ <a href="https://zhangruochi.com/BackPropagation-through-time/2019/10/12/">https://zhangruochi.com/BackPropagation-through-time/2019/10/12/</a><br>æ¢¯åº¦çˆ†ç…§å¯ä»¥é€šè¿‡æ¢¯åº¦è£å‰ªæ¥çŽ¯èŠ‚ï¼Œå½“æ¢¯åº¦å¤§äºŽæŸä¸ªç»™å®šå€¼æ—¶ï¼Œå¯¹æ¢¯åº¦è¿›è¡Œæ”¶ç¼©ã€‚æ¢¯åº¦æ¶ˆå¤±å¯é€šè¿‡ LSTMï¼Œ GRU ç­‰æ¨¡åž‹åŠ å…¥é—¨æŽ§æœºåˆ¶æ¥å¼¥è¡¥ã€‚</p></blockquote></li><li><p>LSTM æ˜¯å¦‚ä½•å®žçŽ°é•¿çŸ­æœŸè®°å¿†åŠŸèƒ½çš„ï¼Ÿ</p><blockquote><p><a href="https://zhangruochi.com/LSTM-Mxnet-Implementation/2019/04/13/">https://zhangruochi.com/LSTM-Mxnet-Implementation/2019/04/13/</a><br>ç»å…¸çš„ LSTMï¼Œç¬¬ t æ­¥çš„æ›´æ–°å…¬å¼ä¸ºï¼š</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{I}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xi} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}_i),\\\boldsymbol{F}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xf} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hf} + \boldsymbol{b}_f),\\\boldsymbol{O}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xo} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{ho} + \boldsymbol{b}_o),\\\tilde{\boldsymbol{C}}_t &= \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xc} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c),\\\boldsymbol{C}_t &= \boldsymbol{F}_t \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t.\end{aligned}</script><p>  ä¸Žä¼ ç»Ÿçš„ RNN ç›¸æ¯”ï¼ŒLSTM ä¾ç„¶æ˜¯åŸºäºŽ$x_t$å’Œ$h_{t-1}$ æ¥è®¡ç®—$h_t$ï¼Œåªä¸è¿‡å¯¹å†…éƒ¨çš„ç»“æž„è¿›è¡Œäº†æ›´åŠ ç²¾å¿ƒçš„è®¾è®¡ï¼ŒåŠ å…¥äº† input gate $i_t$, forget gate $f_t$, output gate $o_t$. input gateæŽ§åˆ¶å½“å‰è®¡ç®—çš„æ–°çŠ¶æ€å¤šå¤§ç¨‹åº¦æ›´æ–°åˆ°å½“å‰momery cell ä¸­ï¼Œforget cellæŽ§åˆ¶å‰ä¸€æ­¥çš„memory cellä¸­çš„ä¿¡æ¯æœ‰å¤šå¤§ç¨‹åº¦è¢«é—å¿˜æŽ‰ï¼Œè¾“å‡ºé—¨æŽ§åˆ¶å½“å‰è¾“å‡ºæœ‰å¤šç¨‹åº¦å–å†³ä¸Žå½“å‰çš„ memory cell.<br>  å½“è¾“å…¥çš„åºåˆ—ä¸­æ²¡æœ‰é‡è¦ä¿¡æ¯æ—¶ï¼ŒLSTM çš„é—å¿˜é—¨çš„å€¼æŽ¥è¿‘äºŽ 1ï¼Œè¾“å…¥é—¨æŽ¥è¿‘äºŽ0. æ­¤æ—¶è¿‡åŽ»çš„è®°å¿†ä¼šè¢«ä¿ç•™ä¸‹æ¥ï¼Œä»Žè€Œå®žçŽ°é•¿æœŸè®°å¿†åŠŸèƒ½ã€‚å½“è¾“å…¥çš„åºåˆ—ä¸­æœ‰é‡è¦ä¿¡æ¯æ—¶ï¼ŒLSTM åº”å½“æŠŠå…¶å­˜è®°å¿†ä¸­ï¼Œæ­¤æ—¶è¾“å…¥é—¨çš„å€¼ä¼šæŽ¥è¿‘äºŽ 1ï¼Œè€Œé—å¿˜é—¨çš„å€¼æŽ¥è¿‘äºŽ0ã€‚ç»è¿‡è¿™æ ·çš„è®¾è®¡ï¼Œæ•´ä¸ªç½‘ç»œæ›´å®¹æ˜“å­¦ä¹ åˆ°åºåˆ—ä¹‹é—´çš„é•¿æœŸä¾èµ–ã€‚</p></blockquote></li><li><p>LSTM é‡Œå„æ¨¡å—åˆ†åˆ«é€‚ç”¨ä»€ä¹ˆæ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥ä½¿ç”¨åˆ«çš„æ¿€æ´»å‡½æ•°æ¿€æ´»å—ï¼Ÿ</p><blockquote><p>ä¸‰ä¸ªé—¨æŽ§å•å…ƒä½¿ç”¨Sigmoidä½œä¸ºæ¿€æ´»å‡½æ•°,ç”Ÿæˆå€™é€‰è®°å¿†æ—¶ï¼Œä½¿ç”¨tanhä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚Sigmoidå‡½æ•°çš„è¾“å‡ºåœ¨(0, 1)ä¹‹é—´ï¼Œç¬¦åˆé—¨æŽ§çš„ç‰©ç†å®šä¹‰ã€‚ä½¿ç”¨ Tanhå‡½æ•°ï¼Œæ˜¯å› ä¸ºå…¶è¾“å‡ºåœ¨(-1,1)ä¹‹é—´ï¼Œè¿™ä¸Žå¤§å¤šæ•°åœºæ™¯ä¸‹ç‰¹å¾åˆ†å¸ƒæ˜¯ 0 ä¸­å¿ƒçš„å»åˆï¼Œæ­¤å¤–ï¼ŒTanhå‡½æ•°åœ¨è¾“å…¥ä¸º0é™„è¿‘ç›¸æ¯”Sigmoidå‡½æ•°æœ‰æ›´å¤§çš„æ¢¯åº¦ï¼Œæ”¶æ•›æ›´å¿«ã€‚</p></blockquote></li></ol><h2 id="Seq2Seq-æ¨¡åž‹"><a href="#Seq2Seq-æ¨¡åž‹" class="headerlink" title="Seq2Seq æ¨¡åž‹"></a>Seq2Seq æ¨¡åž‹</h2><blockquote><p><a href="https://zhangruochi.com/Attention/2019/12/16/">https://zhangruochi.com/Attention/2019/12/16/</a></p></blockquote><ol><li><p>ä»€ä¹ˆæ˜¯ Seq2Seq æ¨¡åž‹ï¼ŒSeq2Seq æ¨¡åž‹æœ‰å“ªäº›ä¼˜ç‚¹ï¼Ÿ</p><blockquote><p>Seq2Seqæ¨¡åž‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œé€šè¿‡æ·±åº¦ç¥žç»ç½‘ç»œå°†è¾“å…¥åºåˆ—æ˜ å°„ä¸ºè¾“å‡ºåºåˆ—ï¼Œè¿™ä¸€è¿‡ç¨‹ç”±encoder ä¸Ž decoder ä¸¤ä¸ªçŽ¯èŠ‚ç»„æˆã€‚åœ¨ç»å…¸å®žçŽ°ä¸­ï¼Œencoder å’Œ decoder éƒ½æ˜¯sequence model. encoderå°†åºåˆ—ç¼–ç æˆ context vectorï¼Œdecoder å°† context vector è§£ç æˆåºåˆ—ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center></blockquote></li><li><p>Seq2Seq æ¨¡åž‹åœ¨è§£ç æ—¶ï¼Œæœ‰å“ªäº›å¸¸ç”¨çš„åŠžæ³•ï¼Ÿ</p><blockquote><p>Seq2Seq æœ€åŸºç¡€çš„è§£ç æ–¹æ³•æ˜¯è´ªå¿ƒæ³•ï¼Œå³é€‰å–ä¸€ç§åº¦é‡æ ‡å‡†åŽï¼Œæ¯æ¬¡éƒ½åœ¨å½“å‰çŠ¶æ€ä¸‹é€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªç»“æžœï¼ŒçŸ¥é“é‡åˆ°ç»“æŸç¬¦ã€‚ä½†æ˜¯è´ªå¿ƒç®—æ³•å¾€å¾€åªèƒ½å¾—åˆ°å±€éƒ¨æœ€ä¼˜è§£ã€‚<br><strong>Beam search</strong> æ˜¯è´ªå¿ƒç®—æ³•çš„æ”¹è¿›ã€‚æ”¹æ–¹æ³•ä¼šä¿å­˜beam size ä¸ªå½“å‰è¾ƒå¥½çš„é€‰æ‹©ï¼Œç„¶åŽè§£ç æ—¶æ¯ä¸€æ­¥æ ¹æ®ä¿å­˜çš„é€‰æ‹©è¿›è¡Œä¸‹ä¸€æ­¥çš„æ‰©å±•å’ŒæŽ’åºï¼ŒæŽ¥ç€é€‰æ‹©å‰bä¸ªè¿›è¡Œä¿å­˜ï¼Œå¾ªçŽ¯è¿­ä»£ï¼ŒçŸ¥é“ç»“æŸåŽé€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªåº§ä½è§£ç ç»“æžœã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center></blockquote></li><li><p>Seq2Seq å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸ºäº†è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿä¸ºä»€ä¹ˆé€‰ç”¨äº†åŒå‘å¾ªçŽ¯ç¥žç»ç½‘ç»œæ¨¡åž‹ï¼Ÿ</p><blockquote><ol><li>éšç€è¾“å…¥åºåˆ—çš„å¢žé•¿ï¼ŒSeq2Seqçš„æ€§èƒ½å‘ç”Ÿæ˜¾è‘—æ€§ä¸‹é™ã€‚è¿™æ˜¯å› ä¸ºç¼–ç æ—¶è¾“å…¥åºåˆ—çš„å…¨éƒ¨ä¿¡æ¯åŽ‹ç¼©åˆ°ä¸€ä¸ª context vectorã€‚éšç€è¾“å…¥åºåˆ—çš„å¢žé•¿ï¼Œå¥å­è¶Šå‰é¢çš„è¯ä¸¢å¤±å°±è¶Šä¸¥é‡ã€‚Attentionæœºåˆ¶çš„å¼•å…¥å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li><li>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</li><li>æœºå™¨ç¿»è¯‘ä¸­ï¼Œä½¿ç”¨åŒå‘RNNæ˜¯å› ä¸ºå½“å‰è¯çš„çŠ¶æ€ä¸ä»…å†³å®šäºŽè¿™ä¸ªè¯ä¹‹å‰çš„è¯ï¼Œè¿˜å†³å®šäºŽè¿™ä¸ªè¯ä¹‹åŽçš„è¯ã€‚æ¯”å¦‚ I was a student two years ago.</li></ol></blockquote></li><li><p>å¦‚ä½•è®¡ç®—attention score.</p><blockquote><ol><li>åˆ©ç”¨RNNç»“æž„å¾—åˆ°encoderä¸­çš„hidden state $(h_1,h_2,\cdots, h_n)$</li><li>å‡è®¾å½“å‰decoderçš„hidden state æ˜¯$s_{t-1}$, æˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸€ä¸ªè¾“å…¥ä½ç½®jçš„ hidden state ä¸Žå½“å‰è¾“å‡ºä½ç½®çš„å…³è”æ€§ï¼Œ$e_{ij} = a(s_{t-1}, h_j)$ï¼Œå…¶ä¸­ [å…¬å¼] æ˜¯ä¸€ç§ç›¸å…³æ€§çš„ç®—ç¬¦ï¼Œä¾‹å¦‚å¸¸è§çš„æœ‰dot product. è¾“å‡ºä½ç½®ä¸Žæ‰€æœ‰çš„è¾“å…¥ä½ç½®çš„å…³è”æ€§å†™æˆå‘é‡å½¢å¼æœ‰ $\vec{e_t} = a(s_{t-1}, h_i), \cdots, a(s_{t-1}, h_T)$</li><li>å¯¹$\vec{e_t}$è¿›è¡Œsoftmaxæ“ä½œï¼Œç„¶åŽå°†å…¶normalizeå¾—åˆ°attenion scoreåˆ†å¸ƒ$\alpha_{tj}$</li><li>åˆ©ç”¨ attention score å¾—åˆ°åŠ æƒçš„context vector. $\vec{c_t} =\sum_{j=1}^{T}\alpha_{tj} h_j$<br>å°†åŠ æƒçš„context vector ä¸Ž decoder çš„ $h_t^{dec}$ æ‹¼æŽ¥åœ¨ä¸€èµ·ã€‚</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      MLP, ç¥žç»ç½‘ç»œè®­ç»ƒæŠ€å·§, CNN, RNN, Seq2Seq
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Optimization</title>
    <link href="https://zhangruochi.com/ML-Interview-Optimization/2020/05/27/"/>
    <id>https://zhangruochi.com/ML-Interview-Optimization/2020/05/27/</id>
    <published>2020-05-26T23:19:28.000Z</published>
    <updated>2020-05-28T23:04:38.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ç›‘ç£å­¦ä¹ çš„æŸå¤±å‡½æ•°"><a href="#ç›‘ç£å­¦ä¹ çš„æŸå¤±å‡½æ•°" class="headerlink" title="ç›‘ç£å­¦ä¹ çš„æŸå¤±å‡½æ•°"></a>ç›‘ç£å­¦ä¹ çš„æŸå¤±å‡½æ•°</h2><ol><li>æœ‰ç›‘ç£å­¦ä¹ æ¶‰åŠçš„æŸå¤±å‡½æ•°æœ‰å“ªäº›ï¼Ÿè¯·åˆ—ä¸¾å¹¶ç®€è¿°å®ƒä»¬çš„ç‰¹ç‚¹ã€‚<blockquote><ol><li>MSE<script type="math/tex; mode=display">L = \sum( Y - f(x))^2</script></li><li>MAE<script type="math/tex; mode=display">L = \sum|Y - f(x)|</script></li><li>Hingeï¼šHingeæŸå¤±ä¸ä»…ä¼šæƒ©ç½šé”™è¯¯çš„é¢„æµ‹ï¼Œè¿˜ä¼šæƒ©ç½šä¸è‡ªä¿¡çš„æ­£ç¡®é¢„æµ‹ã€‚ç”¨äºŽæ”¯æŒå‘é‡æœº(SVM)ä¸­ã€‚<script type="math/tex; mode=display">L_{hinghe}(f,y) = max\{0, 1-f_y\}</script></li><li>Binary Cross Entropy<script type="math/tex; mode=display">L = -y * log(p) - (1-y) * log(1-p)</script></li><li>Cross Entropy <script type="math/tex; mode=display">L(x_i,y_i) = -\sum_{j=1}^{e} y_{ij} * log(p_{ij})</script>where $Y_i$ is one-hot encoded target vector $(y_{i1},\cdots, y_{i2})$.<script type="math/tex; mode=display">y_{ij} = \left\{\begin{aligned}& 1 \quad \text{if i element is in class j} \\ & 0 \quad \text{otherwise}\end{aligned}\right.</script></li><li>Kullback-Leibler Divergenceï¼šè¡¨ç¤ºä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒçš„å·®å¼‚ã€‚Variational Auto-Encoderä¸­ä½¿ç”¨ã€‚<script type="math/tex; mode=display">D_{KL}(p||q) = \sum_{i=1}^{N}p(x_i)\dot(log p(x_i) - log q(x_i))</script></li><li>Huberï¼šç»“åˆ MSE å’Œ MAE çš„ä¼˜ç‚¹<script type="math/tex; mode=display">L = \left\{ \begin{aligned}& \frac{1}{2}(y - f(x))^2, \quad if \ | y - f(x)| \leq \delta, \\ & \delta|y - f(x)| - \frac{1}{2}\delta^2, otherwise\end{aligned}\right.</script></li><li>Dice lossï¼š ä¸¤ä¸ªè½®å»“çš„ç›¸ä¼¼åº¦ï¼Œåº”ç”¨åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸ<script type="math/tex; mode=display">DL(A,B) = 2 \frac{A \cap B }{ |A| + |B|}</script></li></ol></blockquote></li></ol><h2 id="æœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŒ–é—®é¢˜"><a href="#æœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŒ–é—®é¢˜" class="headerlink" title="æœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŒ–é—®é¢˜"></a>æœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŒ–é—®é¢˜</h2><ol><li><p>æœºå™¨å­¦ä¹ ä¸­ï¼Œå“ªäº›æ˜¯å‡¸ä¼˜åŒ–é—®é¢˜ï¼Ÿ</p><blockquote><p>å‡¸å‡½æ•°æ›²é¢ä¸Šä»»æ„ä¸¤ç‚¹è¿žæŽ¥è€Œæˆçš„çº¿æ®µï¼Œå…¶ä¸Šçš„ä»»æ„ä¸€ç‚¹éƒ½ä¸ä¼šå¤„äºŽæ”¹å‡½æ•°æ›²é¢çš„ä¸‹æ–¹ã€‚ä¸€ä¸ªå¸¸ç”¨çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ï¼Œé€»è¾‘å›žå½’ï¼Œå¯¹åº”çš„ä¼˜åŒ–é—®é¢˜å°±æ˜¯å‡¸ä¼˜åŒ–é—®é¢˜ã€‚å› ä¸ºæˆ‘ä»¬å¯ä»¥æ±‚å¾—ä¼˜åŒ–å‡½æ•°çš„ HessiançŸ©é˜µæ˜¯åŠæ­£å®šçš„ã€‚</p><script type="math/tex; mode=display">L(\lambda x + (1-\lambda) y) \leq \lambda L(x) + (1 - \lambda)L(y)</script><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></blockquote></li><li><p>å½“æ•°æ®é‡ç‰¹åˆ«å¤§æ—¶ï¼Œç»å…¸çš„æ¢¯åº¦ä¸‹é™ç®—æ³•æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ</p><blockquote><p>ç»å…¸çš„æ¢¯åº¦ä¸‹é™ç®—æ³•åœ¨æ¯æ¬¡æ¨¡åž‹å‚æ•°è¿›è¡Œæ›´æ–°æ—¶ï¼Œéœ€è¦éåŽ†æ‰€æœ‰çš„è®­ç»ƒæ•°æ®ã€‚å½“ M å¾ˆå¤§æ—¶ï¼Œè¿™éœ€è¦è¿›è¡Œå¾ˆå¤§çš„è®¡ç®—ã€‚ä¸ºäº†è®¡ç®—è¿™ä¸ªé—®é¢˜ï¼Œéšæœºæ¢¯åº¦ä¸‹é™æ³•ç”¨å•ä¸ªæ ·æœ¬æŸå¤±æ¥è¿‘ä¼¼æ‰€æœ‰æ ·æœ¬çš„å¹³å‡æŸå¤±ã€‚ä¸ºäº†é™ä½Žéšæœºæ¢¯åº¦çš„æ–¹å·®ï¼Œä½¿å¾—è¿­ä»£æ›´åŠ ç¨³å®šï¼Œä¸€èˆ¬ä½¿ç”¨å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚å¯¹äºŽå°æ‰¹é‡ä¸‹é™æ³•çš„ä½¿ç”¨ï¼Œéœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š</p><ol><li>ä¸åŒåº”ç”¨ä¸­ï¼Œæ¯ä¸ª batch çš„å¤§å°é€šå¸¸ä¼šä¸ä¸€æ ·ã€‚ä¸€èˆ¬é€‰æ‹© 2 çš„å¹‚æ¬¡å¯ä»¥å……åˆ†åˆ©ç”¨çŸ©é˜µè¿ç®—ã€‚</li><li>ä¸ºäº†é¿å…æ•°æ®çš„ç‰¹å®šé¡ºåºç»™ç®—æ³•æ”¶æ•›å¸¦æ¥çš„å½±å“ï¼Œä¸€èˆ¬ä¼šåœ¨æ¯æ¬¡éåŽ†æ•°æ®ä¹‹å‰ï¼Œå…ˆå¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œshuffleã€‚</li><li>ä¸ºäº†åŠ å¿«æ”¶æ•›é€Ÿåº¦ï¼ŒåŒæ—¶æé«˜æ±‚è§£ç²¾åº¦ï¼Œé€šå¸¸é‡‡ç”¨è¡°å‡å­¦ä¹ é€ŸçŽ‡çš„æ–¹æ¡ˆï¼šä¸€å¼€å§‹ç®—æ³•é‡‡ç”¨è¾ƒå¤§çš„å­¦ä¹ é€ŸçŽ‡ï¼Œå½“è¯¯å·®æ›²çº¿è¿›å…¥å¹³å°æœŸåŽï¼Œå‡å°å­¦ä¹ é€ŸçŽ‡åšæ›´ç²¾ç»†çš„è°ƒæ•´ã€‚</li></ol></blockquote></li><li><p>è¯·ç»™å‡ºéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•å¤±æ•ˆçš„åŽŸå› ã€‚</p><blockquote><p>éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•æ”¾å¼ƒäº†å¯¹æ¢¯åº¦å‡†ç¡®æ€§çš„è¿½æ±‚ï¼Œæ¯æ­¥ä»…ä»…é‡‡ç”¨ä¸€ä¸ªï¼ˆæˆ–å°‘é‡ï¼‰æ ·æœ¬æ¥ä¼°è®¡å½“å‰æ¢¯åº¦ã€‚ä½†æ˜¯ç”±äºŽæ¯æ­¥æŽ¥å—çš„ä¿¡æ¯é‡æœ‰é™ï¼Œéšæœºæ¢¯åº¦ä¸‹é™æ³•å¯¹æ¢¯åº¦çš„ä¼°è®¡å¸¸å¸¸å‡ºçŽ°åå·®ï¼Œé€ æˆç›®æ ‡å‡½æ•°æ”¶æ•›å¾ˆä¸ç¨³å®šï¼Œä¼´æœ‰å‰§çƒˆçš„æ³¢åŠ¨ï¼Œæœ‰æ—¶ç”šè‡³å‡ºçŽ°ä¸æ”¶æ•›çš„æƒ…å†µã€‚å¯¹äºŽéšæœºæ¢¯åº¦ä¸‹é™æ³•æ¥è¯´ï¼Œæœ€å¯æ€•çš„ä¸æ˜¯å±€éƒ¨æœ€ä¼˜ç‚¹ï¼Œè€Œæ˜¯å±±è°·å’Œéžç‚¹ã€‚éžç‚¹å°±æ˜¯ä¸€ç‰‡å¹³æ‘Šçš„åŒºåŸŸï¼Œåœ¨æ¢¯åº¦å‡ ä¹Žä¸ºé›¶çš„åŒºåŸŸï¼Œéšæœºæ¢¯åº¦ä¸‹é™æ³•æ— æ³•è®¡ç®—å‡ºæ¢¯åº¦çš„å¾®å°å˜åŒ–ï¼Œå¯¼è‡´åœ¨æ¥å›žéœ‡è¡ã€‚</p></blockquote></li><li><p>å¦‚ä½•æ”¹è¿›éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Ÿï¼ˆåŠ¨é‡å’ŒçŽ¯å¢ƒæ„ŸçŸ¥ï¼‰</p><blockquote><ol><li>Momentumï¼ˆåŠ¨é‡ï¼‰ï¼šå½“æ¥åˆ°éžç‚¹å¤„ï¼Œåœ¨æƒ¯æ€§ä½œç”¨ä¸‹ç»§ç»­å‰è¡Œï¼Œåˆ™æœ‰æœºä¼šå†²å‡ºå¹³å¦çš„é™·é˜±ã€‚åŠ¨é‡æ³•çš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œæ”¶æ•›æ›²çº¿ä¹Ÿæ›´ç¨³å®šã€‚å®žé™…ä¸Šæ˜¯å¯¹ gradient åš moving average.<script type="math/tex; mode=display">\begin{aligned}& v_t = \eta v_{t-1} + \gamma g_t \\& \theta_{t+1} = \theta_t - v_t \end{aligned}</script></li></ol></blockquote></li><li><p>AdaGrad æ–¹æ³•</p><blockquote><p>éšæœºæ¢¯åº¦ä¸‹é™æ³•å¯¹çŽ¯å¢ƒçš„æ„ŸçŸ¥æ˜¯æŒ‡åœ¨å‚æ•°ç©ºé—´ä¸­ï¼Œæ ¹æ®ä¸åŒå‚æ•°çš„ä¸€äº›ç»éªŒæ€§åˆ¤æ–­ï¼Œè‡ªé€‚åº”åœ°ç¡®å®šå‚æ•°çš„å­¦ä¹ é€ŸçŽ‡ã€‚ä¾‹å¦‚åœ¨æ–‡æœ¬å¤„ç†ä¸­è®­ç»ƒ word embeddingï¼Œæœ‰å†™è¯é¢‘ç¹å‡ºçŽ°ï¼Œæœ‰äº›è¯æžå°‘å‡ºçŽ°ï¼Œæˆ‘ä»¬å¸Œæœ›æžå°‘å‡ºçŽ°çš„è¯æ›´æ–°çš„æ­¥å¹…å¤§ä¸€äº›ã€‚AdaGrad é‡‡ç”¨<code>åŽ†å²æ¢¯åº¦å¹³æ–¹å’Œ</code>æ¥è¡¡é‡ä¸åŒå‚æ•°çš„ç¨€ç–æ€§ï¼Œå–å€¼è¶Šå°è¯´æ˜Žè¶Šç¨€ç–ã€‚ å…·ä½“çš„æ›´æ–°å…¬å¼ä¸º:</p><script type="math/tex; mode=display">\theta_{t+1,i} = \theta_{t,i} - \frac{\gamma}{\sqrt{\sum_{k=0}^{t} g_{k,i}^2 + \epsilon}}</script></blockquote></li><li><p>Adam æ–¹æ³•</p><blockquote><p>Adamæ–¹æ³•é›†æƒ¯æ€§ä¿æŒå’ŒçŽ¯å¢ƒæ„ŸçŸ¥ä¸¤ä¸ªä¼˜ç‚¹äºŽä¸€èº«ã€‚ä¸€æ–¹é¢ï¼ŒAdam è®°å½•æ¢¯åº¦çš„ first momentï¼Œå³è¿‡å¾€æ¢¯åº¦ä¸Žå½“å‰æ¢¯åº¦çš„å¹³å‡ï¼Œè¿™ä½“çŽ°äº†æƒ¯æ€§ä¿æŒï¼›å¦ä¸€æ–¹é¢ï¼ŒAdam è¿˜è®°å½•äº†æ¢¯åº¦çš„ second momentï¼Œå³è¿‡å¾€æ¢¯åº¦å¹³æ–¹ä¸Žå½“å‰æ¢¯åº¦å¹³æ–¹çš„å¹³å‡ï¼Œè¿™ç±»ä¼¼ AdaGradæ–¹æ³•ï¼Œä½“çŽ°çŽ¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›,ä¸ºä¸åŒå‚æ•°äº§ç”Ÿè‡ªé€‚åº”çš„å­¦ä¹ é€ŸçŽ‡ã€‚first and second monent é‡‡ç”¨exponential decay averageï¼Œä½¿å¾—æ—¶é—´ä¹…è¿œçš„æ¢¯åº¦å¯¹å½“å‰å¹³å‡å€¼çš„è´¡çŒ®å‘ˆæŒ‡æ•°è¡°å‡ã€‚</p><script type="math/tex; mode=display">\begin{aligned}& m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\& v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2 \\ & \hat{m_t} = \frac{m_t}{1 - \beta^t_1}   \\& \hat{v_t} = \frac{v_t}{1 - \beta^t_2} \\& \theta_{t+1} = \theta_t - \frac{ \gamma \cdot \hat{m_t} }{ \sqrt{\hat{v_t} + \epsilon} }\end{aligned}</script><p>å…¶ä¸­$\beta_1$, $\beta_2$ ä¸ºè¡°å‡ç³»æ•°ï¼Œ$m_t$æ˜¯ first moment, $v_t$ æ˜¯second moment.</p></blockquote></li></ol><p><a href="https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/">https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/</a></p><ol><li>L1 æ­£åˆ™åŒ–ä¸Žç¨€ç–æ€§åŽŸç†æ˜¯ä»€ä¹ˆï¼Ÿ<blockquote><p>å¸¦æ­£åˆ™é¡¹å’Œå¸¦çº¦æŸæ¡ä»¶æ˜¯ç­‰ä»·çš„ï¼Œä¸ºäº†çº¦æŸwçš„å¯èƒ½å–å€¼ç©ºé—´ä»Žè€Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬ä¸ºè¯¥æœ€ä¼˜åŒ–é—®é¢˜åŠ ä¸Šä¸€ä¸ªçº¦æŸï¼Œå°±æ˜¯wçš„ L2 èŒƒæ•°ä¸èƒ½å¤§äºŽm</p><script type="math/tex; mode=display">\begin{aligned} & \min sum_{i=1}^N(y_i - w^T x_i)^2 \\ & s.t. ||w||^2_2 \leq m\end{aligned}</script><p>ä¸ºäº†æ±‚è§£å¸¦çº¦æŸæ¡ä»¶çš„å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œå†™å‡ºæ‹‰æ ¼æœ—æ—¥å‡½æ•°</p><script type="math/tex; mode=display">sum_{i=1}^N(y_i - w^T x_i)^2 + \lambda(||w||^2_2 - m)</script><p>L2æ­£åˆ™åŒ–ç›¸å½“äºŽä¸ºå‚æ•°å®šä¹‰äº†ä¸€ä¸ªåœ†å½¢çš„è§£ç©ºé—´(å› ä¸ºå¿…é¡»ä¿è¯L2èŒƒæ•°ä¸èƒ½å¤§äºŽm), è€Œ L1 æ­£åˆ™åŒ–æƒ³å½“äºŽå®šä¹‰äº†ä¸€ä¸ªè±å½¢çš„è§£ç©ºé—´ã€‚L1 çš„è§£ç©ºé—´æ˜¾ç„¶æ›´å®¹æ˜“ä¸Žç›®æ ‡å‡½æ•°çš„ç­‰é«˜çº¿åœ¨è§’ç‚¹ç¢°æ’žï¼Œä»Žè€Œäº§ç”Ÿç¨€ç–è§£ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–ç®—æ³•
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Unsupervised-Learning</title>
    <link href="https://zhangruochi.com/ML-Interview-Unsupervised-Learning/2020/05/26/"/>
    <id>https://zhangruochi.com/ML-Interview-Unsupervised-Learning/2020/05/26/</id>
    <published>2020-05-26T01:14:23.000Z</published>
    <updated>2020-05-28T22:49:19.611Z</updated>
    
    <content type="html"><![CDATA[<h2 id="æ— ç›‘ç£å­¦ä¹ "><a href="#æ— ç›‘ç£å­¦ä¹ " class="headerlink" title="æ— ç›‘ç£å­¦ä¹ "></a>æ— ç›‘ç£å­¦ä¹ </h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%"></center><h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><ol><li><p>ç®€è¿° K-Meansç®—æ³•çš„å…·ä½“æ­¥éª¤<br>è¾“å…¥æ˜¯æ ·æœ¬é›†$D=\{x_1,x_2,â€¦x_m\}$,èšç±»çš„ç°‡æ ‘k,æœ€å¤§è¿­ä»£æ¬¡æ•°Nã€‚è¾“å‡ºæ˜¯ç°‡åˆ’åˆ†$C=\{C_1,C_2,â€¦C_k\}$</p><blockquote><ol><li>æ•°æ®é¢„å¤„ç†,å¦‚å½’ä¸€åŒ–ã€ç¦»ç¾¤ç‚¹å¤„ç†ç­‰<br>2.ä»Žæ•°æ®é›†Dä¸­éšæœºé€‰æ‹©kä¸ªæ ·æœ¬ä½œä¸ºåˆå§‹çš„kä¸ªè´¨å¿ƒå‘é‡ï¼š$\{\mu_1,\mu_2,â€¦,\mu_k\}$</li><li>å¯¹äºŽn=1,2,â€¦,N<ul><li>å°†ç°‡åˆ’åˆ†Cåˆå§‹åŒ–ä¸º $C_t = \varnothing \;\; t =1,2â€¦k$</li><li>è®¡ç®—æ ·æœ¬$x_i$å’Œå„ä¸ªè´¨å¿ƒå‘é‡$\mu_j(j=1,2,â€¦k)$çš„è·ç¦»: $d_{ij} = ||x_i - \mu_j||_2^2$ï¼Œå°†$x_i$æ ‡è®°æœ€å°çš„ä¸º$d_{ij}$æ‰€å¯¹åº”çš„ç±»åˆ«$\lambda_i$, æ­¤æ—¶æ›´æ–° $C_{\lambda_i} = C_{\lambda_i} \cup \{x_i\}$</li><li>å¯¹äºŽj=1,2,â€¦,k,å¯¹ $C_j$ä¸­æ‰€æœ‰çš„æ ·æœ¬ç‚¹é‡æ–°è®¡ç®—æ–°çš„è´¨å¿ƒ$\mu_j = \frac{1}{|C_j|}\sum\limits_{x \in C_j}x$</li><li>å¦‚æžœæ‰€æœ‰çš„kä¸ªè´¨å¿ƒå‘é‡éƒ½æ²¡æœ‰å‘ç”Ÿå˜åŒ–ï¼Œåˆ™è½¬åˆ°æ­¥éª¤3ï¼‰</li></ul></li><li>è¾“å‡ºç°‡åˆ’åˆ†$C=\{C_1,C_2,â€¦C_k\}$</li></ol></blockquote></li><li><p>ç®€è¿°K-Means++ä¸Ž K-Meansçš„åŒºåˆ«</p><blockquote><p>K-Meansä¸­kä¸ªåˆå§‹åŒ–çš„è´¨å¿ƒçš„ä½ç½®é€‰æ‹©å¯¹æœ€åŽçš„èšç±»ç»“æžœå’Œè¿è¡Œæ—¶é—´éƒ½æœ‰å¾ˆå¤§çš„å½±å“ï¼Œå› æ­¤éœ€è¦é€‰æ‹©åˆé€‚çš„kä¸ªè´¨å¿ƒã€‚å¦‚æžœä»…ä»…æ˜¯å®Œå…¨éšæœºçš„é€‰æ‹©ï¼Œæœ‰å¯èƒ½å¯¼è‡´ç®—æ³•æ”¶æ•›å¾ˆæ…¢ã€‚K-Means++ç®—æ³•å°±æ˜¯å¯¹K-Meanséšæœºåˆå§‹åŒ–è´¨å¿ƒçš„æ–¹æ³•çš„ä¼˜åŒ–ã€‚K-Means++çš„å¯¹äºŽåˆå§‹åŒ–è´¨å¿ƒçš„ä¼˜åŒ–ç­–ç•¥ä¹Ÿå¾ˆç®€å•ï¼Œå¦‚ä¸‹ï¼š</p><ol><li>ä»Žè¾“å…¥çš„æ•°æ®ç‚¹é›†åˆä¸­éšæœºé€‰æ‹©ä¸€ä¸ªç‚¹ä½œä¸ºç¬¬ä¸€ä¸ªèšç±»ä¸­å¿ƒ$u_1$</li><li>å¯¹äºŽæ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ªç‚¹$x_i$,è®¡ç®—å®ƒä¸Žå·²é€‰æ‹©çš„èšç±»ä¸­å¿ƒä¸­æœ€è¿‘èšç±»ä¸­å¿ƒçš„è·ç¦»<script type="math/tex; mode=display">D(x_i) = arg\;min||x_i- \mu_r||_2^2\;\;r=1,2,...k_{selected}</script></li><li>é€‰æ‹©ä¸€ä¸ªæ–°çš„æ•°æ®ç‚¹ä½œä¸ºæ–°çš„èšç±»ä¸­å¿ƒï¼Œé€‰æ‹©çš„åŽŸåˆ™æ˜¯ï¼š$D(x)$è¾ƒå¤§çš„ç‚¹ï¼Œè¢«é€‰å–ä½œä¸ºèšç±»ä¸­å¿ƒçš„æ¦‚çŽ‡è¾ƒå¤§.</li><li>é‡å¤bå’Œcç›´åˆ°é€‰æ‹©å‡ºkä¸ªèšç±»è´¨å¿ƒ</li><li>åˆ©ç”¨è¿™kä¸ªè´¨å¿ƒæ¥ä½œä¸ºåˆå§‹åŒ–è´¨å¿ƒåŽ»è¿è¡Œæ ‡å‡†çš„K-Meansç®—æ³•</li></ol></blockquote></li><li><p>K-Meanså‡å€¼ç®—æ³•çš„ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><ol><li>Kå€¼çš„é€‰å–ä¸å¥½æŠŠæ¡</li><li>å¯¹äºŽä¸æ˜¯å‡¸çš„æˆ–è€…çƒå½¢çš„æ•°æ®é›†æ¯”è¾ƒéš¾æ”¶æ•›</li><li>å¦‚æžœå„éšå«ç±»åˆ«çš„æ•°æ®ä¸å¹³è¡¡ï¼Œæ¯”å¦‚å„éšå«ç±»åˆ«çš„æ•°æ®é‡ä¸¥é‡å¤±è¡¡ï¼Œæˆ–è€…å„éšå«ç±»åˆ«çš„æ–¹å·®ä¸åŒï¼Œåˆ™èšç±»æ•ˆæžœä¸ä½³ã€‚</li><li>é‡‡ç”¨è¿­ä»£æ–¹æ³•ï¼Œå¾—åˆ°çš„ç»“æžœåªæ˜¯å±€éƒ¨æœ€ä¼˜ã€‚</li><li>å¯¹å™ªéŸ³å’Œå¼‚å¸¸ç‚¹æ¯”è¾ƒçš„æ•æ„Ÿã€‚</li></ol></blockquote></li><li><p>å¦‚ä½•é€‰å– K-Means çš„ K å€¼ï¼Ÿ</p><blockquote><p>K å€¼çš„é€‰æ‹©ä¸€èˆ¬åŸºäºŽç»éªŒå’Œå¤šæ¬¡è¯•éªŒç»“æžœã€‚æ¯”å¦‚å¯ä»¥é‡‡ç”¨æ‰‹è‚˜æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸åŒçš„ K å€¼ï¼Œå¹¶å°†ä¸åŒçš„ K å€¼æ‰€å¯¹åº”çš„æŸå¤±å‡½æ•°ç”»æˆæŠ˜çº¿ã€‚æ‹ç‚¹å°±æ˜¯ K çš„æœ€ä½³å€¼ã€‚</p></blockquote></li><li><p>ä»€ä¹ˆæ˜¯ Kernel K- Means ?</p><blockquote><p>è¿˜ç«¥çš„æ¬§å¼è·ç¦»åº¦é‡æ–¹å¼ï¼Œä½¿å¾— K å‡å€¼ç®—æ³•æœ¬è´¨ä¸Šå‡è®¾äº†å„ä¸ªæ•°æ®ç°‡çš„æ•°æ®å‘ˆçŽ°çƒå½¢æˆ–è€…é«˜ç»´çƒå½¢ï¼Œè¿™ç§åˆ†å¸ƒåœ¨å®žé™…ç”Ÿæ´»ä¸­ä¸å¸¸è§ã€‚é¢å¯¹éžå‡¸çš„æ•°æ®åˆ†å¸ƒæ—¶ï¼Œå¼•å…¥æ ¸å‡½æ•°æ¥è¿›è¡Œéžçº¿æ€§æ˜ å°„ï¼Œå°†è¾“å…¥ç©ºé—´ä¸­çš„æ•°æ®ç‚¹æ˜ å°„åˆ°é«˜ç»´çš„ç‰¹å¾ç©ºé—´ï¼Œå¹¶åœ¨æ–°çš„ç‰¹å¾ä¸­ç©ºé—´è¿›è¡Œèšç±»ã€‚éžçº¿æ€§æ˜ å°„å¢žåŠ äº†æ•°æ®ç‚¹çº¿æ€§å¯åˆ†çš„æ¦‚çŽ‡ã€‚</p></blockquote></li></ol><h3 id="DBSCANS-ï¼ˆå¯†åº¦èšç±»ï¼‰"><a href="#DBSCANS-ï¼ˆå¯†åº¦èšç±»ï¼‰" class="headerlink" title="DBSCANS ï¼ˆå¯†åº¦èšç±»ï¼‰"></a>DBSCANS ï¼ˆå¯†åº¦èšç±»ï¼‰</h3><blockquote><p><a href="https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/">https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/</a></p></blockquote><h3 id="Birch-å±‚æ¬¡èšç±»"><a href="#Birch-å±‚æ¬¡èšç±»" class="headerlink" title="Birch (å±‚æ¬¡èšç±»)"></a>Birch (å±‚æ¬¡èšç±»)</h3><ol><li>ä»€ä¹ˆæ˜¯å±‚æ¬¡èšç±»? å±‚æ¬¡èšç±»çš„æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ<blockquote><p>å±‚æ¬¡èšç±»ä¸æŒ‡å®šå…·ä½“çš„ç°‡æ•°ï¼Œè€Œåªå…³æ³¨ç°‡ä¹‹é—´çš„è¿œè¿‘ï¼Œæœ€ç»ˆä¼šå½¢æˆä¸€ä¸ªæ ‘å½¢å›¾ã€‚<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%"></center><br>æ ¹æ®èšç±»ç°‡ä¹‹é—´è·ç¦»çš„è®¡ç®—æ–¹æ³•çš„ä¸åŒï¼Œå±‚æ¬¡èšç±»ç®—æ³•å¯ä»¥å¤§è‡´åˆ†ä¸ºï¼šå•é“¾æŽ¥ï¼ˆSingle-linkï¼‰ç®—æ³•ï¼Œå…¨é“¾æŽ¥ç®—æ³•ï¼ˆcomplete-linkï¼‰æˆ–å‡é“¾æŽ¥ç®—æ³•ï¼ˆaverage-linkï¼‰ã€‚å•é“¾æŽ¥ç®—æ³•ç”¨ä¸¤ä¸ªèšç±»ç°‡ä¸­æœ€è¿‘çš„æ ·æœ¬è·ç¦»ä½œä¸ºä¸¤ä¸ªç°‡ä¹‹é—´çš„è·ç¦»ï¼›è€Œå…¨é“¾æŽ¥ä½¿ç”¨è®¡ç®—ä¸¤ä¸ªèšç±»ç°‡ä¸­æœ€è¿œçš„æ ·æœ¬è·ç¦»ï¼›å‡é“¾æŽ¥ç®—æ³•ä¸­ä¸¤ä¸ªèšç±»ä¹‹é—´çš„è·ç¦»ç”±ä¸¤ä¸ªç°‡ä¸­æ‰€æœ‰çš„æ ·æœ¬å…±åŒå†³å®šã€‚</p><ol><li>æ¯ä¸€ä¸ªæ ·æœ¬ç‚¹è§†ä¸ºä¸€ä¸ªç°‡ï¼›</li><li>è®¡ç®—å„ä¸ªç°‡ä¹‹é—´çš„è·ç¦»ï¼Œæœ€è¿‘çš„ä¸¤ä¸ªç°‡èšåˆæˆä¸€ä¸ªæ–°ç°‡ï¼›</li><li>é‡å¤ä»¥ä¸Šè¿‡ç¨‹ç›´è‡³æœ€åŽåªæœ‰ä¸€ç°‡ã€‚</li></ol></blockquote></li></ol><h3 id="Gaussian-Mixed-Model-æ¦‚çŽ‡èšç±»"><a href="#Gaussian-Mixed-Model-æ¦‚çŽ‡èšç±»" class="headerlink" title="Gaussian Mixed Model (æ¦‚çŽ‡èšç±»)"></a>Gaussian Mixed Model (æ¦‚çŽ‡èšç±»)</h3><blockquote><p><a href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/">https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/</a></p></blockquote><ol><li>é«˜æ–¯æ··åˆæ¨¡åž‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæ˜¯å¦‚ä½•è¿­ä»£è®¡ç®—çš„ï¼Ÿ<blockquote><p>é«˜æ–¯æ··åˆæ¨¡åž‹å‡è®¾æ•°æ®å¯ä»¥çœ‹ä½œæ˜¯ä»Žå¤šä¸ªé«˜æ–¯åˆ†å¸ƒä¸­ç”Ÿæˆå‡ºæ¥çš„ã€‚æ±‚è§£æ­¥éª¤å¦‚ä¸‹:</p><ol><li>E step: æ ¹æ®å½“å‰å‚æ•°ï¼Œè®¡ç®—æ¯ä¸ªç‚¹å±žäºŽå„ä¸ªé«˜æ–¯åˆ†å¸ƒçš„æ¦‚çŽ‡</li><li>M step: ä½¿ç”¨ä¸Šè¿° E step æ±‚å¾—çš„æ¦‚çŽ‡ï¼Œè®¡ç®—æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒçš„åŠ æƒå¹³å‡å‚æ•°ã€‚</li></ol></blockquote></li></ol><h3 id="èšç±»ç®—æ³•çš„è¯„ä¼°"><a href="#èšç±»ç®—æ³•çš„è¯„ä¼°" class="headerlink" title="èšç±»ç®—æ³•çš„è¯„ä¼°"></a>èšç±»ç®—æ³•çš„è¯„ä¼°</h3><ol><li>ä»¥èšç±»ç®—æ³•ä¸ºä¾‹ï¼Œå‡è®¾æ²¡æœ‰å¤–éƒ¨æ ‡ç­¾æ•°æ®ï¼Œå¦‚ä½•è¯„ä¼°ä¸¤ä¸ªèšç±»ç®—æ³•çš„ä¼˜åŠ£ï¼Ÿ<blockquote><p>åœ¨æ— ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯é€šè¿‡è€ƒå¯Ÿç°‡çš„åˆ†ç¦»æƒ…å†µå’Œç°‡çš„ç´§å‡‘æƒ…å†µæ¥è¯„ä¼°èšç±»çš„æ•ˆæžœã€‚</p><ol><li>è½®å»“ç³»æ•°ï¼šç»™å®šä¸€ä¸ªç‚¹pï¼Œæ”¹ç‚¹çš„è½®å»“ç³»æ•°å®šä¹‰ä¸º<script type="math/tex; mode=display">s(p) = \frac{b(p) - a(p)}{max{a(p), b(p)}}</script>å…¶ä¸­ï¼Œ$a(p)$æ˜¯ç‚¹$p$ä¸ŽåŒä¸€ç°‡ä¸­å…¶ä»–ç‚¹$p\prime$ä¹‹é—´çš„å¹³å‡è·ç¦»ï¼›$b(p)$æ˜¯ç‚¹$p$ä¸Žå¦ä¸€ä¸åŒç°‡ä¸­çš„ç‚¹ä¹‹é—´çš„æœ€å°å¹³å‡è·ç¦»ï¼ˆå¦‚æžœæœ‰nä¸ªç°‡ï¼Œåˆ™åªè®¡ç®—å’Œç‚¹pæœ€æŽ¥è¿‘çš„ä¸€ç°‡ä¸­çš„ç‚¹ä¸Žè¯¥ç‚¹çš„å¹³å‡è·ç¦»). $a(p)$ååº”çš„æ˜¯$p$æ‰€å±žçš„ç°‡ä¸­æ•°æ®çš„ç´§å¯†ç¨‹åº¦ï¼Œ$b(p)$ååº”çš„æ˜¯è¯¥ç°‡ä¸Žå…¶ä»–ä¸´è¿‘ç°‡çš„åˆ†ç¦»ç¨‹åº¦ã€‚æ˜¾ç„¶ï¼Œ$b(p)$è¶Šå¤§ï¼Œ$a(p)$è¶Šå°ï¼Œå¯¹åº”çš„èšç±»çš„è´¨é‡è¶Šå¥½ã€‚</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      æ— ç›‘ç£å­¦ä¹ ï¼ŒK-Meansï¼ŒDBSCANSï¼ŒBirchï¼ŒGMM
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Decomposition</title>
    <link href="https://zhangruochi.com/ML-Interview-Decomposition/2020/05/26/"/>
    <id>https://zhangruochi.com/ML-Interview-Decomposition/2020/05/26/</id>
    <published>2020-05-25T22:56:23.000Z</published>
    <updated>2020-05-28T22:52:58.124Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PCA-æœ€å¤§æ–¹å·®ç†è®º"><a href="#PCA-æœ€å¤§æ–¹å·®ç†è®º" class="headerlink" title="PCA æœ€å¤§æ–¹å·®ç†è®º"></a>PCA æœ€å¤§æ–¹å·®ç†è®º</h2><ol><li><p>å¦‚ä½•å®šä¹‰ä¸»æˆåˆ†ï¼Ÿä»Žè¿™ç§å®šä¹‰å‡ºå‘ï¼Œå¦‚ä½•è®¾è®¡ç›®æ ‡å‡½æ•°ä½¿å¾—é™ç»´è¾¾åˆ°æå–ä¸»æˆåˆ†çš„ç›®çš„ï¼Ÿé’ˆå¯¹è¿™ä¸ªç›®æ ‡å‡½æ•°ï¼Œå¦‚ä½•å¯¹ PCA é—®é¢˜è¿›è¡Œæ±‚è§£ï¼Ÿ</p><blockquote><p>åœ¨ä¿¡å·å¤„ç†é¢†åŸŸï¼Œæˆ‘ä»¬è®¤ä¸ºä¿¡å·å…·æœ‰è¾ƒå¤§çš„æ–¹å·®ï¼Œå™ªå£°å…·æœ‰è¾ƒå°çš„æ–¹å·®ï¼Œä¿¡å·ä¸Žå™ªå£°ä¹‹æ¯”æˆä¸ºä¿¡å™ªæ¯”ã€‚ä¿¡å™ªæ¯”è¶Šå¤§æ„å‘³ç€æ•°æ®çš„è´¨é‡è¶Šå¥½ã€‚x æŠ•å½±ä¹‹åŽçš„æ–¹å·®å°±æ˜¯åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ï¼Œæœ€ä½³æŠ•å½±æ–¹å‘ä¹Ÿå°±æ˜¯åæ–¹å·®çŸ©é˜µæœ€å¤§çš„ç‰¹å¾å€¼ã€‚è‡³æ­¤ï¼Œ<br>PCA çš„æ±‚è§£æ–¹æ³•ä¸ºï¼š</p><ol><li>å¯¹æ ·æœ¬æ•°æ®è¿›è¡Œä¸­å¿ƒåŒ–å¤„ç†</li><li>æ±‚æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µ</li><li>å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå°†ç‰¹å¾å€¼ä»Žå¤§åˆ°å°æŽ’åˆ—</li><li>åŽ»ç‰¹å¾å€¼å‰$d$å¤§å¯¹åº”çš„ç‰¹å¾å‘é‡$w_1,w_2,â€¦,w_d$,<br>é€šè¿‡ä»¥ä¸‹æ˜ å°„å°†nç»´æ ·æœ¬æ˜ å°„åˆ°$d$ç»´åº¦ã€‚<script type="math/tex; mode=display">x_i\prime = \left[\begin{matrix}& w_1^{T}x_i \\& w_2^{T}x_i \\& w_3^{T}x_i \\& \cdots \\& w_d^{T}x_i \end{matrix}\right]</script></li></ol></blockquote></li><li><p>PCA çš„ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ<br>åœ¨ PCA ä¸­ï¼Œç®—æ³•æ²¡æœ‰è€ƒè™‘æ•°æ®çš„æ ‡ç­¾ï¼ˆç±»åˆ«ï¼‰ï¼Œåªæ˜¯æŠŠæ•°æ®æ˜ å°„åˆ°ä¸€äº›æ–¹å·®æ¯”è¾ƒå¤§çš„æ–¹å‘è€Œå·²ã€‚å¦‚ä¸‹å›¾ï¼ŒPCA ç®—æ³•ä¼šæŠŠä¸¤ä¸ªç±»åˆ«çš„æ•°æ®æ˜ å°„åˆ°yè½´ï¼Œä½¿å¾—åˆ†ç±»æ•ˆæžœç‰¹åˆ«å·®ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></li></ol><h2 id="LDA-çº¿æ€§åˆ¤åˆ«åˆ†æž"><a href="#LDA-çº¿æ€§åˆ¤åˆ«åˆ†æž" class="headerlink" title="LDA çº¿æ€§åˆ¤åˆ«åˆ†æž"></a>LDA çº¿æ€§åˆ¤åˆ«åˆ†æž</h2><ol><li><p>å¯¹äºŽå…·æœ‰ç±»åˆ«æ ‡ç­¾çš„æ•°æ®ï¼Œæ˜ å¸¦å¦‚ä½•è®¾è®¡ç›®æ ‡å‡½æ•°ä½¿å¾—é™ç»´çš„è¿‡ç¨‹ä¸­ä¸æŸå¤±ç±»åˆ«ä¿¡æ¯ï¼Ÿåœ¨è¿™ç§ç›®æ ‡ä¸‹ï¼Œåº”å½“å¦‚ä½•æ±‚è§£ï¼Ÿ</p><blockquote><p>æŠ•å½±åŽæ¯ç±»å†…éƒ¨æ–¹å·®æœ€å°ï¼Œç±»é—´æ–¹å·®æœ€å¤§<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><br>ç±»å†…æ•£åº¦çŸ©é˜µ$s_w$:</p><script type="math/tex; mode=display">S_w = \Sigma_0 + \Sigma_1 = \sum\limits_{x \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum\limits_{x \in X_1}(x-\mu_1)(x-\mu_1)^T</script><p>ç±»é—´æ•£åº¦çŸ©é˜µ$s_b$:</p><script type="math/tex; mode=display">S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T</script><p>LDA çš„ä¼˜åŒ–ç›®æ ‡ï¼š</p><script type="math/tex; mode=display">\underbrace{arg\;max}_w\;\;J(w) = \frac{w^TS_bw}{w^TS_ww}</script></blockquote></li><li><p>LDA ç®—æ³•çš„æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><ol><li>è®¡ç®—ç±»å†…æ•£åº¦çŸ©é˜µ$S_w$</li><li>è®¡ç®—ç±»é—´æ•£åº¦çŸ©é˜µ$S_b$</li><li>è®¡ç®—çŸ©é˜µ$S_w^{-1}S_b$</li><li>è®¡ç®—$S_w^{-1}S_b$çš„æœ€å¤§çš„dä¸ªç‰¹å¾å€¼å’Œå¯¹åº”çš„dä¸ªç‰¹å¾å‘é‡$(w_1,w_2,â€¦w_d)$å¾—åˆ°æŠ•å½±çŸ©é˜µ$W$.</li><li>å¯¹æ ·æœ¬é›†ä¸­çš„æ¯ä¸€ä¸ªæ ·æœ¬ç‰¹å¾$x_i$,è½¬åŒ–ä¸ºæ–°çš„æ ·æœ¬$z_i=W^Tx_i$.</li></ol></blockquote></li><li><p>LDA ä¸Ž PCA ä½œä¸ºç»å…¸çš„é™ç»´ç®—æ³•ï¼Œå¦‚ä½•ä»Žåº”ç”¨çš„è§’åº¦åˆ†æžå…¶åŽŸç†çš„å¼‚åŒï¼Ÿ</p><blockquote><p>ä»Žç›®æ ‡å‡ºå‘ï¼ŒPCA é€‰æ‹©çš„æ˜¯æŠ•å½±åŽæ•°æ®æ–¹å·®æœ€å¤§çš„æ–¹å‘ï¼Œç”±äºŽå®ƒæ˜¯æ— ç›‘ç£çš„ï¼Œå› æ­¤ PCA å‡è®¾æ–¹å·®è¶Šå¤§ï¼Œä¿¡æ¯é‡è¶Šå¤šï¼Œç”¨ä¸»æˆåˆ†æ¥è¡¨ç¤ºåŽŸå§‹æ•°æ®å¯ä»¥åŽ»é™¤ç”¨äºŽçš„ç»´åº¦ï¼Œè¾¾åˆ°é™ç»´ã€‚è€Œ LDAé€‰æ‹©çš„æ˜¯æŠ•å½±åŽç±»å†…æ–¹å·®å°ã€ç±»é—´æ–¹å·®å¤§çš„æ–¹å‘ã€‚å…¶ç”¨åˆ°äº†ç±»åˆ«ä¿¡æ¯ï¼Œä¸ºäº†æ‰¾åˆ°æ•°æ®ä¸­å…·æœ‰åˆ¤åˆ«æ€§çš„ç»´åº¦ï¼Œä½¿å¾—åŽŸå§‹æ•°æ®åœ¨è¿™äº›æ–¹å‘ä¸ŠæŠ•å½±åŽï¼Œä¸åŒç±» jinå°½å¯èƒ½åŒºåˆ†å¼€ã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œæˆ‘ä»¬æƒ³ä»Žä¸€æ®µéŸ³é¢‘ä¸­æå–äººçš„è¯­éŸ³ä¿¡å·ï¼Œè¿™æ—¶å¯ä»¥ä½¿ç”¨ PCA å…ˆè¿›è¡Œé™ç»´ï¼Œè¿‡æ»¤æŽ‰ä¸€äº›å›ºå®šé¢‘çŽ‡çš„åŒ—äº¬å™ªå£°ã€‚ä½†å¦‚æžœæˆ‘ä»¬çš„éœ€æ±‚æ˜¯ä»Žè¿™æ®µéŸ³é¢‘ä¸­åŒºåˆ†å‡ºå£°éŸ³å±žäºŽå“ªä¸ªäººï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥ä½¿ç”¨ LDA å¯¹æ•°æ®è¿›è¡Œé™ç»´ï¼Œä½¿å¾—æ¯ä¸ªäººçš„è¯­éŸ³ä¿¡å·å…·æœ‰åŒºåˆ†æ€§ã€‚<br><strong>ä»Žåº”ç”¨çš„è§’åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æŽŒæ¡ä¸€ä¸ªåŸºæœ¬çš„åŽŸåˆ™â€”å¯¹æ— ç›‘ç£çš„ä»»åŠ¡ä½¿ç”¨ PCA è¿›è¡Œé™ç»´ï¼Œå¯¹æœ‰ç›‘ç£çš„åˆ™åº”ç”¨ LDA</strong></p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      Decomposition, PCA,LDA
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Classicial-Algorithms</title>
    <link href="https://zhangruochi.com/ML-Interview-Classicial-Algorithms/2020/05/25/"/>
    <id>https://zhangruochi.com/ML-Interview-Classicial-Algorithms/2020/05/25/</id>
    <published>2020-05-25T00:28:33.000Z</published>
    <updated>2020-05-28T22:48:42.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ç»å…¸ç®—æ³•"><a href="#ç»å…¸ç®—æ³•" class="headerlink" title="ç»å…¸ç®—æ³•"></a>ç»å…¸ç®—æ³•</h2><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><blockquote><p>å‚è€ƒ <a href="https://zhuanlan.zhihu.com/p/35755150" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35755150</a></p></blockquote><p>SVMçš„åŸºæœ¬åž‹ï¼š</p><script type="math/tex; mode=display">min_{w,b} = \frac{1}{2} ||w||^{2}</script><script type="math/tex; mode=display">s.t. \quad y_i(w^T x_i + b) \leq 1, i=1,2,3...m</script><ol><li><p>ç©ºé—´ä¸Šçº¿æ€§å¯åˆ†çš„ä¸¤ç‚¹ï¼Œåˆ†åˆ«å‘svmçš„è¶…å¹³é¢åšæŠ•å½±ï¼ŒæŠ•å½±çš„ç‚¹åœ¨è¶…å¹³é¢ä¸Šä¾ç„¶çº¿æ€§å¯åˆ†å—ï¼Ÿ</p><blockquote><p>ä¸€å®šçº¿æ€§ä¸å¯åˆ†</p></blockquote></li><li><p>ç¡¬é—´éš”å’Œè½¯é—´éš”æ˜¯æŒ‡ä»€ä¹ˆï¼Ÿ</p><blockquote><p>SVMçš„åŸºæœ¬å½¢æ€æ˜¯ä¸€ä¸ªç¡¬é—´éš”åˆ†ç±»å™¨ï¼Œå®ƒè¦æ±‚æ‰€æœ‰æ ·æœ¬éƒ½æ»¡è¶³ç¡¬é—´éš”çº¦æŸ(å³å‡½æ•°é—´éš”è¦å¤§äºŽ1)ï¼Œæ‰€ä»¥å½“æ•°æ®é›†æœ‰å™ªå£°ç‚¹æ—¶ï¼ŒSVMä¸ºäº†æŠŠå™ªå£°ç‚¹ä¹Ÿåˆ’åˆ†æ­£ç¡®ï¼Œè¶…å¹³é¢å°±ä¼šå‘å¦å¤–ä¸€ä¸ªç±»çš„æ ·æœ¬é æ‹¢ï¼Œè¿™å°±ä½¿å¾—åˆ’åˆ†è¶…å¹³é¢çš„å‡ ä½•é—´è·å˜å°ï¼Œé™ä½Žæ¨¡åž‹çš„æ³›åŒ–æ€§èƒ½ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå½“å™ªå£°ç‚¹æ··å…¥å¦å¤–ä¸€ä¸ªç±»æ—¶ï¼Œå¯¹äºŽç¡¬é—´éš”åˆ†ç±»å™¨è€Œè¨€ï¼Œè¿™å°±å˜æˆäº†ä¸€ä¸ªçº¿æ€§ä¸å¯åˆ†çš„é—®é¢˜ï¼ŒäºŽæ˜¯å°±ä½¿ç”¨æ ¸æŠ€å·§ï¼Œé€šè¿‡å°†æ ·æœ¬æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ä½¿å¾—æ ·æœ¬çº¿æ€§å¯åˆ†ï¼Œè¿™æ ·å¾—åˆ°ä¸€ä¸ªå¤æ‚æ¨¡åž‹ï¼Œå¹¶ç”±æ­¤å¯¼è‡´è¿‡æ‹Ÿåˆï¼ˆåŽŸæ ·æœ¬ç©ºé—´å¾—åˆ°çš„åˆ’åˆ†è¶…å¹³é¢ä¼šæ˜¯å¼¯å¼¯æ›²æ›²çš„ï¼Œå®ƒç¡®å®žå¯ä»¥æŠŠæ‰€æœ‰æ ·æœ¬éƒ½åˆ’åˆ†æ­£ç¡®ï¼Œä½†å¾—åˆ°çš„æ¨¡åž‹åªå¯¹è®­ç»ƒé›†æœ‰æ•ˆï¼‰ã€‚<br>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒSVMé€šè¿‡å¼•å…¥æ¾å¼›å˜é‡æž„é€ äº†è½¯é—´éš”åˆ†ç±»å™¨ï¼Œå®ƒå…è®¸åˆ†ç±»å™¨å¯¹ä¸€äº›æ ·æœ¬çŠ¯é”™ï¼Œå…è®¸ä¸€äº›æ ·æœ¬ä¸æ»¡è¶³ç¡¬é—´éš”çº¦æŸæ¡ä»¶ï¼Œè¿™æ ·åšå¯ä»¥é¿å…SVMåˆ†ç±»å™¨è¿‡æ‹Ÿåˆï¼ŒäºŽæ˜¯ä¹Ÿå°±é¿å…äº†æ¨¡åž‹è¿‡äºŽå¤æ‚ï¼Œé™ä½Žäº†æ¨¡åž‹å¯¹å™ªå£°ç‚¹çš„æ•æ„Ÿæ€§ï¼Œæå‡äº†æ¨¡åž‹çš„æ³›åŒ–æ€§èƒ½ã€‚<br>å› ä¸ºæ¾å¼›å˜é‡æ˜¯éžè´Ÿçš„ï¼Œå› æ­¤æ ·æœ¬çš„å‡½æ•°é—´éš”å¯ä»¥æ¯”1å°ã€‚å‡½æ•°é—´éš”æ¯”1å°çš„æ ·æœ¬è¢«å«åšç¦»ç¾¤ç‚¹ï¼Œæˆ‘ä»¬æ”¾å¼ƒäº†å¯¹ç¦»ç¾¤ç‚¹çš„ç²¾ç¡®åˆ†ç±»ï¼Œè¿™å¯¹æˆ‘ä»¬çš„åˆ†ç±»å™¨æ¥è¯´æ˜¯ç§æŸå¤±ã€‚ä½†æ˜¯æ”¾å¼ƒè¿™äº›ç‚¹ä¹Ÿå¸¦æ¥äº†å¥½å¤„ï¼Œé‚£å°±æ˜¯è¶…å¹³é¢ä¸å¿…å‘è¿™äº›ç‚¹çš„æ–¹å‘ç§»åŠ¨ï¼Œå› è€Œå¯ä»¥å¾—åˆ°æ›´å¤§çš„å‡ ä½•é—´éš”ï¼ˆåœ¨ä½Žç»´ç©ºé—´çœ‹æ¥ï¼Œåˆ†ç±»è¾¹ç•Œä¹Ÿæ›´å¹³æ»‘ï¼‰ã€‚æ˜¾ç„¶æˆ‘ä»¬å¿…é¡»æƒè¡¡è¿™ç§æŸå¤±å’Œå¥½å¤„ã€‚</p></blockquote></li><li><p>æ¾å¼›å˜é‡å’Œæƒ©ç½šå› å­æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>æ¾å¼›å˜é‡ï¼šæ¾å¼›å˜é‡è¡¨ç¤ºæ ·æœ¬ç¦»ç¾¤çš„ç¨‹åº¦ï¼Œæ¾å¼›å˜é‡è¶Šå¤§ï¼Œç¦»ç¾¤è¶Šè¿œï¼Œæ¾å¼›å˜é‡ä¸ºé›¶ï¼Œåˆ™æ ·æœ¬æ²¡æœ‰ç¦»ç¾¤ã€‚<br>æƒ©ç½šå› å­ï¼šæƒ©ç½šå› å­è¡¨ç¤ºæˆ‘ä»¬æœ‰å¤šé‡è§†ç¦»ç¾¤ç‚¹å¸¦æ¥çš„æŸå¤±ï¼Œå½“Cå–æ— ç©·å¤§æ—¶ï¼Œä¼šè¿«ä½¿è¶…å¹³é¢å°†æ‰€æœ‰çš„æ ·æœ¬éƒ½åˆ’åˆ†æ­£ç¡®ï¼Œè¿™å°±é€€åŒ–æˆäº†ç¡¬é—´éš”åˆ†ç±»å™¨ã€‚</p></blockquote></li><li><p>æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œä¸»è¦è¿ç”¨äºŽè§£å†³ä¼˜åŒ–é—®é¢˜ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³å°±æ˜¯ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æž„é€ ä¸€ä¸ªæ–°çš„ä¼˜åŒ–å‡½æ•°å°†åŽŸæœ¬çš„çº¦æŸä¼˜åŒ–é—®é¢˜è½¬æ¢æˆç­‰ä»·çš„æ— çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚</p></blockquote></li><li><p>ä»€ä¹ˆæ˜¯å¯¹å¶é—®é¢˜?</p><blockquote><p>å¸¸ä¸€ä¸ªä¼˜åŒ–é—®é¢˜å¯ä»¥ä»Žä¸¤ä¸ªè§’åº¦æ¥è€ƒè™‘ï¼Œå³ä¸»é—®é¢˜(primal problem)å’Œå¯¹å¶é—®é¢˜(dual problem)ã€‚åœ¨çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œå¸¸å¸¸åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§å°†åŽŸå§‹é—®é¢˜ï¼ˆä¸»é—®é¢˜ï¼‰è½¬æ¢æˆå¯¹å¶é—®é¢˜ï¼Œé€šè¿‡è§£å¯¹å¶é—®é¢˜æ¥å¾—åˆ°åŽŸå§‹é—®é¢˜çš„è§£ã€‚è¿™æ ·åšæ˜¯å› ä¸ºå¯¹å¶é—®é¢˜çš„å¤æ‚åº¦å¾€å¾€ä½ŽäºŽä¸»é—®é¢˜ã€‚</p></blockquote></li><li><p>ä»€ä¹ˆæ˜¯ kernel trick?</p><blockquote><p>$x_i$ å’Œ $x_j$ åœ¨ç‰¹å¾ç©ºé—´çš„å…§ç§¯ç­‰äºŽå®ƒä»¬åœ¨åŽŸå§‹çš„æ ·æœ¬ç©ºé—´é€šè¿‡ $k(x_i,x_j)$ è®¡ç®—çš„ç»“æžœã€‚æœ‰äº†è¿™æ ·çš„å‡½æ•°ï¼Œæˆ‘ä»¬ä¸å¿…åŽ»è®¡ç®—é«˜ç»´ç”šè‡³æ— ç©·ç»´ç‰¹å¾ç©ºé—´ä¸­çš„å…§ç§¯ã€‚ d<br>SVM åŸºæœ¬å¼çš„å¯¹å¶é—®é¢˜ä¸º: </p><script type="math/tex; mode=display">max_{\alpha} \sum_{i=1}^{m}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_i y_j \Phi{x_i}^{T}\Phi_{x_j}</script><script type="math/tex; mode=display">s.t. \sum_{i=1}^{m}\alpha_i y_i = 0</script><script type="math/tex; mode=display">\alpha_i \geq 0, i = 1,2,...,m.</script></blockquote></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><h3 id="é€»è¾‘å›žå½’"><a href="#é€»è¾‘å›žå½’" class="headerlink" title="é€»è¾‘å›žå½’"></a>é€»è¾‘å›žå½’</h3><ol><li><p>ä»€ä¹ˆæ˜¯é€»è¾‘å›žå½’ï¼Ÿ</p><blockquote><p>å¯¹æ•°å‡ çŽ‡å›žå½’ã€‚å¯¹é€»è¾‘å›žå½’çš„å…¬å¼è¿›è¡Œæ•´ç†ï¼Œå¾—åˆ°:</p><script type="math/tex; mode=display">log\frac{p}{1-p} = \theta^{T}x</script><script type="math/tex; mode=display">p = P(y=1 | x)</script><p>é€»è¾‘å›žå½’é€šè¿‡æžå¤§ä¼¼ç„¶æ¥å¾—åˆ°æœ€ä½³å‚æ•°</p><script type="math/tex; mode=display">L(\theta) = \prod_{i:y_{i}=1}p(x_{i})\prod_{i^{\prime}:y_{i^{\prime}}=0}(1-p(x_{i^{\prime}}))</script></blockquote></li><li><p>ä½¿ç”¨é€»è¾‘å›žå½’å¤„ç†å¤šæ ‡ç­¾çš„åˆ†ç±»é—®é¢˜æ—¶ï¼Œæœ‰å“ªäº›å¸¸ç”¨åšæ³•ï¼Ÿ</p><blockquote><ol><li>å¦‚æžœä¸€ä¸ªæ ·æœ¬åªå¯¹åº”ä¸€ä¸ªæ ‡ç­¾ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨ sofmax regression</li><li>å½“å­˜åœ¨æ ·æœ¬å±žäºŽå¤šä¸ªæ ‡ç­¾çš„æƒ…å†µï¼Œå¯ä»¥è®­ç»ƒ$i$ä¸ªåˆ†ç±»å™¨ï¼Œç¬¬$i$ä¸ªåˆ†ç±»å™¨ç”¨ä»¥åŒºåˆ†æ¯ä¸ªæ ·æœ¬æ˜¯å¦å¯ä»¥å½’ä¸ºç¬¬iç±»ã€‚å¯ä»¥è®­ç»ƒ softmax regression. è®¾å®šä¸€ä¸ª thresholdï¼Œåˆ¤æ–­æ¯ä¸ªç±»åˆ«çš„æ¦‚çŽ‡æ˜¯å¦é«˜äºŽ threshold.</li></ol></blockquote></li></ol><h3 id="å†³ç­–æ ‘"><a href="#å†³ç­–æ ‘" class="headerlink" title="å†³ç­–æ ‘"></a>å†³ç­–æ ‘</h3><ol><li><p>å†³ç­–æ ‘æœ‰å“ªäº›å¯å‘å‡½æ•°ï¼Ÿ</p><blockquote><p>ID3ï¼ˆæœ€å¤§ä¿¡æ¯å¢žç›Šï¼‰ è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ä¿¡æ¯å¢žç›Šï¼Œç„¶åŽé€‰æ‹©ä¿¡æ¯å¢žç›Šæœ€å¤§çš„ç‰¹å¾æ¥åˆ’åˆ†æ ·æœ¬ï¼Œå®Œæˆå†³ç­–æ ‘çš„å¢žé•¿ã€‚<br>C4.5ï¼ˆæœ€å¤§ä¿¡æ¯å¢žç›Šæ¯”ï¼‰ã€‚<br>CART(æœ€å¤§åŸºå°¼æŒ‡æ•°)</p></blockquote></li><li><p>ä¿¡æ¯ç†µã€ä¿¡æ¯å¢žç›Šã€ä¿¡æ¯å¢žç›Šæ¯”ã€æœ€å¤§åŸºå°¼ç³»æ•°æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><ol><li><strong>ä¿¡æ¯ç†µ</strong> æ˜¯åº¦é‡æ ·æœ¬é›†åˆä¸ç¡®å®šåº¦ï¼ˆçº¯åº¦ï¼‰çš„æœ€å¸¸ç”¨çš„æŒ‡æ ‡ã€‚<br>å½“å‰æ ·æœ¬é›†åˆ D ä¸­ç¬¬ k ç±»æ ·æœ¬æ‰€å çš„æ¯”ä¾‹ä¸º pk ï¼Œåˆ™ D çš„ä¿¡æ¯ç†µå®šä¹‰ä¸º<script type="math/tex; mode=display">Ent(D) = - \sum_{K=1}^{|y|}p_k log_2^{p_k}</script></li><li><strong>ä¿¡æ¯å¢žç›Š</strong> è¡¨ç¤ºå¾—çŸ¥å±žæ€§ a çš„ä¿¡æ¯è€Œä½¿å¾—æ ·æœ¬é›†åˆä¸ç¡®å®šåº¦å‡å°‘çš„ç¨‹åº¦<br>å‡è®¾ç¦»æ•£å±žæ€§ a æœ‰ V ä¸ªå¯èƒ½çš„å–å€¼ {a1,a2,â€¦,aV}ï¼›æ ·æœ¬é›†åˆä¸­ï¼Œå±žæ€§ a ä¸Šå–å€¼ä¸º av çš„æ ·æœ¬é›†åˆï¼Œè®°ä¸º Dvã€‚<script type="math/tex; mode=display">Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{D^v}{D}Ent(D^v)</script>ä¿¡æ¯å¢žç›ŠçŽ‡ = ä¿¡æ¯å¢žç›Š/IV(a),è¯´æ˜Žä¿¡æ¯å¢žç›ŠçŽ‡æ˜¯ä¿¡æ¯å¢žç›Šé™¤äº†ä¸€ä¸ªå±žæ€§açš„å›ºæœ‰å€¼å¾—æ¥çš„ã€‚<script type="math/tex; mode=display">IV(a) = -sum_{v=1}^{v}\frac{D^v}{D}log_2\frac{D^v}{D}</script><strong>Gini</strong>æè¿°çš„æ˜¯æ•°æ®çš„çº¯åº¦<script type="math/tex; mode=display">Gini(D) = 1 - sum_{k=1}^{n}(\frac{|C_k|}{|D|})^2</script>ç‰¹å¾ A çš„ GiniæŒ‡æ•°å®šä¹‰ä¸º:<script type="math/tex; mode=display">Gini(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}Gini(D_i)</script></li></ol></blockquote></li><li><p>ID3,C4.5,CART å„è‡ªçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>ID3å€¾å‘äºŽå–å€¼è¾ƒå¤šçš„ç‰¹å¾,å› ä¸ºä¿¡æ¯å¢žç›Šæ”¾æ˜ çš„æ˜¯ç»™å®šæ¡ä»¶ä»¥åŽä¸ç¡®å®šæ€§å‡å°‘çš„ç¨‹åº¦ï¼Œç‰¹å¾å–å€¼è¶Šå¤šå°±æ„å‘³ç€ç¡®å®šæ€§è¶Šé«˜ï¼Œä¹Ÿå°±æ˜¯æ¡ä»¶ç†µè¶Šå°ã€ä¿¡æ¯å¢žç›Šè¶Šå¤§ã€‚<br>C4.5å®žé™…ä¸Šæ˜¯å¯¹ ID3 è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡å¼•å…¥ä¿¡æ¯å¢žç›Šæ¯”ï¼Œä¸€å®šç¨‹åº¦ä¸Šå¯¹å–å€¼è¾ƒå¤šçš„ç‰¹å¾è¿›è¡Œæƒ©ç½šã€é¿å… ID3 å‡ºçŽ°è¿‡æ‹Ÿåˆã€‚<br>CART ä¸Ž ID3,C4.5ä¸åŒï¼Œå®ƒæ˜¯ä¸€é¢—äºŒå‰æ ‘ï¼Œé‡‡ç”¨äºŒå…ƒåˆ†å‰²æ³•ï¼Œæ¯ä¸€æ­¥å°†æ•°æ®æŒ‰ç…§ç‰¹å¾ A çš„å–å€¼åˆ‡æˆä¸¤ä»½ï¼Œåˆ†åˆ«è¿›å…¥å·¦å³å­æ ‘ã€‚</p></blockquote></li><li><p>Cart åœ¨åš regression å’Œ classification çš„åŒºåˆ«æ˜¯ï¼Ÿ</p><blockquote><p> åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼ŒCART ä½¿ç”¨åŸºå°¼æŒ‡æ•°ï¼ˆGini indexï¼‰ä½œä¸ºé€‰æ‹©ç‰¹å¾ï¼ˆfeatureï¼‰å’Œåˆ’åˆ†ï¼ˆsplitï¼‰çš„ä¾æ®ï¼›åœ¨å›žå½’é—®é¢˜ä¸­ï¼ŒCART ä½¿ç”¨ mseï¼ˆmean square errorï¼‰æˆ–è€… maeï¼ˆmean absolute errorï¼‰ä½œä¸ºé€‰æ‹© feature å’Œ split çš„ criteriaã€‚<br>åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼ŒCARTçš„æ¯ä¸€ç‰‡å¶å­éƒ½ä»£è¡¨çš„æ˜¯ä¸€ä¸ªclassï¼›åœ¨å›žå½’é—®é¢˜ä¸­ï¼ŒCART çš„æ¯ä¸€ç‰‡å¶å­è¡¨ç¤ºçš„æ˜¯ä¸€ä¸ªé¢„æµ‹å€¼ï¼Œå–å€¼æ˜¯è¿žç»­çš„ã€‚é¢„æµ‹å€¼ä¸€èˆ¬æ˜¯è¯¥ç‰‡å¶å­æ‰€å«è®­ç»ƒé›†å…ƒç´ è¾“å‡ºçš„å‡å€¼ã€‚</p></blockquote></li><li><p>å†³ç­–æ ‘å¦‚ä½•è¿›è¡Œå‰ªæžï¼Ÿ</p><blockquote><p>é¢„å‰ªæžï¼š1. å½“æ ‘åˆ°è¾¾ä¸€å®šæ·±åº¦æ—¶ï¼Œåœæ­¢æ ‘çš„ç”Ÿé•¿ï¼›2.å½“åˆ°è¾¾å½“å‰èŠ‚ç‚¹çš„æ ·æœ¬æ•°é‡å°äºŽæŸä¸ªé˜ˆå€¼æ—¶ï¼Œåœæ­¢æ ‘çš„ç”Ÿé•¿ï¼›3. è®¡ç®—æ¯æ¬¡åˆ†è£‚æ—¶æµ‹è¯•é›†çš„å‡†ç¡®åº¦æå‡ï¼Œå½“å°äºŽæŸä¸ªé˜ˆå€¼æ—¶ä¸å†ç»§ç»­æ‰©å±•ã€‚<br>åŽå‰ªæžï¼šåŽå‰ªæžçš„æ–¹æ³•æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚ä»£ä»·å¤æ‚åº¦å‰ªæžã€æ‚²è§‚å‰ªæžã€æœ€å°è¯¯å·®å‰ªæžç­‰ã€‚</p></blockquote></li></ol><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><ol><li><p>ç®€è¿°æœ´ç´ è´å¶æ–¯çš„åŽŸç†ã€‚</p><blockquote><p>æœ´ç´ è´å¶æ–¯é‡‡ç”¨<code>å±žæ€§æ¡ä»¶ç‹¬ç«‹æ€§</code>çš„å‡è®¾ï¼Œå¯¹äºŽç»™å®šçš„å¾…åˆ†ç±»è§‚æµ‹æ•°æ®X, è®¡ç®—åœ¨Xå‡ºçŽ°çš„æ¡ä»¶ä¸‹ï¼Œå„ä¸ªç›®æ ‡ç±»å‡ºçŽ°çš„æ¦‚çŽ‡ï¼ˆå³åŽéªŒæ¦‚çŽ‡ï¼‰ï¼Œ å°†è¯¥åŽéªŒæ¦‚çŽ‡æœ€å¤§çš„ç±»ä½œä¸ºXæ‰€å±žçš„ç±»ã€‚æ–¹æ³•æ˜¯æ ¹æ®å·²æœ‰æ ·æœ¬è¿›è¡Œè´å¶æ–¯ä¼°è®¡å­¦ä¹ å‡ºå…ˆéªŒæ¦‚çŽ‡$P(Y)$å’Œæ¡ä»¶æ¦‚çŽ‡$P(X|Y)$ï¼Œè¿›è€Œæ±‚å‡ºè”åˆåˆ†å¸ƒæ¦‚çŽ‡P(XY),æœ€åŽåˆ©ç”¨è´å¶æ–¯å®šç†æ±‚è§£åŽéªŒæ¦‚çŽ‡P(Y|X).</p></blockquote></li><li><p>æœ´ç´ è´å¶æ–¯â€œæœ´ç´ â€åœ¨å“ªé‡Œï¼Ÿ</p><blockquote><p>åˆ©ç”¨è´å¶æ–¯å®šç†æ±‚è§£è”åˆæ¦‚çŽ‡P(XY)æ—¶ï¼Œéœ€è¦è®¡ç®—æ¡ä»¶æ¦‚çŽ‡P(X|Y)ã€‚åœ¨è®¡ç®—P(X|Y)æ—¶ï¼Œæœ´ç´ è´å¶æ–¯åšäº†ä¸€ä¸ªå¾ˆå¼ºçš„æ¡ä»¶ç‹¬ç«‹å‡è®¾ï¼ˆå½“Yç¡®å®šæ—¶ï¼ŒXçš„å„ä¸ªåˆ†é‡å–å€¼ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼‰ï¼Œå³</p><script type="math/tex; mode=display">P(X_1=x_1,X_2=x_2,\cdots,X_j=x_j|Y=y_k) = P(X_1=x_1|Y=y_k) * P(X_2=x_2|Y=y_k),\cdots,P(X_j=x_j|Y=y_k)</script></blockquote></li><li><p>ä»€ä¹ˆæ˜¯æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æ³•?</p><blockquote><p>æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æ³•æ˜¯æœ´ç´ è´å¶æ–¯ä¸­å¤„ç†é›¶æ¦‚çŽ‡é—®é¢˜çš„ä¸€ç§ä¿®æ­£æ–¹å¼ã€‚åœ¨è¿›è¡Œåˆ†ç±»çš„æ—¶å€™ï¼Œå¯èƒ½ä¼šå‡ºçŽ°æŸä¸ªå±žæ€§åœ¨è®­ç»ƒé›†ä¸­æ²¡æœ‰ä¸ŽæŸä¸ªç±»åŒæ—¶å‡ºçŽ°è¿‡çš„æƒ…å†µï¼Œå¦‚æžœç›´æŽ¥åŸºäºŽæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨çš„è¡¨è¾¾å¼è¿›è¡Œè®¡ç®—çš„è¯å°±ä¼šå‡ºçŽ°é›¶æ¦‚çŽ‡çŽ°è±¡ã€‚ä¸ºäº†é¿å…å…¶ä»–å±žæ€§æ‰€æºå¸¦çš„ä¿¡æ¯è¢«è®­ç»ƒé›†ä¸­æœªå‡ºçŽ°è¿‡çš„å±žæ€§å€¼â€œæŠ¹åŽ»â€ï¼Œæ‰€ä»¥æ‰ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ä¼°è®¡å™¨è¿›è¡Œä¿®æ­£ã€‚å…·ä½“çš„æ–¹æ³•æ˜¯ï¼šåœ¨åˆ†å­ä¸ŠåŠ 1,å¯¹äºŽå…ˆéªŒæ¦‚çŽ‡ï¼Œåœ¨åˆ†æ¯ä¸ŠåŠ ä¸Šè®­ç»ƒé›†ä¸­å¯èƒ½çš„ç±»åˆ«æ•°ï¼›å¯¹äºŽæ¡ä»¶æ¦‚çŽ‡ï¼Œåˆ™åœ¨åˆ†æ¯ä¸ŠåŠ ä¸Šç¬¬iä¸ªå±žæ€§å¯èƒ½çš„å–å€¼æ•°</p></blockquote></li><li><p>æœ´ç´ è´å¶æ–¯ä¸­æœ‰å“ªäº›ä¸åŒçš„æ¨¡åž‹ï¼Ÿ</p><blockquote><p>æœ´ç´ è´å¶æ–¯å«æœ‰3ç§æ¨¡åž‹ï¼Œåˆ†åˆ«æ˜¯<strong>é«˜æ–¯æ¨¡åž‹</strong>ï¼Œå¯¹è¿žç»­åž‹æ•°æ®è¿›è¡Œå¤„ç†ï¼›<strong>å¤šé¡¹å¼æ¨¡åž‹</strong>ï¼Œå¯¹ç¦»æ•£åž‹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œè®¡ç®—æ•°æ®çš„æ¡ä»¶æ¦‚çŽ‡(ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ä¼°è®¡å™¨è¿›è¡Œå¹³æ»‘çš„ä¸€ä¸ªæ¨¡åž‹)ï¼›<strong>ä¼¯åŠªåˆ©æ¨¡åž‹</strong>ï¼Œä¼¯åŠªåˆ©æ¨¡åž‹çš„å–å€¼ç‰¹å¾æ˜¯å¸ƒå°”åž‹ï¼Œå³å‡ºçŽ°ä¸ºture,ä¸å‡ºçŽ°ä¸ºfalse,åœ¨è¿›è¡Œæ–‡æ¡£åˆ†ç±»æ—¶ï¼Œå°±æ˜¯ä¸€ä¸ªå•è¯æœ‰æ²¡æœ‰åœ¨ä¸€ä¸ªæ–‡æ¡£ä¸­å‡ºçŽ°è¿‡ã€‚</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      SVM, Logistic Regression, Decision Tree
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Feature-Engineering-and-Evaluation</title>
    <link href="https://zhangruochi.com/ML-Interview-Feature-Engineering-and-Evaluation/2020/05/24/"/>
    <id>https://zhangruochi.com/ML-Interview-Feature-Engineering-and-Evaluation/2020/05/24/</id>
    <published>2020-05-24T08:02:06.000Z</published>
    <updated>2020-05-29T00:18:35.014Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ç‰¹å¾å·¥ç¨‹"><a href="#ç‰¹å¾å·¥ç¨‹" class="headerlink" title="ç‰¹å¾å·¥ç¨‹"></a>ç‰¹å¾å·¥ç¨‹</h2><ol><li><p>ä¸ºä»€ä¹ˆéœ€è¦å¯¹æ•°å€¼ç±»åž‹çš„ç‰¹å¾åšå½’ä¸€åŒ–ï¼Ÿ</p><blockquote><p>å¸¸ç”¨çš„å½’ä¸€åŒ–æœ‰ï¼šMin Max Scaler / Z-Score<br>å½“ç‰¹å¾çš„ range ä¸åŒæ—¶ï¼Œå½’ä¸€åŒ–ç‰¹å¾å¯ä»¥åŠ å¿«æ¢¯åº¦ä¸‹é™æ”¶æ•›çš„é€Ÿåº¦ã€‚PCA ç­‰ç®—æ³•çš„å‡è®¾æœ‰æ•°æ®æ˜¯å‡å€¼å‡å€¼ä¸º0,æ–¹å·®ä¸º1. </p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></blockquote></li><li><p>åº”è¯¥æ€Žæ ·å¤„ç†ç±»åˆ«ç‰¹å¾ï¼Ÿ</p><blockquote><p>Ordinal Encoding<br>One-hot Encoding<br>Binary Encoding</p></blockquote></li><li><p>ä»€ä¹ˆæ˜¯ç‰¹å¾ç»„åˆï¼Œå¦‚ä½•å¤„ç†é«˜ç»´ç»„åˆç‰¹å¾ï¼Ÿ</p><blockquote><p>ä¸¤ä¸ªæˆ–å¤šä¸ªç‰¹å¾ç»„åˆåœ¨ä¸€èµ·$(x1,\cdots, xn)$ ç­‰å½¢æˆç»„åˆç‰¹å¾.<br>ç‰¹å¾é€‰æ‹©ï¼ŒçŸ©é˜µåˆ†è§£ï¼ŒPCA.</p></blockquote></li><li><p>æ€Žæ ·æœ‰æ•ˆåœ°æ‰¾åˆ°ç»„åˆç‰¹å¾</p><blockquote><ol><li>å†³ç­–æ ‘ä»Žæ ¹èŠ‚ç‚¹åˆ°å¶å­ç»“ç‚¹çš„è·¯å¾„å¯ä»¥çœ‹æˆä¸€ç§ç‰¹å¾ç»„åˆçš„æ–¹å¼<br><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><br>Filter, wrapper, embedding ç­‰æ–¹æ³•è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå½¢æˆç»„åˆç‰¹å¾ã€‚</li></ol></blockquote></li><li><p>æœ‰å“ªäº›æ–‡æœ¬è¡¨ç¤ºæ¨¡åž‹ï¼Œå®ƒä»¬å„è‡ªçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>Bag of Word, å¸¸ç”¨ TF-IDFè¡¨ç¤ºè¯çš„æƒé‡ï¼ˆterm frequency and inverse document frequencyï¼‰; N-gram. æå–è¯ç»„; å› ä¸ºç›¸åŒçš„è¯å¯èƒ½æœ‰å¤šç§è¡¨ç¤ºï¼Œç»å¸¸ä¼šåšè¯å¹²æå–word stemming; ä¸»é¢˜æ¨¡åž‹ï¼ˆå¾—åˆ°æ¯ä¸ªä¸»é¢˜ä¸Šè¯çš„åˆ†å¸ƒç‰¹å¾ï¼‰; Word embedding ; Contextual word embeddings</p></blockquote></li><li><p>word2vecæ˜¯å¦‚ä½•å·¥ä½œçš„</p><blockquote><p>å‚è€ƒ <a href="https://zhangruochi.com/Word-Vectors/2019/12/04/">https://zhangruochi.com/Word-Vectors/2019/12/04/</a><br>CBOW æ ¹æ®ä¸Šä¸‹æ–‡æ¥é¢„æµ‹ä¸­å¿ƒè¯ï¼ŒSkip-gramæ ¹æ®ä¸­å¿ƒè¯æ¥é¢„æµ‹ä¸Šä¸‹æ–‡ã€‚ CBOW å’Œ Skip-gram éƒ½æ˜¯ç”±ä¸‰å±‚çš„ç¥žç»ç½‘ç»œç»„æˆã€‚è¾“å…¥å±‚ä¸ºNç»´ one-hot encodingï¼Œéšè—å±‚ä¸º K ç»´ã€‚åˆ™è¾“å…¥å±‚å’Œéšè—å±‚çš„ weight matrix ï¼ˆN*Kï¼‰å°±æ˜¯ embedding vector. word vector å¯ä»¥ç”±one-hot encoding ä¸Ž weight matrix ç›¸ä¹˜å¾—åˆ°ã€‚éšè—å±‚åˆ°è¾“å‡ºå±‚çš„weightg matrix ä¸º ï¼ˆK*Nï¼‰.è¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªNç»´å‘é‡ï¼Œåˆ™å¯ä»¥æ ¹æ®softmaxæ¥æ±‚æ¯ä¸ªè¯çš„æ¦‚çŽ‡ï¼Œç„¶åŽåº”ç”¨æ¢¯åº¦ä¸‹é™ã€‚<br>ç”±äºŽsoftmaxéœ€è¦å¯¹æ‰€æœ‰è¯è¿›è¡ŒéåŽ†ï¼Œè®¡ç®—é‡å¤§ã€‚æ­¤æ—¶å¯ä»¥ä½¿ç”¨negtive sampling æˆ–è€… hierarchical softmax.</p></blockquote></li><li><p>LSAï¼ˆLatent Semantic Analysisï¼‰ ç®—æ³•æ˜¯æ€Žæ ·å·¥ä½œçš„ï¼Ÿ</p><blockquote><p>LSA ç®—æ³•å…ˆç»Ÿè®¡ term-documentçŸ©é˜µï¼ˆçŸ©é˜µçš„æ¯ä¸ªå…ƒç´ ä¸ºtf-idfï¼‰è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼Œä»Žè€Œå¾—åˆ°termçš„å‘é‡è¡¨ç¤ºå’Œdocumentçš„å‘é‡è¡¨ç¤º. å…¶ç®—æ³•çš„åŸºæœ¬æµç¨‹æ˜¯ï¼š</p><ol><li>åˆ†æžæ–‡æ¡£é›†åˆï¼Œå»ºç«‹è¯æ±‡-æ–‡æœ¬çŸ©é˜µA</li><li>å¯¹è¯æ±‡-æ–‡æœ¬çŸ©é˜µè¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£</li><li>å¯¹SVDåˆ†è§£åŽçš„çŸ©é˜µè¿›è¡Œé™ç»´</li><li>ä½¿ç”¨é™ç»´åŽçš„çŸ©é˜µæž„å»ºæ½œåœ¨è¯­ä¹‰ç©ºé—´</li></ol></blockquote></li><li><p>Glove æ˜¯æ€Žæ ·å·¥ä½œçš„ï¼Ÿ</p><blockquote><ol><li>Construct co-occurrence Matrix</li><li>Construct relationships between word vectors and co-occurrence Matrix<ul><li>Let X denote the word-word co-occurrence matrix, where $X_{ij}$ indicates the number of times word j occur in the context of word i</li><li>$w_{i}$,$\tilde{w_{j}}$ is the word vector</li><li>$b_i,b_j$ is the bias term<script type="math/tex; mode=display">w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} = \log(X_{ij}) \tag{1}</script></li></ul></li><li>Construct loss function: Mean Square Loss<script type="math/tex; mode=display">J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} â€“ \log(X_{ij}) )^2</script><script type="math/tex; mode=display">f(x)=\begin{equation} \begin{cases} (x/x_{max})^{\alpha}  & \text{if} \ x < x_{max} \\ 1 & \text{otherwise} \end{cases} \end{equation}</script></li></ol></blockquote></li><li><p>LSA, word2vec, ä»¥åŠ Glove çš„åŒºåˆ«äºŽè”ç³»ï¼Ÿ</p><blockquote><p>LSAå’Œword2vecä½œä¸ºä¸¤å¤§ç±»æ–¹æ³•çš„ä»£è¡¨ï¼Œä¸€ä¸ªæ˜¯åˆ©ç”¨äº†å…¨å±€ç‰¹å¾çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ï¼Œä¸€ä¸ªæ˜¯åˆ©ç”¨å±€éƒ¨ä¸Šä¸‹æ–‡çš„æ–¹æ³•ã€‚GloVeæ¨¡åž‹å°±æ˜¯å°†è¿™ä¸¤ä¸­ç‰¹å¾åˆå¹¶åˆ°ä¸€èµ·çš„ï¼Œå³ä½¿ç”¨äº†è¯­æ–™åº“çš„å…¨å±€ç»Ÿè®¡ï¼ˆoverall statisticsï¼‰ç‰¹å¾ï¼Œä¹Ÿä½¿ç”¨äº†å±€éƒ¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆå³æ»‘åŠ¨çª—å£ï¼‰ã€‚</p></blockquote></li><li><p>å›¾åƒåˆ†ç±»æ—¶ï¼Œè®­ç»ƒæ•°æ®ä¸è¶³å¦‚ä½•å¤„ç†ã€‚</p><blockquote><p>æ•°æ®ä¸è¶³æœ‰è¿‡æ‹Ÿåˆé£Žé™©ï¼Œæˆ–è€…æ¨¡åž‹ä¸èƒ½æ”¶æ•›ã€‚</p><ol><li>å¯ä»¥ä½¿ç”¨é™ä½Žè¿‡æ‹Ÿåˆé£Žé™©çš„æŽªæ–½ã€‚å¦‚l1/l2,ç»§æ‰¿å­¦ä¹ ,dropout ç­‰</li><li>Data augmentation ï¼ˆæ—‹è½¬ã€å¹³ç§»ã€ç¼©æ”¾ã€åƒç´ æ‰°åŠ¨ã€é¢œè‰²å˜æ¢ã€æ¸…æ™°åº¦ã€å¯¹æ¯”åº¦ç­‰ï¼‰</li><li>Fine tuing or transfer learning</li><li>ç”Ÿæˆå¯¹æŠ—æ¨¡åž‹ç”Ÿæˆæ–°æ ·æœ¬</li><li>å¯¹å›¾åƒè¿›è¡Œç‰¹å¾æå–ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡åž‹ã€‚</li></ol></blockquote></li></ol><h2 id="æ¨¡åž‹è¯„ä¼°"><a href="#æ¨¡åž‹è¯„ä¼°" class="headerlink" title="æ¨¡åž‹è¯„ä¼°"></a>æ¨¡åž‹è¯„ä¼°</h2><ol><li><p>å‡†ç¡®çŽ‡çš„å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><p>å½“æ­£è´Ÿæ•°æ®ä¸å¹³è¡¡æ—¶ä¼šå¤±åŽ»æ„ä¹‰</p></blockquote></li><li><p>Precision å’Œ Recall æ€Žæ ·æƒè¡¡ï¼Ÿ</p><blockquote><p>Precision æ˜¯æŒ‡åˆ†ç±»æ­£ç¡®çš„æ­£æ ·æœ¬/æ¨¡åž‹é¢„æµ‹çš„æ­£æ ·æœ¬, Recall æ˜¯æŒ‡åˆ†ç±»æ­£ç¡®çš„æ­£æ ·æœ¬/å®žé™…çš„æ­£æ ·æœ¬ã€‚P-R æ›²çº¿æ¨ªè½´æ˜¯recallï¼Œçºµè½´æ˜¯precisionã€‚P-R æ›²çº¿æ˜¯å°†é˜™å€¼ä»Žé«˜åˆ°ä½Žæ»‘åŠ¨ç”»å‡ºçš„ã€‚<br>ä½¿ç”¨ P-R æ›²çº¿æ¥ç»¼åˆåˆ¤å®šä¸¤ä¸ªæ¨¡åž‹çš„å¥½åã€‚ F1 å’Œ ROC ä¹Ÿèƒ½ååº”æŽ’åºæ¨¡åž‹çš„å¥½åã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center></blockquote></li><li><p>RMSE çš„å±€é™æ˜¯ä»€ä¹ˆï¼Ÿæ€Žæ ·è§£å†³ï¼Ÿ</p><blockquote><p>å¦‚æžœå­˜åœ¨ä¸ªåˆ«åç¦»ç¨‹åº¦å¤§çš„å¼‚å¸¸å€¼ï¼ŒRMSEçš„æ•ˆæžœä¼šå¾ˆå·®ã€‚</p><ol><li>æ•°æ®é¢„å¤„ç†æ¸…ç† outlierï¼› </li><li>å»ºæ¨¡è€ƒè™‘å¼‚å¸¸æœºåˆ¶ï¼Œå¦‚å¼‚å¸¸ç‚¹æ£€æµ‹ï¼›</li><li>ä½¿ç”¨æ›´åˆé€‚çš„æŒ‡æ ‡å¦‚ MAPE</li></ol></blockquote></li><li><p>ä»€ä¹ˆæ˜¯ ROC æ›²çº¿ï¼Ÿ</p><blockquote><p>æ¨ªè½´æ˜¯ FPRï¼ˆFP/Nï¼‰, çºµè½´æ˜¯ TPRï¼ˆTP/Pï¼‰ã€‚ç»˜åˆ¶ ROC æ›²çº¿ï¼Œéœ€è¦å°†æ¨¡åž‹çš„è¾“å‡ºæ¦‚çŽ‡ä»Žå¤§åˆ°å°æŽ’åºï¼Œç„¶åŽåŠ¨æ€åœ°é€‰æ‹©é˜ˆå€¼ã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center></blockquote></li><li><p>AUCå¦‚ä½•è®¡ç®—ï¼Ÿ</p><blockquote><p>AUC æ˜¯ ROC æ›²çº¿ä¸‹çš„é¢ç§¯å¤§å°ï¼Œè®¡ç®—æ—¶åªç”¨æ²¿ç€ROC æ›²çº¿åšç§¯åˆ†å°±è¡Œäº†ã€‚AUCå–å€¼ä¸€èˆ¬åœ¨[0.5,1]ä¹‹é—´ï¼Œè¶Šå¤§è¶Šå¥½ã€‚</p></blockquote></li><li><p>ROC æ›²çº¿ç›¸æ¯” P-R æ›²çº¿æœ‰ä»€ä¹ˆç‰¹ç‚¹</p><blockquote><p>å½“æ­£è´Ÿæ ·æœ¬çš„åˆ†å¸ƒå‘ç”Ÿæ˜Žæ˜¾å˜åŒ–æ—¶ï¼ŒROCæ›²çº¿åŸºæœ¬ä¸å˜ã€‚å› æ­¤ ROC é€‚ç”¨çš„åœºæ™¯æ›´å¤šã€‚å¦‚ä¸‹å›¾æ˜¯å°†è´Ÿæ ·æœ¬çš„æ•°é‡å¢žåŠ  10 å€ä¹‹åŽçš„ç»“æžœã€‚</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center></blockquote></li><li><p>ä¸ºä»€ä¹ˆåœ¨ä¸€äº›åœºæ™¯ä¸­éœ€è¦ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è€Œä¸æ˜¯æ¬§æ°è·ç¦»ã€‚</p><blockquote><p>ä½™å¼¦ç›¸ä¼¼åº¦åªå…³æ³¨å‘é‡çš„å¤¹è§’ï¼Œå¹¶ä¸å…³å¿ƒå‘é‡çš„ç»å¯¹å€¼å¤§å°ï¼ŒèŒƒå›´ä¸º[-1,1]ã€‚æ¯”å¦‚åœ¨åº¦é‡ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦æ—¶ï¼Œä»¥è¯é¢‘å’Œè¯å‘é‡æœ€ä¸ºç‰¹å¾ã€‚æ–‡æœ¬è¶Šé•¿åˆ™æ¬§å¼è·ç¦»ä¸€å®šè¶Šå¤§ï¼Œä½†æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦åˆ™å¯ä»¥ä¿æŒä¸å˜ã€‚æ€»çš„æ¥è¯´ï¼Œå…³æ³¨ç›¸å¯¹å·®å¼‚ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å…³æ³¨æ•°å€¼ç»å¯¹å·®å¼‚ï¼Œä½¿ç”¨æ¬§å¼è·ç¦»ã€‚</p></blockquote></li><li><p>å¦‚ä½•è¿›è¡Œçº¿ä¸Š A/B æµ‹è¯•ï¼Ÿ</p><blockquote><p>ç”¨æˆ·åˆ†æ¡¶ï¼Œåœ¨åˆ†æ¡¶è¿‡ç¨‹ä¸­ä¸€å®šè¦ä¿è¯ç‹¬ç«‹æ€§å’Œé‡‡æ ·çš„æ— åæ€§ã€‚</p></blockquote></li><li><p>ä¸ºä»€ä¹ˆåœ¨è¿›è¡Œäº†ç¦»çº¿è¯„ä¼°åŽè¿˜è¦è¿›è¡Œçº¿ä¸Šè¯„ä¼°ï¼Ÿ</p><blockquote><ol><li>ç¦»çº¿è¯„ä¼°æ— æ³•å®Œå…¨æ¶ˆé™¤è¿‡æ‹Ÿåˆçš„å½±å“ã€‚</li><li>ç¦»çº¿è¯„ä¼°æ— æ³•å®Œå…¨è¿˜åŽŸçº¿ä¸Šçš„å·¥ç¨‹çŽ¯å¢ƒã€‚</li><li>çº¿ä¸Šç³»ç»Ÿçš„æŸäº›å•†ä¸šæŒ‡æ ‡æ— æ³•åœ¨ç¦»çº¿çŽ¯å¢ƒä¸­è¿˜åŽŸï¼Œå¦‚ç”¨æˆ·ç‚¹å‡»çŽ‡ï¼Œç•™å­˜æ—¶é•¿ç­‰ã€‚</li></ol></blockquote></li><li><p>æ¨¡åž‹è¯„ä¼°æ—¶ï¼Œæœ‰å“ªäº›ä¸»è¦çš„éªŒè¯æ–¹æ³•ï¼Œä»–ä»¬çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ</p><blockquote><ol><li>holdout. åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å‡ºçš„è¯„ä¼°æŒ‡æ ‡ä¸ŽåŽŸå§‹åˆ†ç»„æœ‰å¾ˆå¤§çš„å…³ç³»ã€‚</li><li>k-fold. æŠŠkæ¬¡è¯„ä¼°çš„å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆçš„è¯„ä¼°æŒ‡æ ‡ã€‚</li><li>ç•™ä¸€æ³•. æ¯æ¬¡ç•™ä¸‹ 1 ä¸ªæ ·æœ¬ä½œä¸ºéªŒè¯é›†ã€‚å¼€é”€å¤§ï¼Œå®žé™…å·¥ç¨‹ä¸­è¾ƒå°‘ä½¿ç”¨ã€‚</li><li>è‡ªåŠ©æ³•. åŸºäºŽè‡ªåŠ©é‡‡æ ·çš„æ–¹æ³•ï¼Œå¯¹äºŽæ€»æ•°ä¸ºnçš„æ ·æœ¬é›†åˆï¼Œè¿›è¡Œnæ¬¡æœ‰æ”¾å›žçš„éšæœºé‡‡æ ·ï¼Œå¾—åˆ°å¤§å°ä¸ºnçš„è®­ç»ƒé›†ï¼Œæ²¡æœ‰è¢«é‡‡æ ·çš„æ ·æœ¬ä½œä¸ºæµ‹è¯•é›†ã€‚</li></ol></blockquote></li><li><p>è¶…å‚æ•°æœ‰å“ªäº›è°ƒä¼˜æ–¹æ³•ï¼Ÿ</p><blockquote><ol><li>Grid Searchã€‚ ååˆ†æ¶ˆè€—è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œä¸€èˆ¬å…ˆä½¿ç”¨è¾ƒå¹¿çš„æœç´¢èŒƒå›´å’Œè¾ƒå¤§çš„æ­¥é•¿ï¼Œæˆ–è€…å…ˆç¡®å®šå¯¹æ¨¡åž‹å½±å“æœ€å¤§çš„å‚æ•°ã€‚</li><li>éšæœºæœç´¢ã€‚ ä¸šç•Œå…¬è®¤çš„Random searchæ•ˆæžœä¼šæ¯”Grid searchå¥½ã€‚ ä¾‹å¦‚å‰é¢çš„åœºæ™¯Aæœ‰2ç§é€‰æ‹©ã€Bæœ‰3ç§ã€Cæœ‰5ç§ã€è¿žç»­å€¼éšæœºé‡‡æ ·ï¼Œé‚£ä¹ˆæ¯æ¬¡åˆ†åˆ«åœ¨Aã€Bã€Cä¸­éšæœºå–å€¼ç»„åˆæˆæ–°çš„è¶…å‚æ•°ç»„åˆæ¥è®­ç»ƒã€‚è™½ç„¶æœ‰éšæœºå› ç´ ï¼Œä½†éšæœºæœç´¢å¯èƒ½å‡ºçŽ°æ•ˆæžœç‰¹åˆ«å·®ã€ä¹Ÿå¯èƒ½å‡ºçŽ°æ•ˆæžœç‰¹åˆ«å¥½ï¼Œåœ¨å°è¯•æ¬¡æ•°å’ŒGrid searchç›¸åŒçš„æƒ…å†µä¸‹ä¸€èˆ¬æœ€å€¼ä¼šæ›´å¤§ï¼Œå½“ç„¶varianceä¹Ÿæ›´å¤§ä½†è¿™ä¸å½±å“æœ€ç»ˆç»“æžœã€‚</li><li>è´å¶æ–¯ä¼˜åŒ–ç®—æ³•. æ˜¯åŸºäºŽæ•°æ®ä½¿ç”¨è´å¶æ–¯å®šç†ä¼°è®¡ç›®æ ‡å‡½æ•°çš„åŽéªŒåˆ†å¸ƒï¼Œç„¶åŽå†æ ¹æ®åˆ†å¸ƒé€‰æ‹©ä¸‹ä¸€ä¸ªé‡‡æ ·çš„è¶…å‚æ•°ç»„åˆã€‚å®ƒå……åˆ†åˆ©ç”¨äº†å‰ä¸€ä¸ªé‡‡æ ·ç‚¹çš„ä¿¡æ¯ï¼Œå…¶ä¼˜åŒ–çš„å·¥ä½œæ–¹å¼æ˜¯é€šè¿‡å¯¹ç›®æ ‡å‡½æ•°å½¢çŠ¶çš„å­¦ä¹ ï¼Œå¹¶æ‰¾åˆ°ä½¿ç»“æžœå‘å…¨å±€æœ€å¤§æå‡çš„å‚æ•°</li></ol></blockquote></li><li><p>è¿‡æ‹Ÿåˆã€æ¬ æ‹Ÿåˆå…·ä½“æ˜¯æŒ‡ä»€ä¹ˆçŽ°è±¡ï¼Ÿ</p><blockquote><p>è¿‡æ‹Ÿåˆæ˜¯æŒ‡æ•°æ®æ‹Ÿåˆè¿‡å½“ï¼Œæ¨¡åž‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨çŽ°å¥½ï¼Œä½†æ˜¯æµ‹è¯•é›†å’Œæ–°æ•°æ®ä¸Šè¡¨çŽ°å·®ã€‚æ¬ æ‹Ÿåˆæ˜¯æ¨¡åž‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šéƒ½è¡¨çŽ°ä¸å¥½ã€‚</p></blockquote></li><li><p>èƒ½å¦è¯´å‡ºé›†ä¸­é™ä½Žè¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆé£Žé™©çš„æ–¹æ³•ï¼Ÿ</p><blockquote><p>é™ä½Žè¿‡æ‹Ÿåˆï¼šèŽ·å–æ›´å¤šæ•°æ®ã€é™ä½Žæ¨¡åž‹å¤æ‚åº¦ã€æ­£åˆ™åŒ–ã€é›†æˆå­¦ä¹ <br>é™ä½Žæ¬ æ‹Ÿåˆï¼šæ·»åŠ æ–°ç‰¹å¾ã€å¢žåŠ æ¨¡åž‹è´Ÿè´£åº¦ã€å‡å°‘æ­£åˆ™åŒ–ç³»æ•°</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      ç‰¹å¾å·¥ç¨‹,æ¨¡åž‹è¯„ä¼°ç›¸å…³é—®é¢˜
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Time and Ordering</title>
    <link href="https://zhangruochi.com/Time-and-Ordering/2020/05/09/"/>
    <id>https://zhangruochi.com/Time-and-Ordering/2020/05/09/</id>
    <published>2020-05-08T17:10:01.000Z</published>
    <updated>2020-05-09T05:36:27.341Z</updated>
    
    <content type="html"><![CDATA[<p>åˆ†å¸ƒå¼ç³»ç»Ÿå’Œä¼ ç»Ÿçš„å•æœºç³»ç»Ÿä¸åŒï¼Œå½¼æ­¤æ˜¯é€šè¿‡ç½‘ç»œè€Œä¸æ˜¯â€ä¸»æ¿â€è¿žæŽ¥ã€æ¶ˆæ¯é€šè®¯æ˜¯ä¸å¯é çš„ã€‚å› æ­¤å¦‚æžœæ²¡æœ‰ä»»ä½•åŒæ­¥æœºåˆ¶ï¼ŒåŒä¸€ç³»ç»Ÿçš„æˆå‘˜ä¹‹é—´æ— æ³•ç¡®ä¿æ—¶é—´æˆ³çš„è¯¯å·®æŽ§åˆ¶åœ¨æŸä¸ªèŒƒå›´å†…ã€‚è¿™ä¸ªåŸºæœ¬æ¡ä»¶çš„ç¼ºå¤±ï¼Œä¼šç»™ä¸Šå±‚åº”ç”¨çš„è®¾è®¡å¸¦æ¥å¾ˆå¤šçš„éº»çƒ¦ã€‚æ¯”å¦‚ï¼Œä¸€ä¸ªä¸šåŠ¡æµç¨‹çš„ä¸¤ä¸ªé˜¶æ®µåˆ†åˆ«åœ¨ä¸¤å°æœºå™¨ä¸Šå¤„ç†ï¼Œè€ŒåŽåœ¨ç¬¬ä¸‰å°æœºå™¨ä¸Šå°†å¤„ç†è®°å½•joinèµ·æ¥ï¼Œå°±å¯èƒ½å› ä¸ºæ—¶é—´æˆ³çš„é—®é¢˜å¼•å‘æ··ä¹±ã€‚å¦‚ä½•åšå¥½æ—¶é—´åŒæ­¥çš„åè®®ï¼Œæˆä¸ºäº†åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„ä¸€ä¸ªåŸºæœ¬çš„é—®é¢˜ã€‚</p><p>åœ¨ç³»ç»Ÿå¯¹æ—¶çš„æ—¶å€™ï¼Œæœ‰ä¸¤ç±»åŸºæœ¬çš„åè®®ï¼Œç¬¬ä¸€ä¸ªæ˜¯å¤–éƒ¨å¯¹æ—¶ï¼Œç®€å•çš„è¯´ï¼Œå°±æ˜¯æ•´ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„æ‰€æœ‰æˆå‘˜ï¼Œä¸Žå¤–éƒ¨æŸä¸ªæŒ‡å®šçš„æºå¤´è¿›è¡Œæ—¶é—´åŒæ­¥ï¼Œç¡®ä¿ä¸Žæºå¤´çš„æ—¶é—´çš„diffåœ¨æŸä¸ªè¯¯å·®èŒƒå›´$D$å†…; å¦ä¸€ç§æ˜¯å†…éƒ¨å¯¹æ—¶ï¼Œå³å†…éƒ¨é€šè¿‡å¹¿æ’­ç­‰å„ç§æ‰‹æ®µï¼Œç¡®ä¿ç³»ç»Ÿå†…çš„æˆå‘˜ä¿©ä¿©é—´çš„æ—¶é—´è¯¯å·®åœ¨ä¸€å®šèŒƒå›´å†…ã€‚ä»Žè¿™é‡Œå¯ä»¥çœ‹å‡ºï¼Œå¦‚æžœä¸€ä¸ªé›†ç¾¤ä½¿ç”¨äº†å¤–éƒ¨å¯¹æ—¶ï¼ŒæŽ§åˆ¶è¯¯å·®åœ¨$D$ä»¥å†…ï¼Œé‚£ä¹ˆè¿™ä¸ªé›†ç¾¤å†…éƒ¨çš„æ—¶é—´çš„è¯¯å·®ï¼Œä¹Ÿä¸€å®šèƒ½å¤ŸæŽ§åˆ¶åœ¨$2D$çš„èŒƒå›´å†…ã€‚ä½†åè¿‡æ¥ä¸ä¸€å®šï¼Œå› ä¸ºæœ‰å¯èƒ½æ•´ä¸ªé›†ç¾¤ä¸Žå¤–éƒ¨çš„æ—¶é—´å­˜åœ¨å¾ˆå¤§çš„æ•´ä½“åå·®ï¼Œå°½ç®¡åœ¨å†…éƒ¨å½¼æ­¤çš„åå·®å¾ˆå°ã€‚</p><p>é‚£ä¹ˆå¦‚ä½•è¿›è¡Œæ—¶é—´çš„åŒæ­¥å‘¢ï¼Ÿè¿™é‡Œä»‹ç»ä¸¤ä¸ªç»å…¸çš„åè®®ï¼šCristianå’ŒNTPã€‚</p><h2 id="Cristian"><a href="#Cristian" class="headerlink" title="Cristian"></a>Cristian</h2><p>Cristiançš„åŸºæœ¬è¿‡ç¨‹æ˜¯è¿™æ ·çš„ï¼Œå‡å®šçŽ°åœ¨Pè¿›ç¨‹è¦ä»ŽæŽˆæ—¶æœåŠ¡å™¨SèŽ·å–æ—¶é—´ï¼Œé‚£ä¹ˆæœ€æœ´ç´ çš„åšæ³•å°±æ˜¯På‘Så‘é€è¯·æ±‚ï¼ŒSå°†è‡ªå·±çš„æ—¶é—´tè¿”å›žç»™Pï¼Œè€ŒåŽPè®¾ç½®è‡ªå·±çš„æ—¶é—´ä¸ºtã€‚è¿™ä¸ªåšæ³•å­˜åœ¨ä¸€ä¸ªå¾ˆå…³é”®çš„é—®é¢˜ï¼Œå°±æ˜¯ç”±äºŽç½‘ç»œçš„é€šè®¯æ—¶é—´æ˜¯ä¸ç¡®å®šçš„ï¼ŒPæ‹¿åˆ°tçš„æ—¶å€™ï¼Œå·²ç»ç»è¿‡äº†ä¸ç¡®å®šå¤šä¹…äº†ï¼Œæ— æ³•ä¼°è®¡ç»“æŸåŽPä¸ŽSçš„æ—¶é—´è¯¯å·®èŒƒå›´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†ç½‘ç»œé€šè®¯çš„æ—¶é—´ï¼Œå³RTT(Rount Trip Time)ä¹Ÿè€ƒè™‘è¿›æ¥ã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸‹ï¼ŒRTTæŒ‡çš„æ˜¯Pè¿›ç¨‹å‘å‡ºè¯·æ±‚ï¼Œåˆ°å¾—åˆ°Sçš„å›žåº”æ¶ˆæ¯çš„æ—¶é—´å·®ï¼Œè¿™ä¸ªæ—¶é—´å·®æ˜¯Pè¿›ç¨‹è‡ªå·±å¯ä»¥è®°å½•æ±‚å¾—çš„ã€‚å‡å®šæˆ‘ä»¬çŸ¥é“ä»Ž $P \to S$çš„æœ€å°å»¶æ—¶æ˜¯ $min_1$, $S \to P$çš„æœ€å°å»¶æ—¶æ˜¯$min_2$,é‚£ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥æŽ¨æ–­ï¼ŒçœŸå®žçš„æ—¶é—´åœ¨$[t+min_2, t+RTT-min_1]$é—´å†…ï¼ŒCristiançš„åšæ³•å°±å°†å¯¹æ—¶ç»“æžœè®¾ç½®ä¸ºï¼š$tâ€™=t+\frac{RTT+min_2-min_1}{2}$ è¿™ä¸ªä¸­é—´ä½ç½®ä¸Šã€‚é‚£ä¹ˆï¼Œå…¶è¯¯å·®å°±èƒ½æŽ§åˆ¶åœ¨$\pm \frac{RTT-min_1-min_2}{2}$ çš„èŒƒå›´å†…ã€‚</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><h2 id="NTP"><a href="#NTP" class="headerlink" title="NTP"></a>NTP</h2><p>å¦å¤–ä¸€ä¸ªçŸ¥åçš„æ—¶é—´åŒæ­¥åè®®æ˜¯NTPï¼Œå…¨ç§°Network Time Protocolã€‚NTPåè®®ä¸€èˆ¬åœ¨æŸä¸ªå¤§çš„æœºæž„å†…éƒ¨ç½²ï¼Œå°†æœºæž„å†…çš„è®¾å¤‡ç»„ç»‡æˆæ ‘å½¢ç»“æž„ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½ä»Žçˆ¶èŠ‚ç‚¹å¤„èŽ·å–æ—¶é—´ã€‚æ•´ä¸ªåŒæ­¥è¿‡ç¨‹åˆ†ä¸ºä¸¤è½®ï¼Œç¬¬ä¸€è½®çˆ¶èŠ‚ç‚¹è®°å½•è‡ªå·±å‘é€è¿”å›žçš„æ—¶é—´ç‚¹$ts_1$ï¼Œå­èŠ‚ç‚¹è®°å½•è‡ªå·±æŽ¥æ”¶åˆ°è¿”å›žæ¶ˆæ¯çš„æ—¶é—´ $tr_1$ï¼›è€ŒåŽç¬¬äºŒè½®ï¼Œå­èŠ‚ç‚¹è®°å½•è‡ªå·±çš„å‘é€æ—¶é—´$ts_2$ï¼›ï¼›çˆ¶èŠ‚ç‚¹è®°å½•æ”¶åˆ°è¯·æ±‚çš„æ—¶é—´$tr_2$åŽå°†$ts_1$ å’Œ $tr_2$è¿”å›žã€‚é‚£ä¹ˆå­èŠ‚ç‚¹å¯ä»¥è®¡ç®—å‡ºè‡ªå·±å’Œçˆ¶èŠ‚ç‚¹ä¹‹é—´çš„æ—¶é—´åå·®ä¸º: $o=\frac{(tr_{1}-tr_{2}+ts_{2}-ts{1})}{2}$ï¼Œå¹¶ä»¥æ­¤ä¸ºä¾æ®è¿›è¡Œä¿®æ­£(ä¸€èˆ¬éœ€è¦ç¡®ä¿æ—¶é—´ä¸èƒ½â€œå€’æµâ€)ã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆ$o$æ˜¯è¿™ä¹ˆè®¡ç®—å‘¢ï¼Ÿå‡å®šå­èŠ‚ç‚¹ä¸Žçˆ¶èŠ‚ç‚¹çš„æ—¶é—´åå·®(offset)ä¸º$o\prime$ã€çˆ¶èŠ‚ç‚¹å¾€å­èŠ‚ç‚¹çš„é€šè®¯æ—¶å»¶ä¸º$L_1$ã€å­èŠ‚ç‚¹å¾€çˆ¶èŠ‚ç‚¹çš„é€šè®¯æ—¶å»¶ä¸º$L_2$ï¼Œé‚£ä¹ˆ:</p><script type="math/tex; mode=display">\begin{align*} & tr_{1}=ts_{1}+L_{1}+o' \\& tr_{2}=ts_{2}+L_{2}-o' \\\end{align*}</script><p>ç›¸å‡å¯ä»¥å¾—åˆ°:</p><script type="math/tex; mode=display">o'= \frac{(tr_1-tr_2+ts_2-ts_1)}{2} + \frac{(L_2 - L1)}{2} = o + \frac{(L_2-L1)}{2}</script><p>å› æ­¤:</p><script type="math/tex; mode=display">\lvert o'-o \rvert \leqslant \lvert \frac{(L_2-L_1)}{2}\rvert < \frac{(L_{1}+L_{2})}{2} = \frac{RTT}{2}</script><p>ç”±æ­¤å¯çŸ¥oçš„è¿™ä¸ªå€¼ä¹Ÿåœ¨RTTç›¸å…³çš„ä¸€ä¸ªè¯¯å·®èŒƒå›´å†…ï¼Œæ˜¯å¯ä¼°è®¡çš„ã€‚<br>ä»Žä¸Šé¢ä¸¤ä¸ªåè®®å¯ä»¥çœ‹å‡ºï¼Œå¯¹æ—¶çš„è¯¯å·®æ˜¯ä¸ŽRTTå¼ºç›¸å…³çš„ã€‚ç”±äºŽæ¶ˆæ¯çš„ä¼ é€’å—åˆ¶äºŽå…‰é€Ÿã€è·ç¦»è¶Šè¿œæ—¶é—´å‡†ç¡®åº¦çš„ä¿è¯å°±è¶Šå·®ã€‚å¯¹äºŽé‚£äº›å‡å®šäº†æ—¶é—´è¯¯å·®åœ¨æŸä¸ªèŒƒå›´å†…çš„åˆ†å¸ƒå¼åè®®ï¼Œåœ¨è·¨è¶Šè·ç¦»å¾ˆå¤§çš„æ—¶å€™ï¼Œæˆ‘ä»¬å°±å¿…é¡»è¦å°†è¿™ä¸ªè¯¯å·®å¯¹ç³»ç»Ÿçš„å½±å“è€ƒè™‘åœ¨å†…ï¼Œè¿™å°†æ˜¾è‘—å¢žåŠ åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡çš„å¤æ‚åº¦ã€æˆ–è€…å½±å“è®¾è®¡å‡ºæ¥çš„ç³»ç»Ÿçš„åžå(å°¤å…¶æ˜¯æœ‰é«˜ä¸€è‡´æ€§è¦æ±‚çš„äº‹åŠ¡åž‹ç³»ç»Ÿ)ã€‚</p><p>æœ€åŽï¼Œä¸è®ºæ˜¯Cristianè¿˜æ˜¯NTPï¼Œéƒ½åªæè¿°äº†ä¸€æ¬¡å¯¹æ—¶å¦‚ä½•å°†æ—¶é—´çš„åç§»(clow skew)æŽ§åˆ¶åœ¨ä¸€å®šèŒƒå›´å†…ã€‚ç”±äºŽä¸åŒæœºå™¨çš„æ—¶é’Ÿçš„è¡Œè¿›é€Ÿåº¦(clock drift)æ˜¯ä¸åŒçš„ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ¯éš”ä¸€æ®µæ—¶é—´ï¼Œè¿›è¡Œä¸€æ¬¡ä¿®æ­£ï¼Œä»¥æ¶ˆé™¤æ—¶é’ŸèŠ‚å¥ä¸åŒçš„å½±å“ã€‚å¤šä¹…éœ€è¦åšä¸€æ¬¡åŒæ­¥å‘¢? è¿™ä¸ªåšä¸€ä¸ªç®€å•çš„è®¡ç®—å°±å¯ä»¥å¾—åˆ°ã€‚å‡å®šç³»ç»Ÿæ•´ä½“æ—¶é’Ÿçš„è¡Œè¿›é€ŸçŽ‡ä¸Žæ ‡å‡†æ—¶é’Ÿçš„é€ŸçŽ‡å°äºŽMDR(Max Drift Rate, ä¸€èˆ¬ç”±æ—¶é’Ÿçš„å®žçŽ°æ–¹å¼å†³å®š)ï¼Œé‚£ä¹ˆç³»ç»Ÿå†…ä¿©ä¿©æ—¶é’Ÿçš„è¡Œè¿›é€ŸçŽ‡å·®å°äºŽ2MDRã€‚å¦‚æžœæˆ‘ä»¬è¦æ±‚ç³»ç»Ÿå†…æ—¶é—´å·®ä¸èƒ½è¶…è¿‡Mï¼Œé‚£å°±å¿…é¡»ä»¥ä¸ä½ŽäºŽ$\delta = \frac{M}{2 \times {MDR}}$çš„é—´éš”è¿›è¡Œæ—¶é—´åŒæ­¥ã€‚åœ¨çŽ°å®žçš„ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—åˆç†çš„Mï¼Œä»¥é¿å…ç³»ç»Ÿå†…å‡ºçŽ°è¿‡å¤šçš„æ—¶é—´åŒæ­¥æ¶ˆæ¯ã€‚</p><p>åœ¨ä¸Šé¢éƒ¨åˆ†ï¼Œæˆ‘ä»¬è°ˆåˆ°äº†åˆ†å¸ƒå¼ç³»ç»Ÿé‡Œè¿›ç¨‹å½¼æ­¤çš„ç‰©ç†æ—¶é—´æ˜¯å¦‚ä½•è¿›è¡ŒåŒæ­¥çš„ï¼Œå¹¶ä»‹ç»äº†ä¸€äº›ç»å…¸çš„æ—¶é—´åŒæ­¥ç®—æ³•ã€‚ä½†é™ä¸‹å¿ƒæ¥ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘ä»¬å¸Œæœ›è¿›è¡Œæ—¶é—´åŒæ­¥ï¼Œå¾ˆå¤šæ—¶å€™æ˜¯å¸Œæœ›ä¸åŒçš„è¿›ç¨‹ï¼Œå¯¹ç³»ç»Ÿå†…äº‹ä»¶çš„é¡ºåºè¾¾æˆä¸€è‡´ã€‚è‡³äºŽæ˜¯å¦æ˜¯ä½¿ç”¨çœŸå®žä¸–ç•Œçš„é‚£ä¸ªæ—¶é—´æ¥æŽ’åºï¼Œå¾€å¾€å¹¶ä¸æ˜¯é‚£ä¹ˆé‡è¦ã€‚<br>é‚£ä¹ˆï¼Œå¦‚ä½•åœ¨ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œå¯¹å‘ç”Ÿåœ¨ä¼—å¤šèŠ‚ç‚¹ä¸Šçš„äº‹ä»¶è¿›è¡Œå®šåºå‘¢ï¼Ÿç›®å‰å·²çŸ¥çš„åšæ³•åŒ…æ‹¬ä»¥ä¸‹å‡ ç§ï¼š</p><ul><li>ä½¿ç”¨ç‰©ç†æ—¶é—´åŒæ­¥çš„æ–¹æ³•ï¼Œç¡®ä¿ä¼—å¤šèŠ‚ç‚¹çš„æ—¶é—´åå·®åœ¨æŸä¸ªèŒƒå›´å†…ã€‚è€ŒåŽè®°å½•äº‹ä»¶çš„å‘ç”Ÿæ—¶é—´åŠç†è®ºè¯¯å·®èŒƒå›´ï¼Œæ¯”å¦‚å°†æ¯ä¸ªäº‹ä»¶çš„å‘ç”Ÿæ—¶é—´ç™»è®°ä¸º$(t \pm \Delta)$å¦‚æžœä¸¤ä¸ªäº‹ä»¶çš„æ—¶é—´èŒƒå›´æ²¡æœ‰overlapï¼Œé‚£ä¹ˆå°±è‡ªç„¶çš„å¯ä»¥æŽ’åºåˆ¤æ–­ï¼›å¦åˆ™ï¼Œåˆ™éœ€è¦å¼•å…¥ä¸€ä¸ªæ–°çš„æŽ’åºè§„åˆ™(æ¯”å¦‚ä»¥èŠ‚ç‚¹id)ï¼Œå¯¹è¿™ä¸¤ä¸ªäº‹ä»¶çº¦å®šä¸€ä¸ªæŽ’åºã€‚spannerä¸­é‡‡ç”¨äº†è¿™ç§æ–¹å¼ã€‚</li><li>é‡‡ç”¨Lamport TimestampåŠå…¶å¼•ç”³ç®—æ³•è¿›è¡Œå®šåºï¼Œç¡®ä¿äº‹ä»¶æ»¡è¶³causality consistencyçš„æ€§è´¨ï¼Œæˆä¸ºåŽç»­æ›´é«˜å±‚æ¬¡çš„åˆ†å¸ƒå¼ç®—æ³•è®¾è®¡çš„åŸºç¡€ã€‚æœ¬æ–‡åŽé¢ä¸»è¦å°†å±•å¼€è¿™ç±»ç®—æ³•ï¼Œå¹¶å¼•å‡ºåˆ†å¸ƒå¼ç³»ç»Ÿä¸­ä¸€äº›åŸºç¡€æ¦‚å¿µã€‚è¿™äº›åŸºç¡€æ¦‚å¿µæ˜¯ç†è§£åˆ†å¸ƒå¼å…±è¯†é—®é¢˜(consensus problem)çš„åŸºç¡€ã€‚</li></ul><h2 id="Lamport"><a href="#Lamport" class="headerlink" title="Lamport"></a>Lamport</h2><p>ä¸ºæ˜Žç¡®è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å…ˆå¯¹äº‹ä»¶çš„åº(happen-before)åšå‡ºä¸€ä¸ªå®šä¹‰ã€‚åœ¨Lamportçš„ä½“ç³»ä¸­ï¼Œäº‹ä»¶çš„å…ˆåŽå…³ç³»æ˜¯æŒ‰ç…§å¦‚ä¸‹åŽŸåˆ™è®¾å®šçš„ï¼š</p><ul><li>è§„åˆ™ä¸€ï¼šå¦‚æžœAã€Bä¸¤ä¸ªäº‹ä»¶éƒ½å‘ç”Ÿåœ¨åŒä¸€ä¸ªè¿›ç¨‹å†…ï¼Œé‚£ä¹ˆï¼ŒAã€Bä¹‹é—´çš„åºè‡ªç„¶å¯ä»¥ç”±è¿™ä¸ªè¿›ç¨‹ç»™å‡ºã€‚å‡å¦‚è¿›ç¨‹å…ˆæ‰§è¡Œäº†AåŽæ‰§è¡Œäº†Bï¼Œé‚£ä¹ˆå¯ä»¥è¯´Aåœ¨Bä¹‹å‰å‘ç”Ÿï¼Œè®°ä¸º$A \prec B$;</li><li>è§„åˆ™äºŒï¼šå¦‚æžœè¿›ç¨‹xå¾€è¿›ç¨‹yå‘é€äº†ä¸€æ¡æ¶ˆæ¯Mï¼›è®¾åœ¨è¿›ç¨‹xçš„æ¶ˆæ¯å‘é€äº‹ä»¶ä¸ºAï¼Œè¿›ç¨‹yæ”¶åˆ°æ¶ˆæ¯çš„äº‹ä»¶ä¸ºBï¼Œåˆ™æ˜¾ç„¶æˆ‘ä»¬åº”å½“è®¤ä¸ºAåœ¨Bä¹‹å‰å‘ç”Ÿï¼ŒåŒæ ·è®°ä¸º$A \prec B$.</li></ul><p>ç”±æ­¤å¼•å‡ºäº†Lamport timestampçš„ç®—æ³•ï¼Œè¿™ä¸ªç®—æ³•å°±æ˜¯ä¸€ç§ç»™äº‹ä»¶æ‰“ä¸Šé€»è¾‘æ—¶é—´æˆ³ã€ç¡®ä¿å…¶æ»¡è¶³causalityçš„åŸºæœ¬å±žæ€§ã€‚è¿™ä¸ªç®—æ³•çš„åŸºæœ¬è¿‡ç¨‹ä¸ºï¼š</p><ul><li>æ¯ä¸ªè¿›ç¨‹éƒ½è®°å½•è‡ªå·±çš„ä¸€ä¸ªå½“å‰æ—¶é—´æˆ³ï¼Œåˆå§‹çš„æ—¶å€™ï¼Œå¤§å®¶éƒ½æ˜¯0</li><li>å¦‚æžœè¿›ç¨‹å†…éƒ¨å‘ç”Ÿäº†ä¸€ä¸ªæ–°çš„äº‹ä»¶ï¼Œé‚£ä¹ˆå°†å½“å‰æ—¶é—´æˆ³è®°ä¸º $tâ€™=t+1$ï¼Œå¹¶è®¤ä¸ºäº‹ä»¶å‘ç”ŸäºŽ$tâ€™$æ—¶åˆ»</li><li>å¦‚æžœè¿›ç¨‹Aå‘è¿›ç¨‹Bé€šè®¯ï¼Œåˆ™å‘é€æ¶ˆæ¯çš„æ—¶å€™ï¼Œè¿›ç¨‹Açš„æ—¶é—´æˆ³$tâ€™_A = t_A + 1$å¹¶éšæ¶ˆæ¯å‘é€åˆ°Bï¼ŒBæ›´æ–°è‡ªå·±çš„æ—¶é—´æˆ³ä¸º$tâ€™_B = max(tâ€™_B, tâ€™_A) + 1$.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><h3 id="Concurrent-Events"><a href="#Concurrent-Events" class="headerlink" title="Concurrent Events"></a>Concurrent Events</h3><ul><li>A pair of concurrent events doesnâ€™t have a causal path from one event to another (either way, in the pair)</li><li>Lamport timestamps not guaranteed to be ordered or unequal for concurrent events</li><li>Ok, since concurrent events are not causality related!</li><li>Remember</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">E1 -&gt; E2 =&gt; timestamp(E1) &lt; timestamp (E2), </span><br><span class="line">BUT timestamp(E1) &lt; timestamp (E2) =&gt; &#123;E1 -&gt; E2&#125; OR &#123;E1 and E2 concurrent&#125;</span><br></pre></td></tr></table></figure><h2 id="Vector-timestamps"><a href="#Vector-timestamps" class="headerlink" title="Vector timestamps"></a>Vector timestamps</h2><ul><li>Used in key-value stores like Riak</li><li>Each process uses a vector of integer clocks</li><li>Suppose there are N processes in the group 1â€¦N</li><li>Each vector has N elements</li><li>Process i maintains vector Vi[1â€¦N]</li><li>$j_{th}$ element of vector clock at process $i$, $V_i[j]$, is $iâ€™s$ knowledge of latest events at process $j$</li></ul><p>Incrementing vector clocks</p><ol><li>On an instruction or send event at process $i$, it increments only its $i_{th}$ element of its vector clock.</li><li>Each message carries the send-eventâ€™s vector timestamp V_{message}[1â€¦N]</li><li>On receiving a message at process $i$:</li></ol><script type="math/tex; mode=display">\begin{align*} &V_i[i] = V_i[i] + 1 \\& V_i[j] = max(V_{message}[j], V_i[j]) \quad for \ quad j \neq i \\\end{align*}</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center><h3 id="Causality"><a href="#Causality" class="headerlink" title="Causality"></a>Causality</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://lvsizhe.github.io/course/2018/09/time-in-distributed-systems-part1.html" target="_blank" rel="noopener">https://lvsizhe.github.io/course/2018/09/time-in-distributed-systems-part1.html</a></li><li>lecture slide from <a href="https://www.coursera.org/learn/cloud-computing/lecture/dy8wf/2-5-vector-clocks" target="_blank" rel="noopener">https://www.coursera.org/learn/cloud-computing/lecture/dy8wf/2-5-vector-clocks</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;åˆ†å¸ƒå¼ç³»ç»Ÿå’Œä¼ ç»Ÿçš„å•æœºç³»ç»Ÿä¸åŒï¼Œå½¼æ­¤æ˜¯é€šè¿‡ç½‘ç»œè€Œä¸æ˜¯â€ä¸»æ¿â€è¿žæŽ¥ã€æ¶ˆæ¯é€šè®¯æ˜¯ä¸å¯é çš„ã€‚å› æ­¤å¦‚æžœæ²¡æœ‰ä»»ä½•åŒæ­¥æœºåˆ¶ï¼ŒåŒä¸€ç³»ç»Ÿçš„æˆå‘˜ä¹‹é—´æ— æ³•ç¡®ä¿æ—¶é—´æˆ³çš„è¯¯å·®æŽ§åˆ¶åœ¨æŸä¸ªèŒƒå›´å†…ã€‚è¿™ä¸ªåŸºæœ¬æ¡ä»¶çš„ç¼ºå¤±ï¼Œä¼šç»™ä¸Šå±‚åº”ç”¨çš„è®¾è®¡å¸¦æ¥å¾ˆå¤šçš„éº»çƒ¦ã€‚æ¯”å¦‚ï¼Œä¸€ä¸ªä¸šåŠ¡æµç¨‹çš„ä¸¤ä¸ªé˜¶æ®µåˆ†åˆ«åœ¨ä¸¤å°æœºå™¨ä¸Šå¤„ç†ï¼Œè€Œ
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Distributed &amp; Cloud Computing" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Distributed-Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>Programming Language: New Types, Pattern Matching, Tail Recursion</title>
    <link href="https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/05/03/"/>
    <id>https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/05/03/</id>
    <published>2020-05-02T19:29:05.000Z</published>
    <updated>2020-05-03T07:29:53.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Conceptual-Ways-to-Build-New-Types"><a href="#Conceptual-Ways-to-Build-New-Types" class="headerlink" title="Conceptual Ways to Build New Types"></a>Conceptual Ways to Build New Types</h2><p>To create a compound type, there are really only three essential building blocks. Any decent programming language provides these building blocks in some way:</p><ul><li><strong>Each-of</strong>: A compound type t describes values that contain each of values of type t1, t2, â€¦, and tn. Tuples are an example: int * bool describes values that contain an int and a bool. A <strong>Java class</strong> with fields is also an each-of sort of thing.</li><li><strong>One-of</strong>: A compound type t describes values that contain a value of one of the types t1, t2, â€¦, or tn. For a type that contains an int or a bool in ML, we need <code>datatype bindings</code>. In object-oriented languages with classes like Java, one-of types are achieved with <strong>subclassing</strong>, but that is a topic for much later in the course.</li><li><strong>Self-reference</strong>: A compound type t may refer to itself in its definition in order to describe recursive data structures like lists and trees. This is useful in combination with each-of and one-of types. For example, int list describes values that either contain nothing or contain an int and another int list. </li></ul><h2 id="Records-Another-Approach-to-Each-of-Types"><a href="#Records-Another-Approach-to-Each-of-Types" class="headerlink" title="Records: Another Approach to Each-of Types"></a>Records: Another Approach to <strong>Each-of</strong> Types</h2><p>Record types are â€œeach-ofâ€ types where each component is a <code>named field</code>.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;foo : <span class="built_in">int</span>, bar : <span class="built_in">int</span>*<span class="built_in">bool</span>, baz : <span class="built_in">bool</span>*<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><p>In ML, we do not have to declare that we want a record type with particular field names and field types â€” we just write down a record expression and the type-checker gives it the right type.</p><p>Now that we know how to build record values, we need a way to access their pieces. For now, we will use <code>#foo e</code> where <code>foo</code> is a field name. </p><h3 id="The-truth-of-tuple"><a href="#The-truth-of-tuple" class="headerlink" title="The truth of tuple"></a>The truth of tuple</h3><p>In fact, this is how ML actually defines tuples: A tuple is a record. That is, all the syntax for tuples is just a convenient way to write down and use records. The REPL just always uses the tuple syntax where possible, so if you evaluate {2=1+2, 1=3+4} it will print the result as (7,3). Using the tuple syntax is better style, but we did not need to give tuples their own semantics: we can instead use the â€œanother way of writingâ€ rules above and then reuse the semantics for records.</p><p>This is the first of many examples we will see of <code>syntactic sugar</code>. We say, Tuples are just syntactic sugar for records with fields named 1, 2, â€¦, n.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> z = (<span class="number">3</span>,<span class="number">7</span>) : <span class="built_in">int</span> * <span class="built_in">int</span></span><br><span class="line"><span class="keyword">val</span> z = &#123;<span class="number">1</span>=<span class="number">3</span>,<span class="number">3</span>=<span class="number">7</span>&#125; : &#123;<span class="number">1</span>:<span class="built_in">int</span>, <span class="number">3</span>:<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="Datatype-Bindings-Our-Own-One-of-Types"><a href="#Datatype-Bindings-Our-Own-One-of-Types" class="headerlink" title="Datatype Bindings: Our Own One-of Types"></a>Datatype Bindings: Our Own <strong>One-of</strong> Types</h2><p>We now introduce datatype bindings, our third kind of binding after variable bindings and function bindings.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">datatype mytype = TwoInts of int * int</span><br><span class="line">                | Str of string</span><br><span class="line">                | Pizza</span><br></pre></td></tr></table></figure><p>Roughly, this defines a new type where values have an int <em> int or a string or nothing. Any value will also be <code>tagged</code> with information that lets us know which variant it is: These tags, which we will call <em>*constructors</em></em>, are <code>TwoInts</code>, <code>Str</code>, and <code>Pizza</code>.</p><p>More precisely, the example above adds four things to the environment:</p><ul><li>A new type mytype that we can now use just like any other type</li><li>Three constructors TwoInts, Str, and Pizza</li></ul><p>A constructor is two different things. First, it is either a function for creating values of the new type (if the variant has of t for some type t) or it is actually a value of the new type (otherwise). In our example, TwoInts is a function of type int*int -&gt; mytype, Str is a function of type string-&gt;mytype, and Pizza is a value of type mytype. Second, we use constructors in case-expressions as described further below.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> mytype = <span class="type">TwoInts</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span> </span><br><span class="line">                | <span class="type">Str</span> <span class="keyword">of</span> <span class="built_in">string</span> </span><br><span class="line">                | <span class="type">Pizza</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a = <span class="type">Str</span> <span class="string">"hi"</span></span><br><span class="line"><span class="keyword">val</span> b = <span class="type">Str</span></span><br><span class="line"><span class="keyword">val</span> c = <span class="type">Pizza</span></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">TwoInts</span>(<span class="number">1</span>+<span class="number">2</span>,<span class="number">3</span>+<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> e = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">(* val a = Str "hi" : mytype</span></span><br><span class="line"><span class="comment">val b = fn : string -&gt; mytype</span></span><br><span class="line"><span class="comment">val c = Pizza : mytype</span></span><br><span class="line"><span class="comment">val d = TwoInts (3,7) : mytype</span></span><br><span class="line"><span class="comment">val e = Str "hi" : mytype *)</span></span><br></pre></td></tr></table></figure><h2 id="How-ML-Provides-Access-to-Datatype-Values-Case-Expressions"><a href="#How-ML-Provides-Access-to-Datatype-Values-Case-Expressions" class="headerlink" title="How ML Provides Access to Datatype Values: Case Expressions"></a>How ML Provides Access to Datatype Values: Case Expressions</h2><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> f x = <span class="comment">(* f has type mytype -&gt; int*)</span> </span><br><span class="line">    <span class="keyword">case</span> x <span class="keyword">of</span></span><br><span class="line">      <span class="type">Pizza</span> =&gt; <span class="number">3</span></span><br><span class="line">    | <span class="type">TwoInts</span>(i1,i2) =&gt; i1 + i2 </span><br><span class="line">    | <span class="type">Str</span> s =&gt; <span class="type">String</span>.size s</span><br></pre></td></tr></table></figure><p>In one sense, a case-expression is like a more powerful if-then-else expression: Like a conditional expression, it evaluates two of its subexpressions: first the expression between the case and of keywords and second the expression in the first branch that matches. But instead of having two branches (one for true and one for false), we can have one branch for each variant of our datatype (and we will generalize this further below). Like conditional expressions, each branchâ€™s expression must have the same type (int in the example above) because the type-checker cannot know what branch will be used.<br>Each branch has the form <code>p =&gt; e</code> where p is a pattern and e is an expression, and we separate the branches with the | character. Patterns look like expressions, but do not think of them as expressions. Instead they are used to match against the result of evaluating the caseâ€™s first expression (the part after case). This is why evaluating a case-expression is called pattern-matching.</p><h3 id="Datatype-Bindings-and-Case-Expressions-So-Far-Precisely"><a href="#Datatype-Bindings-and-Case-Expressions-So-Far-Precisely" class="headerlink" title="Datatype Bindings and Case Expressions So Far, Precisely"></a>Datatype Bindings and Case Expressions So Far, Precisely</h3><p>We can summarize what we know about datatypes and pattern matching so far as follows: The binding</p><blockquote><p>datatype t = C1 of t1 | C2 of t2 | â€¦ | Cn of tn</p></blockquote><p>introduces a new type t and each constructor Ci is a function of type ti-&gt;t. One omits the â€œof tiâ€ for a variant that â€œcarries nothingâ€ and such a constructor just has type t. To â€œget at the piecesâ€ of a t we use a case expression:</p><blockquote><p>case e of p1 =&gt; e1 | p2 =&gt; e2 | â€¦ | pn =&gt; en</p></blockquote><p>A case expression evaluates e to a value v, finds the first pattern pi that matches v, and evaluates ei to produce the result for the whole case expression. So far, patterns have looked like Ci(x1,â€¦,xn) where Ci is a constructor of type t1 <em> â€¦ </em> tn -&gt; t (or just Ci if Ci carries nothing). Such a pattern matches a value of the form Ci(v1,â€¦,vn) and binds each xi to vi for evaluating the corresponding ei.</p><h2 id="Type-Synonyms"><a href="#Type-Synonyms" class="headerlink" title="Type Synonyms"></a>Type Synonyms</h2><p>A <strong>type synonym</strong> simply creates another name for an existing type that is entirely interchangeable with the existing type.</p><p>For example, if we write:<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> foo = <span class="built_in">int</span></span><br></pre></td></tr></table></figure></p><p>then we can write foo wherever we write int and vice-versa.</p><p>for more complicated types, it can be convenient to create type synonyms. Here are some examples for types we created above:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> card = suit * rank</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> name_record = &#123; student_num : <span class="built_in">int</span> <span class="built_in">option</span>,</span><br><span class="line">                    first : <span class="built_in">string</span>,</span><br><span class="line">                    middle : <span class="built_in">string</span> <span class="built_in">option</span>,</span><br><span class="line">                    last : <span class="built_in">string</span> &#125;</span><br></pre></td></tr></table></figure><h2 id="Lists-and-Options-are-Datatypes"><a href="#Lists-and-Options-are-Datatypes" class="headerlink" title="Lists and Options are Datatypes"></a>Lists and Options are Datatypes</h2><p>Because datatype definitions can be recursive, we can use them to create our own types for lists. For example, this binding works well for a linked list of integers:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> my_int_list = <span class="type">Empty</span></span><br><span class="line">                        | <span class="type">Cons</span> <span class="keyword">of</span> <span class="built_in">int</span> * my_int_list</span><br></pre></td></tr></table></figure><p>We can use the constructors Empty and Cons to make values of my_int_list and we can use case expressions to use such values:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> one_two_three = <span class="type">Cons</span>(<span class="number">1</span>,<span class="type">Cons</span>(<span class="number">2</span>,<span class="type">Cons</span>(<span class="number">3</span>,<span class="type">Empty</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> append_mylist (xs,ys) = </span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">        <span class="type">Empty</span> =&gt; ys</span><br><span class="line">    | <span class="type">Cons</span>(x,xsâ€™) =&gt; <span class="type">Cons</span>(x, append_mylist(xsâ€™,ys))</span><br></pre></td></tr></table></figure><p>For options, all you need to know is SOME and NONE are constructors, which we use to create values (just like before) and in patterns to access the values. Here is a short example of the latter:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> inc_or_zero intoption = <span class="keyword">case</span> intoption <span class="keyword">of</span></span><br><span class="line">        <span class="type">NONE</span> =&gt; <span class="number">0</span></span><br><span class="line">      | <span class="type">SOME</span> i =&gt; i+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">&lt;!-- <span class="keyword">val</span> inc_or_zero = <span class="keyword">fn</span> : <span class="built_in">int</span> <span class="built_in">option</span> -&gt; <span class="built_in">int</span> --&gt;</span><br></pre></td></tr></table></figure><p>The story for lists is similar with a few convenient syntactic peculiarities: [] really is a constructor that carries nothing and :: really is a constructor that carries two things, but :: is unusual because it is an infix operator (it is placed between its two operands), both when creating things and in patterns:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_list xs = </span><br><span class="line">    <span class="keyword">case</span> xs:</span><br><span class="line">        <span class="literal">[]</span> =&gt; <span class="number">0</span></span><br><span class="line">        | x::xs' =&gt; x + sum_list xs'</span><br></pre></td></tr></table></figure><p>Notice here x and xsâ€™ are nothing but local variables introduced via pattern-matching. We can use any names for the variables we want. </p><h2 id="Pattern-Matching-for-Each-Of-Types-The-Truth-About-Val-Bindings"><a href="#Pattern-Matching-for-Each-Of-Types-The-Truth-About-Val-Bindings" class="headerlink" title="Pattern-Matching for Each-Of Types: The Truth About Val-Bindings"></a>Pattern-Matching for Each-Of Types: The Truth About Val-Bindings</h2><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> full_name (r : &#123;first:<span class="built_in">string</span>,middle:<span class="built_in">string</span>,last:<span class="built_in">string</span>&#125;) = <span class="keyword">case</span> r <span class="keyword">of</span></span><br><span class="line">        &#123;first=x,middle=y,last=z&#125; =&gt; x ^ <span class="string">" "</span> ^ y ^ <span class="string">" "</span> ^z</span><br></pre></td></tr></table></figure><p>However, a case-expression with one branch is poor style â€” it looks strange because the purpose of such expressions is to distinguish cases, plural. So how should we use pattern-matching for each-of types, when we know that a single pattern will definitely match so we are using pattern-matching just for the convenient <strong>extraction of values</strong>? It turns out you can use patterns in val-bindings too! So this approach is better style:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">fun</span> full_name (r : &#123;first:<span class="built_in">string</span>,middle:<span class="built_in">string</span>,last:<span class="built_in">string</span>&#125;) = </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">val</span> &#123;first=x,middle=y,last=z&#125; = r</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        x ^ <span class="string">" "</span> ^ y ^ <span class="string">" "</span> ^z </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum_triple (triple : <span class="built_in">int</span>*<span class="built_in">int</span>*<span class="built_in">int</span>) = </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">val</span> (x,y,z) = triple</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        x+y+z</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_triple (x,y,z) = x+y+z</span><br></pre></td></tr></table></figure><p>This version of sum_triple should intrigue you: It takes a triple as an argument and uses pattern-matching to bind three variables to the three pieces for use in the function body. But it looks exactly like a function that takes three arguments of type int. Indeed, is the type int<em>int</em>int-&gt;int for three-argument functions or for one argument functions that take triples?<br>It turns out we have been basically lying: There is no such thing as a multi-argument function in ML: <strong>Every function in ML takes exactly one argument!</strong> Every time we write a multi-argument function, we are really writing a one-argument function that takes a tuple as an argument and uses pattern-matching to extract the pieces. This is such a common idiom that it is easy to forget about and it is totally fine to talk about â€œmulti-argument functionsâ€ when discussing your ML code with friends. But in terms of the actual language definition, it really is a one-argument function: syntactic sugar for expanding out to the first version of sum_triple with a one-arm case expression.</p><h2 id="Digression-Type-inference"><a href="#Digression-Type-inference" class="headerlink" title="Digression: Type inference"></a>Digression: Type inference</h2><p>In ML, every variable and function has a type (or your program fails to type-check) â€” type inference only means you do not need to write down the type.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_triple triple = <span class="keyword">case</span> triple <span class="keyword">of</span></span><br><span class="line">    (x,y,z) =&gt; z + y + x</span><br></pre></td></tr></table></figure><p>In fact, type inference sometimes reveals that functions are more general than you might have thought. Consider this code, which does use part of a tuple/record:<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> partial_sum (x,y,z) = x + z</span><br><span class="line"><span class="keyword">fun</span> partial_name &#123;first=x, middle=y, last=z&#125; = x ^ <span class="string">" "</span> ^ z</span><br></pre></td></tr></table></figure></p><p>In both cases, the inferred function types reveal that the type of y can be any type, so we can call partial_sum (3,4,5) or partial_sum (3,false,5). This is okay because the polymorphism indicates that partial_sum has a more gen- eral type. If you can take a type containing â€™a, â€™b, â€™c, etc. and replace each of these type variables consistently to get the type you â€œwant,â€ then you have a more general type than the one you want.</p><h2 id="Nested-Patterns"><a href="#Nested-Patterns" class="headerlink" title="Nested Patterns"></a>Nested Patterns</h2><p>It turns out the definition of patterns is recursive: anywhere we have been putting a variable in our patterns, we can instead put another pattern. Roughly speaking, the semantics of pattern-matching is that the value being matched must have the same â€œshapeâ€ as the pattern and variables are bound to the â€œright pieces.â€ (This is very hand-wavy explanation which is why a precise definition is described below.) For example, the pattern a::(b::(c::d)) would match any list with at least 3 elements and it would bind a to the first element, b to the second, c to the third, and d to the list holding all the other elements (if any). The pattern a::(b::(c::[])) on the other hand, would match only lists with exactly three elements. Another nested patterns is (a,b,c)::d, which matches any non-empty list of triples, binding a to the first component of the head, b to the second component of the head, c to the third component of the head, and d to the tail of the list.</p><p>In general, pattern-matching is about taking a value and a pattern and (1) deciding if the pattern matches the value and (2) if so, binding variables to the right parts of the value. Here are some key parts to the elegant recursive definition of pattern matching:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">BadTriple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> zip3 list_triple = <span class="keyword">case</span> list_triple <span class="keyword">of</span></span><br><span class="line">    (<span class="literal">[]</span>,<span class="literal">[]</span>,<span class="literal">[]</span>) =&gt; <span class="literal">[]</span></span><br><span class="line">        | (hd1::tl1,hd2::tl2,hd3::tl3) =&gt; (hd1,hd2,hd3)::zip3(tl1,tl2,tl3) </span><br><span class="line">        | _ =&gt; <span class="keyword">raise</span> <span class="type">BadTriple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> unzip3 lst =</span><br><span class="line">    <span class="keyword">case</span> lst <span class="keyword">of</span></span><br><span class="line">        <span class="literal">[]</span> =&gt; (<span class="literal">[]</span>,<span class="literal">[]</span>,<span class="literal">[]</span>)</span><br><span class="line">      | (a,b,c)::tl =&gt; <span class="keyword">let</span> <span class="keyword">val</span> (l1,l2,l3) = unzip3 tl</span><br><span class="line">                       <span class="keyword">in</span></span><br><span class="line">                           (a::l1,b::l2,c::l3)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="Exceptions"><a href="#Exceptions" class="headerlink" title="Exceptions"></a>Exceptions</h2><p>ML has a built-in notion of exception. You can raise (also known as throw) an exception with the raise primitive. For example, the hd function in the standard library raises the List.Empty exception when called with []:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> hd xs =</span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line"><span class="literal">[]</span> =&gt; <span class="keyword">raise</span> <span class="type">List</span>.<span class="type">Empty</span> | x::_ =&gt; x</span><br></pre></td></tr></table></figure><p>You can create your own kinds of exceptions with an exception binding. Exceptions can optionally carry values with them, which let the code raising the exception provide more information:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">MyUndesirableCondition</span></span><br><span class="line"><span class="keyword">exception</span> <span class="type">MyOtherException</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span></span><br></pre></td></tr></table></figure><p>Kinds of exceptions are a lot like constructors of a datatype binding. Indeed, they are functions (if they carry values) or values (if they donâ€™t) that create values of type exn rather than the type of a datatype. So Empty, MyUndesirableCondition, and MyOtherException(3,9) are all values of type exn, whereas MyOtherException has type int*int-&gt;exn.</p><h2 id="Tail-Recursion-and-Accumulators"><a href="#Tail-Recursion-and-Accumulators" class="headerlink" title="Tail Recursion and Accumulators"></a>Tail Recursion and Accumulators</h2><p>This topic involves new programming idioms, but no new language constructs. It defines tail recursion, describes how it relates to writing efficient recursive functions in functional languages like ML, and presents how to use accumulators as a technique to make some functions tail recursive.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum1 xs =</span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">        <span class="literal">[]</span> =&gt; <span class="number">0</span></span><br><span class="line">      | i::xsâ€™ =&gt; i + sum1 xsâ€™</span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum2 xs =</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">fun</span> f (xs,acc) =</span><br><span class="line">            <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">                <span class="literal">[]</span> =&gt; acc</span><br><span class="line">              | i::xsâ€™ =&gt; f(xsâ€™,i+acc)</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        f(xs,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Why might sum2 be preferred when it is clearly more complicated? To answer, we need to understand a little bit about how function calls are implemented. Conceptually, there is a <strong>call stack</strong>, which is a stack (the data structure with push and pop operations) with one element for each function call that has been started but has not yet completed. Each element stores things like the value of local variables and what part of the function has not been evaluated yet. When the evaluation of one function body calls another function, a new element is pushed on the call stack and it is popped off when the called function completes.</p><blockquote><p> there is nothing more for the caller to do after the callee returns except return the calleeâ€™s result.</p></blockquote><p>This situation is called a tail call (letâ€™s not try to figure out why itâ€™s called this) and functional languages like ML typically promise an essential optimization: When a call is a tail call, the callerâ€™s stack-frame is popped before the call â€” the calleeâ€™s stack-frame just replaces the callerâ€™s. This makes sense: the caller was just going to return the calleeâ€™s result anyway. Therefore, calls to sum2 never use more than 1 stack frame.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Conceptual-Ways-to-Build-New-Types&quot;&gt;&lt;a href=&quot;#Conceptual-Ways-to-Build-New-Types&quot; class=&quot;headerlink&quot; title=&quot;Conceptual Ways to Build
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>EDA Summary</title>
    <link href="https://zhangruochi.com/EDA-Summary/2020/04/30/"/>
    <id>https://zhangruochi.com/EDA-Summary/2020/04/30/</id>
    <published>2020-04-30T11:55:33.000Z</published>
    <updated>2020-05-01T00:02:49.967Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-main-goals-of-EDA-are"><a href="#The-main-goals-of-EDA-are" class="headerlink" title="The main goals of EDA are:"></a>The main goals of EDA are:</h2><ul><li>Provide summary level insight into a data set</li><li>Uncover underlying patterns and structure in the data</li><li>Identify outliers, missing data and class balance issues</li><li>Carry out quality control checks</li></ul><h2 id="The-principal-steps-in-the-process-of-EDA-are"><a href="#The-principal-steps-in-the-process-of-EDA-are" class="headerlink" title="The principal steps in the process of EDA are:"></a>The principal steps in the process of EDA are:</h2><ol><li>Summarize the data - Generally done using dataframes in R or Python</li><li>Tell the Story - Summarize the details of what connects the dataset to the business opportunity</li><li>Deal with missing data - Identify the strategy for dealing with missing data</li><li>Investigate - Using data visualization and hypothesis testing delve into the relationship between the dataset and the business opportunity</li><li>Communicate - Communicate the findings from the above steps</li></ol><h2 id="Data-visualization"><a href="#Data-visualization" class="headerlink" title="Data visualization"></a>Data visualization</h2><ol><li>Jupyter notebooks in combination with pandas and simple plots are the basis for modern EDA when using Python as a principal language</li></ol><h3 id="Advantages-of-Jupyter-notebooks"><a href="#Advantages-of-Jupyter-notebooks" class="headerlink" title="Advantages of Jupyter notebooks:"></a>Advantages of Jupyter notebooks:</h3><ul><li>They are portable: then can be used locally on private servers, public cloud, and as part of IBM Watson Studio</li><li>They work with dozens of languages</li><li>They mix markdown with executable code in a way that works naturally with storytelling and investigation</li><li>matplotlib itself and its numerous derivative works like seaborn are the core of the Python data visualization landscape</li><li>pandas and specifically the dataframe class works naturally with Jupyter, matplotlib and downstream modeling frameworks like sklearn</li></ul><h3 id="EDA-and-Data-Visualization-best-practices"><a href="#EDA-and-Data-Visualization-best-practices" class="headerlink" title="EDA and Data Visualization best practices"></a>EDA and Data Visualization best practices</h3><ol><li>The majority of code for any data science project should be contained within text files. This is a software engineering best practice that ensures re-usability, allows for unit testing and works naturally with version control. &gt;In Python the text files can be executable scripts, modules, a full Python package or some combination of these.</li><li>Keep a record of plots and visualization code that you create. It is difficult to remember all of the details of how visualizations were created. Extracting the visualization code to a specific place will ensure that similar plots for future projects will be quick to create.</li><li>Use you plots as a quality assurance tool. Given what you know about the data it can be useful to make an educated guess before you execute the cell or run the script. This habit is surprisingly useful for quality assurance of both data and code.</li></ol><h2 id="Missing-values"><a href="#Missing-values" class="headerlink" title="Missing values"></a>Missing values</h2><ul><li>Dealing with missing data sits at the intersection of EDA and data ingestion in the AI enterprise workflow</li><li>Ignoring missing data may have unintended consequences in terms of model performance that may not be easy to detect</li><li>Removing either complete rows or columns in a feature matrix that contain missing values is called complete case analysis</li><li>Complete case analysis, although commonly used, can lead to undesirable resultsâ€”the extent to which depends on the category of missingness</li></ul><h3 id="The-categories-of-missingness-are"><a href="#The-categories-of-missingness-are" class="headerlink" title="The categories of missingness are:"></a>The categories of missingness are:</h3><ol><li>Missing completely at random or MCAR:</li></ol><p>When data are MCAR, missing cases are, on average, identical to non-missing cases, with respect to the feature matrix. Complete case analysis will reduce the power of the analysis, but will not affect bias.</p><ol><li>Missing at random or MAR:</li></ol><p>When data are MAR the missing data often have some dependence on measured values, and models can be used to help impute what the likely data would be. For example, in an MLB survey, there may be a gender bias when it comes to completing all of the questions.</p><ol><li>Missing not at random or MNAR:</li></ol><p>In this case the missing data depend on unmeasured or unknown variables. There is no information available to account for the missingness.</p><ul><li>The best case scenario is that the data are MCAR. It should be noted that imputing values under the other two types of missingness can result in an increase in bias.</li><li>In statistics the process of replacing missing data with substituted values is known as imputation.</li><li>It is a common practice to perform multiple imputations.</li><li>The practice of imputing missing values introduces uncertainty into the results of a data science project.</li><li>One way to deal with that additional uncertainty is to try a range of different values for imputation and measure how the results vary between each set of imputations. This technique is known as multiple imputation.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;The-main-goals-of-EDA-are&quot;&gt;&lt;a href=&quot;#The-main-goals-of-EDA-are&quot; class=&quot;headerlink&quot; title=&quot;The main goals of EDA are:&quot;&gt;&lt;/a&gt;The main g
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="EDA" scheme="https://zhangruochi.com/categories/AI-Workflow/EDA/"/>
    
    
  </entry>
  
  <entry>
    <title>Model Training Tricks (2)</title>
    <link href="https://zhangruochi.com/Model-Training-Tricks-2/2020/04/28/"/>
    <id>https://zhangruochi.com/Model-Training-Tricks-2/2020/04/28/</id>
    <published>2020-04-27T20:04:20.000Z</published>
    <updated>2020-04-28T21:09:54.664Z</updated>
    
    <content type="html"><![CDATA[<p>If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a really long time. The reason that deep learning is not straightforward is because your data, memory, and time is limited. If you are running out of memory or time, then the solution is to train a smaller model. If you are not able to train for long enough to overfit, then you are not taking advantage of the capacity of your model.</p><p>So step one is to get to the point that you can overfit. Then, the question is how to reduce that overfitting.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>Many practitioners when faced with an overfitting model start at exactly the wrong end of this diagram. Their starting point is to use a smaller model, or more regularisation. Using a smaller model should be absolutely the last step you take, unless your model is taking up too much time or memory. Reducing the size of your model as reducing the ability of your model to learn subtle relationships in your data.<br>Instead, your first step should be to seek to create more data. That could involve adding more labels to data that you already have in your organisation, finding additional tasks that your model could be asked to solve (or to think of it another way, identifying different kinds of labels that you could model), or creating additional synthetic data via using more or different data augmentation. Thanks to the development of mixup and similar approaches, effective data augmentation is now available for nearly all kinds of data.<br>Once youâ€™ve got as much data as you think you can reasonably get a hold of, and are using it as effectively as possible by taking advantage of all of the labels that you can find, and all of the augmentation that make sense, if you are still overfitting and you should think about using more generalisable architectures. For instance, adding batch normalisation may improve generalisation.<br>If you are still overfitting after doing the best you can at using your data and tuning your architecture, then you can take a look at regularisation. Generally speaking, adding dropout to the last layer or two will do a good job of regularising your model. However, as we learnt from the story of the development of AWD-LSTM, it is often the case that adding dropout of different types throughout your model can help regularise even better. Generally speaking, a larger model with more regularisation is more flexible, and can therefore be more accurate than a smaller model with less regularisation.<br>Only after considering all of these options would be recommend that you try using smaller versions of your architectures.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/fastai/fastbook/blob/master/15_arch_details.ipynb" target="_blank" rel="noopener">https://github.com/fastai/fastbook/blob/master/15_arch_details.ipynb</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a r
      
    
    </summary>
    
    
      <category term="Competition" scheme="https://zhangruochi.com/categories/Competition/"/>
    
      <category term="Tricks" scheme="https://zhangruochi.com/categories/Competition/Tricks/"/>
    
    
  </entry>
  
  <entry>
    <title>Model Training Tricks (1)</title>
    <link href="https://zhangruochi.com/Model-Training-Tricks-1/2020/04/27/"/>
    <id>https://zhangruochi.com/Model-Training-Tricks-1/2020/04/27/</id>
    <published>2020-04-27T08:21:11.000Z</published>
    <updated>2020-04-28T08:04:35.236Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dls</span><span class="params">(bs, size)</span>:</span></span><br><span class="line">    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),</span><br><span class="line">                   get_items=get_image_files,</span><br><span class="line">                   get_y=parent_label,</span><br><span class="line">                   item_tfms=Resize(<span class="number">460</span>),</span><br><span class="line">                   batch_tfms=[*aug_transforms(size=size, min_scale=<span class="number">0.75</span>),</span><br><span class="line">                               Normalize.from_stats(*imagenet_stats)])</span><br><span class="line">    <span class="keyword">return</span> dblock.dataloaders(path, bs=bs)</span><br></pre></td></tr></table></figure><p>Normalization becomes especially important when using pretrained models. The pretrained model only knows how to work with data of the type that it has seen before. If the average pixel was zero in the data it was trained with, but your data has zero as the minimum possible value of a pixel, then the model is going to be seeing something very different to what is intended.</p><p>This means that when you distribute a model, you need to also distribute the statistics used for normalization, since anyone using it for inference, or transfer learning, will need to use the same statistics. By the same token, if youâ€™re using a model that someone else has trained, make sure you find out what normalization statistics they used, and match them.</p><h2 id="Progressive-resizing"><a href="#Progressive-resizing" class="headerlink" title="Progressive resizing"></a>Progressive resizing</h2><blockquote><p>Gradually using larger and larger images as you train</p></blockquote><p>Start training using small images, and end training using large images. By spending most of the epochs training with small images, training completed much faster. By completing training using large images, the final accuracy was much higher. We call this approach progressive resizing.</p><p>Note that for transfer learning, progressive resizing may actually hurt performance. This would happen if your pretrained model was quite <code>similar</code> to your transfer learning task and dataset, and was trained on similar sized images, so the weights donâ€™t need to be changed much. In that case, training on smaller images may damage the pretrained weights. On the other hand, if the transfer learning task is going to be on images that are of different sizes, shapes, or style to those used in the pretraining tasks, progressive resizing will probably help. As always, the answer to â€œdoes it help?â€ is â€œtry it!â€.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">dls = get_dls(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">learn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(), </span><br><span class="line">                metrics=accuracy)</span><br><span class="line">learn.fit_one_cycle(<span class="number">4</span>, <span class="number">3e-3</span>)</span><br><span class="line"></span><br><span class="line">learn.dls = get_dls(<span class="number">64</span>, <span class="number">224</span>)</span><br><span class="line">learn.fine_tune(<span class="number">5</span>, <span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><h2 id="Test-time-augmentation"><a href="#Test-time-augmentation" class="headerlink" title="Test time augmentation"></a>Test time augmentation</h2><blockquote><p>During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image</p></blockquote><p>Select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we could do this not just for different crops, but for different values across all of our test time augmentation parameters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds,targs = learn.tta()</span><br><span class="line">accuracy(preds, targs).item()</span><br></pre></td></tr></table></figure><h2 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h2><p>Mixup works as follows, for each image:</p><ol><li>Select another image from your dataset at random</li><li>Pick a weight at random</li><li>Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable</li><li>Take a weighted average (with the same weight) of this imageâ€™s labels with your imageâ€™s labels; this will be your dependent variable</li></ol><p>In pseudo-code, weâ€™re doing (where t is the weight for our weighted average):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image2,target2 = dataset[randint(<span class="number">0</span>,len(dataset)]</span><br><span class="line">t = random_float(<span class="number">0.5</span>,<span class="number">1.0</span>)</span><br><span class="line">new_image = t * image1 + (<span class="number">1</span>-t) * image2</span><br><span class="line">new_target = t * target1 + (<span class="number">1</span>-t) * target2</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>The third image is built by adding 0.3 times the first one and 0.7 times the second. In this example, should the model predict church? gas station? The right answer is 30% church and 70% gas station since thatâ€™s what weâ€™ll get if we take the linear combination of the one-hot encoded targets. For instance, if church has for index 2 and gas station as for index 7, the one-hot-encoded representations are</p><blockquote><p>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]<br>[0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0]</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = xresnet50()</span><br><span class="line">learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), </span><br><span class="line">                metrics=accuracy, cbs=Mixup)</span><br><span class="line">learn.fit_one_cycle(<span class="number">5</span>, <span class="number">3e-3</span>)</span><br></pre></td></tr></table></figure><h2 id="Label-smoothing"><a href="#Label-smoothing" class="headerlink" title="Label smoothing"></a>Label smoothing</h2><p>In the theoretical expression of the loss, in classification problems, our targets are one-hot encoded (in practice we tend to avoid doing it to save memory, but what we compute is the same loss as if we had used one-hot encoding). That means the model is trained to return 0 for all categories but one, for which it is trained to return 1. Even 0.999 is not good enough, the model will get gradients and learn to predict activations that are even more confident. This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category even if itâ€™s not too sure, just because it was trained this way. <strong>It can become very harmful if your data is not perfectly labeled.</strong></p><p>This is how label smoothing works in practice: we start with one-hot encoded labels, then replace all zeros by</p><script type="math/tex; mode=display">\frac{\epsilon}{N}</script><p>where $N$ is the number of classes and $\epsilon$ is a parameter (usually 0.1, which would mean we are 10% unsure of our labels). Since you want the labels to add up to 1, replace the 1 by </p><p><script type="math/tex">1-\epsilon + \frac{\epsilon}{N}</script>. </p><p>This way, we donâ€™t encourage the model to predict something overconfident: in our Imagenette example where we have 10 classes, the targets become something like:</p><blockquote><p>[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Normalization&quot;&gt;&lt;a href=&quot;#Normalization&quot; class=&quot;headerlink&quot; title=&quot;Normalization&quot;&gt;&lt;/a&gt;Normalization&lt;/h2&gt;&lt;figure class=&quot;highlight pyth
      
    
    </summary>
    
    
      <category term="Competition" scheme="https://zhangruochi.com/categories/Competition/"/>
    
      <category term="Tricks" scheme="https://zhangruochi.com/categories/Competition/Tricks/"/>
    
    
  </entry>
  
</feed>
