<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-09-30T08:50:40.877Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Dyna-Q and Dyna-Q+</title>
    <link href="https://zhangruochi.com/Dyna-Q-and-Dyna-Q/2020/09/30/"/>
    <id>https://zhangruochi.com/Dyna-Q-and-Dyna-Q/2020/09/30/</id>
    <published>2020-09-30T08:50:06.000Z</published>
    <updated>2020-09-30T08:50:40.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-Dyna-Q-and-Dyna-Q"><a href="#Assignment-Dyna-Q-and-Dyna-Q" class="headerlink" title="Assignment: Dyna-Q and Dyna-Q+"></a>Assignment: Dyna-Q and Dyna-Q+</h1><p>Welcome to this programming assignment! In this notebook, you will:</p><ol><li>implement the Dyna-Q and Dyna-Q+ algorithms. </li><li>compare their performance on an environment which changes to become ‘better’ than it was before, that is, the task becomes easier. </li></ol><p>We will give you the environment and infrastructure to run the experiment and visualize the performance. The assignment will be graded automatically by comparing the behavior of your agent to our implementations of the algorithms. The random seed will be set explicitly to avoid different behaviors due to randomness. </p><p>Please go through the cells in order. </p><h2 id="The-Shortcut-Maze-Environment"><a href="#The-Shortcut-Maze-Environment" class="headerlink" title="The Shortcut Maze Environment"></a>The Shortcut Maze Environment</h2><p>In this maze environment, the goal is to reach the goal state (G) as fast as possible from the starting state (S). There are four actions â€“ up, down, right, left â€“ which take the agent deterministically from a state to the corresponding neighboring states, except when movement is blocked by a wall (denoted by grey) or the edge of the maze, in which case the agent remains where it is. The reward is +1 on reaching the goal state, 0 otherwise. On reaching the goal state G, the agent returns to the start state S to being a new episode. This is a discounted, episodic task with $\gamma = 0.95$.</p><p><img src="shortcut_env.png" alt="environment" width="400"></p><p>Later in the assignment, we will use a variant of this maze in which a ‘shortcut’ opens up after a certain number of timesteps. We will test if the the Dyna-Q and Dyna-Q+ agents are able to find the newly-opened shorter route to the goal state.</p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>We import the following libraries that are required for this assignment. Primarily, we shall be using the following libraries:</p><ol><li>numpy: the fundamental package for scientific computing with Python.</li><li>matplotlib: the library for plotting graphs in Python.</li><li>RL-Glue: the library for reinforcement learning experiments.</li></ol><p><strong>Please do not import other libraries</strong> as this will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> ShortcutMazeEnvironment</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">'font.size'</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">'figure.figsize'</span>: [<span class="number">8</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure><h2 id="Section-1-Dyna-Q"><a href="#Section-1-Dyna-Q" class="headerlink" title="Section 1: Dyna-Q"></a>Section 1: Dyna-Q</h2><p>Let’s start with a quick recap of the tabular Dyna-Q algorithm.</p><div style="width:80%"><img src="DynaQ.png" alt="DynaQ_pseudocode"></div><p>Dyna-Q involves four basic steps:</p><ol><li>Action selection: given an observation, select an action to be performed (here, using the $\epsilon$-greedy method).</li><li>Direct RL: using the observed next state and reward, update the action values (here, using one-step tabular Q-learning).</li><li>Model learning: using the observed next state and reward, update the model (here, updating a table as the environment is assumed to be deterministic).</li><li>Planning: update the action values by generating $n$ simulated experiences using certain starting states and actions (here, using the random-sample one-step tabular Q-planning method). This is also known as the ‘Indirect RL’ step. The process of choosing the state and action to simulate an experience with is known as ‘search control’.</li></ol><p>Steps 1 and 2 are parts of the <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=153" target="_blank" rel="noopener">tabular Q-learning algorithm</a> and are denoted by line numbers (a)â€“(d) in the pseudocode above. Step 3 is performed in line (e), and Step 4 in the block of lines (f).</p><p>We highly recommend revising the Dyna videos in the course and the material in the RL textbook (in particular, <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=183" target="_blank" rel="noopener">Section 8.2</a>).</p><p>Alright, let’s begin coding.</p><p>As you already know by now, you will develop an agent which interacts with the given environment via RL-Glue. More specifically, you will implement the usual methods <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code> in your <code>DynaQAgent</code> class, along with a couple of helper methods specific to Dyna-Q, namely <code>update_model</code> and <code>planning_step</code>. We will provide detailed comments in each method describing what your code should do. </p><p>Let’s break this down in pieces and do it one-by-one.</p><p>First of all, check out the <code>agent_init</code> method below. As in earlier assignments, some of the attributes are initialized with the data passed inside <code>agent_info</code>. In particular, pay attention to the attributes which are new to <code>DynaQAgent</code>, since you shall be using them later. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynaQAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                num_states (int): The number of states,</span></span><br><span class="line"><span class="string">                num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">                epsilon (float): The parameter for epsilon-greedy exploration,</span></span><br><span class="line"><span class="string">                step_size (float): The step-size,</span></span><br><span class="line"><span class="string">                discount (float): The discount factor,</span></span><br><span class="line"><span class="string">                planning_steps (int): The number of planning steps per environmental interaction</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                random_seed (int): the seed for the RNG used in epsilon-greedy</span></span><br><span class="line"><span class="string">                planning_random_seed (int): the seed for the RNG used in the planner</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, we get the relevant information from agent_info </span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> we use np.random.RandomState(seed) to set the two different RNGs</span></span><br><span class="line">        <span class="comment"># for the planner and the rest of the code</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.num_states = agent_info[<span class="string">"num_states"</span>]</span><br><span class="line">            self.num_actions = agent_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">"You need to pass both 'num_states' and 'num_actions' \</span></span><br><span class="line"><span class="string">                   in agent_info to initialize the action-value table"</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">"discount"</span>, <span class="number">0.95</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">"step_size"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">"epsilon"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.planning_steps = agent_info.get(<span class="string">"planning_steps"</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">'random_seed'</span>, <span class="number">42</span>))</span><br><span class="line">        self.planning_rand_generator = np.random.RandomState(agent_info.get(<span class="string">'planning_random_seed'</span>, <span class="number">42</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, we initialize the attributes required by the agent, e.g., q_values, model, etc.</span></span><br><span class="line">        <span class="comment"># A simple way to implement the model is to have a dictionary of dictionaries, </span></span><br><span class="line">        <span class="comment">#        mapping each state to a dictionary which maps actions to (reward, next state) tuples.</span></span><br><span class="line">        self.q_values = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.actions = list(range(self.num_actions))</span><br><span class="line">        self.past_action = <span class="number">-1</span></span><br><span class="line">        self.past_state = <span class="number">-1</span></span><br><span class="line">        self.model = &#123;&#125; <span class="comment"># model is a dictionary of dictionaries, which maps states to actions to </span></span><br><span class="line">                        <span class="comment"># (reward, next_state) tuples</span></span><br></pre></td></tr></table></figure><p>Now let’s create the <code>update_model</code> method, which performs the ‘Model Update’ step in the pseudocode. It takes a <code>(s, a, s&#39;, r)</code> tuple and stores the next state and reward corresponding to a state-action pair.</p><p>Remember, because the environment is deterministic, an easy way to implement the model is to have a dictionary of encountered states, each mapping to a dictionary of actions taken in those states, which in turn maps to a tuple of next state and reward. In this way, the model can be easily accessed by <code>model[s][a]</code>, which would return the <code>(s&#39;, r)</code> tuple.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_model</span><span class="params">(self, past_state, past_action, state, reward)</span>:</span></span><br><span class="line">    <span class="string">"""updates the model </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        past_state       (int): s</span></span><br><span class="line"><span class="string">        past_action      (int): a</span></span><br><span class="line"><span class="string">        state            (int): s'</span></span><br><span class="line"><span class="string">        reward           (int): r</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Update the model with the (s,a,s',r) tuple (1~4 lines)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> past_state <span class="keyword">in</span> self.model:</span><br><span class="line">        self.model[past_state] = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> past_action <span class="keyword">in</span> self.model[past_state]:</span><br><span class="line">        self.model[past_state][past_action] = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    self.model[past_state][past_action] = (state,reward)</span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-update-model"><a href="#Test-update-model" class="headerlink" title="Test update_model()"></a>Test <code>update_model()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (past_state, past_action, state, reward)</span></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="comment"># action 2 in state 0 leads back to state 0 with a reward of 1</span></span><br><span class="line">    <span class="comment"># or taking action 3 leads to state 1 with reward of 2</span></span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="comment"># taking action 0 in state 2 leads to state 1 with a reward of 1</span></span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><p>Next, you will implement the planning step, the crux of the Dyna-Q algorithm. You shall be calling this <code>planning_step</code> method at every timestep of every trajectory.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">planning_step</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""performs planning, i.e. indirect RL.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The indirect RL step:</span></span><br><span class="line">    <span class="comment"># - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Query the model with this state-action pair for the predicted next state and reward.(~1 line)</span></span><br><span class="line">    <span class="comment"># - Update the action values with this simulated experience.                            (2~4 lines)</span></span><br><span class="line">    <span class="comment"># - Repeat for the required number of planning steps.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the update equation is different for terminal and non-terminal transitions. </span></span><br><span class="line">    <span class="comment"># To differentiate between a terminal and a non-terminal next state, assume that the model stores</span></span><br><span class="line">    <span class="comment"># the terminal state as a dummy state like -1</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Important: remember you have a random number generator 'planning_rand_generator' as </span></span><br><span class="line">    <span class="comment">#     a part of the class which you need to use as self.planning_rand_generator.choice()</span></span><br><span class="line">    <span class="comment">#     For the sake of reproducibility and grading, *do not* use anything else like </span></span><br><span class="line">    <span class="comment">#     np.random.choice() for performing search control.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.planning_steps):</span><br><span class="line">    </span><br><span class="line">        s = self.planning_rand_generator.choice(list(self.model.keys()))</span><br><span class="line">        a = self.planning_rand_generator.choice(list(self.model[s].keys()))</span><br><span class="line"></span><br><span class="line">        (next_s,r) = self.model[s][a]</span><br><span class="line">        <span class="keyword">if</span> next_s == <span class="number">-1</span>:</span><br><span class="line">            expect = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            expect = self.gamma * np.max(self.q_values[next_s])</span><br><span class="line">        self.q_values[s,a] += self.step_size * (r + expect - self.q_values[s,a])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-planning-step"><a href="#Test-planning-step" class="headerlink" title="Test planning_step()"></a>Test <code>planning_step()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">2</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">agent.planning_step()</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q_values, expected_values))</span><br></pre></td></tr></table></figure><p>Now before you move on to implement the rest of the agent methods, here are the helper functions that you’ve used in the previous assessments for choosing an action using an $\epsilon$-greedy policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">    <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q_values (Numpy array): the array of action values</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        action (int): an action with the highest value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    top = float(<span class="string">"-inf"</span>)</span><br><span class="line">    ties = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">        <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">            top = q_values[i]</span><br><span class="line">            ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">            ties.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action_egreedy</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""returns an action using an epsilon-greedy policy w.r.t. the current action-value function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Important: assume you have a random number generator 'rand_generator' as a part of the class</span></span><br><span class="line"><span class="string">                which you can use as self.rand_generator.choice() or self.rand_generator.rand()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (List): coordinates of the agent (two elements)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action taken w.r.t. the aforementioned epsilon-greedy policy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">        action = self.rand_generator.choice(self.actions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        values = self.q_values[state]</span><br><span class="line">        action = self.argmax(values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><p>Next, you will implement the rest of the agent-related methods, namely <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the experiment starts, </span></span><br><span class="line"><span class="string">    called after the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) the first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># given the state, select the action using self.choose_action_egreedy()), </span></span><br><span class="line">    <span class="comment"># and save current state and action (~2 lines)</span></span><br><span class="line">    <span class="comment">### self.past_state = ?</span></span><br><span class="line">    <span class="comment">### self.past_action = ?</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = self.choose_action_egreedy(state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">            last step</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The action the agent takes given this state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># - Direct-RL step (~1-3 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step (~1 line)</span></span><br><span class="line">    <span class="comment"># - `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment"># - Action Selection step (~1 line)</span></span><br><span class="line">    <span class="comment"># Save the current state and action before returning the action to be performed. (~2 lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward + self.gamma * np.max(self.q_values[state]) - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, state, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    action = self.choose_action_egreedy(state)</span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = action</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Called when the agent terminates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">            terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># - Direct RL update with this final transition (1~2 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step with this final transition (~1 line)</span></span><br><span class="line">    <span class="comment"># - One final `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: the final transition needs to be handled carefully. Since there is no next state, </span></span><br><span class="line">    <span class="comment">#       you will have to pass a dummy state (like -1), which you will be using in the planning_step() to </span></span><br><span class="line">    <span class="comment">#       differentiate between updates with usual terminal and non-terminal transitions.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward  - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, <span class="number">-1</span>, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-agent-start-agent-step-and-agent-end"><a href="#Test-agent-start-agent-step-and-agent-end" class="headerlink" title="Test agent_start(), agent_step(), and agent_end()"></a>Test <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">2</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ----------------</span></span><br><span class="line"><span class="comment"># test agent start</span></span><br><span class="line"><span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.model == &#123;&#125;</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q_values == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.3439</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">1</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.41051</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.01</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br></pre></td></tr></table></figure><h3 id="Experiment-Dyna-Q-agent-in-the-maze-environment"><a href="#Experiment-Dyna-Q-agent-in-the-maze-environment" class="headerlink" title="Experiment: Dyna-Q agent in the maze environment"></a>Experiment: Dyna-Q agent in the maze environment</h3><p>Alright. Now we have all the components of the <code>DynaQAgent</code> ready. Let’s try it out on the maze environment! </p><p>The next cell runs an experiment on this maze environment to test your implementation. The initial action values are $0$, the step-size parameter is $0.125$. and the exploration parameter is $\epsilon=0.1$. After the experiment, the sum of rewards in each episode should match the correct result.</p><p>We will try planning steps of $0,5,50$ and compare their performance in terms of the average number of steps taken to reach the goal state in the aforementioned maze environment. For scientific rigor, we will run each experiment $30$ times. In each experiment, we set the initial random-number-generator (RNG) seeds for a fair comparison across algorithms.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_episodes = exp_parameters[<span class="string">'num_episodes'</span>]</span><br><span class="line">    planning_steps_all = agent_parameters[<span class="string">'planning_steps'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = env_parameters                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  <span class="comment"># We pass the agent the information it needs. </span></span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"epsilon"</span>: agent_parameters[<span class="string">"epsilon"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes)) <span class="comment"># for collecting metrics </span></span><br><span class="line">    log_data = &#123;<span class="string">'planning_steps_all'</span> : planning_steps_all&#125;                     <span class="comment"># that shall be plotted later</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Planning steps : '</span>, planning_steps)</span><br><span class="line">        os.system(<span class="string">'sleep 0.5'</span>)                    <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"planning_steps"</span>] = planning_steps  </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = i</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = i</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)          <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_episodes):</span><br><span class="line"></span><br><span class="line">                rl_glue.rl_start()                <span class="comment"># We start an episode. Here we aren't using rl_glue.rl_episode()</span></span><br><span class="line">                                                  <span class="comment"># like the other assessments because we'll be requiring some </span></span><br><span class="line">                is_terminal = <span class="keyword">False</span>               <span class="comment"># data from within the episodes in some of the experiments here </span></span><br><span class="line">                num_steps = <span class="number">0</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, _, action, is_terminal = rl_glue.rl_step()  <span class="comment"># The environment and agent take a step </span></span><br><span class="line">                    num_steps += <span class="number">1</span>                                      <span class="comment"># and return the reward and action taken.</span></span><br><span class="line"></span><br><span class="line">                all_averages[idx][i][j] = num_steps</span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'all_averages'</span>] = all_averages</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_data</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_steps_per_episode</span><span class="params">(data)</span>:</span></span><br><span class="line">    all_averages = data[<span class="string">'all_averages'</span>]</span><br><span class="line">    planning_steps_all = data[<span class="string">'planning_steps_all'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line">        plt.plot(np.mean(all_averages[i], axis=<span class="number">0</span>), label=<span class="string">'Planning steps = '</span>+str(planning_steps))</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Episodes'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Steps\nper\nepisode'</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">    plt.axhline(y=<span class="number">16</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">40</span>,                 <span class="comment"># The number of episodes per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">0</span>, <span class="number">5</span>, <span class="number">50</span>]       <span class="comment"># The list of planning_steps we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">dataq = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_steps_per_episode(dataq)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  0100%|██████████| 30/30 [00:07&lt;00:00,  3.97it/s]Planning steps :  5100%|██████████| 30/30 [00:09&lt;00:00,  3.24it/s]Planning steps :  50100%|██████████| 30/30 [00:53&lt;00:00,  1.79s/it]</code></pre><p><img src="output_27_6.png" alt="png"></p><p>What do you notice?</p><p>As the number of planning steps increases, the number of episodes taken to reach the goal decreases rapidly. Remember that the RNG seed was set the same for all the three values of planning steps, resulting in the same number of steps taken to reach the goal in the first episode. Thereafter, the performance improves. The slowest improvement is when there are $n=0$ planning steps, i.e., for the non-planning Q-learning agent, even though the step size parameter was optimized for it. Note that the grey dotted line shows the minimum number of steps required to reach the goal state under the optimal greedy policy.</p><hr><h3 id="Experiment-s-Dyna-Q-agent-in-the-changing-maze-environment"><a href="#Experiment-s-Dyna-Q-agent-in-the-changing-maze-environment" class="headerlink" title="Experiment(s): Dyna-Q agent in the _changing_ maze environment"></a>Experiment(s): Dyna-Q agent in the _changing_ maze environment</h3><p>Great! Now let us see how Dyna-Q performs on the version of the maze in which a shorter path opens up after 3000 steps. The rest of the transition and reward dynamics remain the same. </p><p><img src="shortcut_env_after.png" alt="environment" width="800"></p><p>Before you proceed, take a moment to think about what you expect to see. Will Dyna-Q find the new, shorter path to the goal? If so, why? If not, why not?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment_with_state_visitations</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters, result_file_name)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_max_steps = exp_parameters[<span class="string">'num_max_steps'</span>]</span><br><span class="line">    planning_steps_all = agent_parameters[<span class="string">'planning_steps'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">"change_at_n"</span> : env_parameters[<span class="string">"change_at_n"</span>]&#125;                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  </span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"epsilon"</span>: agent_parameters[<span class="string">"epsilon"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    state_visits_before_change = np.zeros((len(planning_steps_all), num_runs, <span class="number">54</span>))  <span class="comment"># For saving the number of</span></span><br><span class="line">    state_visits_after_change = np.zeros((len(planning_steps_all), num_runs, <span class="number">54</span>))   <span class="comment">#     state-visitations </span></span><br><span class="line">    cum_reward_all = np.zeros((len(planning_steps_all), num_runs, num_max_steps))   <span class="comment"># For saving the cumulative reward</span></span><br><span class="line">    log_data = &#123;<span class="string">'planning_steps_all'</span> : planning_steps_all&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Planning steps : '</span>, planning_steps)</span><br><span class="line">        os.system(<span class="string">'sleep 1'</span>)          <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"planning_steps"</span>] = planning_steps  <span class="comment"># We pass the agent the information it needs. </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = run</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = run</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)  <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            num_steps = <span class="number">0</span></span><br><span class="line">            cum_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line"></span><br><span class="line">                state, _ = rl_glue.rl_start()  <span class="comment"># We start the experiment. We'll be collecting the </span></span><br><span class="line">                is_terminal = <span class="keyword">False</span>            <span class="comment"># state-visitation counts to visiualize the learned policy</span></span><br><span class="line">                <span class="keyword">if</span> num_steps &lt; env_parameters[<span class="string">"change_at_n"</span>]: </span><br><span class="line">                    state_visits_before_change[idx][run][state] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state_visits_after_change[idx][run][state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal <span class="keyword">and</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()  </span><br><span class="line">                    num_steps += <span class="number">1</span></span><br><span class="line">                    cum_reward += reward</span><br><span class="line">                    cum_reward_all[idx][run][num_steps] = cum_reward</span><br><span class="line">                    <span class="keyword">if</span> num_steps &lt; env_parameters[<span class="string">"change_at_n"</span>]:</span><br><span class="line">                        state_visits_before_change[idx][run][state] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        state_visits_after_change[idx][run][state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'state_visits_before'</span>] = state_visits_before_change</span><br><span class="line">    log_data[<span class="string">'state_visits_after'</span>] = state_visits_after_change</span><br><span class="line">    log_data[<span class="string">'cum_reward_all'</span>] = cum_reward_all</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cumulative_reward</span><span class="params">(data_all, item_key, y_key, y_axis_label, legend_prefix, title)</span>:</span></span><br><span class="line">    data_y_all = data_all[y_key]</span><br><span class="line">    items = data_all[item_key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(items):</span><br><span class="line">        plt.plot(np.mean(data_y_all[i], axis=<span class="number">0</span>), label=legend_prefix+str(item))</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=<span class="number">3000</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Timesteps'</span>)</span><br><span class="line">    plt.ylabel(y_axis_label, rotation=<span class="number">0</span>, labelpad=<span class="number">60</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>Did you notice that the environment changes after a fixed number of _steps_ and not episodes? </p><p>This is because the environment is separate from the agent, and the environment changes irrespective of the length of each episode (i.e., the number of environmental interactions per episode) that the agent perceives. And hence we are now plotting the data per step or interaction of the agent and the environment, in order to comfortably see the differences in the behaviours of the agents before and after the environment changes.  </p><p>Okay, now we will first plot the cumulative reward obtained by the agent per interaction with the environment, averaged over 10 runs of the experiment on this changing world. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">10</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">5</span>, <span class="number">10</span>, <span class="number">50</span>]      <span class="comment"># The list of planning_steps we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">dataq = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, <span class="string">"Dyna-Q_shortcut_steps"</span>)    </span><br><span class="line">plot_cumulative_reward(dataq, <span class="string">'planning_steps_all'</span>, <span class="string">'cum_reward_all'</span>, <span class="string">'Cumulative\nreward'</span>, <span class="string">'Planning steps = '</span>, <span class="string">'Dyna-Q : Varying planning_steps'</span>)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  5100%|██████████| 10/10 [00:10&lt;00:00,  1.08s/it]Planning steps :  10100%|██████████| 10/10 [00:16&lt;00:00,  1.70s/it]Planning steps :  50100%|██████████| 10/10 [01:19&lt;00:00,  7.99s/it]</code></pre><p><img src="output_34_6.png" alt="png"></p><p>We observe that the slope of the curves is almost constant. If the agent had discovered the shortcut and begun using it, we would expect to see an increase in the slope of the curves towards the later stages of training. This is because the agent can get to the goal state faster and get the positive reward. Note that the timestep at which the shortcut opens up is marked by the grey dotted line.</p><p>Note that this trend is constant across the increasing number of planning steps.</p><p>Now let’s check the heatmap of the state visitations of the agent with <code>planning_steps=10</code> during training, before and after the shortcut opens up after 3000 timesteps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_state_visitations</span><span class="params">(data, plot_titles, idx)</span>:</span></span><br><span class="line">    data_keys = [<span class="string">"state_visits_before"</span>, <span class="string">"state_visits_after"</span>]</span><br><span class="line">    positions = [<span class="number">211</span>,<span class="number">212</span>]</span><br><span class="line">    titles = plot_titles</span><br><span class="line">    wall_ends = [<span class="keyword">None</span>,<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line"></span><br><span class="line">        state_visits = data[data_keys[i]][idx]</span><br><span class="line">        average_state_visits = np.mean(state_visits, axis=<span class="number">0</span>)</span><br><span class="line">        grid_state_visits = np.rot90(average_state_visits.reshape((<span class="number">6</span>,<span class="number">9</span>)).T)</span><br><span class="line">        grid_state_visits[<span class="number">2</span>,<span class="number">1</span>:wall_ends[i]] = np.nan <span class="comment"># walls</span></span><br><span class="line">        <span class="comment">#print(average_state_visits.reshape((6,9)))</span></span><br><span class="line">        plt.subplot(positions[i])</span><br><span class="line">        plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">'gray'</span>, linewidth=<span class="number">1</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">        plt.text(<span class="number">3</span>+<span class="number">0.5</span>, <span class="number">0</span>+<span class="number">0.5</span>, <span class="string">'S'</span>, horizontalalignment=<span class="string">'center'</span>, verticalalignment=<span class="string">'center'</span>)</span><br><span class="line">        plt.text(<span class="number">8</span>+<span class="number">0.5</span>, <span class="number">5</span>+<span class="number">0.5</span>, <span class="string">'G'</span>, horizontalalignment=<span class="string">'center'</span>, verticalalignment=<span class="string">'center'</span>)</span><br><span class="line">        plt.title(titles[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        cm = plt.get_cmap()</span><br><span class="line">        cm.set_bad(<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">1.</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    cbar = plt.colorbar(cax=cax)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">plot_state_visitations(dataq, [<span class="string">'Dyna-Q : State visitations before the env changes'</span>, <span class="string">'Dyna-Q : State visitations after the env changes'</span>], <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="output_37_0.png" alt="png"></p><p>What do you observe?</p><p>The state visitation map looks almost the same before and after the shortcut opens. This means that the Dyna-Q agent hasn’t quite discovered and started exploiting the new shortcut.</p><p>Now let’s try increasing the exploration parameter $\epsilon$ to see if it helps the Dyna-Q agent discover the shortcut. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment_only_cumulative_reward</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_max_steps = exp_parameters[<span class="string">'num_max_steps'</span>]</span><br><span class="line">    epsilons = agent_parameters[<span class="string">'epsilons'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">"change_at_n"</span> : env_parameters[<span class="string">"change_at_n"</span>]&#125;                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  </span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"planning_steps"</span>: agent_parameters[<span class="string">"planning_steps"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    log_data = &#123;<span class="string">'epsilons'</span> : epsilons&#125; </span><br><span class="line">    cum_reward_all = np.zeros((len(epsilons), num_runs, num_max_steps))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> eps_idx, epsilon <span class="keyword">in</span> enumerate(epsilons):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Agent : Dyna-Q, epsilon : %f'</span> % epsilon)</span><br><span class="line">        os.system(<span class="string">'sleep 1'</span>)          <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"epsilon"</span>] = epsilon</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = run</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = run</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)  <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            num_steps = <span class="number">0</span></span><br><span class="line">            cum_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line"></span><br><span class="line">                rl_glue.rl_start()  <span class="comment"># We start the experiment</span></span><br><span class="line">                is_terminal = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal <span class="keyword">and</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line">                    reward, _, action, is_terminal = rl_glue.rl_step()  <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                    <span class="comment"># the reward, and action taken.</span></span><br><span class="line">                    num_steps += <span class="number">1</span></span><br><span class="line">                    cum_reward += reward</span><br><span class="line">                    cum_reward_all[eps_idx][run][num_steps] = cum_reward</span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'cum_reward_all'</span>] = cum_reward_all</span><br><span class="line">    <span class="keyword">return</span> log_data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : <span class="number">10</span>,</span><br><span class="line">    <span class="string">"epsilons"</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]    <span class="comment"># The list of epsilons we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">data = run_experiment_only_cumulative_reward(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_cumulative_reward(data, <span class="string">'epsilons'</span>, <span class="string">'cum_reward_all'</span>, <span class="string">'Cumulative\nreward'</span>, <span class="string">r'$\epsilon$ = '</span>, <span class="string">r'Dyna-Q : Varying $\epsilon$'</span>)</span><br></pre></td></tr></table></figure><pre><code>Agent : Dyna-Q, epsilon : 0.100000100%|██████████| 30/30 [00:52&lt;00:00,  1.75s/it]Agent : Dyna-Q, epsilon : 0.200000100%|██████████| 30/30 [00:49&lt;00:00,  1.65s/it]Agent : Dyna-Q, epsilon : 0.400000100%|██████████| 30/30 [00:50&lt;00:00,  1.69s/it]Agent : Dyna-Q, epsilon : 0.800000100%|██████████| 30/30 [00:52&lt;00:00,  1.74s/it]</code></pre><p><img src="output_40_8.png" alt="png"></p><p>What do you observe?</p><p>Increasing the exploration via the $\epsilon$-greedy strategy does not seem to be helping. In fact, the agent’s cumulative reward decreases because it is spending more and more time trying out the exploratory actions.</p><p>Can we do better…? </p><h2 id="Section-2-Dyna-Q"><a href="#Section-2-Dyna-Q" class="headerlink" title="Section 2: Dyna-Q+"></a>Section 2: Dyna-Q+</h2><p>The motivation behind Dyna-Q+ is to give a bonus reward for actions that haven’t been tried for a long time, since there is a greater chance that the dynamics for that actions might have changed.</p><p>In particular, if the modeled reward for a transition is $r$, and the transition has not been tried in $\tau(s,a)$ time steps, then planning updates are done as if that transition produced a reward of $r + \kappa \sqrt{ \tau(s,a)}$, for some small $\kappa$. </p><p>Let’s implement that!</p><p>Based on your <code>DynaQAgent</code>, create a new class <code>DynaQPlusAgent</code> to implement the aforementioned exploration heuristic. Additionally :</p><ol><li>actions that had never been tried before from a state should now be allowed to be considered in the planning step,</li><li>and the initial model for such actions is that they lead back to the same state with a reward of zero.</li></ol><p>At this point, you might want to refer to the video lectures and <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=188" target="_blank" rel="noopener">Section 8.3</a> of the RL textbook for a refresher on Dyna-Q+.</p><p>As usual, let’s break this down in pieces and do it one-by-one.</p><p>First of all, check out the <code>agent_init</code> method below. In particular, pay attention to the attributes which are new to <code>DynaQPlusAgent</code>â€“ state-visitation counts $\tau$ and the scaling parameter $\kappa$ â€“ because you shall be using them later. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynaQPlusAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                num_states (int): The number of states,</span></span><br><span class="line"><span class="string">                num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">                epsilon (float): The parameter for epsilon-greedy exploration,</span></span><br><span class="line"><span class="string">                step_size (float): The step-size,</span></span><br><span class="line"><span class="string">                discount (float): The discount factor,</span></span><br><span class="line"><span class="string">                planning_steps (int): The number of planning steps per environmental interaction</span></span><br><span class="line"><span class="string">                kappa (float): The scaling factor for the reward bonus</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                random_seed (int): the seed for the RNG used in epsilon-greedy</span></span><br><span class="line"><span class="string">                planning_random_seed (int): the seed for the RNG used in the planner</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, we get the relevant information from agent_info </span></span><br><span class="line">        <span class="comment"># Note: we use np.random.RandomState(seed) to set the two different RNGs</span></span><br><span class="line">        <span class="comment"># for the planner and the rest of the code</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.num_states = agent_info[<span class="string">"num_states"</span>]</span><br><span class="line">            self.num_actions = agent_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">"You need to pass both 'num_states' and 'num_actions' \</span></span><br><span class="line"><span class="string">                   in agent_info to initialize the action-value table"</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">"discount"</span>, <span class="number">0.95</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">"step_size"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">"epsilon"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.planning_steps = agent_info.get(<span class="string">"planning_steps"</span>, <span class="number">10</span>)</span><br><span class="line">        self.kappa = agent_info.get(<span class="string">"kappa"</span>, <span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">'random_seed'</span>, <span class="number">42</span>))</span><br><span class="line">        self.planning_rand_generator = np.random.RandomState(agent_info.get(<span class="string">'planning_random_seed'</span>, <span class="number">42</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, we initialize the attributes required by the agent, e.g., q_values, model, tau, etc.</span></span><br><span class="line">        <span class="comment"># The visitation-counts can be stored as a table as well, like the action values </span></span><br><span class="line">        self.q_values = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.tau = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.actions = list(range(self.num_actions))</span><br><span class="line">        self.past_action = <span class="number">-1</span></span><br><span class="line">        self.past_state = <span class="number">-1</span></span><br><span class="line">        self.model = &#123;&#125;</span><br></pre></td></tr></table></figure><p>Now first up, implement the <code>update_model</code> method. Note that this is different from Dyna-Q in the aforementioned way.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_model</span><span class="params">(self, past_state, past_action, state, reward)</span>:</span></span><br><span class="line">    <span class="string">"""updates the model </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        past_state  (int): s</span></span><br><span class="line"><span class="string">        past_action (int): a</span></span><br><span class="line"><span class="string">        state       (int): s'</span></span><br><span class="line"><span class="string">        reward      (int): r</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Recall that when adding a state-action to the model, if the agent is visiting the state</span></span><br><span class="line">    <span class="comment">#    for the first time, then the remaining actions need to be added to the model as well</span></span><br><span class="line">    <span class="comment">#    with zero reward and a transition into itself.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: do *not* update the visitation-counts here. We will do that in `agent_step`.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># (3 lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> past_state <span class="keyword">not</span> <span class="keyword">in</span> self.model:</span><br><span class="line">        self.model[past_state] = &#123;past_action : (state, reward)&#125;</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> self.actions:</span><br><span class="line">            <span class="keyword">if</span> action != past_action:</span><br><span class="line">                self.model[past_state][action] = (past_state, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.model[past_state][past_action] = (state, reward)</span><br></pre></td></tr></table></figure><h3 id="Test-update-model-1"><a href="#Test-update-model-1" class="headerlink" title="Test update_model()"></a>Test <code>update_model()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><p>Next, you will implement the <code>planning_step()</code> method. This will be very similar to the one you implemented in <code>DynaQAgent</code>, but here you will be adding the exploration bonus to the reward in the simulated transition.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">planning_step</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""performs planning, i.e. indirect RL.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The indirect RL step:</span></span><br><span class="line">    <span class="comment"># - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Query the model with this state-action pair for the predicted next state and reward.(~1 line)</span></span><br><span class="line">    <span class="comment"># - **Add the bonus to the reward** (~1 line)</span></span><br><span class="line">    <span class="comment"># - Update the action values with this simulated experience.                            (2~4 lines)</span></span><br><span class="line">    <span class="comment"># - Repeat for the required number of planning steps.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the update equation is different for terminal and non-terminal transitions. </span></span><br><span class="line">    <span class="comment"># To differentiate between a terminal and a non-terminal next state, assume that the model stores</span></span><br><span class="line">    <span class="comment"># the terminal state as a dummy state like -1</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Important: remember you have a random number generator 'planning_rand_generator' as </span></span><br><span class="line">    <span class="comment">#     a part of the class which you need to use as self.planning_rand_generator.choice()</span></span><br><span class="line">    <span class="comment">#     For the sake of reproducibility and grading, *do not* use anything else like </span></span><br><span class="line">    <span class="comment">#     np.random.choice() for performing search control.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.planning_steps):</span><br><span class="line">    </span><br><span class="line">        s = self.planning_rand_generator.choice(list(self.model.keys()))</span><br><span class="line">        a = self.planning_rand_generator.choice(list(self.model[s].keys()))</span><br><span class="line"></span><br><span class="line">        (next_s,r) = self.model[s][a]</span><br><span class="line">        r += self.kappa * np.sqrt(self.tau[s][a])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> next_s == <span class="number">-1</span>:</span><br><span class="line">            expect = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            expect = self.gamma * np.max(self.q_values[next_s])</span><br><span class="line">        self.q_values[s,a] += self.step_size * (r + expect - self.q_values[s,a])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-planning-step-1"><a href="#Test-planning-step-1" class="headerlink" title="Test planning_step()"></a>Test <code>planning_step()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test code for planning_step() ##</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"kappa"</span>: <span class="number">0.001</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">2</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">2</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.planning_step()</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;, </span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.10014142</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.00036373</span>, <span class="number">0</span>, <span class="number">0.00017321</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br></pre></td></tr></table></figure><p>Again, before you move on to implement the rest of the agent methods, here are the couple of helper functions that you’ve used in the previous assessments for choosing an action using an $\epsilon$-greedy policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">    <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q_values (Numpy array): the array of action values</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        action (int): an action with the highest value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    top = float(<span class="string">"-inf"</span>)</span><br><span class="line">    ties = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">        <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">            top = q_values[i]</span><br><span class="line">            ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">            ties.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action_egreedy</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""returns an action using an epsilon-greedy policy w.r.t. the current action-value function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Important: assume you have a random number generator 'rand_generator' as a part of the class</span></span><br><span class="line"><span class="string">                which you can use as self.rand_generator.choice() or self.rand_generator.rand()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (List): coordinates of the agent (two elements)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action taken w.r.t. the aforementioned epsilon-greedy policy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">        action = self.rand_generator.choice(self.actions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        values = self.q_values[state]</span><br><span class="line">        action = self.argmax(values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><p>Now implement the rest of the agent-related methods, namely <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>. Again, these will be very similar to the ones in the <code>DynaQAgent</code>, but you will have to think of a way to update the counts since the last visit.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># given the state, select the action using self.choose_action_egreedy(), </span></span><br><span class="line">    <span class="comment"># and save current state and action (~2 lines)</span></span><br><span class="line">    <span class="comment">### self.past_state = ?</span></span><br><span class="line">    <span class="comment">### self.past_action = ?</span></span><br><span class="line">    <span class="comment"># Note that the last-visit counts are not updated here.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = self.choose_action_egreedy(state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">            last step</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The action the agent is taking.</span></span><br><span class="line"><span class="string">    """</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update the last-visited counts (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Direct-RL step (1~3 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step (~1 line)</span></span><br><span class="line">    <span class="comment"># - `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment"># - Action Selection step (~1 line)</span></span><br><span class="line">    <span class="comment"># Save the current state and action before returning the action to be performed. (~2 lines)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.tau += <span class="number">1</span></span><br><span class="line">    self.tau[self.past_state,self.past_action] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward + self.gamma * np.max(self.q_values[state]) - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, state, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    action = self.choose_action_egreedy(state)</span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = action</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Called when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">            terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Again, add the same components you added in agent_step to augment Dyna-Q into Dyna-Q+</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.tau += <span class="number">1</span></span><br><span class="line">    self.tau[self.past_state,self.past_action] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward  - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, <span class="number">-1</span>, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-agent-start-agent-step-and-agent-end-1"><a href="#Test-agent-start-agent-step-and-agent-end-1" class="headerlink" title="Test agent_start(), agent_step(), and agent_end()"></a>Test <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>,</span><br><span class="line">              <span class="string">"kappa"</span>: <span class="number">0.001</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>) <span class="comment"># state</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.tau, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> agent.model == &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_tau = np.array([</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.tau == expected_tau)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.0191</span>, <span class="number">0.271</span>, <span class="number">0.0</span>, <span class="number">0.0191</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.000183847763</span>, <span class="number">0.000424264069</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;, </span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_tau = np.array([</span><br><span class="line">    [<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.tau == expected_tau)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.0191</span>, <span class="number">0.344083848</span>, <span class="number">0</span>, <span class="number">0.0444632051</span>],</span><br><span class="line">    [<span class="number">0.0191732051</span>, <span class="number">0.19</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.000183847763</span>, <span class="number">0.000424264069</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;<span class="number">0</span>: &#123;<span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>), <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">0</span>, <span class="number">0</span>), <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>)&#125;, <span class="number">2</span>: &#123;<span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">0</span>: (<span class="number">2</span>, <span class="number">0</span>), <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>)&#125;, <span class="number">1</span>: &#123;<span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>), <span class="number">0</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>)&#125;&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><h3 id="Experiment-Dyna-Q-agent-in-the-changing-environment"><a href="#Experiment-Dyna-Q-agent-in-the-changing-environment" class="headerlink" title="Experiment: Dyna-Q+ agent in the _changing_ environment"></a>Experiment: Dyna-Q+ agent in the _changing_ environment</h3><p>Okay, now we’re ready to test our Dyna-Q+ agent on the Shortcut Maze. As usual, we will average the results over 30 independent runs of the experiment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">50</span>]      </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQPlusAgent          <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">data_qplus = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, <span class="string">"Dyna-Q+"</span>)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  50100%|██████████| 30/30 [04:21&lt;00:00,  8.72s/it]</code></pre><p>Let’s compare the Dyna-Q and Dyna-Q+ agents with <code>planning_steps=50</code> each.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cumulative_reward_comparison</span><span class="params">(data1, data2)</span>:</span></span><br><span class="line"></span><br><span class="line">    cum_reward_q = data1[<span class="string">'cum_reward_all'</span>][<span class="number">2</span>]</span><br><span class="line">    cum_reward_qPlus = data2[<span class="string">'cum_reward_all'</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.plot(np.mean(cum_reward_qPlus, axis=<span class="number">0</span>), label=<span class="string">'Dyna-Q+'</span>)</span><br><span class="line">    plt.plot(np.mean(cum_reward_q, axis=<span class="number">0</span>), label=<span class="string">'Dyna-Q'</span>)</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=<span class="number">3000</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Timesteps'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Cumulative\nreward'</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">60</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.title(<span class="string">'Average performance of Dyna-Q and Dyna-Q+ agents in the Shortcut Maze\n'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">plot_cumulative_reward_comparison(dataq, data_qplus)</span><br></pre></td></tr></table></figure><p><img src="output_64_0.png" alt="png"></p><p>What do you observe? (For reference, your graph should look like <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=189" target="_blank" rel="noopener">Figure 8.5 in Chapter 8</a> of the RL textbook)</p><p>The slope of the curve increases for the Dyna-Q+ curve shortly after the shortcut opens up after 3000 steps, which indicates that the rate of receiving the positive reward increases. This implies that the Dyna-Q+ agent finds the shorter path to the goal.</p><p>To verify this, let us plot the state-visitations of the Dyna-Q+ agent before and after the shortcut opens up.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">plot_state_visitations(data_qplus, [<span class="string">'Dyna-Q+ : State visitations before the env changes'</span>, <span class="string">'Dyna-Q+ : State visitations after the env changes'</span>], <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="output_66_0.png" alt="png"></p><p>What do you observe?</p><p>Before the shortcut opens up, like Dyna-Q, the Dyna-Q+ agent finds the sole, long path to the goal. But because the Dyna-Q+ agent keeps exploring, it succeeds in discovering the shortcut once it opens up, which leads to the goal faster. So the bonus reward heuristic is effective in helping the agent explore and find changes in the environment without degrading the performance. </p><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations! You have:</p><ol><li>implemented Dyna-Q, a model-based approach to RL;</li><li>implemented Dyna-Q+, a variant of Dyna-Q with an exploration bonus that encourages exploration; </li><li>conducted scientific experiments to empirically validate the exploration/exploitation dilemma in the planning context on an environment that changes with time.</li></ol><p>Some points to ponder about:</p><ol><li>At what cost does Dyna-Q+ improve over Dyna-Q?</li><li>In general, what is the trade-off of using model-based methods like Dyna-Q over model-free methods like Q-learning?</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-Dyna-Q-and-Dyna-Q&quot;&gt;&lt;a href=&quot;#Assignment-Dyna-Q-and-Dyna-Q&quot; class=&quot;headerlink&quot; title=&quot;Assignment: Dyna-Q and Dyna-Q+&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Planning and learning with Tabular Methods</title>
    <link href="https://zhangruochi.com/Planning-and-learning-with-Tabular-Methods/2020/09/29/"/>
    <id>https://zhangruochi.com/Planning-and-learning-with-Tabular-Methods/2020/09/29/</id>
    <published>2020-09-29T08:09:45.000Z</published>
    <updated>2020-09-29T08:19:02.711Z</updated>
    
    <content type="html"><![CDATA[<p>We develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heuristic search, and methods that can be used without a model, such as Monte Carlo and temporal-difference methods. These are respectively called <strong>model-based</strong> and <strong>model-free</strong> reinforcement learning methods. Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning. Although there are real di↵erences between these two kinds of methods, there are also great similarities.</p><ul><li>All state-space planning methods involve computing value functions as a key intermediate step toward improving the policy</li><li>They compute value functions by updates or backup operations applied to simulated experience.</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions.</li><li>Distribution model produce a description of all possibilities and their probabilities. Sample model produce just one of the possibilities and their probabilities.</li><li>当给定一个 state 和一个 action 时，distribution model 可以生成所有可能的状态转移，而sample model只能给出一个可能的状态转移</li><li>当给定一个 state 和 Policy 时，distribution model 可以获得所有可能的 episode 并得到他们出现的概率，但 sample model 只能给出一个 episode</li></ul><p>总之，distribution model 比 sample model包含更多信息，但现实中往往更容易获得sample model。简单来说，distribution model 包含了所有状态的转移概率，但sample model更像是管中窥豹，可见一斑。在DP中，我们用到的是distribution model，而在MC中我们用到的是sample model。model 是对环境的一种表达方式，（不一定是真实或完全正确的），可以用来产生仿真经验（simulation experience）。</p><h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p>从Model中生成或提升Policy 的计算过程称为 Planning:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>注意本文讨论的Planning都是state space Planning，这种Planning有两个特点：</p><ul><li>通过计算values function 来进行Policy 提升</li><li>根据simulated experience来计算value function</li></ul><p>Planning（如DP） 和learning（如MC、TD）方法的核心都是用backing-up 更新公式计算value function 的估计值。区别在于Planning 所用经验是有模型生成的simulated exprience，而learning method使用的经验是由真实环境生成的real exprience。  但两者都满足上述state space Planning结构，这表示很多思想和算法可以相互借鉴，在应用中常常用 learning 中 value function 估计值的更新公式取代 Planning 中的 value function 估计值的更新公式。例如，我们可以将Q learning 和 planning 结合，得到random-sample one-step tabular Q-planning 方法：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%"></center><p>one-step tabular Q-learning最终会收敛到一个对应于真实环境的optimal Policy，而 random-sample one-step tabular Q-planning 则收敛到一个对应于model 的optimal Policy。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heur
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Q-Learning and Expected Sarsa </title>
    <link href="https://zhangruochi.com/Q-Learning-and-Expected-Sarsa/2020/09/29/"/>
    <id>https://zhangruochi.com/Q-Learning-and-Expected-Sarsa/2020/09/29/</id>
    <published>2020-09-29T02:52:00.000Z</published>
    <updated>2020-09-29T08:10:50.768Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Q-Learning-and-Expected-Sarsa"><a href="#Assignment-2-Q-Learning-and-Expected-Sarsa" class="headerlink" title="Assignment 2 - Q-Learning and Expected Sarsa"></a>Assignment 2 - Q-Learning and Expected Sarsa</h1><p>Welcome to Course 2 Programming Assignment 2. In this notebook, you will:</p><ol><li>Implement Q-Learning with $\epsilon$-greedy action selection</li><li>Implement Expected Sarsa with $\epsilon$-greedy action selection</li><li>Investigate how these two algorithms behave on Cliff World (described on page 132 of the textbook)</li></ol><p>We will provide you with the environment and infrastructure to run an experiment (called the experiment program in RL-Glue). This notebook will provide all the code you need to run your experiment and visualise learning performance.</p><p>This assignment will be graded automatically by comparing the behavior of your agent to our implementations of Expected Sarsa and Q-learning. The random seed will be set to avoid different behavior due to randomness. We will highlight the functions you have to use for generating random samples and the number of times these functions should be called. </p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>You will need the following libraries for this assignment. We are using:</p><ol><li>numpy: the fundamental package for scientific computing with Python.</li><li>scipy: a Python library for scientific and technical computing.</li><li>matplotlib: library for plotting graphs in Python.</li><li>RL-Glue: library for reinforcement learning experiments.</li></ol><p>DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> sem</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> cliffworld_env</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">'font.size'</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">'figure.figsize'</span>: [<span class="number">10</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure><h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>In this section you will implement and test a Q-Learning agent with $\epsilon$-greedy action selection (Section 6.5 in the textbook). </p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_init_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">"num_states"</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">"epsilon"</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">"step_size"</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">"discount"</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">"seed"</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action]  += self.step_size * (reward + self.discount * np.max(current_q) - \</span><br><span class="line">                                                                        self.q[self.prev_state][self.prev_action] )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">        <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        top = float(<span class="string">"-inf"</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p><p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">3</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = QLearningAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q == expected_values)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>,  <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.02</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>, <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.1</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure><h1 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h1><p>In this section you will implement an Expected Sarsa agent with $\epsilon$-greedy action selection (Section 6.6 in the textbook). </p><h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpectedSarsaAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_init_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">"num_states"</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">"epsilon"</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">"step_size"</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">"discount"</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">"seed"</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        expect = (<span class="number">1</span> - self.epsilon) * np.max(current_q) + self.epsilon * np.average(current_q)</span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward + self.discount * expect - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">        <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        top = float(<span class="string">"-inf"</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure><h3 id="Test-1"><a href="#Test-1" class="headerlink" title="Test"></a>Test</h3><p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p><p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">3</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = ExpectedSarsaAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q == expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.28</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure><h1 id="Solving-the-Cliff-World"><a href="#Solving-the-Cliff-World" class="headerlink" title="Solving the Cliff World"></a>Solving the Cliff World</h1><p>We described the Cliff World environment in the video “Expected Sarsa in the Cliff World” in Lesson 3. This is an undiscounted episodic task and thus we set $\gamma$=1. The agent starts in the bottom left corner of the gridworld below and takes actions that move it in the four directions. Actions that would move the agent off of the cliff incur a reward of -100 and send the agent back to the start state. The reward for all other transitions is -1. An episode terminates when the agent reaches the bottom right corner. </p><p><img src="cliffworld.png" alt="Drawing" style="width: 600px;"></p><p>Using the experiment program in the cell below we now compare the agents on the Cliff World environment and plot the sum of rewards during each episode for the two agents.</p><p>The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">"Q-learning"</span>: QLearningAgent,</span><br><span class="line">    <span class="string">"Expected Sarsa"</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125; <span class="comment"># Contains sum of rewards during episode</span></span><br><span class="line">all_state_visits = &#123;&#125; <span class="comment"># Contains state visit counts during the last 10 episodes</span></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">48</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.5</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">100</span> <span class="comment"># The number of runs</span></span><br><span class="line">num_episodes = <span class="number">100</span> <span class="comment"># The number of episodes in each run</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    all_reward_sums[algorithm] = []</span><br><span class="line">    all_state_visits[algorithm] = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">        rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">        reward_sums = []</span><br><span class="line">        state_visits = np.zeros(<span class="number">48</span>)</span><br><span class="line">        last_episode_total_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">            <span class="keyword">if</span> episode &lt; num_episodes - <span class="number">10</span>:</span><br><span class="line">                <span class="comment"># Runs an episode</span></span><br><span class="line">                rl_glue.rl_episode(<span class="number">10000</span>) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="comment"># Runs an episode while keeping track of visited states</span></span><br><span class="line">                state, action = rl_glue.rl_start()</span><br><span class="line">                state_visits[state] += <span class="number">1</span></span><br><span class="line">                is_terminal = <span class="keyword">False</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">                    state_visits[state] += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            reward_sums.append(rl_glue.rl_return() - last_episode_total_reward)</span><br><span class="line">            last_episode_total_reward = rl_glue.rl_return()</span><br><span class="line">            </span><br><span class="line">        all_reward_sums[algorithm].append(reward_sums)</span><br><span class="line">        all_state_visits[algorithm].append(state_visits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot results</span></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    plt.plot(np.mean(all_reward_sums[algorithm], axis=<span class="number">0</span>), label=algorithm)</span><br><span class="line">plt.xlabel(<span class="string">"Episodes"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Sum of\n rewards\n during\n episode"</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line">plt.ylim(<span class="number">-30</span>,<span class="number">0</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 100/100 [00:12&lt;00:00,  8.12it/s]100%|██████████| 100/100 [00:16&lt;00:00,  6.15it/s]</code></pre><p><img src="output_26_1.png" alt="png"></p><p>To see why these two agents behave differently, let’s inspect the states they visit most. Run the cell below to generate plots showing the number of timesteps that the agents spent in each state over the last 10 episodes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm, position <span class="keyword">in</span> [(<span class="string">"Q-learning"</span>, <span class="number">211</span>), (<span class="string">"Expected Sarsa"</span>, <span class="number">212</span>)]:</span><br><span class="line">    plt.subplot(position)</span><br><span class="line">    average_state_visits = np.array(all_state_visits[algorithm]).mean(axis=<span class="number">0</span>)</span><br><span class="line">    grid_state_visits = average_state_visits.reshape((<span class="number">4</span>,<span class="number">12</span>))</span><br><span class="line">    grid_state_visits[<span class="number">0</span>,<span class="number">1</span>:<span class="number">-1</span>] = np.nan</span><br><span class="line">    plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">'gray'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.title(algorithm)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    cm = plt.get_cmap()</span><br><span class="line">    cm.set_bad(<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">0.85</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    </span><br><span class="line">cbar = plt.colorbar(cax=cax)</span><br><span class="line">cbar.ax.set_ylabel(<span class="string">"Visits during\n the last 10\n episodes"</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">70</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_28_0.png" alt="png"></p><p>The Q-learning agent learns the optimal policy, one that moves along the cliff and reaches the goal in as few steps as possible. However, since the agent does not follow the optimal policy and uses $\epsilon$-greedy exploration, it occasionally falls off the cliff. The Expected Sarsa agent takes exploration into account and follows a safer path.</p><p>Previously we used a fixed step-size of 0.5 for the agents. What happens with other step-sizes? Does this difference in performance persist?</p><p>In the next experiment we will try 10 different step-sizes from 0.1 to 1.0 and compare the sum of rewards per episode averaged over the first 100 episodes (similar to the interim performance curves in Figure 6.3 of the textbook). Shaded regions show standard errors.</p><p>This cell takes around 10 minutes to run. The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">"Q-learning"</span>: QLearningAgent,</span><br><span class="line">    <span class="string">"Expected Sarsa"</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line">step_sizes = np.linspace(<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">10</span>)</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">48</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">30</span></span><br><span class="line">num_episodes = <span class="number">100</span></span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line"></span><br><span class="line">algorithms = [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]</span><br><span class="line">cross_product = list(product(algorithms, step_sizes, range(num_runs)))</span><br><span class="line"><span class="keyword">for</span> algorithm, step_size, run <span class="keyword">in</span> tqdm(cross_product):</span><br><span class="line">    <span class="keyword">if</span> (algorithm, step_size) <span class="keyword">not</span> <span class="keyword">in</span> all_reward_sums:</span><br><span class="line">        all_reward_sums[(algorithm, step_size)] = []</span><br><span class="line"></span><br><span class="line">    agent_info[<span class="string">"step_size"</span>] = step_size</span><br><span class="line">    agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">    rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    last_episode_total_reward = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>)</span><br><span class="line">    all_reward_sums[(algorithm, step_size)].append(rl_glue.rl_return()/num_episodes)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    algorithm_stds = np.array([sem(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    plt.plot(step_sizes, algorithm_means, marker=<span class="string">'o'</span>, linestyle=<span class="string">'solid'</span>, label=algorithm)</span><br><span class="line">    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">"Step-size"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Sum of\n rewards\n per episode"</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">50</span>)</span><br><span class="line">plt.xticks(step_sizes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 600/600 [01:38&lt;00:00,  6.08it/s]</code></pre><p><img src="output_30_1.png" alt="png"></p><p>Expected Sarsa shows an advantage over Q-learning in this problem across a wide range of step-sizes.</p><p>Congratulations! Now you have:</p><ul><li>implemented Q-Learning with $\epsilon$-greedy action selection</li><li>implemented Expected Sarsa with $\epsilon$-greedy action selection</li><li>investigated the behavior of these two algorithms on Cliff World</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Q-Learning-and-Expected-Sarsa&quot;&gt;&lt;a href=&quot;#Assignment-2-Q-Learning-and-Expected-Sarsa&quot; class=&quot;headerlink&quot; title=&quot;Assignme
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Policy Evaluation in Cliff Walking Environment</title>
    <link href="https://zhangruochi.com/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/"/>
    <id>https://zhangruochi.com/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/</id>
    <published>2020-09-28T08:04:11.000Z</published>
    <updated>2020-09-28T08:06:19.087Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-Policy-Evaluation-in-Cliff-Walking-Environment"><a href="#Assignment-Policy-Evaluation-in-Cliff-Walking-Environment" class="headerlink" title="Assignment: Policy Evaluation in Cliff Walking Environment"></a>Assignment: Policy Evaluation in Cliff Walking Environment</h1><p>Welcome to the Course 2 Module 2 Programming Assignment! In this assignment, you will implement one of the fundamental sample and bootstrapping based model free reinforcement learning agents for prediction. This is namely one that uses one-step temporal difference learning, also known as TD(0). The task is to design an agent for policy evaluation in the Cliff Walking environment. Recall that policy evaluation is the prediction problem where the goal is to accurately estimate the values of states given some policy.</p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ul><li>Implement parts of the Cliff Walking environment, to get experience specifying MDPs [Section 1].</li><li>Implement an agent that uses bootstrapping and, particularly, TD(0) [Section 2].</li><li>Apply TD(0) to estimate value functions for different policies, i.e., run policy evaluation experiments [Section 3].</li></ul><h2 id="The-Cliff-Walking-Environment"><a href="#The-Cliff-Walking-Environment" class="headerlink" title="The Cliff Walking Environment"></a>The Cliff Walking Environment</h2><p>The Cliff Walking environment is a gridworld with a discrete state space and discrete action space. The agent starts at grid cell S. The agent can move (deterministically) to the four neighboring cells by taking actions Up, Down, Left or Right. Trying to move out of the boundary results in staying in the same location. So, for example, trying to move left when at a cell on the leftmost column results in no movement at all and the agent remains in the same location. The agent receives -1 reward per step in most states, and -100 reward when falling off of the cliff. This is an episodic task; termination occurs when the agent reaches the goal grid cell G. Falling off of the cliff results in resetting to the start state, without termination.</p><p>The diagram below showcases the description above and also illustrates two of the policies we will be evaluating.</p><p><img src="cliffwalk.png" style="height:400px"></p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages."></a>Packages.</h2><p>We import the following libraries that are required for this assignment. We shall be using the following libraries:</p><ol><li>jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li><li>numpy: the fundamental package for scientific computing with Python.</li><li>matplotlib: the library for plotting graphs in Python.</li><li>RL-Glue: the library for reinforcement learning experiments.</li><li>BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.</li><li>Manager: the file allowing for visualization and testing.</li><li>itertools.product: the function that can be used easily to compute permutations.</li><li>tqdm.tqdm: Provides progress bars for visualizing the status of loops.</li></ol><p><strong>Please do not import other libraries</strong> — this will break the autograder.</p><p><strong>NOTE: For this notebook, there is no need to make any calls to methods of random number generators. Spurious or missing calls to random number generators may affect your results.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> Agent <span class="keyword">import</span> BaseAgent </span><br><span class="line"><span class="keyword">from</span> Environment <span class="keyword">import</span> BaseEnvironment  </span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> manager <span class="keyword">import</span> Manager</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure><h2 id="Section-1-Environment"><a href="#Section-1-Environment" class="headerlink" title="Section 1. Environment"></a>Section 1. Environment</h2><p>In the first part of this assignment, you will get to see how the Cliff Walking environment is implemented. You will also get to implement parts of it to aid your understanding of the environment and more generally how MDPs are specified. In particular, you will implement the logic for:</p><ol><li>Converting 2-dimensional coordinates to a single index for the state,</li><li>One of the actions (action up), and,</li><li>Reward and termination.</li></ol><p>Given below is an annotated diagram of the environment with more details that may help in completing the tasks of this part of the assignment. Note that we will be creating a more general environment where the height and width positions can be variable but the start, goal and cliff grid cells have the same relative positions (bottom left, bottom right and the cells between the start and goal grid cells respectively).</p><p><img src="cliffwalk-annotated.png" style="height:400px"></p><p>Once you have gone through the code and begun implementing solutions, it may be a good idea to come back here and see if you can convince yourself that the diagram above is an accurate representation of the code given and the code you have written.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty CliffWalkEnvironment class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CliffWalkEnvironment</span><span class="params">(BaseEnvironment)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># helper method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span><span class="params">(self, loc)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h2 id="env-init"><a href="#env-init" class="headerlink" title="env_init()"></a>env_init()</h2><p>The first function we add to the environment is the initialization function which is called once when an environment object is created. In this function, the grid dimensions and special locations (start and goal locations and the cliff locations) are stored for easy use later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_init</span><span class="params">(self, env_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the environment called when the experiment first starts.</span></span><br><span class="line"><span class="string">        Note:</span></span><br><span class="line"><span class="string">            Initialize a tuple with the reward, first state, boolean</span></span><br><span class="line"><span class="string">            indicating if it's terminal.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Note, we can setup the following variables later, in env_start() as it is equivalent. </span></span><br><span class="line">        <span class="comment"># Code is left here to adhere to the note above, but these variables are initialized once more</span></span><br><span class="line">        <span class="comment"># in env_start() [See the env_start() function below.]</span></span><br><span class="line">        </span><br><span class="line">        reward = <span class="keyword">None</span></span><br><span class="line">        state = <span class="keyword">None</span> <span class="comment"># See Aside</span></span><br><span class="line">        termination = <span class="keyword">None</span></span><br><span class="line">        self.reward_state_term = (reward, state, termination)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably </span></span><br><span class="line">        <span class="comment"># used with the term "state" for our purposes and for this assignment in particular. </span></span><br><span class="line">        <span class="comment"># A difference arises in the use of the terms when we have what is called Partial Observability where </span></span><br><span class="line">        <span class="comment"># the environment may return states that may not fully represent all the information needed to </span></span><br><span class="line">        <span class="comment"># predict values or make decisions (i.e., the environment is non-Markovian.)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the default height to 4 and width to 12 (as in the diagram given above)</span></span><br><span class="line">        self.grid_h = env_info.get(<span class="string">"grid_height"</span>, <span class="number">4</span>) </span><br><span class="line">        self.grid_w = env_info.get(<span class="string">"grid_width"</span>, <span class="number">12</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Now, we can define a frame of reference. Let positive x be towards the direction down and </span></span><br><span class="line">        <span class="comment"># positive y be towards the direction right (following the row-major NumPy convention.)</span></span><br><span class="line">        <span class="comment"># Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1 </span></span><br><span class="line">        <span class="comment"># and max y is then grid_w - 1. So, we have:</span></span><br><span class="line">        <span class="comment"># Starting location of agent is the bottom-left corner, (max x, min y). </span></span><br><span class="line">        self.start_loc = (self.grid_h - <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Goal location is the bottom-right corner. (max x, max y).</span></span><br><span class="line">        self.goal_loc = (self.grid_h - <span class="number">1</span>, self.grid_w - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The cliff will contain all the cells between the start_loc and goal_loc.</span></span><br><span class="line">        self.cliff = [(self.grid_h - <span class="number">1</span>, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, (self.grid_w - <span class="number">1</span>))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to </span></span><br><span class="line">        <span class="comment"># verify that your understanding of the above code is correct for the default case, i.e., where </span></span><br><span class="line">        <span class="comment"># height = 4 and width = 12.</span></span><br></pre></td></tr></table></figure><h2 id="Implement-state"><a href="#Implement-state" class="headerlink" title="Implement state()"></a><em>Implement</em> state()</h2><p>The agent location can be described as a two-tuple or coordinate (x, y) describing the agent’s position.<br>However, we can convert the (x, y) tuple into a single index and provide agents with just this integer.<br>One reason for this choice is that the spatial aspect of the problem is secondary and there is no need<br>for the agent to know about the exact dimensions of the environment.<br>From the agent’s viewpoint, it is just perceiving some states, accessing their corresponding values<br>in a table, and updating them. Both the coordinate (x, y) state representation and the converted coordinate representation are thus equivalent in this sense.</p><p>Given a grid cell location, the state() function should return the state; a single index corresponding to the location in the grid.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Example: Suppose grid_h is 2 and grid_w is 2. Then, we can write the grid cell two-tuple or coordinate</span><br><span class="line">states as follows (following the usual 0-index convention):</span><br><span class="line">|(0, 0) (0, 1)| |0 1|</span><br><span class="line">|(1, 0) (1, 1)| |2 3|</span><br><span class="line">Assuming row-major order as NumPy does,  we can flatten the latter to get a vector [0 1 2 3].</span><br><span class="line">So, if loc = (0, 0) we return 0. While, for loc = (1, 1) we return 3.</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [state]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Modify the return statement of this function to return a correct single index as </span></span><br><span class="line"><span class="comment"># the state (see the logic for this in the previous cell.)</span></span><br><span class="line"><span class="comment"># Lines: 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state</span><span class="params">(self, loc)</span>:</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> loc[<span class="number">0</span>] * <span class="number">12</span> + loc[<span class="number">1</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR STATE (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below corresponds to the annotated diagram for the environment</span></span><br><span class="line"><span class="comment">#       given previously and is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_state</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    coords_to_test = [(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">11</span>), (<span class="number">1</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">0</span>), (<span class="number">3</span>, <span class="number">9</span>), (<span class="number">3</span>, <span class="number">11</span>)]</span><br><span class="line">    true_states = [<span class="number">0</span>, <span class="number">11</span>, <span class="number">17</span>, <span class="number">36</span>, <span class="number">45</span>, <span class="number">47</span>]</span><br><span class="line">    output_states = [env.state(coords) <span class="keyword">for</span> coords <span class="keyword">in</span> coords_to_test]</span><br><span class="line">    <span class="keyword">assert</span>(output_states == true_states)</span><br><span class="line">test_state()</span><br></pre></td></tr></table></figure><h2 id="env-start"><a href="#env-start" class="headerlink" title="env_start()"></a>env_start()</h2><p>In env_start(), we initialize the agent location to be the start location and return the state corresponding to it as the first state for the agent to act upon. Additionally, we also set the reward and termination terms to be 0 and False respectively as they are consistent with the notion that there is no reward nor termination before the first action is even taken.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_start</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the episode starts, called before the</span></span><br><span class="line"><span class="string">    agent starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first state from the environment.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    reward = <span class="number">0</span></span><br><span class="line">    <span class="comment"># agent_loc will hold the current location of the agent</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br><span class="line">    <span class="comment"># state is the one dimensional state representation of the agent location.</span></span><br><span class="line">    state = self.state(self.agent_loc)</span><br><span class="line">    termination = <span class="keyword">False</span></span><br><span class="line">    self.reward_state_term = (reward, state, termination)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h2 id="Implement-env-step"><a href="#Implement-env-step" class="headerlink" title="Implement env_step()"></a><em>Implement</em> env_step()</h2><p>Once an action is taken by the agent, the environment must provide a new state, reward and termination signal. </p><p>In the Cliff Walking environment, agents move around using a 4-cell neighborhood called the Von Neumann neighborhood (<a href="https://en.wikipedia.org/wiki/Von_Neumann_neighborhood" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Von_Neumann_neighborhood</a>). Thus, the agent has 4 available actions at each state. Three of the actions have been implemented for you and your first task is to implement the logic for the fourth action (Action UP).</p><p>Your second task for this function is to implement the reward logic. Look over the environment description given earlier in this notebook if you need a refresher for how the reward signal is defined.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [env_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the code for action UP and implement the logic for reward and termination.</span></span><br><span class="line"><span class="comment"># Lines: ~7.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        action: The action taken by the agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (float, state, Boolean): a tuple of the reward, state,</span></span><br><span class="line"><span class="string">            and boolean indicating if it's terminal.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> action == <span class="number">0</span>: <span class="comment"># UP (Task 1)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Hint: Look at the code given for the other actions and think about the logic in them.</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] - <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &gt;= <span class="number">0</span>:</span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">1</span>: <span class="comment"># LEFT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &gt;= <span class="number">0</span>: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">2</span>: <span class="comment"># DOWN</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] + <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &lt; self.grid_h: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">3</span>: <span class="comment"># RIGHT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &lt; self.grid_w: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">raise</span> Exception(str(action) + <span class="string">" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!"</span>)</span><br><span class="line"></span><br><span class="line">    reward = <span class="number">-1</span></span><br><span class="line">    terminal = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Consider the initialization of reward and terminal variables above. Then, note the </span></span><br><span class="line">    <span class="comment"># conditional statements and comments given below and carefully ensure to set the variables reward </span></span><br><span class="line">    <span class="comment"># and terminal correctly for each case.</span></span><br><span class="line">    <span class="keyword">if</span> self.agent_loc == self.goal_loc: <span class="comment"># Reached Goal!</span></span><br><span class="line">        terminal = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">elif</span> self.agent_loc <span class="keyword">in</span> self.cliff: <span class="comment"># Fell into the cliff!</span></span><br><span class="line">        reward = <span class="number">-100</span></span><br><span class="line">        self.agent_loc = self.start_loc</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)</span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR ACTION UP (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is again limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_action_up</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">test_action_up()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR REWARD &amp; TERMINATION (10 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_reward</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">0</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-100</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">2</span>, <span class="number">11</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">11</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">True</span>)</span><br><span class="line">test_reward()</span><br></pre></td></tr></table></figure><h2 id="env-cleanup"><a href="#env-cleanup" class="headerlink" title="env_cleanup()"></a>env_cleanup()</h2><p>There is not much cleanup to do for the Cliff Walking environment. Here, we simply reset the agent location to be the start location in this function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Cleanup done after the environment ends"""</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br></pre></td></tr></table></figure><h2 id="Section-2-Agent"><a href="#Section-2-Agent" class="headerlink" title="Section 2. Agent"></a>Section 2. Agent</h2><p>In this second part of the assignment, you will be implementing the key updates for Temporal Difference Learning. There are two cases to consider depending on whether an action leads to a terminal state or not.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty TDAgent class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span><span class="params">(self)</span>:</span>        </span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h2 id="agent-init"><a href="#agent-init" class="headerlink" title="agent_init()"></a>agent_init()</h2><p>As we did with the environment, we first initialize the agent once when a TDAgent object is created. In this function, we create a random number generator, seeded with the seed provided in the agent_info dictionary to get reproducible results. We also set the policy, discount and step size based on the agent_info dictionary. Finally, with a convention that the policy is always specified as a mapping from states to actions and so is an array of size (# States, # Actions), we initialize a values array of shape (# States,) to zeros.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">    <span class="string">"""Setup for the agent called when the experiment first starts."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a random number generator with the provided seed to seed the agent for reproducibility.</span></span><br><span class="line">    self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Policy will be given, recall that the goal is to accurately estimate its corresponding value function. </span></span><br><span class="line">    self.policy = agent_info.get(<span class="string">"policy"</span>)</span><br><span class="line">    <span class="comment"># Discount factor (gamma) to use in the updates.</span></span><br><span class="line">    self.discount = agent_info.get(<span class="string">"discount"</span>)</span><br><span class="line">    <span class="comment"># The learning rate or step size parameter (alpha) to use in updates.</span></span><br><span class="line">    self.step_size = agent_info.get(<span class="string">"step_size"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize an array of zeros that will hold the values.</span></span><br><span class="line">    <span class="comment"># Recall that the policy can be represented as a (# States, # Actions) array. With the </span></span><br><span class="line">    <span class="comment"># assumption that this is the case, we can use the first dimension of the policy to</span></span><br><span class="line">    <span class="comment"># initialize the array for values.</span></span><br><span class="line">    self.values = np.zeros((self.policy.shape[<span class="number">0</span>],))</span><br></pre></td></tr></table></figure><h1 id="agent-start"><a href="#agent-start" class="headerlink" title="agent_start()"></a>agent_start()</h1><p>In agent_start(), we choose an action based on the initial state and policy we are evaluating. We also cache the state so that we can later update its value when we perform a Temporal Difference update. Finally, we return the action chosen so that the RL loop can continue and the environment can execute this action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># The policy can be represented as a (# States, # Actions) array. So, we can use </span></span><br><span class="line">    <span class="comment"># the second dimension here when choosing an action.</span></span><br><span class="line">    action = self.rand_generator.choice(range(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="Implement-agent-step"><a href="#Implement-agent-step" class="headerlink" title="Implement agent_step()"></a><em>Implement</em> agent_step()</h2><p>In agent_step(), the agent must:</p><ul><li>Perform an update to improve the value estimate of the previously visited state, and</li><li>Act based on the state provided by the environment.</li></ul><p>The latter of the two steps above has been implemented for you. Implement the former. Note that, unlike later in agent_end(), the episode has not yet ended in agent_step(). in other words, the previously observed state was not a terminal state.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step after the last action, i.e., where the agent ended up after the</span></span><br><span class="line"><span class="string">            last action</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action the agent is taking.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: We should perform an update with the last state given that we now have the reward and</span></span><br><span class="line">    <span class="comment"># next state. We break this into two steps. Recall for example that the Monte-Carlo update </span></span><br><span class="line">    <span class="comment"># had the form: V[S_t] = V[S_t] + alpha * (target - V[S_t]), where the target was the return, G_t.</span></span><br><span class="line">    target = reward + self.discount * self.values[state]</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Having updated the value for the last state, we now act based on the current </span></span><br><span class="line">    <span class="comment"># state, and set the last state to be current one as we will next be making an </span></span><br><span class="line">    <span class="comment"># update with it when agent_step is called next once the action we return from this function </span></span><br><span class="line">    <span class="comment"># is executed in the environment.</span></span><br><span class="line"></span><br><span class="line">    action = self.rand_generator.choice(range(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="Implement-agent-end"><a href="#Implement-agent-end" class="headerlink" title="Implement agent_end()"></a><em>Implement</em> agent_end()</h2><p>Implement the TD update for the case where an action leads to a terminal state.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_end]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Here too, we should perform an update with the last state given that we now have the </span></span><br><span class="line">    <span class="comment"># reward. Note that in this case, the action led to termination. Once more, we break this into </span></span><br><span class="line">    <span class="comment"># two steps, computing the target and the update itself that uses the target and the </span></span><br><span class="line">    <span class="comment"># current value estimate for the state whose value we are updating.</span></span><br><span class="line">    target = reward</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><h2 id="agent-cleanup"><a href="#agent-cleanup" class="headerlink" title="agent_cleanup()"></a>agent_cleanup()</h2><p>In cleanup, we simply reset the last state to be None to ensure that we are not storing any states past an episode.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Cleanup done after the agent ends."""</span></span><br><span class="line">    self.last_state = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><h2 id="agent-message"><a href="#agent-message" class="headerlink" title="agent_message()"></a>agent_message()</h2><p>agent_message() can generally be used to get different kinds of information about an RLGlue agent in the interaction loop of RLGlue. Here, we conditonally check for a message matching “get_values” and use it to retrieve the values table the agent has been updating over time.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">    <span class="string">"""A function used to pass information from the agent to the experiment.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        message: The message passed to the agent.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The response (or answer) to the message.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">"get_values"</span>:</span><br><span class="line">        <span class="keyword">return</span> self.values</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"TDAgent.agent_message(): Message not understood!"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR TD-UPDATES (20 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test belows serve as a good check in debugging your code for the TD updates. However, </span></span><br><span class="line"><span class="comment">#       as with the other tests, it is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_td_updates</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -1 and does not lead to a terminal state. This is in a simple two state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The first state's current value estimate is 0 while the second is 1.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>], [<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">"policy"</span>: np.array(policy_list), <span class="string">"discount"</span>: <span class="number">0.99</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>, <span class="number">1.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = <span class="number">-1</span></span><br><span class="line">    next_state = <span class="number">1</span></span><br><span class="line">    agent.agent_step(reward, next_state)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], <span class="number">-0.001</span>) <span class="keyword">and</span> np.isclose(agent.values[<span class="number">1</span>], <span class="number">1.</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -100 and lead to a terminal state. This is in a simple one state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The state's current value estimate is 0.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">"policy"</span>: np.array(policy_list), <span class="string">"discount"</span>: <span class="number">0.99</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = <span class="number">-100</span></span><br><span class="line">    next_state = <span class="number">0</span></span><br><span class="line">    agent.agent_end(reward)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], <span class="number">-10</span>))</span><br><span class="line">    </span><br><span class="line">test_td_updates()</span><br></pre></td></tr></table></figure><h2 id="Section-3-Policy-Evaluation-Experiments"><a href="#Section-3-Policy-Evaluation-Experiments" class="headerlink" title="Section 3. Policy Evaluation Experiments"></a>Section 3. Policy Evaluation Experiments</h2><p>Finally, in this last part of the assignment, you will get to see the TD policy evaluation algorithm in action by looking at the estimated values, the per state value error and after the experiment is complete, the Mean Squared Value Error curve vs. episode number, summarizing how the value error changed over time.</p><p>The code below runs one run of an experiment given env_info and agent_info dictionaries. A “manager” object is created for visualizations and is used in part for the autograder. By default, the run will be for 5000 episodes. The true_values_file is specified to compare the learned value function with the values stored in the true_values_file. Plotting of the learned value  function occurs by default after every 100 episodes. In addition, when true_values_file is specified, the value error per state and the root mean square value error will also be plotted.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No. </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(env_info, agent_info, </span></span></span><br><span class="line"><span class="function"><span class="params">                   num_episodes=<span class="number">5000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   experiment_name=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                   plot_freq=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   true_values_file=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                   value_error_threshold=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment</span><br><span class="line">    agent = TDAgent</span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line"></span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line">        <span class="keyword">if</span> episode % plot_freq == <span class="number">0</span>:</span><br><span class="line">            values = rl_glue.agent.agent_message(<span class="string">"get_values"</span>)</span><br><span class="line">            manager.visualize(values, episode)</span><br><span class="line"></span><br><span class="line">    values = rl_glue.agent.agent_message(<span class="string">"get_values"</span>)</span><br><span class="line">    <span class="keyword">if</span> true_values_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># Grading: The Manager will check that the values computed using your TD agent match </span></span><br><span class="line">        <span class="comment"># the true values (within some small allowance) across the states. In addition, it also</span></span><br><span class="line">        <span class="comment"># checks whether the root mean squared value error is close to 0.</span></span><br><span class="line">        manager.run_tests(values, value_error_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> values</span><br></pre></td></tr></table></figure><p>The cell below just runs a policy evaluation experiment with the determinstic optimal policy that strides just above the cliff. You should observe that the per state value error and RMSVE curve asymptotically go towards 0. The arrows in the four directions denote the probabilities of taking each action. This experiment is ungraded but should serve as a good test for the later experiments. The true values file provided for this experiment may help with debugging as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">"discount"</span>: <span class="number">1</span>, <span class="string">"step_size"</span>: <span class="number">0.01</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Optimal Policy that strides just along the cliff</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line"></span><br><span class="line">true_values_file = <span class="string">"optimal_policy_value_fn.npy"</span></span><br><span class="line">_ = run_experiment(env_info, agent_info, num_episodes=<span class="number">5000</span>, experiment_name=<span class="string">"Policy Evaluation on Optimal Policy"</span>,</span><br><span class="line">                   plot_freq=<span class="number">500</span>, true_values_file=true_values_file)</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The Safe Policy</span></span><br><span class="line"><span class="comment"># Hint: Fill in the array below (as done in the previous cell) based on the safe policy illustration </span></span><br><span class="line"><span class="comment"># in the environment diagram. This is the policy that strides as far as possible away from the cliff. </span></span><br><span class="line"><span class="comment"># We call it a "safe" policy because if the environment has any stochasticity, this policy would do a good job in </span></span><br><span class="line"><span class="comment"># keeping the agent from falling into the cliff (in contrast to the optimal policy shown before). </span></span><br><span class="line"><span class="comment"># BOILERPLATE:</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">24</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">12</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">11</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">policy[<span class="number">11</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">23</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH SAFE POLICY</span></span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line">v = run_experiment(env_info, agent_info,</span><br><span class="line">               experiment_name=<span class="string">"Policy Evaluation On Safe Policy"</span>,</span><br><span class="line">               num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">500</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A Near Optimal Stochastic Policy</span></span><br><span class="line"><span class="comment"># Now, we try a stochastic policy that deviates a little from the optimal policy seen above. </span></span><br><span class="line"><span class="comment"># This means we can get different results due to randomness.</span></span><br><span class="line"><span class="comment"># We will thus average the value function estimates we get over multiple runs. </span></span><br><span class="line"><span class="comment"># This can take some time, upto about 5 minutes from previous testing. </span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The autograder will compare . Re-run this cell upon making any changes.</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">"discount"</span>: <span class="number">1</span>, <span class="string">"step_size"</span>: <span class="number">0.01</span>&#125;</span><br><span class="line"></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line">agent_info.update(&#123;<span class="string">"step_size"</span>: <span class="number">0.01</span>&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH NEAR OPTIMAL STOCHASTIC POLICY (40 POINTS)</span></span><br><span class="line">arr = []</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(<span class="number">30</span>)):</span><br><span class="line">    env_info[<span class="string">'seed'</span>] = i</span><br><span class="line">    agent_info[<span class="string">'seed'</span>] = i</span><br><span class="line">    v = run_experiment(env_info, agent_info,</span><br><span class="line">                   experiment_name=<span class="string">"Policy Evaluation On Optimal Policy"</span>,</span><br><span class="line">                   num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">10000</span>)</span><br><span class="line">    arr.append(v)</span><br><span class="line">average_v = np.array(arr).mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations, you have completed assignment 2! In this assignment, we investigated a very useful concept for sample-based online learning: temporal difference. We particularly looked at the prediction problem where the goal is to find the value function corresponding to a given policy. In the next assignment, by learning the action-value function instead of the state-value function, you will get to see how temporal difference learning can be used in control as well.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-Policy-Evaluation-in-Cliff-Walking-Environment&quot;&gt;&lt;a href=&quot;#Assignment-Policy-Evaluation-in-Cliff-Walking-Environment&quot; clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Assignment 4: Chatbot</title>
    <link href="https://zhangruochi.com/Assignment-4-Chatbot/2020/09/28/"/>
    <id>https://zhangruochi.com/Assignment-4-Chatbot/2020/09/28/</id>
    <published>2020-09-28T05:50:01.000Z</published>
    <updated>2020-09-28T09:00:27.567Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-4-Chatbot"><a href="#Assignment-4-Chatbot" class="headerlink" title="Assignment 4: Chatbot"></a>Assignment 4: Chatbot</h1><p><img src="cbot.jpg" height="400" width="400"> </p><p>Welcome to the last assignment of Course 4. Before you get started, we want to congratulate you on getting here. It is your 16th programming assignment in this Specialization and we are very proud of you! In this assignment, you are going to use the <a href="https://arxiv.org/abs/2001.04451" target="_blank" rel="noopener">Reformer</a>, also known as the efficient Transformer, to generate a dialogue between two bots. You will feed conversations to your model and it will learn how to understand the context of each one. Not only will it learn how to answer questions but it will also know how to ask questions if it needs more info. For example, after a customer asks for a train ticket, the chatbot can ask what time the said customer wants to leave. You can use this concept to automate call centers, hotel receptions, personal trainers, or any type of customer service. By completing this assignment, you will:</p><ul><li>Understand how the Reformer works</li><li>Explore the <a href="https://arxiv.org/abs/1810.00278" target="_blank" rel="noopener">MultiWoz</a> dataset</li><li>Process the data to feed it into the model</li><li>Train your model</li><li>Generate a dialogue by feeding a question to the model</li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">Part 1:   Exploring the MultiWoz dataset</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li><li><a href="#2">Part 2:   Processing the data for Reformer inputs</a><ul><li><a href="#2.1">2.1   Tokenizing, batching with bucketing</a></li></ul></li><li><a href="#3">Part 3:   Reversible layers</a><ul><li><a href="#ex02">Exercise 02</a></li><li><a href="#ex03">Exercise 03</a></li><li><a href="#3.1">3.1   Reversible layers and randomness</a></li></ul></li><li><a href="#4">Part 4:   ReformerLM Training</a><ul><li><a href="#ex04">Exercise 04</a></li><li><a href="#ex05">Exercise 05</a></li></ul></li><li><a href="#5">Part 5:   Decode from a pretrained model</a><ul><li><a href="#ex06">Exercise 06</a></li></ul></li></ul><p><a name="1"></a></p><h1 id="Part-1-Exploring-the-MultiWoz-dataset"><a href="#Part-1-Exploring-the-MultiWoz-dataset" class="headerlink" title="Part 1:   Exploring the MultiWoz dataset"></a>Part 1:   Exploring the MultiWoz dataset</h1><p>You will start by exploring the MultiWoz dataset. The dataset you are about to use has more than 10,000 human annotated dialogues and spans multiple domains and topics. Some dialogues include multiple domains and others include single domains. In this section, you will load and explore this dataset, as well as develop a function to extract the dialogues.</p><p>Let’s first import the modules we will be using:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax   </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line">!pip list | grep trax</span><br></pre></td></tr></table></figure><p>Let’s also declare some constants we will be using in the exercises.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># filename of the MultiWOZ dialogue dataset</span></span><br><span class="line">DATA_FILE = <span class="string">'data.json'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data directory</span></span><br><span class="line">DATA_DIR = <span class="string">'./data'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dictionary where we will load the dialogue dataset</span></span><br><span class="line">DIALOGUE_DB = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary filename</span></span><br><span class="line">VOCAB_FILE = <span class="string">'en_32k.subword'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary file directory</span></span><br><span class="line">VOCAB_DIR = <span class="string">'data/vocabs'</span></span><br></pre></td></tr></table></figure><p>Let’s now load the MultiWOZ 2.1 dataset. We have already provided it for you in your workspace. It is in JSON format so we should load it as such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># help function to load a JSON file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_json</span><span class="params">(directory, file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">f'<span class="subst">&#123;directory&#125;</span>/<span class="subst">&#123;file&#125;</span>'</span>) <span class="keyword">as</span> file: </span><br><span class="line">        db = json.load(file)</span><br><span class="line">    <span class="keyword">return</span> db</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the dialogue data set into our dictionary</span></span><br><span class="line">DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)</span><br></pre></td></tr></table></figure><p>Let’s see how many dialogues we have in the dictionary. 1 key-value pair is one dialogue so we can just get the dictionary’s length.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The number of dialogues is: <span class="subst">&#123;len(DIALOGUE_DB)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>The number of dialogues is: 10438</code></pre><p>The dialogues are composed of multiple files and the filenames are used as keys in our dictionary. Those with multi-domain dialogues have “MUL” in their filenames while single domain dialogues have either “SNG” or “WOZ”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print 7 keys from the dataset to see the filenames</span></span><br><span class="line">print(list(DIALOGUE_DB.keys())[<span class="number">0</span>:<span class="number">7</span>])</span><br></pre></td></tr></table></figure><pre><code>[&#39;SNG01856.json&#39;, &#39;SNG0129.json&#39;, &#39;PMUL1635.json&#39;, &#39;MUL2168.json&#39;, &#39;SNG0073.json&#39;, &#39;SNG01445.json&#39;, &#39;MUL2105.json&#39;]</code></pre><p>As you can see from the cells above, there are 10,438 conversations, each in its own file.  You will train your model on all those conversations. Each file is also loaded into a dictionary and each has two keys which are the following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get keys of the fifth file in the list above</span></span><br><span class="line">print(DIALOGUE_DB[<span class="string">'SNG0073.json'</span>].keys())</span><br></pre></td></tr></table></figure><pre><code>dict_keys([&#39;goal&#39;, &#39;log&#39;])</code></pre><p>The <code>goal</code> also points to a dictionary and it contains several keys pertaining to the objectives of the conversation. For example below, we can see that the conversation will be about booking a taxi.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'goal'</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;taxi&#39;: {&#39;info&#39;: {&#39;leaveAt&#39;: &#39;17:15&#39;,   &#39;destination&#39;: &#39;pizza hut fen ditton&#39;,   &#39;departure&#39;: &quot;saint john&#39;s college&quot;},  &#39;reqt&#39;: [&#39;car type&#39;, &#39;phone&#39;],  &#39;fail_info&#39;: {}}, &#39;police&#39;: {}, &#39;hospital&#39;: {}, &#39;hotel&#39;: {}, &#39;attraction&#39;: {}, &#39;train&#39;: {}, &#39;message&#39;: [&quot;You want to book a &lt;span class=&#39;emphasis&#39;&gt;taxi&lt;/span&gt;. The taxi should go to &lt;span class=&#39;emphasis&#39;&gt;pizza hut fen ditton&lt;/span&gt; and should depart from &lt;span class=&#39;emphasis&#39;&gt;saint john&#39;s college&lt;/span&gt;&quot;,  &quot;The taxi should &lt;span class=&#39;emphasis&#39;&gt;leave after 17:15&lt;/span&gt;&quot;,  &quot;Make sure you get &lt;span class=&#39;emphasis&#39;&gt;car type&lt;/span&gt; and &lt;span class=&#39;emphasis&#39;&gt;contact number&lt;/span&gt;&quot;], &#39;restaurant&#39;: {}}</code></pre><p>The <code>log</code> on the other hand contains the dialog. It is a list of dictionaries and each element of this list contains several descriptions as well. Let’s look at an example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get first element of the log list</span></span><br><span class="line">DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;text&#39;: &quot;I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton.&quot;, &#39;metadata&#39;: {}, &#39;dialog_act&#39;: {&#39;Taxi-Inform&#39;: [[&#39;Dest&#39;, &#39;pizza hut fen ditton&#39;],   [&#39;Depart&#39;, &quot;saint john &#39;s college&quot;]]}, &#39;span_info&#39;: [[&#39;Taxi-Inform&#39;, &#39;Dest&#39;, &#39;pizza hut fen ditton&#39;, 11, 14],  [&#39;Taxi-Inform&#39;, &#39;Depart&#39;, &quot;saint john &#39;s college&quot;, 6, 9]]}</code></pre><p>For this assignment, we are only interested in the conversation which is in the <code>text</code> field.<br>The conversation goes back and forth between two persons. Let’s call them ‘Person 1’ and ‘Person 2’. This implies that<br>data[‘SNG0073.json’][‘log’][0][‘text’] is ‘Person 1’ and<br>data[‘SNG0073.json’][‘log’][1][‘text’] is ‘Person 2’ and so on. The even offsets are ‘Person 1’ and the odd offsets are ‘Person 2’.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">' Person 1: '</span>, DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>][<span class="string">'text'</span>])</span><br><span class="line">print(<span class="string">' Person 2: '</span>,DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">1</span>][<span class="string">'text'</span>])</span><br></pre></td></tr></table></figure><pre><code> Person 1:  I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton. Person 2:  What time do you want to leave and what time do you want to arrive by?</code></pre><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p>You will now implement the <code>get_conversation()</code> function that will extract the conversations from the dataset’s file.</p><p><strong>Instructions:</strong> Implement a function to extract conversations from the input file.<br>As described above, the conversation is in the <code>text</code> field in each of the elements in the <code>log</code> list of the file. If the log list has <code>x</code> number of elements, then the function will get the <code>text</code> entries of each of those elements. Your function should return the conversation, prepending each field with either ‘ Person 1: ‘ if ‘x’ is even or ‘ Person 2: ‘ if ‘x’ is odd. You can use the Python modulus operator ‘%’ to help select the even/odd entries. Important note: Do not print a newline character (i.e. <code>\n</code>) when generating the string. For example, in the code cell above, your function should output something like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person 1: I would like a taxi from Saint John&apos;s college to Pizza Hut Fen Ditton. Person 2: What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure><p>and <strong>not</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1:  I would like a taxi from Saint John&apos;s college to Pizza Hut Fen Ditton.</span><br><span class="line">Person 2:  What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_conversation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conversation</span><span class="params">(file, data_db)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file (string): filename of the dialogue file saved as json</span></span><br><span class="line"><span class="string">        data_db (dict): dialogue database</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: A string containing the 'text' fields of  data[file]['log'][x]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize empty string</span></span><br><span class="line">    result = <span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get length of file's log list</span></span><br><span class="line">    len_msg_log = len(data_db[file][<span class="string">'log'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set the delimiter strings</span></span><br><span class="line">    delimiter_1 = <span class="string">' Person 1: '</span></span><br><span class="line">    delimiter_2 = <span class="string">' Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over the file's log list</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len_msg_log):</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># get i'th element of file log list</span></span><br><span class="line">        cur_log = data_db[file][<span class="string">'log'</span>][i][<span class="string">'text'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check if i is even</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:                   </span><br><span class="line">            <span class="comment"># append the 1st delimiter string</span></span><br><span class="line">            result += delimiter_1</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="comment"># append the 2nd delimiter string</span></span><br><span class="line">            result += delimiter_2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the message text from the log</span></span><br><span class="line">        result += cur_log</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> w4_unittest</span><br><span class="line">w4_unittest.test_get_conversation(get_conversation)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file = <span class="string">'SNG01856.json'</span></span><br><span class="line">conversation = get_conversation(file, DIALOGUE_DB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print raw output</span></span><br><span class="line">print(conversation)</span><br></pre></td></tr></table></figure><pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</code></pre><p><strong>Expected Result:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&apos;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&apos;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.</span><br><span class="line">Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</span><br></pre></td></tr></table></figure></p><p>We can have a utility pretty print function just so we can visually follow the conversation more easily.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_conversation</span><span class="params">(conversation)</span>:</span></span><br><span class="line">    </span><br><span class="line">    delimiter_1 = <span class="string">'Person 1: '</span></span><br><span class="line">    delimiter_2 = <span class="string">'Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    split_list_d1 = conversation.split(delimiter_1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> sublist <span class="keyword">in</span> split_list_d1[<span class="number">1</span>:]:</span><br><span class="line">        split_list_d2 = sublist.split(delimiter_2)</span><br><span class="line">        print(colored(<span class="string">f'Person 1: <span class="subst">&#123;split_list_d2[<span class="number">0</span>]&#125;</span>'</span>, <span class="string">'red'</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> len(split_list_d2) &gt; <span class="number">1</span>:</span><br><span class="line">            print(colored(<span class="string">f'Person 2: <span class="subst">&#123;split_list_d2[<span class="number">1</span>]&#125;</span>'</span>, <span class="string">'green'</span>))</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">print_conversation(conversation)</span><br></pre></td></tr></table></figure><p>For this assignment, we will just use the outputs of the calls to <code>get_conversation</code> to train the model. But just to expound, there are also other information in the MultiWoz dataset that can be useful in other contexts. Each element of the log list has more information about it. For example, above, if you were to look at the other fields for the following, “am looking for a place to stay that has cheap price range it should be in a type of hotel”, you will get the following. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">'SNG01856.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;text&#39;: &#39;am looking for a place to to stay that has cheap price range it should be in a type of hotel&#39;, &#39;metadata&#39;: {}, &#39;dialog_act&#39;: {&#39;Hotel-Inform&#39;: [[&#39;Type&#39;, &#39;hotel&#39;], [&#39;Price&#39;, &#39;cheap&#39;]]}, &#39;span_info&#39;: [[&#39;Hotel-Inform&#39;, &#39;Type&#39;, &#39;hotel&#39;, 20, 20],  [&#39;Hotel-Inform&#39;, &#39;Price&#39;, &#39;cheap&#39;, 10, 10]]}</code></pre><p>The dataset also comes with hotel, hospital, taxi, train, police, and restaurant databases. For example, in case you need to call a doctor, or a hotel, or a taxi, this will allow you to automate the entire conversation. Take a look at the files accompanying the data set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the attractions file</span></span><br><span class="line">attraction_file = open(<span class="string">'data/attraction_db.json'</span>)</span><br><span class="line">attractions = json.load(attraction_file)</span><br><span class="line">print(attractions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;pool way, whitehill road, off newmarket road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;entrance fee&#39;: &#39;?&#39;, &#39;id&#39;: &#39;1&#39;, &#39;location&#39;: [52.208789, 0.154883], &#39;name&#39;: &#39;abbey pool and astroturf pitch&#39;, &#39;openhours&#39;: &#39;?&#39;, &#39;phone&#39;: &#39;01223902088&#39;, &#39;postcode&#39;: &#39;cb58nt&#39;, &#39;pricerange&#39;: &#39;?&#39;, &#39;type&#39;: &#39;swimmingpool&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hospital file</span></span><br><span class="line">hospital_file = open(<span class="string">'data/hospital_db.json'</span>)</span><br><span class="line">hospitals = json.load(hospital_file)</span><br><span class="line">print(hospitals[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;department&#39;: &#39;neurosciences critical care unit&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223216297&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hotel file</span></span><br><span class="line">hotel_file = open(<span class="string">'data/hotel_db.json'</span>)</span><br><span class="line">hotels = json.load(hotel_file)</span><br><span class="line">print(hotels[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;124 tenison road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;internet&#39;: &#39;yes&#39;, &#39;parking&#39;: &#39;no&#39;, &#39;id&#39;: &#39;0&#39;, &#39;location&#39;: [52.1963733, 0.1987426], &#39;name&#39;: &#39;a and b guest house&#39;, &#39;phone&#39;: &#39;01223315702&#39;, &#39;postcode&#39;: &#39;cb12dp&#39;, &#39;price&#39;: {&#39;double&#39;: &#39;70&#39;, &#39;family&#39;: &#39;90&#39;, &#39;single&#39;: &#39;50&#39;}, &#39;pricerange&#39;: &#39;moderate&#39;, &#39;stars&#39;: &#39;4&#39;, &#39;takesbookings&#39;: &#39;yes&#39;, &#39;type&#39;: &#39;guesthouse&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the police file</span></span><br><span class="line">police_file = open(<span class="string">'data/police_db.json'</span>)</span><br><span class="line">police = json.load(police_file)</span><br><span class="line">print(police[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;name&#39;: &#39;Parkside Police Station&#39;, &#39;address&#39;: &#39;Parkside, Cambridge&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223358966&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of a restuarant file</span></span><br><span class="line">restaurant_file = open(<span class="string">'data/restaurant_db.json'</span>)</span><br><span class="line">restaurants = json.load(restaurant_file)</span><br><span class="line">print(restaurants[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;Regent Street City Centre&#39;, &#39;area&#39;: &#39;centre&#39;, &#39;food&#39;: &#39;italian&#39;, &#39;id&#39;: &#39;19210&#39;, &#39;introduction&#39;: &#39;Pizza hut is a large chain with restaurants nationwide offering convenience pizzas pasta and salads to eat in or take away&#39;, &#39;location&#39;: [52.20103, 0.126023], &#39;name&#39;: &#39;pizza hut city centre&#39;, &#39;phone&#39;: &#39;01223323737&#39;, &#39;postcode&#39;: &#39;cb21ab&#39;, &#39;pricerange&#39;: &#39;cheap&#39;, &#39;type&#39;: &#39;restaurant&#39;}</code></pre><p>For more information about the multiwoz 2.1 data set, please run the cell below to read the <code>ReadMe.txt</code> file. Feel free to open any other file to explore it. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data/README'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    print(file.read())</span><br></pre></td></tr></table></figure><pre><code>###########################################################################################################  Copyright Cambridge Dialogue Systems Group, 2018 ###########################################################################################################Dataset contains the following files:1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have &quot;MUL&quot; in their names. Single domain dialogues have either &quot;SNG&quot; or &quot;WOZ&quot; in their names.2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.7. police_db.json: the Cambridge police station information.8. taxi_db.json: slot-value list for taxi domain.9. valListFile.txt: list of dialogues for validation.10. testListFile.txt: list of dialogues for testing.11. system_acts.json:  There are 6 domains (&#39;Booking&#39;, &#39;Restaurant&#39;, &#39;Hotel&#39;, &#39;Attraction&#39;, &#39;Taxi&#39;, &#39;Train&#39;) and 1 dummy domain (&#39;general&#39;).  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. &#39;Hotel-inform&#39; means it is an &#39;inform&#39; act in the Hotel domain.  Dialogue acts which cannot take slots, e.g., &#39;good bye&#39;, are defined under the &#39;general&#39; domain.  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.  If a dialogue act takes no slots, e.g., dialogue act &#39;offer booking&#39; for an utterance &#39;would you like to take a reservation?&#39;, its slot-value pair is [&#39;none&#39;, &#39;none&#39;]  There are four types of values:  1) If a slot takes a binary value, e.g., &#39;has Internet&#39; or &#39;has park&#39;, the value is either &#39;yes&#39; or &#39;no&#39;.  2) If a slot is under the act &#39;request&#39;, e.g., &#39;request&#39; about &#39;area&#39;, the value is expressed as &#39;?&#39;.  3) The value that appears in the utterance e.g., the name of a restaurant.  4) If for some reason the turn does not have an annotation then it is labeled as &quot;No Annotation.&quot;12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. </code></pre><p>As you can see, there are many other aspects of the MultiWoz dataset. Nonetheless, you’ll see that even with just the conversations, your model will still be able to generate useful responses. This concludes our exploration of the dataset. In the next section, we will do some preprocessing before we feed it into our model for training.</p><p><a name="2"></a></p><h1 id="Part-2-Processing-the-data-for-Reformer-inputs"><a href="#Part-2-Processing-the-data-for-Reformer-inputs" class="headerlink" title="Part 2:   Processing the data for Reformer inputs"></a>Part 2:   Processing the data for Reformer inputs</h1><p>You will now use the <code>get_conversation()</code> function to process the data. The Reformer expects inputs of this form: </p><p><strong>Person 1: Why am I so happy? Person 2: Because you are learning NLP Person 1: … Person 2: …*</strong></p><p>And the conversation keeps going with some text. As you can see ‘Person 1’ and ‘Person 2’ act as delimiters so the model automatically recognizes the person and who is talking. It can then come up with the corresponding text responses for each person. Let’s proceed to process the text in this fashion for the Reformer. First, let’s grab all the conversation strings from all dialogue files and put them in a list.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the keys are the file names</span></span><br><span class="line">all_files = DIALOGUE_DB.keys()</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize empty list</span></span><br><span class="line">untokenized_data = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># loop over all files</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> all_files:</span><br><span class="line">    <span class="comment"># this is the graded function you coded</span></span><br><span class="line">    <span class="comment"># returns a string delimited by Person 1 and Person 2</span></span><br><span class="line">    result = get_conversation(file, DIALOGUE_DB)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># append to the list</span></span><br><span class="line">    untokenized_data.append(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the first element to check if it's the same as the one we got before</span></span><br><span class="line">print(untokenized_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</code></pre><p>Now let us split the list to a train and eval dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle the list we generated above</span></span><br><span class="line">random.shuffle(untokenized_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a cutoff (5% of the total length for this assignment)</span></span><br><span class="line"><span class="comment"># convert to int because we will use it as a list index</span></span><br><span class="line">cut_off = int(len(untokenized_data) * <span class="number">.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># slice the list. the last elements after the cut_off value will be the eval set. the rest is for training. </span></span><br><span class="line">train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'number of conversations in the data set: <span class="subst">&#123;len(untokenized_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'number of conversations in train set: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'number of conversations in eval set: <span class="subst">&#123;len(eval_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>number of conversations in the data set: 10438number of conversations in train set: 9917number of conversations in eval set: 521</code></pre><p><a name="2.1"></a></p><h2 id="2-1-Tokenizing-batching-with-bucketing"><a href="#2-1-Tokenizing-batching-with-bucketing" class="headerlink" title="2.1   Tokenizing, batching with bucketing"></a>2.1   Tokenizing, batching with bucketing</h2><p>We can now proceed in generating tokenized batches of our data. Let’s first define a utility generator function to yield elements from our data sets:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stream</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># loop over the entire data</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="comment"># get a random element</span></span><br><span class="line">        d = random.choice(data)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># yield a tuple pair of identical values </span></span><br><span class="line">        <span class="comment"># (i.e. our inputs to the model will also be our targets during training)</span></span><br><span class="line">        <span class="keyword">yield</span> (d, d)</span><br></pre></td></tr></table></figure><p>Now let’s define our data pipeline for tokenizing and batching our data. As in the previous assignments, we will bucket by length and also have an upper bound on the token length.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trax allows us to use combinators to generate our data pipeline</span></span><br><span class="line">data_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># randomize the stream</span></span><br><span class="line">    trax.data.Shuffle(),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># tokenize the data</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=VOCAB_DIR,</span><br><span class="line">                       vocab_file=VOCAB_FILE),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># filter too long sequences</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># bucket by length</span></span><br><span class="line">    trax.data.BucketByLength(boundaries=[<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>],</span><br><span class="line">                             batch_sizes=[<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,   <span class="number">2</span>, <span class="number">1</span>]),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add loss weights but do not add it to the padding tokens (i.e. 0)</span></span><br><span class="line">    trax.data.AddLossWeights(id_to_mask=<span class="number">0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply the data pipeline to our train and eval sets</span></span><br><span class="line">train_stream = data_pipeline(stream(train_data))</span><br><span class="line">eval_stream = data_pipeline(stream(eval_data))</span><br></pre></td></tr></table></figure><p>Peek into the train stream.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the stream generators will yield (input, target, weights). let's just grab the input for inspection</span></span><br><span class="line">inp, _, _ = next(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the shape. format is (batch size, token length)</span></span><br><span class="line">print(<span class="string">"input shape: "</span>, inp.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># detokenize the first element</span></span><br><span class="line">print(trax.data.detokenize(inp[<span class="number">0</span>], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))</span><br></pre></td></tr></table></figure><pre><code>input shape:  (4, 512) Person 1: I need a place to stay that has free wifi.  Person 2: There are 32 options in Cambridge, what price range are you looking for? Person 1: I&#39;m looking for something in the cheap price range, but I need it to have a 4 star rating. I don&#39;t need any parking though. Person 2: Again I have many to choose from that meet those criteria. Would you like a suggestion? Person 1: Ok, yes, if you could suggest one that comes with free parking that would be great! Person 2: I will book it for you,is there anything else I can do for you ? Person 1: I also need a Vietnamese restaurant. Person 2: My apologies it appears that I forgot to book your lodging. I recommend Alexander Bed and Breakfast, would you like me to book it for you? Person 1: Oh yes, please do. I need it for 8 people and 5 nights, beginning friday Person 2: You are booked with the reference number E9100B48. I can help you with the Vietnamese restaurant now. Do you have an area in mind? Person 1: I just want the restaurant to be in the same price range as my hotel Person 2: There is one cheap vietnamese restaurant in town. It is thanh binh. Do you want to book? Person 1: No, just provide me with the address and area for that restaurant if you could Person 2: The restaurant is located at 17 Magdalene Street City Centre in the West.  Can I help you with anything else? Person 1: Yes, will you book me a taxi to the restaurant from the hotel, please Person 2: And what time would you like that taxi? Person 1: I would like to leave the hotel by 22:15. Person 2: Your taxi service was book with a red volkswagen. The contact number is 07797935179 in case you need to contact them. Person 1: Thank you, that will be all. Person 2: You are welcome enjoy your meal. Have a good evenening</code></pre><p><a name="3"></a></p><h1 id="Part-3-Reversible-layers"><a href="#Part-3-Reversible-layers" class="headerlink" title="Part 3:   Reversible layers"></a>Part 3:   Reversible layers</h1><p>When running large deep models, you will often run out of memory as each layer allocates memory to store activations for use in backpropagation. To save this resource, you need to be able to recompute these activations during the backward pass without storing them during the forward pass. Take a look first at the leftmost diagram below. </p><p><img src="reversible2.PNG" height="400" width="600"></p><p>This is how the residual networks are implemented in the standard Transformer. It follows that, given <code>F()</code> is Attention and <code>G()</code> is Feed-forward(FF).<br>: </p><p>\begin{align}<br>\mathrm{y}_\mathrm{a} &amp;= \mathrm{x} + \mathrm{F}\left(\mathrm{x}\right)\tag{1} \\<br>\mathrm{y}_{b}&amp;=\mathrm{y}_{a}+\mathrm{G}\left(\mathrm{y}_{a}\right)\tag{2}\\<br>\end{align}</p><p>As you can see, it requires that $\mathrm{x}$ and $\mathrm{y}_{a}$ be saved so it can be used during backpropagation. We want to avoid this to conserve memory and this is where reversible residual connections come in. They are shown in the middle and rightmost diagrams above. The key idea is that we will start with two copies of the input to the model and at each layer we will only update one of them. The activations that we <em>don’t</em> update are the ones that will be used to compute the residuals. </p><p>Now in this reversible set up you get the following instead: </p><p>\begin{align}<br>\mathrm{y}_{1}&amp;=\mathrm{x}_{1}+\mathrm{F}\left(\mathrm{x}_{2}\right)\tag{3}\\<br>\mathrm{y}_{2}&amp;=\mathrm{x}_{2}+\mathrm{G}\left(\mathrm{y}_{1}\right)\tag{4}\\<br>\end{align}<br>To recover $\mathrm{(x_1,x_2)}$ from $\mathrm{(y_1, y_2)}$ </p><p>\begin{align}<br>\mathrm{x}_{2}&amp;=\mathrm{y}_{2}-\mathrm{G}\left(\mathrm{y}_{1}\right)\tag{5}\\<br>\mathrm{x}_{1}&amp;=\mathrm{y}_{1}-\mathrm{F}\left(\mathrm{x}_{2}\right)\tag{6}\\<br>\end{align}</p><p>With this configuration, we’re now able to run the network fully in reverse. You’ll notice that during the backward pass, $\mathrm{x2}$ and $\mathrm{x1}$ can be recomputed based solely on the values of $\mathrm{y2}$ and $\mathrm{y1}$. No need to save it during the forward pass.</p><p><a name="ex02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> You will implement the <code>reversible_layer_forward</code> function using equations 3 and 4 above. This function takes in the input vector <code>x</code> and the functions <code>f</code> and <code>g</code> and returns the concatenation of $y_1 and y_2$. For this exercise, we will be splitting <code>x</code> before going through the reversible residual steps$\mathrm{^1}$.  We can then use those two vectors for the <code>reversible_layer_reverse</code> function. Utilize <code>np.concatenate()</code> to form the output being careful to match the axis of the <code>np.split()</code>.</p><p>$\mathrm{^1}$<em>Take note that this is just for demonstrating the concept in this exercise and there are other ways of processing the input. As you’ll see in the Reformer architecture later, the initial input (i.e. <code>x</code>) can instead be duplicated instead of split.</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_forward</span><span class="params">(x, f, g)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        x (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by 'x', f and g</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    x1, x2 = np.split(x, <span class="number">2</span>, axis=<span class="number">-1</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y1 using equation 3</span></span><br><span class="line">    y1 = x1 + f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y2 using equation 4</span></span><br><span class="line">    y2 = x2 + g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray</span></span><br><span class="line">    y = np.concatenate((y1,y2), axis = <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_forward(reversible_layer_forward)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p>You will now implement the <code>reversible_layer_reverse</code> function  which is possible because at every time step you have $x_1$ and $x_2$ and $y_2$ and $y_1$, along with the function <code>f</code>, and <code>g</code>. Where <code>f</code> is the attention and <code>g</code> is the feedforward. This allows you to compute equations 5 and 6.</p><p><strong>Instructions:</strong> Implement the <code>reversible_layer_reverse</code>. Your function takes in the output vector from  <code>reversible_layer_forward</code> and functions f and g. Using equations 5 and 6 above, it computes the inputs to the layer,  $x_1$ and $x_2$.  The output, x, is the concatenation of  $x_1, x_2$. Utilize <code>np.concatenate()</code>  to form the output being careful to match the axis of the <code>np.split()</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_reverse</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_reverse</span><span class="params">(y, f, g)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        y (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix of the form of 'y'</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix of the form of 'y'</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by 'y', f and g</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    y1, y2 = np.split(y, <span class="number">2</span>, axis=<span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x2 using equation 5</span></span><br><span class="line">    x2 = y2 - g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x1 using equation 6</span></span><br><span class="line">    x1 = y1 - f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate x1 and x2 along the depth dimension</span></span><br><span class="line">    x = np.concatenate((x1,x2),axis = <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_reverse(reversible_layer_reverse)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: assert at the end can be used in grading as well</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + <span class="number">2</span></span><br><span class="line">g = <span class="keyword">lambda</span> x: x * <span class="number">3</span></span><br><span class="line">input_vector = np.random.uniform(size=(<span class="number">32</span>,))</span><br><span class="line"></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector)</span><br></pre></td></tr></table></figure><p><a name="3.1"></a></p><h2 id="3-1-Reversible-layers-and-randomness"><a href="#3-1-Reversible-layers-and-randomness" class="headerlink" title="3.1   Reversible layers and randomness"></a>3.1   Reversible layers and randomness</h2><p>This is why we were learning about fastmath’s random functions and keys in Course 3 Week 1. Utilizing the same key, <code>trax.fastmath.random.uniform()</code> will return the same values. This is required for the backward pass to return the correct layer inputs when random noise is introduced in the layer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Layers like dropout have noise, so let's simulate it here:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + np.random.uniform(size=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that the above doesn't work any more:</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="keyword">not</span> np.allclose(reversed_vector, input_vector)  <span class="comment"># Fails!!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It failed because the noise when reversing used a different random seed.</span></span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">27686</span></span><br><span class="line">rng = trax.fastmath.random.get_prng(random_seed)</span><br><span class="line">f = <span class="keyword">lambda</span> x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that it works now as the same rng is used on forward and reverse.</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector,  atol=<span class="number">1e-07</span>)</span><br></pre></td></tr></table></figure><p><a name="4"></a></p><h1 id="Part-4-ReformerLM-Training"><a href="#Part-4-ReformerLM-Training" class="headerlink" title="Part 4:   ReformerLM Training"></a>Part 4:   ReformerLM Training</h1><p>You will now proceed to training your model. Since you have already know the two main components that differentiates it from the standard Transformer, LSH in Course 1 and reversible layers above, you can just use the pre-built model already implemented in Trax. It will have this architecture:</p><p><img src="Reformer.jpg"></p><p>Similar to the Transformer you learned earlier, you want to apply an attention and feed forward layer to your inputs. For the Reformer, we improve the memory efficiency by using <strong>reversible decoder blocks</strong> and you can picture its implementation in Trax like below:</p><p><img src="ReversibleDecoder.png"></p><p>You can see that it takes the initial inputs <code>x1</code> and <code>x2</code> and does the first equation of the reversible networks you learned in Part 3. As you’ve also learned, the reversible residual has two equations for the forward-pass so doing just one of them will just constitute half of the reversible decoder block. Before doing the second equation (i.e. second half of the reversible residual), it first needs to swap the elements to take into account the stack semantics in Trax. It simply puts <code>x2</code> on top of the stack so it can be fed to the add block of the half-residual layer. It then swaps the two outputs again so it can be fed to the next layer of the network. All of these arrives at the two equations in Part 3 and it can be used to recompute the activations during the backward pass.</p><p>These are already implemented for you in Trax and in the following exercise, you’ll get to practice how to call them to build your network.</p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement a wrapper function that returns a Reformer Language Model. You can use Trax’s <a href="https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.reformer.reformer.ReformerLM" target="_blank" rel="noopener">ReformerLM</a> to do this quickly. It will have the same architecture as shown above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM</span><span class="params">(vocab_size=<span class="number">33000</span>, n_layers=<span class="number">2</span>, mode=<span class="string">'train'</span>, attention_type=tl.SelfAttention)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        vocab_size (int): size of the vocabulary</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers</span></span><br><span class="line"><span class="string">        mode (string): setting of the model which can be 'train', 'eval', or 'predict' </span></span><br><span class="line"><span class="string">        attention_type(class): attention class to use </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        model (ReformerLM): a reformer language model implemented in Trax</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    <span class="comment"># initialize an instance of Trax's ReformerLM class</span></span><br><span class="line">    model = trax.models.reformer.ReformerLM( </span><br><span class="line">        <span class="comment"># set vocab size</span></span><br><span class="line">        vocab_size = vocab_size,</span><br><span class="line">        <span class="comment"># set number of layers</span></span><br><span class="line">        n_layers = n_layers,</span><br><span class="line">        <span class="comment"># set mode</span></span><br><span class="line">        mode = mode,</span><br><span class="line">        <span class="comment"># set attention type</span></span><br><span class="line">        attention_type = attention_type</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># display the model</span></span><br><span class="line">temp_model = ReformerLM(<span class="string">'train'</span>)</span><br><span class="line">print(str(temp_model))</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> temp_model</span><br></pre></td></tr></table></figure><pre><code>Serial[  ShiftRight(1)  Embedding_train_512  Dropout  PositionalEncoding  Dup_out2  ReversibleSerial_in2_out2[    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm      ]      SelfAttention    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm        Dense_2048        Dropout        FastGelu        Dense_512        Dropout      ]    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm      ]      SelfAttention    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm        Dense_2048        Dropout        FastGelu        Dense_512        Dropout      ]    ]    ReversibleSwap_in2_out2  ]  Concatenate_in2  LayerNorm  Dropout  Dense_train  LogSoftmax]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_ReformerLM(ReformerLM)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><p><a name="ex05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>You will now write a function that takes in your model and trains it. </p><p><strong>Instructions:</strong> Implement the <code>training_loop</code> below to train the neural network above. Here is a list of things you should do:</p><ul><li>Create <code>TrainTask</code> and <code>EvalTask</code></li><li>Create the training loop <code>trax.supervised.training.Loop</code></li><li>Pass in the following depending to train_task :<ul><li><code>labeled_data=train_gen</code></li><li><code>loss_layer=tl.CrossEntropyLoss()</code></li><li><code>optimizer=trax.optimizers.Adam(0.01)</code></li><li><code>lr_schedule=lr_schedule</code></li><li><code>n_steps_per_checkpoint=10</code>  </li></ul></li></ul><p>You will be using your CrossEntropyLoss loss function with Adam optimizer. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam" target="_blank" rel="noopener">trax</a> documentation to get a full understanding. </p><ul><li>Pass in the following to eval_task:<ul><li><code>labeled_data=eval_gen</code></li><li><code>metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]</code></li></ul></li></ul><p>This function should return a <code>training.Loop</code> object. To read more about this check the <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop" target="_blank" rel="noopener">docs</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span><span class="params">(ReformerLM, train_gen, eval_gen, output_dir = <span class="string">"./model/"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you are building</span></span><br><span class="line"><span class="string">        train_gen (generator): train data generator.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Validation generator. </span></span><br><span class="line"><span class="string">        output_dir (string): Path to save the model output. Defaults to './model/'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop for the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># use the warmup_and_rsqrt_decay learning rate schedule</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(</span><br><span class="line">        n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the train task</span></span><br><span class="line">    train_task = training.TrainTask(            </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = train_gen,</span><br><span class="line">        <span class="comment"># loss layer</span></span><br><span class="line">        loss_layer = tl.CrossEntropyLoss(),</span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line">        optimizer=trax.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">        <span class="comment"># lr_schedule</span></span><br><span class="line">        lr_schedule=lr_schedule,</span><br><span class="line">        <span class="comment"># n_steps</span></span><br><span class="line">        n_steps_per_checkpoint=<span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define the eval task</span></span><br><span class="line">    eval_task = training.EvalTask(                      </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = eval_gen,</span><br><span class="line">        metrics = [tl.CrossEntropyLoss(), tl.Accuracy()]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    loop = training.Loop(ReformerLM(mode=<span class="string">'train'</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: Use the train task and eval task for grading train_model</span></span><br><span class="line">test_loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">train_task = test_loop._task</span><br><span class="line">eval_task = test_loop._eval_task</span><br><span class="line"></span><br><span class="line">print(train_task)</span><br><span class="line">print(eval_task)</span><br></pre></td></tr></table></figure><pre><code>&lt;trax.supervised.training.TrainTask object at 0x7fd4ddf95dd0&gt;&lt;trax.supervised.training.EvalTask object at 0x7fd4dc2a2f50&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_tasks(train_task, eval_task)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we will now test your function</span></span><br><span class="line">!rm -f model/model.pkl.gz</span><br><span class="line">loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>Step      1: Ran 1 train steps in 58.71 secsStep      1: train CrossEntropyLoss |  10.41530514Step      1: eval  CrossEntropyLoss |  10.41272354Step      1: eval          Accuracy |  0.00000000Step     10: Ran 9 train steps in 163.46 secsStep     10: train CrossEntropyLoss |  10.25675583Step     10: eval  CrossEntropyLoss |  9.94296360Step     10: eval          Accuracy |  0.11201393</code></pre><p><strong>Approximate Expected output:</strong>  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Step      1: Ran 1 train steps in 55.73 secs</span><br><span class="line">Step      1: train CrossEntropyLoss |  10.41907787</span><br><span class="line">Step      1: eval  CrossEntropyLoss |  10.41005802</span><br><span class="line">Step      1: eval          Accuracy |  0.00000000</span><br><span class="line"></span><br><span class="line">Step     10: Ran 9 train steps in 108.21 secs</span><br><span class="line">Step     10: train CrossEntropyLoss |  10.15449715</span><br><span class="line">Step     10: eval  CrossEntropyLoss |  9.63478279</span><br><span class="line">Step     10: eval          Accuracy |  0.16350447</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">&lt;a name=&quot;5&quot;&gt;&lt;/a&gt;</span><br><span class="line"># Part 5:   Decode from a pretrained model</span><br><span class="line"></span><br><span class="line">We will now proceed on decoding using the model architecture you just implemented. As in the previous weeks, we will be giving you a pretrained model so you can observe meaningful output during inference. You will be using the [autoregressive_sample_stream()](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample_stream) decoding method from Trax to do fast inference. Let&apos;s define a few parameters to initialize our model.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention</span><br><span class="line">def attention(*args, **kwargs):</span><br><span class="line">    # number of input positions to remember in a cache when doing fast inference. </span><br><span class="line">    kwargs[&apos;predict_mem_len&apos;] = 120</span><br><span class="line">    # number of input elements to drop once the fast inference input cache fills up.</span><br><span class="line">    kwargs[&apos;predict_drop_len&apos;] = 120</span><br><span class="line">    # return the attention layer with the parameters defined above</span><br><span class="line">    return tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"># define the model using the ReformerLM function you implemented earlier.</span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=33000,</span><br><span class="line">    n_layers=6,</span><br><span class="line">    mode=&apos;predict&apos;,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.</span><br><span class="line">shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)</span><br></pre></td></tr></table></figure><p>We can now initialize our model from a file containing the pretrained weights. We will save this starting state so we can reset the model state when we generate a new conversation. This will become clearer in the <code>generate_dialogue()</code> function later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize from file</span></span><br><span class="line">model.init_from_file(<span class="string">'chatbot_model1.pkl.gz'</span>,</span><br><span class="line">                     weights_only=<span class="keyword">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save the starting state</span></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure><p>Let’s define a few utility functions as well to help us tokenize and detokenize. We can use the <a href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize" target="_blank" rel="noopener">tokenize()</a> and <a href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize" target="_blank" rel="noopener">detokenize()</a> from <code>trax.data.tf_inputs</code> to do this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentence, vocab_file, vocab_dir)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span><span class="params">(tokens, vocab_file, vocab_dir)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)</span><br></pre></td></tr></table></figure><p>We are now ready to define our decoding function. This will return a generator that yields that next symbol output by the model. It will be able to predict the next words by just feeding it a starting sentence.</p><p><a name="ex06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement the function below to return a generator that predicts the next word of the conversation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM_output_gen</span><span class="params">(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create input tokens using the the tokenize function</span></span><br><span class="line">    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add batch dimension to array. Convert from (n,) to (x, n) where </span></span><br><span class="line">    <span class="comment"># x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)</span></span><br><span class="line">    input_tokens_with_batch = np.expand_dims(input_tokens, axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># call the autoregressive_sample_stream function from trax</span></span><br><span class="line">    output_gen = trax.supervised.decoding.autoregressive_sample_stream( </span><br><span class="line">        <span class="comment"># model</span></span><br><span class="line">        ReformerLM,</span><br><span class="line">        <span class="comment"># inputs will be the tokens with batch dimension</span></span><br><span class="line">        inputs = input_tokens_with_batch,</span><br><span class="line">        <span class="comment"># temperature</span></span><br><span class="line">        temperature = temperature</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output_gen</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">WEIGHTS_FROM_FILE = ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'weights'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    WEIGHTS_FROM_FILE = pickle.load(file)</span><br><span class="line"></span><br><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    kwargs[<span class="string">'predict_mem_len'</span>] = <span class="number">120</span></span><br><span class="line">    kwargs[<span class="string">'predict_drop_len'</span>] = <span class="number">120</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">test_model = ReformerLM(vocab_size=<span class="number">5</span>, n_layers=<span class="number">1</span>, mode=<span class="string">'predict'</span>, attention_type=attention)</span><br><span class="line"></span><br><span class="line">test_output_gen = ReformerLM_output_gen(test_model, <span class="string">"test"</span>, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_model.init_weights_and_state(shape11)</span><br><span class="line"></span><br><span class="line">test_model.weights = WEIGHTS_FROM_FILE</span><br><span class="line"></span><br><span class="line">output = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    output.append(next(test_output_gen)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> test_model </span><br><span class="line"><span class="keyword">del</span> WEIGHTS_FROM_FILE</span><br><span class="line"><span class="keyword">del</span> test_output_gen</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><pre><code>[1, 0, 4, 3, 0, 4]</code></pre><p><strong><em>Expected value:</em></strong></p><p>[1, 0, 4, 3, 0, 4]</p><p>Great! Now you will be able to see the model in action. The utility function below will call the generator you just implemented and will just format the output to be easier to read. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    kwargs[<span class="string">'predict_mem_len'</span>] = <span class="number">120</span>  <span class="comment"># max length for predictions</span></span><br><span class="line">    kwargs[<span class="string">'predict_drop_len'</span>] = <span class="number">120</span>  <span class="comment"># never drop old stuff</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=<span class="number">33000</span>,</span><br><span class="line">    n_layers=<span class="number">6</span>,</span><br><span class="line">    mode=<span class="string">'predict'</span>,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.init_from_file(<span class="string">'chatbot_model1.pkl.gz'</span>,</span><br><span class="line">                     weights_only=<span class="keyword">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_dialogue</span><span class="params">(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        model_state (np.array): initial state of the model before decoding</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        max_len (int): maximum number of tokens to generate </span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    """</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the delimiters we used during training</span></span><br><span class="line">    delimiter_1 = <span class="string">'Person 1: '</span> </span><br><span class="line">    delimiter_2 = <span class="string">'Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize detokenized output</span></span><br><span class="line">    sentence = <span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># token counter</span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output tokens. we insert a ': ' for formatting</span></span><br><span class="line">    result = [tokenize(<span class="string">': '</span>, vocab_file=vocab_file, vocab_dir=vocab_dir)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># reset the model state when starting a new dialogue</span></span><br><span class="line">    ReformerLM.state = model_state</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calls the output generator implemented earlier</span></span><br><span class="line">    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print the starting sentence</span></span><br><span class="line">    print(start_sentence.split(delimiter_2)[<span class="number">0</span>].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.</span></span><br><span class="line">    <span class="keyword">for</span> o <span class="keyword">in</span> output:</span><br><span class="line">        </span><br><span class="line">        result.append(o)</span><br><span class="line">        </span><br><span class="line">        sentence = detokenize(np.concatenate(result, axis=<span class="number">0</span>), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> sentence.endswith(delimiter_1):</span><br><span class="line">            sentence = sentence.split(delimiter_1)[<span class="number">0</span>]</span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;delimiter_2&#125;</span><span class="subst">&#123;sentence&#125;</span>'</span>)</span><br><span class="line">            sentence = <span class="string">''</span></span><br><span class="line">            result.clear()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> sentence.endswith(delimiter_2):</span><br><span class="line">            sentence = sentence.split(delimiter_2)[<span class="number">0</span>]</span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;delimiter_1&#125;</span><span class="subst">&#123;sentence&#125;</span>'</span>)</span><br><span class="line">            sentence = <span class="string">''</span></span><br><span class="line">            result.clear()</span><br><span class="line"></span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> counter &gt; max_len:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>We can now feed in different starting sentences and see how the model generates the dialogue. You can even input your own starting sentence. Just remember to ask a question that covers the topics in the Multiwoz dataset so you can generate a meaningful conversation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Are there theatres in town? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Are there theatres in town?Person 2: : There are 4 theatres in town. Do you have a preference on area? Person 1: No, I don&#39;t care. Which one do you recommend? Person 2: I would recommend the Mumford Theatre. Would you like more information on it? Person 1: Yes, could I get the postcode and phone number please? Person 2: The phone number is 08451962320 and the postcode is cb11pt. The phone number is 084519/ 15/15 - would you like to book a table? </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Is there a hospital nearby? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Is there a hospital nearby?Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need anything else? Person 1: No, that&#39;s all I need. Thanks. Person 2: You&#39;re welcome. Have a good day.Good bye.Person 1: Thanks again. Goodbye. Person 2: You&#39;re welcome. Have a good day.Good bye.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Can you book a taxi? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Can you book a taxi?Person 2: : I sure can. Where are you going? Person 1: I&#39;m going to be picked up from the city centre north b and b. Person 2: I have booked you a grey volkswagen. The contact number is 0783212843. Person 1: Thank you. That&#39;s all I need. Person 2: Thank you for using our services. Have a great day!k you.Good bye.Person 1: Actually, I&#39;ry about there. </code></pre><p><strong>Congratulations! You just wrapped up the final assignment of this course and the entire specialization!</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-4-Chatbot&quot;&gt;&lt;a href=&quot;#Assignment-4-Chatbot&quot; class=&quot;headerlink&quot; title=&quot;Assignment 4: Chatbot&quot;&gt;&lt;/a&gt;Assignment 4: Chatbot&lt;/h1
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Assignment 3: Question Answering</title>
    <link href="https://zhangruochi.com/Assignment-3-Question-Answering/2020/09/27/"/>
    <id>https://zhangruochi.com/Assignment-3-Question-Answering/2020/09/27/</id>
    <published>2020-09-27T09:13:45.000Z</published>
    <updated>2020-09-27T09:15:12.027Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-3-Question-Answering"><a href="#Assignment-3-Question-Answering" class="headerlink" title="Assignment 3: Question Answering"></a>Assignment 3: Question Answering</h1><p>Welcome to this week’s assignment of course 4. In this you will explore question answering. You will implement the “Text to Text Transfer from Transformers” (better known as T5). Since you implemented transformers from scratch last week you will now be able to use them. </p><p><img src="qa.png"> </p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">Overview</a></li><li><a href="#0">Part 0: Importing the Packages</a></li><li><a href="#1">Part 1: C4 Dataset</a><ul><li><a href="#1.1">1.1 Pre-Training Objective</a></li><li><a href="#1.2">1.2 Process C4</a><ul><li><a href="#1.2.1">1.2.1 Decode to natural language</a></li></ul></li><li><a href="#1.3">1.3 Tokenizing and Masking</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li><li><a href="#1.4">1.4 Creating the Pairs</a></li></ul></li><li><a href="#2">Part 2: Transfomer</a><ul><li><a href="#2.1">2.1 Transformer Encoder</a><ul><li><a href="#2.1.1">2.1.1 The Feedforward Block</a><ul><li><a href="#ex02">Exercise 02</a></li></ul></li><li><a href="#2.1.2">2.1.2 The Encoder Block</a><ul><li><a href="#ex03">Exercise 03</a></li></ul></li><li><a href="#2.1.3">2.1.3 The Transformer Encoder</a>            <ul><li><a href="#ex04">Exercise 04</a></li></ul></li></ul></li></ul></li></ul><p><a name="0"></a></p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>This assignment will be different from the two previous ones. Due to memory and time constraints of this environment you will not be able to train a model and use it for inference. Instead you will create the necessary building blocks for the transformer encoder model and will use a pretrained version of the same model in two ungraded labs after this assignment.</p><p>After completing these 3 (1 graded and 2 ungraded) labs you will:</p><ul><li>Implement the code neccesary for Bidirectional Encoder Representation from Transformer (BERT).</li><li>Understand how the C4 dataset is structured.</li><li>Use a pretrained model for inference.</li><li>Understand how the “Text to Text Transfer from Transformers” or T5 model works. </li></ul><p><a name="0"></a></p><h1 id="Part-0-Importing-the-Packages"><a href="#Part-0-Importing-the-Packages" class="headerlink" title="Part 0: Importing the Packages"></a>Part 0: Importing the Packages</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> decoding</span><br><span class="line"></span><br><span class="line"><span class="comment"># Will come handy later.</span></span><br><span class="line">wrapper = textwrap.TextWrapper(width=<span class="number">70</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p><a name="1"></a></p><h2 id="Part-1-C4-Dataset"><a href="#Part-1-C4-Dataset" class="headerlink" title="Part 1: C4 Dataset"></a>Part 1: C4 Dataset</h2><p>The <a href="https://www.tensorflow.org/datasets/catalog/c4" target="_blank" rel="noopener">C4</a> is a huge data set. For the purpose of this assignment you will use a few examples out of it which are present in <code>data.txt</code>. C4 is based on the <a href="https://commoncrawl.org/" target="_blank" rel="noopener">common crawl</a> project. Feel free to read more on their website. </p><p>Run the cell below to see how the examples look like. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load example jsons</span></span><br><span class="line">example_jsons = list(map(ast.literal_eval, open(<span class="string">'data.txt'</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Printing the examples to see how the data looks like</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(<span class="string">f'example number <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>: \n\n<span class="subst">&#123;example_jsons[i]&#125;</span> \n'</span>)</span><br></pre></td></tr></table></figure><pre><code>example number 1: {&#39;content-length&#39;: b&#39;1970&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.&#39;, &#39;timestamp&#39;: b&#39;2019-04-25T12:57:54Z&#39;, &#39;url&#39;: b&#39;https://klyq.com/beginners-bbq-class-taking-place-in-missoula/&#39;} example number 2: {&#39;content-length&#39;: b&#39;12064&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Discussion in \&#39;Mac OS X Lion (10.7)\&#39; started by axboi87, Jan 20, 2012.\nI\&#39;ve got a 500gb internal drive and a 240gb SSD.\nWhen trying to restore using disk utility i\&#39;m given the error &quot;Not enough space on disk ____ to restore&quot;\nBut I shouldn\&#39;t have to do that!!!\nAny ideas or workarounds before resorting to the above?\nUse Carbon Copy Cloner to copy one drive to the other. I\&#39;ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\&#39;t be bootable. CCC usually works in &quot;file mode&quot; and it can easily copy a larger drive (that\&#39;s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\nI\&#39;ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\&#39;t fit is there was slightly more than 4 GB of data.&#39;, &#39;timestamp&#39;: b&#39;2019-04-21T10:07:13Z&#39;, &#39;url&#39;: b&#39;https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/&#39;} example number 3: {&#39;content-length&#39;: b&#39;5235&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.&#39;, &#39;timestamp&#39;: b&#39;2019-04-25T10:40:23Z&#39;, &#39;url&#39;: b&#39;https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way&#39;} example number 4: {&#39;content-length&#39;: b&#39;4967&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&quot;How many backlinks per day for new site?\nDiscussion in &#39;Black Hat SEO&#39; started by Omoplata, Dec 3, 2010.\n1) for a newly created site, what&#39;s the max # backlinks per day I should do to be safe?\n2) how long do I have to let my site age before I can start making more blinks?\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?&quot;, &#39;timestamp&#39;: b&#39;2019-04-21T12:46:19Z&#39;, &#39;url&#39;: b&#39;https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/&#39;} example number 5: {&#39;content-length&#39;: b&#39;4499&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\xe2\x80\x99s included in the mill levy measure.&#39;, &#39;timestamp&#39;: b&#39;2019-04-20T14:33:21Z&#39;, &#39;url&#39;: b&#39;http://bond.dpsk12.org/category/news/&#39;} </code></pre><p>Notice the <code>b</code> before each string? This means that this data comes as bytes rather than strings. Strings are actually lists of bytes so for the rest of the assignments the name <code>strings</code> will be used to describe the data. </p><p>To check this run the following cell:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type(example_jsons[<span class="number">0</span>].get(<span class="string">'text'</span>))</span><br></pre></td></tr></table></figure><pre><code>bytes</code></pre><p><a name="1.1"></a></p><h3 id="1-1-Pre-Training-Objective"><a href="#1-1-Pre-Training-Objective" class="headerlink" title="1.1 Pre-Training Objective"></a>1.1 Pre-Training Objective</h3><p><strong>Note:</strong> The word “mask” will be used throughout this assignment in context of hiding/removing word(s)</p><p>You will be implementing the BERT loss as shown in the following image. </p><p><img src="loss.png" width="600" height="400"></p><p>Assume you have the following text: <span style="color:blue"> **Thank you <span style="color:red">for inviting </span> me to your party <span style="color:red">last</span>  week** &lt;/span&gt; </span></p><p>Now as input you will mask the words in red in the text: </p><p><span style="color:blue"> <strong>Input:</strong></span> Thank you  <strong>X</strong> me to your party <strong>Y</strong> week.</p><p><span style="color:blue"><strong>Output:</strong></span> The model should predict the words(s) for <strong>X</strong> and <strong>Y</strong>. </p><p><strong>Z</strong> is used to represent the end.</p><p><a name="1.2"></a></p><h3 id="1-2-Process-C4"><a href="#1-2-Process-C4" class="headerlink" title="1.2 Process C4"></a>1.2 Process C4</h3><p>C4 only has the plain string <code>text</code> field, so you will tokenize and have <code>inputs</code> and <code>targets</code> out of it for supervised learning. Given your inputs, the goal is to predict the targets during training. </p><p>You will now take the <code>text</code> and convert it to <code>inputs</code> and <code>targets</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Grab text field from dictionary</span></span><br><span class="line">natural_language_texts = [example_json[<span class="string">'text'</span>] <span class="keyword">for</span> example_json <span class="keyword">in</span> example_jsons]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First text example</span></span><br><span class="line">natural_language_texts[<span class="number">4</span>]</span><br></pre></td></tr></table></figure><pre><code>b&#39;The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\xe2\x80\x99s included in the mill levy measure.&#39;</code></pre><p><a name="1.2.1"></a></p><h4 id="1-2-1-Decode-to-natural-language"><a href="#1-2-1-Decode-to-natural-language" class="headerlink" title="1.2.1 Decode to natural language"></a>1.2.1 Decode to natural language</h4><p>The following functions will help you <code>detokenize</code> and<code>tokenize</code> the text data.  </p><p>The <code>sentencepiece</code> vocabulary was used to convert from text to ids. This vocabulary file is loaded and used in this helper functions.</p><p><code>natural_language_texts</code> has the text from the examples we gave you. </p><p>Run the cells below to see what is going on. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Special tokens</span></span><br><span class="line">PAD, EOS, UNK = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span><span class="params">(np_array)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(</span><br><span class="line">        np_array,</span><br><span class="line">        vocab_type=<span class="string">'sentencepiece'</span>,</span><br><span class="line">        vocab_file=<span class="string">'sentencepiece.model'</span>,</span><br><span class="line">        vocab_dir=<span class="string">'.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(s)</span>:</span></span><br><span class="line">  <span class="comment"># The trax.data.tokenize function operates on streams,</span></span><br><span class="line">  <span class="comment"># that's why we have to create 1-element stream with iter</span></span><br><span class="line">  <span class="comment"># and later retrieve the result with next.</span></span><br><span class="line">    <span class="keyword">return</span> next(trax.data.tokenize(</span><br><span class="line">        iter([s]),</span><br><span class="line">        vocab_type=<span class="string">'sentencepiece'</span>,</span><br><span class="line">        vocab_file=<span class="string">'sentencepiece.model'</span>,</span><br><span class="line">        vocab_dir=<span class="string">'.'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># printing the encoding of each word to see how subwords are tokenized</span></span><br><span class="line">tokenized_text = [(tokenize(word).tolist(), word) <span class="keyword">for</span> word <span class="keyword">in</span> natural_language_texts[<span class="number">0</span>].split()]</span><br><span class="line">print(tokenized_text, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><pre><code>[([12847, 277], b&#39;Beginners&#39;), ([15068], b&#39;BBQ&#39;), ([4501], b&#39;Class&#39;), ([3, 12297], b&#39;Taking&#39;), ([3399], b&#39;Place&#39;), ([16], b&#39;in&#39;), ([5964, 7115, 9, 55], b&#39;Missoula!&#39;), ([531], b&#39;Do&#39;), ([25], b&#39;you&#39;), ([241], b&#39;want&#39;), ([12], b&#39;to&#39;), ([129], b&#39;get&#39;), ([394], b&#39;better&#39;), ([44], b&#39;at&#39;), ([492], b&#39;making&#39;), ([3326], b&#39;delicious&#39;), ([15068, 58], b&#39;BBQ?&#39;), ([148], b&#39;You&#39;), ([56], b&#39;will&#39;), ([43], b&#39;have&#39;), ([8], b&#39;the&#39;), ([1004, 6], b&#39;opportunity,&#39;), ([474], b&#39;put&#39;), ([48], b&#39;this&#39;), ([30], b&#39;on&#39;), ([39], b&#39;your&#39;), ([4793], b&#39;calendar&#39;), ([230, 5], b&#39;now.&#39;), ([2721, 6], b&#39;Thursday,&#39;), ([1600], b&#39;September&#39;), ([1630, 727], b&#39;22nd&#39;), ([1715], b&#39;join&#39;), ([1150], b&#39;World&#39;), ([4501], b&#39;Class&#39;), ([15068], b&#39;BBQ&#39;), ([16127, 6], b&#39;Champion,&#39;), ([9137], b&#39;Tony&#39;), ([2659, 5595], b&#39;Balay&#39;), ([45], b&#39;from&#39;), ([301, 782, 3624], b&#39;Lonestar&#39;), ([14627, 15], b&#39;Smoke&#39;), ([12612, 277, 5], b&#39;Rangers.&#39;), ([216], b&#39;He&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([2119], b&#39;teaching&#39;), ([3, 9], b&#39;a&#39;), ([19529], b&#39;beginner&#39;), ([593], b&#39;level&#39;), ([853], b&#39;class&#39;), ([21], b&#39;for&#39;), ([921], b&#39;everyone&#39;), ([113], b&#39;who&#39;), ([2746], b&#39;wants&#39;), ([12], b&#39;to&#39;), ([129], b&#39;get&#39;), ([394], b&#39;better&#39;), ([28], b&#39;with&#39;), ([70], b&#39;their&#39;), ([17712], b&#39;culinary&#39;), ([1098, 5], b&#39;skills.&#39;), ([216], b&#39;He&#39;), ([56], b&#39;will&#39;), ([3884], b&#39;teach&#39;), ([25], b&#39;you&#39;), ([762], b&#39;everything&#39;), ([25], b&#39;you&#39;), ([174], b&#39;need&#39;), ([12], b&#39;to&#39;), ([214], b&#39;know&#39;), ([12], b&#39;to&#39;), ([5978], b&#39;compete&#39;), ([16], b&#39;in&#39;), ([3, 9], b&#39;a&#39;), ([3, 23405, 4547], b&#39;KCBS&#39;), ([15068], b&#39;BBQ&#39;), ([2259, 6], b&#39;competition,&#39;), ([379], b&#39;including&#39;), ([2097, 6], b&#39;techniques,&#39;), ([5459, 6], b&#39;recipes,&#39;), ([13618, 7, 6], b&#39;timelines,&#39;), ([3604], b&#39;meat&#39;), ([1801], b&#39;selection&#39;), ([11], b&#39;and&#39;), ([27856, 6], b&#39;trimming,&#39;), ([303], b&#39;plus&#39;), ([24190], b&#39;smoker&#39;), ([11], b&#39;and&#39;), ([1472], b&#39;fire&#39;), ([251, 5], b&#39;information.&#39;), ([37], b&#39;The&#39;), ([583], b&#39;cost&#39;), ([12], b&#39;to&#39;), ([36], b&#39;be&#39;), ([16], b&#39;in&#39;), ([8], b&#39;the&#39;), ([853], b&#39;class&#39;), ([19], b&#39;is&#39;), ([25264], b&#39;$35&#39;), ([399], b&#39;per&#39;), ([568, 6], b&#39;person,&#39;), ([11], b&#39;and&#39;), ([21], b&#39;for&#39;), ([21380, 7], b&#39;spectators&#39;), ([34], b&#39;it&#39;), ([19], b&#39;is&#39;), ([339, 5], b&#39;free.&#39;), ([15746, 26], b&#39;Included&#39;), ([16], b&#39;in&#39;), ([8], b&#39;the&#39;), ([583], b&#39;cost&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([893], b&#39;either&#39;), ([3, 9], b&#39;a&#39;), ([3, 17, 18, 9486], b&#39;t-shirt&#39;), ([42], b&#39;or&#39;), ([3, 9, 1409, 29], b&#39;apron&#39;), ([11], b&#39;and&#39;), ([25], b&#39;you&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([12246], b&#39;tasting&#39;), ([5977], b&#39;samples&#39;), ([13], b&#39;of&#39;), ([284], b&#39;each&#39;), ([3604], b&#39;meat&#39;), ([24], b&#39;that&#39;), ([19], b&#39;is&#39;), ([2657, 5], b&#39;prepared.&#39;)] </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We can see that detokenize successfully undoes the tokenization</span></span><br><span class="line">print(<span class="string">f"tokenized: <span class="subst">&#123;tokenize(<span class="string">'Beginners'</span>)&#125;</span>\ndetokenized: <span class="subst">&#123;detokenize(tokenize(<span class="string">'Beginners'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>tokenized: [12847   277]detokenized: Beginners</code></pre><p>As you can see above, you were able to take a piece of string and tokenize it. </p><p>Now you will create <code>input</code> and <code>target</code> pairs that will allow you to train your model. T5 uses the ids at the end of the vocab file as sentinels. For example, it will replace: </p><ul><li><code>vocab_size - 1</code> by <code>&lt;Z&gt;</code></li><li><code>vocab_size - 2</code> by <code>&lt;Y&gt;</code></li><li>and so forth. </li></ul><p>It assigns every word a <code>chr</code>.</p><p>The <code>pretty_decode</code> function below, which you will use in a bit, helps in handling the type when decoding. Take a look and try to understand what the function is doing.</p><p>Notice that:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">string.ascii_letters = <span class="string">'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'</span></span><br></pre></td></tr></table></figure></p><p><strong>NOTE:</strong> Targets may have more than the 52 sentinels we replace, but this is just to give you an idea of things.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = trax.data.vocab_size(</span><br><span class="line">    vocab_type=<span class="string">'sentencepiece'</span>,</span><br><span class="line">    vocab_file=<span class="string">'sentencepiece.model'</span>,</span><br><span class="line">    vocab_dir=<span class="string">'.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sentinels</span><span class="params">(vocab_size=vocab_size, display=False)</span>:</span></span><br><span class="line">    sentinels = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(reversed(string.ascii_letters), <span class="number">1</span>):</span><br><span class="line">        decoded_text = detokenize([vocab_size - i]) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Sentinels, ex: &lt;Z&gt; - &lt;a&gt;</span></span><br><span class="line">        sentinels[decoded_text] = <span class="string">f'&lt;<span class="subst">&#123;char&#125;</span>&gt;'</span>    </span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> display:</span><br><span class="line">            print(<span class="string">f'The sentinel is &lt;<span class="subst">&#123;char&#125;</span>&gt; and the decoded token is:'</span>, decoded_text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sentinels</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sentinels = get_sentinels(vocab_size, display=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>The sentinel is &lt;Z&gt; and the decoded token is: InternaționalThe sentinel is &lt;Y&gt; and the decoded token is: erwachseneThe sentinel is &lt;X&gt; and the decoded token is: CushionThe sentinel is &lt;W&gt; and the decoded token is: imunitarThe sentinel is &lt;V&gt; and the decoded token is: IntellectualThe sentinel is &lt;U&gt; and the decoded token is: traditiThe sentinel is &lt;T&gt; and the decoded token is: disguiseThe sentinel is &lt;S&gt; and the decoded token is: exerceThe sentinel is &lt;R&gt; and the decoded token is: nourisheThe sentinel is &lt;Q&gt; and the decoded token is: predominantThe sentinel is &lt;P&gt; and the decoded token is: amitiéThe sentinel is &lt;O&gt; and the decoded token is: erkenntThe sentinel is &lt;N&gt; and the decoded token is: dimensionThe sentinel is &lt;M&gt; and the decoded token is: inférieurThe sentinel is &lt;L&gt; and the decoded token is: refugiThe sentinel is &lt;K&gt; and the decoded token is: cheddarThe sentinel is &lt;J&gt; and the decoded token is: unterliegThe sentinel is &lt;I&gt; and the decoded token is: garanteazThe sentinel is &lt;H&gt; and the decoded token is: făcuteThe sentinel is &lt;G&gt; and the decoded token is: réglageThe sentinel is &lt;F&gt; and the decoded token is: pedepseThe sentinel is &lt;E&gt; and the decoded token is: GermainThe sentinel is &lt;D&gt; and the decoded token is: distinctlyThe sentinel is &lt;C&gt; and the decoded token is: SchraubThe sentinel is &lt;B&gt; and the decoded token is: emanatThe sentinel is &lt;A&gt; and the decoded token is: trimestreThe sentinel is &lt;z&gt; and the decoded token is: disrespectThe sentinel is &lt;y&gt; and the decoded token is: ErasmusThe sentinel is &lt;x&gt; and the decoded token is: AustraliaThe sentinel is &lt;w&gt; and the decoded token is: permeabilThe sentinel is &lt;v&gt; and the decoded token is: deseoriThe sentinel is &lt;u&gt; and the decoded token is: manipulatedThe sentinel is &lt;t&gt; and the decoded token is: suggérThe sentinel is &lt;s&gt; and the decoded token is: corespundThe sentinel is &lt;r&gt; and the decoded token is: nitroThe sentinel is &lt;q&gt; and the decoded token is: oyonsThe sentinel is &lt;p&gt; and the decoded token is: AccountThe sentinel is &lt;o&gt; and the decoded token is: échéanThe sentinel is &lt;n&gt; and the decoded token is: launderingThe sentinel is &lt;m&gt; and the decoded token is: genealogyThe sentinel is &lt;l&gt; and the decoded token is: QuickBooksThe sentinel is &lt;k&gt; and the decoded token is: constitutedThe sentinel is &lt;j&gt; and the decoded token is: FertigungThe sentinel is &lt;i&gt; and the decoded token is: goutteThe sentinel is &lt;h&gt; and the decoded token is: regulăThe sentinel is &lt;g&gt; and the decoded token is: overwhelminglyThe sentinel is &lt;f&gt; and the decoded token is: émergThe sentinel is &lt;e&gt; and the decoded token is: broyeurThe sentinel is &lt;d&gt; and the decoded token is: poveștiThe sentinel is &lt;c&gt; and the decoded token is: emulatorThe sentinel is &lt;b&gt; and the decoded token is: halloweenThe sentinel is &lt;a&gt; and the decoded token is: combustibil</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretty_decode</span><span class="params">(encoded_str_list, sentinels=sentinels)</span>:</span></span><br><span class="line">    <span class="comment"># If already a string, just do the replacements.</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(encoded_str_list, (str, bytes)):</span><br><span class="line">        <span class="keyword">for</span> token, char <span class="keyword">in</span> sentinels.items():</span><br><span class="line">            encoded_str_list = encoded_str_list.replace(token, char)</span><br><span class="line">        <span class="keyword">return</span> encoded_str_list</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># We need to decode and then prettyfy it.</span></span><br><span class="line">    <span class="keyword">return</span> pretty_decode(detokenize(encoded_str_list))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretty_decode(<span class="string">"I want to dress up as an Intellectual this halloween."</span>)</span><br></pre></td></tr></table></figure><pre><code>&#39;I want to dress up as an &lt;V&gt; this &lt;b&gt;.&#39;</code></pre><p>The functions above make your <code>inputs</code> and <code>targets</code> more readable. For example, you might see something like this once you implement the masking function below. </p><ul><li><span style="color:red"> Input sentence: </span> Younes and Lukasz were working together in the lab yesterday after lunch. </li><li><span style="color:red">Input: </span> Younes and Lukasz  <strong>Z</strong> together in the <strong>Y</strong> yesterday after lunch.</li><li><span style="color:red">Target: </span> <strong>Z</strong> were working <strong>Y</strong> lab.</li></ul><p><a name="1.3"></a></p><h3 id="1-3-Tokenizing-and-Masking"><a href="#1-3-Tokenizing-and-Masking" class="headerlink" title="1.3 Tokenizing and Masking"></a>1.3 Tokenizing and Masking</h3><p>You will now implement the <code>tokenize_and_mask</code> function. This function  will allow you to tokenize and mask input words with a noise probability. We usually mask 15% of the words.</p><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: tokenize_and_mask</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_mask</span><span class="params">(text, vocab_size=vocab_size, noise=<span class="number">0.15</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                      randomizer=np.random.uniform, tokenize=tokenize)</span>:</span></span><br><span class="line">    <span class="string">"""Tokenizes and masks a given input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        text (str or bytes): Text input.</span></span><br><span class="line"><span class="string">        vocab_size (int, optional): Size of the vocabulary. Defaults to vocab_size.</span></span><br><span class="line"><span class="string">        noise (float, optional): Probability of masking a token. Defaults to 0.15.</span></span><br><span class="line"><span class="string">        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.</span></span><br><span class="line"><span class="string">        tokenize (function, optional): Tokenizer function. Defaults to tokenize.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tuple: Tuple of lists of integers associated to inputs and targets.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current sentinel number (starts at 0)</span></span><br><span class="line">    cur_sentinel_num = <span class="number">0</span></span><br><span class="line">    <span class="comment"># inputs</span></span><br><span class="line">    inps = []</span><br><span class="line">    <span class="comment"># targets</span></span><br><span class="line">    targs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># prev_no_mask is True if the previous token was NOT masked, False otherwise</span></span><br><span class="line">    <span class="comment"># set prev_no_mask to True</span></span><br><span class="line">    prev_no_mask = <span class="keyword">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop through tokenized `text`</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokenize(text):</span><br><span class="line">        <span class="comment"># check if the `noise` is greater than a random value (weighted coin flip)</span></span><br><span class="line">        <span class="keyword">if</span> randomizer() &lt; noise:</span><br><span class="line">            <span class="comment"># check to see if the previous token was not masked</span></span><br><span class="line">            <span class="keyword">if</span> prev_no_mask==<span class="keyword">True</span>: <span class="comment"># add new masked token at end_id</span></span><br><span class="line">                <span class="comment"># number of masked tokens increases by 1</span></span><br><span class="line">                cur_sentinel_num += <span class="number">1</span></span><br><span class="line">                <span class="comment"># compute `end_id` by subtracting current sentinel value out of the total vocabulary size</span></span><br><span class="line">                end_id = vocab_size - cur_sentinel_num</span><br><span class="line">                <span class="comment"># append `end_id` at the end of the targets</span></span><br><span class="line">                targs.append(end_id)</span><br><span class="line">                <span class="comment"># append `end_id` at the end of the inputs</span></span><br><span class="line">                inps.append(end_id)</span><br><span class="line">            <span class="comment"># append `token` at the end of the targets</span></span><br><span class="line">            targs.append(token)</span><br><span class="line">            <span class="comment"># set prev_no_mask accordingly</span></span><br><span class="line">            prev_no_mask = <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># don't have two masked tokens in a row</span></span><br><span class="line">            <span class="comment"># append `token ` at the end of the inputs</span></span><br><span class="line">            inps.append(token)</span><br><span class="line">            <span class="comment"># set prev_no_mask accordingly</span></span><br><span class="line">            prev_no_mask = <span class="keyword">True</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> inps, targs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Some logic to mock a np.random value generator</span></span><br><span class="line"><span class="comment"># Needs to be in the same cell for it to always generate same output</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testing_rnd</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dummy_generator</span><span class="params">()</span>:</span></span><br><span class="line">        vals = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">        cyclic_vals = itertools.cycle(vals)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            <span class="keyword">yield</span> next(cyclic_vals)</span><br><span class="line"></span><br><span class="line">    dumr = itertools.cycle(dummy_generator())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dummy_randomizer</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> next(dumr)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy_randomizer</span><br><span class="line"></span><br><span class="line">input_str = natural_language_texts[<span class="number">0</span>]</span><br><span class="line">print(<span class="string">f"input string:\n\n<span class="subst">&#123;input_str&#125;</span>\n"</span>)</span><br><span class="line">inps, targs = tokenize_and_mask(input_str, randomizer=testing_rnd())</span><br><span class="line">print(<span class="string">f"tokenized inputs:\n\n<span class="subst">&#123;inps&#125;</span>\n"</span>)</span><br><span class="line">print(<span class="string">f"targets:\n\n<span class="subst">&#123;targs&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>input string:b&#39;Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.&#39;tokenized inputs:[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5, 216, 31993, 2119, 3, 9, 19529, 593, 853, 21, 921, 31992, 12, 129, 394, 28, 70, 17712, 1098, 5, 31991, 3884, 25, 762, 25, 174, 12, 214, 12, 31990, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 31989, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 31988, 6, 303, 24190, 11, 1472, 251, 5, 37, 31987, 36, 16, 8, 853, 19, 25264, 399, 568, 31986, 21, 21380, 7, 34, 19, 339, 5, 15746, 31985, 8, 583, 56, 36, 893, 3, 9, 3, 31984, 9486, 42, 3, 9, 1409, 29, 11, 25, 31983, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 31982]targets:[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 31993, 56, 36, 31992, 113, 2746, 31991, 216, 56, 31990, 5978, 16, 31989, 379, 2097, 31988, 11, 27856, 31987, 583, 12, 31986, 6, 11, 31985, 26, 16, 31984, 17, 18, 31983, 56, 36, 31982, 5]</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">b'Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'</span><br><span class="line"></span><br><span class="line">tokenized inputs:</span><br><span class="line"></span><br><span class="line">[<span class="number">31999</span>, <span class="number">15068</span>, <span class="number">4501</span>, <span class="number">3</span>, <span class="number">12297</span>, <span class="number">3399</span>, <span class="number">16</span>, <span class="number">5964</span>, <span class="number">7115</span>, <span class="number">31998</span>, <span class="number">531</span>, <span class="number">25</span>, <span class="number">241</span>, <span class="number">12</span>, <span class="number">129</span>, <span class="number">394</span>, <span class="number">44</span>, <span class="number">492</span>, <span class="number">31997</span>, <span class="number">58</span>, <span class="number">148</span>, <span class="number">56</span>, <span class="number">43</span>, <span class="number">8</span>, <span class="number">1004</span>, <span class="number">6</span>, <span class="number">474</span>, <span class="number">31996</span>, <span class="number">39</span>, <span class="number">4793</span>, <span class="number">230</span>, <span class="number">5</span>, <span class="number">2721</span>, <span class="number">6</span>, <span class="number">1600</span>, <span class="number">1630</span>, <span class="number">31995</span>, <span class="number">1150</span>, <span class="number">4501</span>, <span class="number">15068</span>, <span class="number">16127</span>, <span class="number">6</span>, <span class="number">9137</span>, <span class="number">2659</span>, <span class="number">5595</span>, <span class="number">31994</span>, <span class="number">782</span>, <span class="number">3624</span>, <span class="number">14627</span>, <span class="number">15</span>, <span class="number">12612</span>, <span class="number">277</span>, <span class="number">5</span>, <span class="number">216</span>, <span class="number">31993</span>, <span class="number">2119</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">19529</span>, <span class="number">593</span>, <span class="number">853</span>, <span class="number">21</span>, <span class="number">921</span>, <span class="number">31992</span>, <span class="number">12</span>, <span class="number">129</span>, <span class="number">394</span>, <span class="number">28</span>, <span class="number">70</span>, <span class="number">17712</span>, <span class="number">1098</span>, <span class="number">5</span>, <span class="number">31991</span>, <span class="number">3884</span>, <span class="number">25</span>, <span class="number">762</span>, <span class="number">25</span>, <span class="number">174</span>, <span class="number">12</span>, <span class="number">214</span>, <span class="number">12</span>, <span class="number">31990</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">23405</span>, <span class="number">4547</span>, <span class="number">15068</span>, <span class="number">2259</span>, <span class="number">6</span>, <span class="number">31989</span>, <span class="number">6</span>, <span class="number">5459</span>, <span class="number">6</span>, <span class="number">13618</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">3604</span>, <span class="number">1801</span>, <span class="number">31988</span>, <span class="number">6</span>, <span class="number">303</span>, <span class="number">24190</span>, <span class="number">11</span>, <span class="number">1472</span>, <span class="number">251</span>, <span class="number">5</span>, <span class="number">37</span>, <span class="number">31987</span>, <span class="number">36</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">853</span>, <span class="number">19</span>, <span class="number">25264</span>, <span class="number">399</span>, <span class="number">568</span>, <span class="number">31986</span>, <span class="number">21</span>, <span class="number">21380</span>, <span class="number">7</span>, <span class="number">34</span>, <span class="number">19</span>, <span class="number">339</span>, <span class="number">5</span>, <span class="number">15746</span>, <span class="number">31985</span>, <span class="number">8</span>, <span class="number">583</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">893</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">31984</span>, <span class="number">9486</span>, <span class="number">42</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">1409</span>, <span class="number">29</span>, <span class="number">11</span>, <span class="number">25</span>, <span class="number">31983</span>, <span class="number">12246</span>, <span class="number">5977</span>, <span class="number">13</span>, <span class="number">284</span>, <span class="number">3604</span>, <span class="number">24</span>, <span class="number">19</span>, <span class="number">2657</span>, <span class="number">31982</span>]</span><br><span class="line"></span><br><span class="line">targets:</span><br><span class="line"></span><br><span class="line">[<span class="number">31999</span>, <span class="number">12847</span>, <span class="number">277</span>, <span class="number">31998</span>, <span class="number">9</span>, <span class="number">55</span>, <span class="number">31997</span>, <span class="number">3326</span>, <span class="number">15068</span>, <span class="number">31996</span>, <span class="number">48</span>, <span class="number">30</span>, <span class="number">31995</span>, <span class="number">727</span>, <span class="number">1715</span>, <span class="number">31994</span>, <span class="number">45</span>, <span class="number">301</span>, <span class="number">31993</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">31992</span>, <span class="number">113</span>, <span class="number">2746</span>, <span class="number">31991</span>, <span class="number">216</span>, <span class="number">56</span>, <span class="number">31990</span>, <span class="number">5978</span>, <span class="number">16</span>, <span class="number">31989</span>, <span class="number">379</span>, <span class="number">2097</span>, <span class="number">31988</span>, <span class="number">11</span>, <span class="number">27856</span>, <span class="number">31987</span>, <span class="number">583</span>, <span class="number">12</span>, <span class="number">31986</span>, <span class="number">6</span>, <span class="number">11</span>, <span class="number">31985</span>, <span class="number">26</span>, <span class="number">16</span>, <span class="number">31984</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">31983</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">31982</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure><p>You will now use the inputs and the targets from the <code>tokenize_and_mask</code> function you implemented above. Take a look at the masked sentence using your <code>inps</code> and <code>targs</code> from the sentence above. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Inputs: \n\n'</span>, pretty_decode(inps))</span><br><span class="line">print(<span class="string">'\nTargets: \n\n'</span>, pretty_decode(targs))</span><br></pre></td></tr></table></figure><pre><code>Inputs:  &lt;Z&gt; BBQ Class Taking Place in Missoul &lt;Y&gt; Do you want to get better at making &lt;X&gt;? You will have the opportunity, put &lt;W&gt; your calendar now. Thursday, September 22 &lt;V&gt; World Class BBQ Champion, Tony Balay &lt;U&gt;onestar Smoke Rangers. He &lt;T&gt; teaching a beginner level class for everyone&lt;S&gt; to get better with their culinary skills.&lt;R&gt; teach you everything you need to know to &lt;Q&gt; a KCBS BBQ competition,&lt;P&gt;, recipes, timelines, meat selection &lt;O&gt;, plus smoker and fire information. The&lt;N&gt; be in the class is $35 per person &lt;M&gt; for spectators it is free. Include &lt;L&gt; the cost will be either a  &lt;K&gt;shirt or apron and you &lt;J&gt; tasting samples of each meat that is prepared &lt;I&gt;Targets:  &lt;Z&gt; Beginners &lt;Y&gt;a! &lt;X&gt; delicious BBQ &lt;W&gt; this on &lt;V&gt;nd join &lt;U&gt; from L &lt;T&gt; will be&lt;S&gt; who wants&lt;R&gt; He will &lt;Q&gt; compete in&lt;P&gt; including techniques &lt;O&gt; and trimming&lt;N&gt; cost to &lt;M&gt;, and &lt;L&gt;d in &lt;K&gt;t- &lt;J&gt; will be &lt;I&gt;.</code></pre><p><a name="1.4"></a></p><h3 id="1-4-Creating-the-Pairs"><a href="#1-4-Creating-the-Pairs" class="headerlink" title="1.4 Creating the Pairs"></a>1.4 Creating the Pairs</h3><p>You will now create pairs using your dataset. You will iterate over your data and create (inp, targ) pairs using the functions that we have given you. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply tokenize_and_mask</span></span><br><span class="line">inputs_targets_pairs = [tokenize_and_mask(text) <span class="keyword">for</span> text <span class="keyword">in</span> natural_language_texts]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_input_target_pairs</span><span class="params">(inputs_targets_pairs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i, inp_tgt_pair <span class="keyword">in</span> enumerate(inputs_targets_pairs, <span class="number">1</span>):</span><br><span class="line">        inps, tgts = inp_tgt_pair</span><br><span class="line">        inps, tgts = pretty_decode(inps), pretty_decode(tgts)</span><br><span class="line">        print(<span class="string">f'[<span class="subst">&#123;i&#125;</span>]\n\n'</span></span><br><span class="line">              <span class="string">f'inputs:\n<span class="subst">&#123;wrapper.fill(text=inps)&#125;</span>\n\n'</span></span><br><span class="line">              <span class="string">f'targets:\n<span class="subst">&#123;wrapper.fill(text=tgts)&#125;</span>\n\n\n\n'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_input_target_pairs(inputs_targets_pairs)</span><br></pre></td></tr></table></figure><pre><code>[1]inputs:Beginners BBQ Class Taking &lt;Z&gt; in Missoul &lt;Y&gt;! Do you want to getbetter at making delicious &lt;X&gt;? You will have the opportunity, &lt;W&gt;this on &lt;V&gt; calendar now. Thursday &lt;U&gt; September 22 &lt;T&gt; join&lt;S&gt; ClassBBQ Champion, Tony Balay from Lonestar Smoke&lt;R&gt;ers &lt;Q&gt; He will beteaching a beginner&lt;P&gt; class &lt;O&gt; everyone who wants&lt;N&gt; get better withtheir &lt;M&gt; skills &lt;L&gt; He will teach &lt;K&gt; everything you need to know to&lt;J&gt; in a KCBS BBQ &lt;I&gt; techniques, recipes, timelines, meat&lt;H&gt; andtrimming, plus smoker and fire information. The cost to be&lt;G&gt; theclass is $35 &lt;F&gt; person, and&lt;E&gt; spectators it is free. Included in thecost will&lt;D&gt; either &lt;C&gt; t- &lt;B&gt; or apron and you will be tastingsamples &lt;A&gt; each meat that &lt;z&gt; prepared.targets:&lt;Z&gt; Place &lt;Y&gt;a &lt;X&gt; BBQ &lt;W&gt; put &lt;V&gt; your &lt;U&gt;, &lt;T&gt;nd&lt;S&gt; World&lt;R&gt; Rang&lt;Q&gt;.&lt;P&gt; level &lt;O&gt; for&lt;N&gt; to &lt;M&gt; culinary &lt;L&gt;. &lt;K&gt; you &lt;J&gt; compete &lt;I&gt;competition, including&lt;H&gt; selection&lt;G&gt; in &lt;F&gt; per&lt;E&gt; for&lt;D&gt; be&lt;C&gt;a&lt;B&gt;shirt &lt;A&gt; of &lt;z&gt; is[2]inputs:&lt;Z&gt; in &#39;Mac OS X &lt;Y&gt; (10 &lt;X&gt;7)&#39; started by axb &lt;W&gt;i87, Jan 20, 2012.I&#39;ve got &lt;V&gt;a 500g &lt;U&gt; drive &lt;T&gt; a 240gb SSD. When trying to restoreusing&lt;S&gt; utility i&#39;m given the error &quot;Not enough space on disk&lt;R&gt;____to restore &lt;Q&gt; But I shouldn&#39;t have to do that!!! Any ideas orwork&lt;P&gt;s before &lt;O&gt;ing to the above? Use Carbon Copy Cloner to copyone drive to the other. I&#39;&lt;N&gt; done &lt;M&gt; several times going from &lt;L&gt;Dto &lt;K&gt; SSD and I wound &lt;J&gt; a bootable SSD drive. One step you &lt;I&gt;remember not to skip is to use Disk Utility to partition the SSD asGUID partition scheme&lt;H&gt; doing the &lt;G&gt;ne. If it came Apple &lt;F&gt;itionScheme, even if&lt;E&gt; let&lt;D&gt;CC do the clone, the resulting drive&lt;C&gt; boot&lt;B&gt;. C &lt;A&gt; usually works &lt;z&gt; &quot;file mode&quot; and it can easily copy alarger drive (that&#39;s mostly empty &lt;y&gt; onto a smaller drive.&lt;x&gt; you&lt;w&gt;CCC to clone a drive you did&lt;v&gt; boot&lt;u&gt;, it can work &lt;t&gt; copy mode &lt;s&gt;destination&lt;r&gt; must be&lt;q&gt; size or larger than the drive youare&lt;p&gt;cloning from &lt;o&gt;if &lt;n&gt; recall &lt;m&gt;ve actually done this somehowon Disk Utility &lt;l&gt; times&lt;k&gt;booting from &lt;j&gt;a different drive (or eventhe dvd)&lt;i&gt; not running disk utility from the drive your clo&lt;h&gt;ing)and had it work just fine from larger to smaller bootable clo&lt;g&gt;.Definitely format the drive cloning to first &lt;f&gt; as bootable Appleetc.. Thanks for &lt;e&gt; this out. My only experience &lt;d&gt; DU to go largerto smaller was when &lt;c&gt; trying to make  &lt;b&gt; install stick and I wasunable to restore InstallESD &lt;a&gt;dmg to a 4 GB Théâtre ofKeep thereason that wouldn&#39;t fit isdürftig was slightly moreutti GB of data.targets:&lt;Z&gt; Discussion &lt;Y&gt; Lion &lt;X&gt;. &lt;W&gt;o &lt;V&gt;  &lt;U&gt;b internal &lt;T&gt; and&lt;S&gt;disk&lt;R&gt;  &lt;Q&gt;&quot;&lt;P&gt;around &lt;O&gt; resort&lt;N&gt;ve &lt;M&gt; this &lt;L&gt; larger HD &lt;K&gt;smaller &lt;J&gt; up with &lt;I&gt; have to&lt;H&gt; HFS+ before&lt;G&gt;clo &lt;F&gt; Part&lt;E&gt;you&lt;D&gt; C&lt;C&gt; won&#39;t be &lt;B&gt;able &lt;A&gt;CC &lt;z&gt; in &lt;y&gt;)&lt;x&gt; If&lt;w&gt; tell&lt;v&gt; NOT&lt;u&gt;from &lt;t&gt; in block &lt;s&gt; where the&lt;r&gt; drive&lt;q&gt; the same&lt;p&gt;  &lt;o&gt; ( &lt;n&gt; I&lt;m&gt;). I&#39; &lt;l&gt; several&lt;k&gt; ( &lt;j&gt; &lt;i&gt; so&lt;h&gt;n&lt;g&gt;ne &lt;f&gt;,&lt;e&gt;pointing &lt;d&gt;using &lt;c&gt; I was &lt;b&gt;a Lion &lt;a&gt;. Théâtre USB stick butKeep coursedürftigthereutti than 4[3]inputs:&lt;Z&gt;il plaid &lt;Y&gt;lycra &lt;X&gt; spandex shortall with metallic slinky&lt;W&gt;sets. Attache &lt;V&gt; metallic elastic belt with O &lt;U&gt;ring. Head &lt;T&gt;included. Great hip hop&lt;S&gt; jazz dance costume.&lt;R&gt; in the USA.targets:&lt;Z&gt; Fo &lt;Y&gt;  &lt;X&gt; and &lt;W&gt; in &lt;V&gt;d &lt;U&gt;- &lt;T&gt;band&lt;S&gt; or&lt;R&gt; Made[4]inputs:How many backlink &lt;Z&gt; per day for new site? Discussion &lt;Y&gt; &#39;Black &lt;X&gt;SEO&#39; started by Omoplata, Dec 3, 2010. 1) for a &lt;W&gt; created site,what&#39;s &lt;V&gt; max &lt;U&gt;links per day I should do to be safe? 2) how &lt;T&gt; doI have&lt;S&gt; let my site&lt;R&gt; before I can start making more blinks? I didabout 6000 forum profiles every 24 hours for 10 days for &lt;Q&gt; of mysites&lt;P&gt; had a brand new domain. There is &lt;O&gt; backlinks for every&lt;N&gt;these &lt;M&gt; profile so &lt;L&gt;s 18 000 backlinks every 24 hours and nothinghappened in terms of being penalized &lt;K&gt; sandboxed. This is now maybe3 months ago &lt;J&gt; the site &lt;I&gt; ranking on first page for&lt;H&gt;a lot&lt;G&gt; mytargeted keywords. build more you can in starting &lt;F&gt; do manualsubmission and not spammy&lt;E&gt; means manual +&lt;D&gt; to&lt;C&gt; post.. &lt;B&gt; after1 month you can &lt;A&gt; a &lt;z&gt; blast.. Wow, dude, you built 18k backlink&lt;y&gt; a day&lt;x&gt; a brand&lt;w&gt;? How quickly did&lt;v&gt; rank up? What kind ofcompetition/search&lt;u&gt; did &lt;t&gt; keywords have?targets:&lt;Z&gt;s &lt;Y&gt; in &lt;X&gt; Hat &lt;W&gt; newly &lt;V&gt; the &lt;U&gt; # back &lt;T&gt; long&lt;S&gt; to&lt;R&gt; age&lt;Q&gt; one&lt;P&gt; which &lt;O&gt; three&lt;N&gt; of &lt;M&gt; forum &lt;L&gt; that &lt;K&gt; or &lt;J&gt; and &lt;I&gt;is&lt;H&gt; &lt;G&gt; of &lt;F&gt; but&lt;E&gt; type&lt;D&gt; relevant&lt;C&gt; the &lt;B&gt; then &lt;A&gt; make &lt;z&gt;big &lt;y&gt;s&lt;x&gt; on&lt;w&gt; new site&lt;v&gt; you&lt;u&gt;es &lt;t&gt; those[5]inputs:The Denver Board of Education opened the 2017-18 school year with anupdate &lt;Z&gt; projects that include new construction &lt;Y&gt; upgrades, heatmitigation &lt;X&gt; quality learning environments. We &lt;W&gt; excited &lt;V&gt;Denver students will be the beneficiaries &lt;U&gt;a four year, $572 millionGeneral Oblig &lt;T&gt; Bond.&lt;S&gt; the passage of the bond, our constructionteam has worked to schedule&lt;R&gt; projects over &lt;Q&gt; four-year term&lt;P&gt;bond. Denver voters on Tuesday approved bond and mill funding &lt;O&gt;for&lt;N&gt; in Denver Public Schools, agreeing to invest $572 million inbond funding &lt;M&gt; build and improve schools and &lt;L&gt;6.6 million inoperating dollars to support proven initiatives, &lt;K&gt; as early &lt;J&gt;Denver voters say &lt;I&gt; to bond and mill levy funding&lt;H&gt; for&lt;G&gt;PSstudents and schools. Click to learn more about the details of thevoter-approved &lt;F&gt; measure. Denver voters&lt;E&gt;. 8 approved bond and millfunding&lt;D&gt; for DPS students and schools. Learn more about what’sincluded in the mill &lt;C&gt;y measure.targets:&lt;Z&gt; on &lt;Y&gt;, &lt;X&gt; and &lt;W&gt; are &lt;V&gt; that &lt;U&gt; of  &lt;T&gt;ation&lt;S&gt; Since&lt;R&gt; the&lt;Q&gt; the&lt;P&gt; of the &lt;O&gt; measures&lt;N&gt; students &lt;M&gt; to &lt;L&gt; $5 &lt;K&gt; such &lt;J&gt;literacy. &lt;I&gt; yes&lt;H&gt; support&lt;G&gt; D &lt;F&gt; bond&lt;E&gt; on Nov&lt;D&gt; measures&lt;C&gt;lev</code></pre><p><a name="2"></a></p><h1 id="Part-2-Transfomer"><a href="#Part-2-Transfomer" class="headerlink" title="Part 2: Transfomer"></a>Part 2: Transfomer</h1><p>We now load a Transformer model checkpoint that has been pre-trained using the above C4 dataset and decode from it. This will save you a lot of time rather than have to train your model yourself. Later in this notebook, we will show you how to fine-tune your model.</p><p><img src="fulltransformer.png" width="300" height="600"></p><p>Start by loading in the model. We copy the checkpoint to local dir for speed, otherwise initialization takes a very long time. Last week you implemented the decoder part for the transformer. Now you will implement the encoder part. Concretely you will implement the following. </p><p><img src="encoder.png" width="300" height="600"></p><p><a name="2.1"></a></p><h3 id="2-1-Transformer-Encoder"><a href="#2-1-Transformer-Encoder" class="headerlink" title="2.1 Transformer Encoder"></a>2.1 Transformer Encoder</h3><p>You will now implement the transformer encoder. Concretely you will implement two functions. The first function is <code>FeedForwardBlock</code>.</p><p><a name="2.1.1"></a></p><h4 id="2-1-1-The-Feedforward-Block"><a href="#2-1-1-The-Feedforward-Block" class="headerlink" title="2.1.1 The Feedforward Block"></a>2.1.1 The Feedforward Block</h4><p>The <code>FeedForwardBlock</code> function is an important one so you will start by implementing it. To do so, you need to return a list of the following: </p><ul><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm" target="_blank" rel="noopener"><code>tl.LayerNorm()</code></a> = layer normalization.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener"><code>tl.Dense(d_ff)</code></a> = fully connected layer.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu" target="_blank" rel="noopener"><code>activation</code></a> = activation relu, tanh, sigmoid etc. </li><li><code>dropout_middle</code> = we gave you this function (don’t worry about its implementation).</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener"><code>tl.Dense(d_model)</code></a> = fully connected layer with same dimension as the model.</li><li><code>dropout_final</code> = we gave you this function (don’t worry about its implementation).</li></ul><p>You can always take a look at <a href="https://trax-ml.readthedocs.io/en/latest/" target="_blank" rel="noopener">trax documentation</a> if needed.</p><p><strong>Instructions</strong>: Implement the feedforward part of the transformer. You will be returning a list. </p><p><a name="ex02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: FeedForwardBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FeedForwardBlock</span><span class="params">(d_model, d_ff, dropout, dropout_shared_axes, mode, activation)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a list of layers implementing a feed-forward block.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: int:  depth of embedding</span></span><br><span class="line"><span class="string">        d_ff: int: depth of feed-forward layer</span></span><br><span class="line"><span class="string">        dropout: float: dropout rate (how much to drop out)</span></span><br><span class="line"><span class="string">        dropout_shared_axes: list of integers, axes to share dropout mask</span></span><br><span class="line"><span class="string">        mode: str: 'train' or 'eval'</span></span><br><span class="line"><span class="string">        activation: the non-linearity in feed-forward layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A list of layers which maps vectors to vectors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    dropout_middle = tl.Dropout(rate=dropout,</span><br><span class="line">                                shared_axes=dropout_shared_axes, </span><br><span class="line">                                mode=mode)</span><br><span class="line">  </span><br><span class="line">    dropout_final = tl.Dropout(rate=dropout, </span><br><span class="line">                               shared_axes=dropout_shared_axes, </span><br><span class="line">                               mode=mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    ff_block = [ </span><br><span class="line">        <span class="comment"># trax Layer normalization </span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># trax Dense layer using `d_ff`</span></span><br><span class="line">        tl.Dense(d_ff),</span><br><span class="line">        <span class="comment"># activation() layer - you need to call (use parentheses) this func!</span></span><br><span class="line">        activation(),</span><br><span class="line">        <span class="comment"># dropout middle layer</span></span><br><span class="line">        dropout_middle,</span><br><span class="line">        <span class="comment"># trax Dense layer using `d_model`</span></span><br><span class="line">        tl.Dense(d_model),</span><br><span class="line">        <span class="comment"># dropout final layer</span></span><br><span class="line">        dropout_final,</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ff_block</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the block layout</span></span><br><span class="line">feed_forward_example = FeedForwardBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.8</span>, dropout_shared_axes=<span class="number">0</span>, mode = <span class="string">'train'</span>, activation = tl.Relu)</span><br><span class="line">print(feed_forward_example)</span><br></pre></td></tr></table></figure><pre><code>[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]</span><br></pre></td></tr></table></figure><p><a name="2.1.2"></a></p><h4 id="2-1-2-The-Encoder-Block"><a href="#2-1-2-The-Encoder-Block" class="headerlink" title="2.1.2 The Encoder Block"></a>2.1.2 The Encoder Block</h4><p>The encoder block will use the <code>FeedForwardBlock</code>. </p><p>You will have to build two residual connections. Inside the first residual connection you will have the <code>tl.layerNorm()</code>, <code>attention</code>, and <code>dropout_</code> layers. The second residual connection will have the <code>feed_forward</code>.  </p><p>You will also need to implement <code>feed_forward</code>, <code>attention</code> and <code>dropout_</code> blocks. </p><p>So far you haven’t seen the <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.Attention" target="_blank" rel="noopener"><code>tl.Attention()</code></a> and <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual" target="_blank" rel="noopener"><code>tl.Residual()</code></a> layers so you can check the docs by clicking on them.</p><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: EncoderBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EncoderBlock</span><span class="params">(d_model, d_ff, n_heads, dropout, dropout_shared_axes,</span></span></span><br><span class="line"><span class="function"><span class="params">                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns a list of layers that implements a Transformer encoder block.</span></span><br><span class="line"><span class="string">    The input to the layer is a pair, (activations, mask), where the mask was</span></span><br><span class="line"><span class="string">    created from the original source tokens to prevent attending to the padding</span></span><br><span class="line"><span class="string">    part of the input.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model (int): depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        dropout_shared_axes (int): axes on which to share dropout mask.</span></span><br><span class="line"><span class="string">        mode (str): 'train' or 'eval'.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string">        FeedForwardBlock (function): A function that returns the feed forward block.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: A list of layers that maps (activations, mask) to (activations, mask).</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Attention block</span></span><br><span class="line">    attention = tl.Attention( </span><br><span class="line">        <span class="comment"># Use dimension of the model</span></span><br><span class="line">        d_feature=d_model,</span><br><span class="line">        <span class="comment"># Set it equal to number of attention heads</span></span><br><span class="line">        n_heads=n_heads,</span><br><span class="line">        <span class="comment"># Set it equal `dropout`</span></span><br><span class="line">        dropout=dropout,</span><br><span class="line">        <span class="comment"># Set it equal `mode`</span></span><br><span class="line">        mode=mode</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Call the function `FeedForwardBlock` (implemented before) and pass in the parameters</span></span><br><span class="line">    feed_forward = FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes, mode, ff_activation)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Dropout block</span></span><br><span class="line">    dropout_ = tl.Dropout( </span><br><span class="line">        <span class="comment"># set it equal to `dropout`</span></span><br><span class="line">        rate=dropout,</span><br><span class="line">        <span class="comment"># set it equal to the axes on which to share dropout mask</span></span><br><span class="line">        shared_axes=dropout_shared_axes,</span><br><span class="line">        <span class="comment"># set it equal to `mode`</span></span><br><span class="line">        mode=mode</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    encoder_block = [ </span><br><span class="line">        <span class="comment"># add `Residual` layer</span></span><br><span class="line">        tl.Residual(</span><br><span class="line">            <span class="comment"># add norm layer</span></span><br><span class="line">            tl.LayerNorm(),</span><br><span class="line">            <span class="comment"># add attention</span></span><br><span class="line">            attention,</span><br><span class="line">            <span class="comment"># add dropout</span></span><br><span class="line">            dropout_,</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># add another `Residual` layer</span></span><br><span class="line">        tl.Residual(</span><br><span class="line">            <span class="comment"># add feed forward</span></span><br><span class="line">            feed_forward,</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> encoder_block</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the block layout</span></span><br><span class="line">encoder_example = EncoderBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, n_heads=<span class="number">6</span>, dropout=<span class="number">0.8</span>, dropout_shared_axes=<span class="number">0</span>, mode = <span class="string">'train'</span>, ff_activation=tl.Relu)</span><br><span class="line">print(encoder_example)</span><br></pre></td></tr></table></figure><pre><code>[Serial_in2_out2[  Branch_in2_out3[    None    Serial_in2_out2[      LayerNorm      Serial_in2_out2[        Dup_out2        Dup_out2        Serial_in4_out2[          Parallel_in3_out3[            Dense_512            Dense_512            Dense_512          ]          PureAttention_in4_out2          Dense_512        ]      ]      Dropout    ]  ]  Add_in2], Serial[  Branch_out2[    None    Serial[      LayerNorm      Dense_2048      Relu      Dropout      Dense_512      Dropout    ]  ]  Add_in2]]</code></pre><h4 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[Serial_in2_out2[</span><br><span class="line">  Branch_in2_out3[</span><br><span class="line">    None</span><br><span class="line">    Serial_in2_out2[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Serial_in2_out2[</span><br><span class="line">        Dup_out2</span><br><span class="line">        Dup_out2</span><br><span class="line">        Serial_in4_out2[</span><br><span class="line">          Parallel_in3_out3[</span><br><span class="line">            Dense_512</span><br><span class="line">            Dense_512</span><br><span class="line">            Dense_512</span><br><span class="line">          ]</span><br><span class="line">          PureAttention_in4_out2</span><br><span class="line">          Dense_512</span><br><span class="line">        ]</span><br><span class="line">      ]</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">], Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Dense_2048</span><br><span class="line">      Relu</span><br><span class="line">      Dropout</span><br><span class="line">      Dense_512</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">]]</span><br></pre></td></tr></table></figure><p><a name="2.1.3"></a></p><h3 id="2-1-3-The-Transformer-Encoder"><a href="#2-1-3-The-Transformer-Encoder" class="headerlink" title="2.1.3 The Transformer Encoder"></a>2.1.3 The Transformer Encoder</h3><p>Now that you have implemented the <code>EncoderBlock</code>, it is time to build the full encoder. BERT, or Bidirectional Encoder Representations from Transformers is one such encoder. </p><p>You will implement its core code in the function below by using the functions you have coded so far. </p><p>The model takes in many hyperparameters, such as the <code>vocab_size</code>, the number of classes, the dimension of your model, etc. You want to build a generic function that will take in many parameters, so you can use it later. At the end of the day, anyone can just load in an API and call transformer, but we think it is important to make sure you understand how it is built. Let’s get started. </p><p><strong>Instructions:</strong> For this encoder you will need a <code>positional_encoder</code> first (which is already provided) followed by <code>n_layers</code> encoder blocks, which are the same encoder blocks you previously built. Once you store the <code>n_layers</code> <code>EncoderBlock</code> in a list, you are going to encode a <code>Serial</code> layer with the following sublayers: </p><ul><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch" target="_blank" rel="noopener"><code>tl.Branch</code></a>: helps with the branching and has the following sublayers:<ul><li><code>positional_encoder</code>.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PaddingMask" target="_blank" rel="noopener"><code>tl.PaddingMask()</code></a>: layer that maps integer sequences to padding masks.</li></ul></li><li>Your list of <code>EncoderBlock</code>s</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select" target="_blank" rel="noopener"><code>tl.Select([0], n_in=2)</code></a>:  Copies, reorders, or deletes stack elements according to indices.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm" target="_blank" rel="noopener"><code>tl.LayerNorm()</code></a>.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean" target="_blank" rel="noopener"><code>tl.Mean()</code></a>: Mean along the first axis.</li><li><code>tl.Dense()</code> with n_units set to n_classes. </li><li><code>tl.LogSoftmax()</code>   </li></ul><p>Please refer to the <a href="https://trax-ml.readthedocs.io/en/latest/" target="_blank" rel="noopener">trax documentation</a> for further information. </p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TransformerEncoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TransformerEncoder</span><span class="params">(vocab_size=vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                       n_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       d_ff=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       n_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       dropout_shared_axes=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                       max_len=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       mode=<span class="string">'train'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       ff_activation=tl.Relu,</span></span></span><br><span class="line"><span class="function"><span class="params">                       EncoderBlock=EncoderBlock)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns a Transformer encoder model.</span></span><br><span class="line"><span class="string">    The input to the model is a tensor of tokens.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int): vocab size. Defaults to vocab_size.</span></span><br><span class="line"><span class="string">        n_classes (int): how many classes on output. Defaults to 10.</span></span><br><span class="line"><span class="string">        d_model (int): depth of embedding. Defaults to 512.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer. Defaults to 2048.</span></span><br><span class="line"><span class="string">        n_layers (int): number of encoder/decoder layers. Defaults to 6.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads. Defaults to 8.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out). Defaults to 0.1.</span></span><br><span class="line"><span class="string">        dropout_shared_axes (int): axes on which to share dropout mask. Defaults to None.</span></span><br><span class="line"><span class="string">        max_len (int): maximum symbol length for positional encoding. Defaults to 2048.</span></span><br><span class="line"><span class="string">        mode (str): 'train' or 'eval'. Defaults to 'train'.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer. Defaults to tl.Relu.</span></span><br><span class="line"><span class="string">        EncoderBlock (function): Returns the encoder block. Defaults to EncoderBlock.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: A Transformer model as a layer that maps</span></span><br><span class="line"><span class="string">        from a tensor of tokens to activations over a set of output classes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    positional_encoder = [</span><br><span class="line">        tl.Embedding(vocab_size, d_model),</span><br><span class="line">        tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode),</span><br><span class="line">        tl.PositionalEncoding(max_len=max_len)</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use the function `EncoderBlock` (implemented above) and pass in the parameters over `n_layers`</span></span><br><span class="line">    encoder_blocks = [EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,</span><br><span class="line">                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_layers)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Assemble and return the model.</span></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        <span class="comment"># Encode</span></span><br><span class="line">        tl.Branch(</span><br><span class="line">            <span class="comment"># Use `positional_encoder`</span></span><br><span class="line">            positional_encoder,</span><br><span class="line">            <span class="comment"># Use trax padding mask</span></span><br><span class="line">            tl.PaddingMask(),</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># Use `encoder_blocks`</span></span><br><span class="line">        encoder_blocks,</span><br><span class="line">        <span class="comment"># Use select layer</span></span><br><span class="line">        tl.Select([<span class="number">0</span>], n_in=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># Use trax layer normalization</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># Map to output categories.</span></span><br><span class="line">        <span class="comment"># Use trax mean. set axis to 1</span></span><br><span class="line">        tl.Mean(axis = <span class="number">1</span>),</span><br><span class="line">        <span class="comment"># Use trax Dense using `n_classes`</span></span><br><span class="line">        tl.Dense(n_classes),</span><br><span class="line">        <span class="comment"># Use trax log softmax</span></span><br><span class="line">        tl.LogSoftmax(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this cell to see the structure of your model</span></span><br><span class="line"><span class="comment"># Only 1 layer is used to keep the output readable</span></span><br><span class="line">TransformerEncoder(n_layers=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>Serial[  Branch_out2[    [Embedding_32000_512, Dropout, PositionalEncoding]    PaddingMask(0)  ]  Serial_in2_out2[    Branch_in2_out3[      None      Serial_in2_out2[        LayerNorm        Serial_in2_out2[          Dup_out2          Dup_out2          Serial_in4_out2[            Parallel_in3_out3[              Dense_512              Dense_512              Dense_512            ]            PureAttention_in4_out2            Dense_512          ]        ]        Dropout      ]    ]    Add_in2  ]  Serial[    Branch_out2[      None      Serial[        LayerNorm        Dense_2048        Relu        Dropout        Dense_512        Dropout      ]    ]    Add_in2  ]  Select[0]_in2  LayerNorm  Mean  Dense_10  LogSoftmax]</code></pre><h4 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    [Embedding_32000_512, Dropout, PositionalEncoding]</span><br><span class="line">    PaddingMask(<span class="number">0</span>)</span><br><span class="line">  ]</span><br><span class="line">  Serial_in2_out2[</span><br><span class="line">    Branch_in2_out3[</span><br><span class="line">      None</span><br><span class="line">      Serial_in2_out2[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Serial_in2_out2[</span><br><span class="line">          Dup_out2</span><br><span class="line">          Dup_out2</span><br><span class="line">          Serial_in4_out2[</span><br><span class="line">            Parallel_in3_out3[</span><br><span class="line">              Dense_512</span><br><span class="line">              Dense_512</span><br><span class="line">              Dense_512</span><br><span class="line">            ]</span><br><span class="line">            PureAttention_in4_out2</span><br><span class="line">            Dense_512</span><br><span class="line">          ]</span><br><span class="line">        ]</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Dense_2048</span><br><span class="line">        Relu</span><br><span class="line">        Dropout</span><br><span class="line">        Dense_512</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Select[<span class="number">0</span>]_in2</span><br><span class="line">  LayerNorm</span><br><span class="line">  Mean</span><br><span class="line">  Dense_10</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p><strong>NOTE Congratulations! You have completed all of the graded functions of this assignment.</strong> Since the rest of the assignment takes a lot of time and memory to run we are providing some extra ungraded labs for you to see this model in action.</p><p><strong>Keep it up!</strong></p><p>To see this model in action continue to the next 2 ungraded labs. <strong>We strongly recommend you to try the colab versions of them as they will yield a much smoother experience.</strong> The links to the colabs can be found within the ungraded labs or if you already know how to open files within colab here are some shortcuts (if not, head to the ungraded labs which contain some extra instructions):</p><p><a href="https://drive.google.com/file/d/1EHAbMnW6u-GqYWh5r3Z8uLbz4KNpKOAv/view?usp=sharing" target="_blank" rel="noopener">BERT Loss Model Colab</a></p><p><a href="https://drive.google.com/file/d/1c-8KJkTySRGqCx_JjwjvXuRBTNTqEE0N/view?usp=sharing" target="_blank" rel="noopener">T5 SQuAD Model Colab</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-3-Question-Answering&quot;&gt;&lt;a href=&quot;#Assignment-3-Question-Answering&quot; class=&quot;headerlink&quot; title=&quot;Assignment 3: Question Answeri
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Transformer Summarizer</title>
    <link href="https://zhangruochi.com/Transformer-Summarizer/2020/09/27/"/>
    <id>https://zhangruochi.com/Transformer-Summarizer/2020/09/27/</id>
    <published>2020-09-27T07:23:13.000Z</published>
    <updated>2020-09-27T07:24:19.391Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Transformer-Summarizer"><a href="#Assignment-2-Transformer-Summarizer" class="headerlink" title="Assignment 2: Transformer Summarizer"></a>Assignment 2: Transformer Summarizer</h1><p>Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. </p><p><img src="transformerNews.png"></p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">Introduction</a></li><li><a href="#1">Part 1: Importing the dataset</a><ul><li><a href="#1.1">1.1 Encode &amp; Decode helper functions</a></li><li><a href="#1.2">1.2 Defining parameters</a></li><li><a href="#1.3">1.3 Exploring the data</a></li></ul></li><li><a href="#2">Part 2: Summarization with transformer</a><ul><li><a href="#2.1">2.1 Dot product attention</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li><li><a href="#2.2">2.2 Causal Attention</a><ul><li><a href="#ex02">Exercise 02</a></li></ul></li><li><a href="#2.3">2.3 Transformer decoder block</a><ul><li><a href="#ex03">Exercise 03</a></li></ul></li><li><a href="#2.4">2.4 Transformer Language model</a><ul><li><a href="#ex04">Exercise 04</a></li></ul></li></ul></li><li><a href="#3">Part 3: Training</a><ul><li><a href="#3.1">3.1 Training the model</a><ul><li><a href="#ex05">Exercise 05</a></li></ul></li></ul></li><li><a href="#4">Part 4: Evaluation</a><ul><li><a href="#4.1">4.1 Loading in a trained model</a></li></ul></li><li><a href="#5">Part 5: Testing with your own input</a> <ul><li><a href="#ex06">Exercise 6</a></li><li><a href="#5.1">5.1 Greedy decoding</a><ul><li><a href="#ex07">Exercise 07</a></li></ul></li></ul></li></ul><p><a name="0"></a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let’s get started, by completing this assignment you will learn to:  </p><ul><li>Use built-in functions to preprocess your data</li><li>Implement DotProductAttention</li><li>Implement Causal Attention</li><li>Understand how attention works</li><li>Build the transformer model</li><li>Evaluate your model</li><li>Summarize an article</li></ul><p>As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line">wrapper = textwrap.TextWrapper(width=<span class="number">70</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.fastmath <span class="keyword">import</span> numpy <span class="keyword">as</span> jnp</span><br><span class="line"></span><br><span class="line"><span class="comment"># to print the entire np array</span></span><br><span class="line">np.set_printoptions(threshold=sys.maxsize)</span><br></pre></td></tr></table></figure><p><a name="1"></a></p><h2 id="Part-1-Importing-the-dataset"><a href="#Part-1-Importing-the-dataset" class="headerlink" title="Part 1: Importing the dataset"></a>Part 1: Importing the dataset</h2><p>Trax makes it easy to work with Tensorflow’s datasets:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This will download the dataset if no data_dir is specified.</span></span><br><span class="line"><span class="comment"># Downloading and processing can take bit of time,</span></span><br><span class="line"><span class="comment"># so we have the data already in 'data/' for you</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Importing CNN/DailyMail articles dataset</span></span><br><span class="line">train_stream_fn = trax.data.TFDS(<span class="string">'cnn_dailymail'</span>,</span><br><span class="line">                                 data_dir=<span class="string">'data/'</span>,</span><br><span class="line">                                 keys=(<span class="string">'article'</span>, <span class="string">'highlights'</span>),</span><br><span class="line">                                 train=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This should be much faster as the data is downloaded already.</span></span><br><span class="line">eval_stream_fn = trax.data.TFDS(<span class="string">'cnn_dailymail'</span>,</span><br><span class="line">                                data_dir=<span class="string">'data/'</span>,</span><br><span class="line">                                keys=(<span class="string">'article'</span>, <span class="string">'highlights'</span>),</span><br><span class="line">                                train=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p><a name="1.1"></a></p><h2 id="1-1-Tokenize-amp-Detokenize-helper-functions"><a href="#1-1-Tokenize-amp-Detokenize-helper-functions" class="headerlink" title="1.1 Tokenize &amp; Detokenize helper functions"></a>1.1 Tokenize &amp; Detokenize helper functions</h2><p>Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your <a href="https://github.com/google/trax" target="_blank" rel="noopener">Trax</a> models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: </p><ul><li><span style="color:blue"> word2Ind: </span> a dictionary mapping the word to its index.</li><li><span style="color:blue"> ind2Word:</span> a dictionary mapping the index to its word.</li><li><span style="color:blue"> word2Count:</span> a dictionary mapping the word to the number of times it appears. </li><li><span style="color:blue"> num_words:</span> total number of words that have appeared. </li></ul><p>Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:</p><ul><li><span style="color:blue"> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.</li><li><span style="color:blue"> detokenize: </span> converts a token list to its corresponding sentence (i.e. string).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(input_str, EOS=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Input str to features dict, ready for inference"""</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Use the trax.data.tokenize method. It takes streams and returns streams,</span></span><br><span class="line">    <span class="comment"># we get around it by making a 1-element stream with `iter`.</span></span><br><span class="line">    inputs =  next(trax.data.tokenize(iter([input_str]),</span><br><span class="line">                                      vocab_dir=<span class="string">'vocab_dir/'</span>,</span><br><span class="line">                                      vocab_file=<span class="string">'summarize32k.subword.subwords'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Mark the end of the sentence with EOS</span></span><br><span class="line">    <span class="keyword">return</span> list(inputs) + [EOS]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span><span class="params">(integers)</span>:</span></span><br><span class="line">    <span class="string">"""List of ints to str"""</span></span><br><span class="line">  </span><br><span class="line">    s = trax.data.detokenize(integers,</span><br><span class="line">                             vocab_dir=<span class="string">'vocab_dir/'</span>,</span><br><span class="line">                             vocab_file=<span class="string">'summarize32k.subword.subwords'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> wrapper.fill(s)</span><br></pre></td></tr></table></figure><p><a name="1.2"></a></p><h2 id="1-2-Preprocessing-for-Language-Models-Concatenate-It"><a href="#1-2-Preprocessing-for-Language-Models-Concatenate-It" class="headerlink" title="1.2 Preprocessing for Language Models: Concatenate It!"></a>1.2 Preprocessing for Language Models: Concatenate It!</h2><p>This week you will use a language model — Transformer Decoder — to solve<br>an input-output problem. As you know, language models only predict the next<br>word, they have no notion of inputs. To create a single input suitable for<br>a language model, we concatenate inputs with targets putting a separator<br>in between. We also need to create a mask — with 0s at inputs and 1s at targets — so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Special tokens</span></span><br><span class="line">SEP = <span class="number">0</span> <span class="comment"># Padding or separator token</span></span><br><span class="line">EOS = <span class="number">1</span> <span class="comment"># End of sentence token</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenate tokenized inputs and targets using 0 as separator.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(stream)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> (article, summary) <span class="keyword">in</span> stream:</span><br><span class="line">        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])</span><br><span class="line">        mask = [<span class="number">0</span>] * (len(list(article)) + <span class="number">2</span>) + [<span class="number">1</span>] * (len(list(summary)) + <span class="number">1</span>) <span class="comment"># Accounting for EOS and SEP</span></span><br><span class="line">        <span class="keyword">yield</span> joint, joint, np.array(mask)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can combine a few data preprocessing steps into a pipeline like this.</span></span><br><span class="line">input_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># Tokenizes</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=<span class="string">'vocab_dir/'</span>,</span><br><span class="line">                       vocab_file=<span class="string">'summarize32k.subword.subwords'</span>),</span><br><span class="line">    <span class="comment"># Uses function defined above</span></span><br><span class="line">    preprocess,</span><br><span class="line">    <span class="comment"># Filters out examples longer than 2048</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply preprocessing to data streams.</span></span><br><span class="line">train_stream = input_pipeline(train_stream_fn())</span><br><span class="line">eval_stream = input_pipeline(eval_stream_fn())</span><br><span class="line"></span><br><span class="line">train_input, train_target, train_mask = next(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> sum((train_input - train_target)**<span class="number">2</span>) == <span class="number">0</span>  <span class="comment"># They are the same in Language Model (LM).</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prints mask, 0s on article, 1s on summary</span></span><br><span class="line">print(<span class="string">f'Single example mask:\n\n <span class="subst">&#123;train_mask&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>Single example mask: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prints: [Example][&lt;EOS&gt;][&lt;pad&gt;][Example Summary][&lt;EOS&gt;]</span></span><br><span class="line">print(<span class="string">f'Single example:\n\n <span class="subst">&#123;detokenize(train_input)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>Single example: By . Margot Peppers . Nigerian and Cameroonian pop star Dencia has hitout at Lupita Nyong&#39;o for her new contract with Lancome, accusing herof bowing to &#39;white people companies&#39;. In an angry tweet directed atthe 12 Years A Slave star, she wrote: &#39;Oh @Lupita_Nyongo cln&#39;t talkabt the bleaching creams white people (Companies) make cuz the whiteman pays her, they own her!! [sic]&#39;. The comment comes just a monthafter Miss Nyong&#39;o mentioned Dencia - who has been accused ofmarketing her own brand of skin-bleaching cream called Whitenicious -in a speech about learning to value the color of her own skin. Scrolldown for video . Butting heads: Nigerian and Cameroonian pop starDencia has hit out at Lupita Nyong&#39;o for her new contract withLancome, accusing her of bowing to &#39;white people companies&#39; Fightingwords: In a tweet directed at the 12 Years A Slave star, she wrote:&#39;Oh @Lupita_Nyongo cln&#39;t talk abt the bleaching creams white people(Companies) make cuz the white man pays her, they own her!! [sic]&#39; Thepop star is no stranger to . controversy; in a February interview withEbony, she all but admitted . that Whitenicious is intended as a skin-lightener, not as a cure for . dark spots as it claims. &#39;When . youtake that picture and you put a picture of Dencia darker, this is .what you&#39;re telling people - the product really works,&#39; she said. &#39;Andguess what? People really want to buy it. It&#39;s what it is. I don&#39;treally care.&#39; Given her defiant and hypocritical attitude, it&#39;s nosurprise the fiery singer was angered when Miss Nyong&#39;o called her outin a speech at Essence&#39;s Black Women in Hollywood event on February27. Influential: In a recent speech, Miss Nyong&#39;o read out loud aletter from a fan who said she decided not to buy Dencia&#39;s skin-whitening cream Whitenicious because the actress had inspired her tolove her own skin . On-screen: Miss Nyong&#39;o won an Oscar for BestSupporting Actress for her role in 2013 film 12 Years A Slave . In hertalk, the 30-year-old opened up about how conventional standards ofbeauty once affected her self-esteem, reading aloud a letter writtento her by a young girl who viewed her as a role model. &#39;Dear Lupita,&#39;reads the letter. &#39;I think you&#39;re really lucky to be this black butyet this successful in Hollywood overnight. I was just about to buyDencia&#39;s Whitenicious cream to lighten my skin when you appeared onthe world map and saved me.&#39; &#39;My heart bled a little when I read thosewords,&#39; the actress said through tears, explaining how as a child,she, too, would pray that she&#39;d one day wake up with lighter skin.Hypocritical: Dencia is no stranger to controversy; in a Februaryinterview with Ebony, she essentially admitted that Whitenicious isintended as a skin-lightener, not as a cure for dark spots as itclaims . Perpetuating the problem: &#39;When you take that picture and youput a picture of Dencia darker, this is what you&#39;re telling people -the product really works,&#39; she said. &#39;And guess what? People reallywant to buy it&#39; But while the actress saw the letter as a source ofinspiration, Dencia took it as a personal attack. After her angrytweet at Miss Nyong&#39;o, criticism poured in, with one person tweeting:&#39;B**** lupita is the new face of Lancôme!! SHE WINS!! And you&#39;re justTRASH [sic]&#39;. In her response, Dencia said of the cosmetics company:&#39;But they sell bleaching cream tho [sic]&#39;. The pop star is likelyreferring to Lancome&#39;s Blanc Expert range of cosmetics, which areactually advertised as &#39;brighteners&#39; that &#39;regulate melanin productionand awaken the luminosity of the skin&#39;. And as far as Dencia&#39;s claimthat Lancome is a &#39;white people company&#39;, a quick perusal of thewebsite reveals that it has a number of concealers and foundations indarker skin tones.&lt;EOS&gt;&lt;pad&gt;Dencia&#39;s comment is hypocriticalconsidering she recently courted controversy for marketing &#39;dark spotremover&#39; Whitenicious, which is frequently used as a skin-whiteningcream .&lt;EOS&gt;</code></pre><p><a name="1.3"></a></p><h2 id="1-3-Batching-with-bucketing"><a href="#1-3-Batching-with-bucketing" class="headerlink" title="1.3 Batching with bucketing"></a>1.3 Batching with bucketing</h2><p>As in the previous week, we use bucketing to create batches of data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bucketing to create batched generators.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Buckets are defined in terms of boundaries and batch sizes.</span></span><br><span class="line"><span class="comment"># Batch_sizes[i] determines the batch size for items with length &lt; boundaries[i]</span></span><br><span class="line"><span class="comment"># So below, we'll take a batch of 16 sentences of length &lt; 128 , 8 of length &lt; 256,</span></span><br><span class="line"><span class="comment"># 4 of length &lt; 512. And so on. </span></span><br><span class="line">boundaries =  [<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>]</span><br><span class="line">batch_sizes = [<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,    <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the streams.</span></span><br><span class="line">train_batch_stream = trax.data.BucketByLength(</span><br><span class="line">    boundaries, batch_sizes)(train_stream)</span><br><span class="line"></span><br><span class="line">eval_batch_stream = trax.data.BucketByLength(</span><br><span class="line">    boundaries, batch_sizes)(eval_stream)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Every execution will result in generation of a different article</span></span><br><span class="line"><span class="comment"># Try running this cell multiple times to see how the length of the examples affects the batch size</span></span><br><span class="line">input_batch, _, mask_batch = next(train_batch_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Shape of the input_batch</span></span><br><span class="line">input_batch.shape</span><br></pre></td></tr></table></figure><pre><code>(2, 1024)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print corresponding integer values</span></span><br><span class="line">print(input_batch[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>[   27  1091    23    46  3873  1248 16013   256 11599 23297   102    68 24308     7     5  1037  1958   320  1477   105  2557   186  4133    28 18175  1348  1287     3  4927  7577    28  8478 10120 19134  7951   364  7317  4990    79     2   393     2   186  8962  2995  9813  4476  3632  2270     5     2   705     2   721 10731    16   186 17136    16   193    54   102    41  1459   320    31 16946    47     2   119  3770   278   355    28   622   263    78  2613     3   312  4543     4  8662  3788  3632  2270     5     6  3048 23524     2  1210     2  1958   320  2033   105    61     2 19134  7951   364  7317  4990    79 24810    17   213  1091     2   931   320   213 16946    47   415 20579 20964    58  1782   863   213  7726     2   213  7599  3938  4133    28 26719     4   752  1480  2868   132    68   583  3898 20579 20964    58   240   197     3  4531  9531  2959   127   132    28 27439  9275  1628  1602     3  8406  5364    11  4927  7577    28  8478 10120 19134  7951   364  7317  4990    79     2   393     2   497     2   186    68 24308  8962  2995  9813  4476  3632  2270     5     2   705     2   721 10731    16   186 17136    16   193    54   809    31   278    78  2613  7511    15  1037 20274    21   379 21549  7150    11  9813  4476  3632  2270     5    80 18649  1496   667   213 17136    45    78    15   882  1838   213  2439  7883   379    27  1147     6   104     6   292   966    43 11850   213  1621     2   931   320   213 13021     4     2    35    22   206    19  5632   213  1018   111   213  2948   186   213 25931     4     3  2713  7801   320    28  6105    32   922  1838   213  6350   141   102 24114    75    78  2613   186  7511    41  2362     2    41   233  3632  2270     5     6  3048 23524  1955    78    28 11261  1797  1782   198    25    92  3787  3103   527 13747   320   213  7599     2   487   159   213   669 27884     4  1622 27872   391  5977  3103   527  2918   186  1472   320    18    46   810   132    28  2439  7726  3898   213 13021     4   127     3    34    31 18649  3347     2   148 19134  7951   364  7317  4990    79   186  9813  4476  3632  2270     5    18 17136    45    78    31  5369   186 19175     5     3     9  2789    25 11203     2   412    25   213   966   186    54  1697     3 12849    14    11  7317  4990    79 12365   146 24810    17   213  1091  4617 27439  9275  1628  4543     4  8662  3788  3632  2270     5     6  3048 23524     2   186   131  4133    28 26719     4  6901   809   213    60     6  1797  6350   809   213   414     8 12370    21    12   186   710   171   864  2362   809   213  1610   379     9 16946    47   415  3357 15581    81     7     5  1431  1890   163  4336  7188    20    78  3632  2270     5     6  3048 23524     7     5   661     2    35   646    25 17926 25290 16741    20  4140     2   213 13021     4   127     3 19134  7951   364  7317  4990    79  1353  3873  1248 11599 23297     2 17260  8041   893   213  5627   527    28   966     2  2439  1740  1524  7726   186 23638    16 24668 21273   204     2   931   320  1882     3  4531  9531  2959   127 19134  7951   364  7317  4990    79    43  9363     4   760    70    35    62    19  2851  2754   103  1353    70  1480 22646   272  7304   132    28  1501   809   213  1881  1610     3   305  1353   475   809   213 16946    47   415  7411    84    78   281  3997    88   226 20934     4     3  9813  4476  3632  2270     5  1353    43  3873  1248   966 17260  8041 16704   464   186  2439  1740  1524  7726   186  1233   320   213  1156 10835    78   281  1696    88   226 20934     4     3    27   924  3729    23    46  4648  1019  3112  1859   809 18235  5333  9141 25733   812    10     1     0  4927  7577    28  8478 10120 19134  7951   364  7317  4990    79     2   393     2   186  8962  2995  9813  4476  3632  2270     5     2   705     2   721  2557   102  4925  1838    28   622    78  2613  1859 16346 27439  6774  1628   312    15  1037     2  4543     4  8662  3788  3632  2270     5     6  3048 23524     2  1210     2  1958   320  1477   105     2 19134  7951   364  7317  4990    79 12365 24810    17    68 16346 27439  6774  1628   305  4133 26719     4  6901   186   864  2362   320   423    68  1955 16346 27439  6774  1628    27  1147     6   104     6   292  2635 11850   213  1621  2104     1     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0]</code></pre><p>Things to notice:</p><ul><li>First we see the corresponding values of the words.</li><li>The first 1, which represents the <code>&lt;EOS&gt;</code> tag of the article.</li><li>Followed by a 0, which represents a <code>&lt;pad&gt;</code> tag.</li><li>After the first 0 (<code>&lt;pad&gt;</code> tag) the corresponding values are of the words that are used for the summary of the article.</li><li>The second 1 represents the <code>&lt;EOS&gt;</code> tag for the summary.</li><li>All the trailing 0s represent <code>&lt;pad&gt;</code> tags which are appended to maintain consistent length (If you don’t see them then it would mean it is already of max length)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print the article and its summary</span></span><br><span class="line">print(<span class="string">'Article:\n\n'</span>, detokenize(input_batch[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>Article: A woman has been charged with reckless manslaughter after herboyfriend&#39;s mother tried to stop them fighting and suffered a fatalheart attack. Claudia Yanira Hernandez Soriano, 25, and Juan FranciscoMartinez Rojas, 28, started punching and scratching each other afterthey returned to their Bergen, New Jersey home following a party earlyon Monday. When Ana Angelina Rojas-Jovel, 45, tried to break them up,Hernandez Soriano assaulted the woman, according to the Bergen CountyProsecutor. &#39;During the assault, the victim apparently suffered acardiac event which resulted in her death,&#39; Prosecutor John L.Molinelli said in a statement. Fight: Claudia Yanira HernandezSoriano, 25, above, and her boyfriend Juan Francisco Martinez Rojas,28, started punching and scratching each other at their home on Mondaywhen his mother intervened . Injured: Martinez Rojas&#39; booking shotshows the scratches on his face from the domestic dispute . A seven-year-old child also witnessed the fight, according to the prosecutor,but he did not reveal the relationship between the adults and theyoungster. Police responded to a 911 call from the apartment justafter 4am on Monday and when they arrived, they found Rojas-Jovel deadon a bedroom floor. &#39;There were no obvious signs of trauma to thevictim, however... the [couple] displayed signs of injury and appearedto have been involved in a domestic assault,&#39; the prosecutor said. Intheir booking photos, both Hernandez Soriano and Martinez Rojas havescratches on their faces and necks. The pair were interviewed, as werethe child and other residents. Scene: Soriano allegedly then assaultedthe woman, Ana Angelina Rojas-Jovel, and she suffered a cardiac arrestat the first-floor apartment at the house (pictured) and died beforepolice arrived at the scene . The Bergen County Medical Examiner&#39;sOffice conducted an autopsy on Rojas-Jovel&#39;s body, but results werepending toxicology tests, the prosecutor said. Hernandez Soriano wascharged with manslaughter, endangering the welfare of a child,domestic violence simple assault and hindering apprehension, accordingto authorities. Molinelli said Hernandez Soriano also hid evidence -but would not detail what it was - which investigators later recoveredin a search at the crime scene. She was held at the Bergen County Jailon $250,000 bail. Martinez Rojas was also charged with childendangerment and domestic violence simple assault and sent to thecounty jail on $75,000 bail. A court hearing has been scheduled forThursday morning at Hackensack Superior Court.&lt;EOS&gt;&lt;pad&gt;ClaudiaYaniraHernandez Soriano, 25, and Juan Francisco Martinez Rojas, 28, startedfighting after returning from a party on Monday morning . When hismother, Ana Angelina Rojas-Jovel, 45, tried to stop them, HernandezSoriano allegedly assaulted her . She suffered cardiac arrest andpolice arrived to find her dead . A seven-year-old girl witnessed thefight .&lt;EOS&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;</code></pre><p>You can see that the data has the following structure:</p><ul><li><span style="color:blue"> [Article] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; <code>&lt;pad&gt;</code> -&gt; <span style="color:blue"> [Article Summary] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; (possibly) multiple <code>&lt;pad&gt;</code></li></ul><p>The loss is taken only on the summary using cross_entropy as loss function. </p><p><a name="2"></a></p><h1 id="Part-2-Summarization-with-transformer"><a href="#Part-2-Summarization-with-transformer" class="headerlink" title="Part 2: Summarization with transformer"></a>Part 2: Summarization with transformer</h1><p>Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps. </p><p>You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.</p><p><img src="transformer_decoder_zoomin.png"></p><p><a name="2.1"></a></p><h2 id="2-1-Dot-product-attention"><a href="#2-1-Dot-product-attention" class="headerlink" title="2.1 Dot product attention"></a>2.1 Dot product attention</h2><p>Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output. </p><p><img src="dotproduct.png"></p><p>Here are some helper functions that will help you create tensors and display useful information:</p><ul><li><code>create_tensor</code>  creates a <code>jax numpy array</code> from a list of lists.</li><li><code>display_tensor</code> prints out the shape and the actual tensor.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tensor</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="string">"""Create tensor from list of lists"""</span></span><br><span class="line">    <span class="keyword">return</span> jnp.array(t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_tensor</span><span class="params">(t, name)</span>:</span></span><br><span class="line">    <span class="string">"""Display shape and tensor"""</span></span><br><span class="line">    print(<span class="string">f'<span class="subst">&#123;name&#125;</span> shape: <span class="subst">&#123;t.shape&#125;</span>\n'</span>)</span><br><span class="line">    print(<span class="string">f'<span class="subst">&#123;t&#125;</span>\n'</span>)</span><br></pre></td></tr></table></figure><p>Before implementing it yourself, you can play around with a toy example of <code>dot product attention</code> without the softmax  operation. Technically it would not be <code>dot product attention</code> without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.</p><p>The formula for attention is this one:</p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\</script><p>$d_{k}$ stands for the dimension of queries and keys.</p><p>The <code>query</code>, <code>key</code>, <code>value</code> and <code>mask</code> vectors are provided for this example.</p><p>Notice that the masking is done using very negative values that will yield a similar effect to using $-\infty $. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">q = create_tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">display_tensor(q, <span class="string">'query'</span>)</span><br><span class="line">k = create_tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">display_tensor(k, <span class="string">'key'</span>)</span><br><span class="line">v = create_tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">display_tensor(v, <span class="string">'value'</span>)</span><br><span class="line">m = create_tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">-1e9</span>, <span class="number">0</span>]])</span><br><span class="line">display_tensor(m, <span class="string">'mask'</span>)</span><br></pre></td></tr></table></figure><pre><code>query shape: (2, 3)[[1 0 0] [0 1 0]]key shape: (2, 3)[[1 2 3] [4 5 6]]value shape: (2, 3)[[0 1 0] [1 0 1]]mask shape: (2, 2)[[ 0.e+00  0.e+00] [-1.e+09  0.e+00]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">query shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">key shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">value shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">mask shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">0.e+00</span>  <span class="number">0.e+00</span>]</span><br><span class="line"> [<span class="number">-1.e+09</span>  <span class="number">0.e+00</span>]]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q_dot_k = q @ k.T / jnp.sqrt(<span class="number">3</span>)</span><br><span class="line">display_tensor(q_dot_k, <span class="string">'query dot key'</span>)</span><br></pre></td></tr></table></figure><pre><code>query dot key shape: (2, 2)[[0.57735026 2.309401  ] [1.1547005  2.8867514 ]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">query dot key shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0.57735026</span> <span class="number">2.309401</span>  ]</span><br><span class="line"> [<span class="number">1.1547005</span>  <span class="number">2.8867514</span> ]]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">masked = q_dot_k + m</span><br><span class="line">display_tensor(masked, <span class="string">'masked query dot key'</span>)</span><br></pre></td></tr></table></figure><pre><code>masked query dot key shape: (2, 2)[[ 5.7735026e-01  2.3094010e+00] [-1.0000000e+09  2.8867514e+00]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">masked query dot key shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">5.7735026e-01</span>  <span class="number">2.3094010e+00</span>]</span><br><span class="line"> [<span class="number">-1.0000000e+09</span>  <span class="number">2.8867514e+00</span>]]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(masked @ v, <span class="string">'masked query dot key dot value'</span>)</span><br></pre></td></tr></table></figure><pre><code>masked query dot key dot value shape: (2, 3)[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00] [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">masked query dot key dot value shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">2.3094010e+00</span>  <span class="number">5.7735026e-01</span>  <span class="number">2.3094010e+00</span>]</span><br><span class="line"> [ <span class="number">2.8867514e+00</span> <span class="number">-1.0000000e+09</span>  <span class="number">2.8867514e+00</span>]]</span><br></pre></td></tr></table></figure></p><p>In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">q_with_batch = q[<span class="keyword">None</span>,:]</span><br><span class="line">display_tensor(q_with_batch, <span class="string">'query with batch dim'</span>)</span><br><span class="line">k_with_batch = k[<span class="keyword">None</span>,:]</span><br><span class="line">display_tensor(k_with_batch, <span class="string">'key with batch dim'</span>)</span><br><span class="line">v_with_batch = v[<span class="keyword">None</span>,:]</span><br><span class="line">display_tensor(v_with_batch, <span class="string">'value with batch dim'</span>)</span><br><span class="line">m_bool = create_tensor([[<span class="keyword">True</span>, <span class="keyword">True</span>], [<span class="keyword">False</span>, <span class="keyword">True</span>]])</span><br><span class="line">display_tensor(m_bool, <span class="string">'boolean mask'</span>)</span><br></pre></td></tr></table></figure><pre><code>query with batch dim shape: (1, 2, 3)[[[1 0 0]  [0 1 0]]]key with batch dim shape: (1, 2, 3)[[[1 2 3]  [4 5 6]]]value with batch dim shape: (1, 2, 3)[[[0 1 0]  [1 0 1]]]boolean mask shape: (2, 2)[[ True  True] [False  True]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">query with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">key with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]]</span><br><span class="line"></span><br><span class="line">value with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]]</span><br><span class="line"></span><br><span class="line">boolean mask shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ True  True]</span><br><span class="line"> [False  True]]</span><br></pre></td></tr></table></figure></p><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Implement the dot product attention. Concretely, implement the following equation</p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\</script><p>$Q$ - query,<br>$K$ - key,<br>$V$ - values,<br>$M$ - mask,<br>${d_k}$ - depth/dimension of the queries and keys (used for scaling down)</p><p>You can implement this formula either by <code>trax</code> numpy (trax.math.numpy) or regular <code>numpy</code> but it is recommended to use <code>jnp</code>.</p><p>Something to take into consideration is that within trax, the masks are tensors of <code>True/False</code> values not 0’s and $-\infty$ as in the previous example. Within the graded function don’t think of applying the mask by summing up matrices, instead use <code>jnp.where()</code> and treat the <strong>mask as a tensor of boolean values with <code>False</code> for values that need to be masked and True for the ones that don’t.</strong></p><p>Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as <code>@</code> for dot product or <code>.T</code> for transposing. Use <code>jnp.matmul()</code> and <code>jnp.swapaxes()</code> instead.</p><p>This is the self-attention block for the transformer decoder. Good luck!  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: DotProductAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DotProductAttention</span><span class="params">(query, key, value, mask)</span>:</span></span><br><span class="line">    <span class="string">"""Dot product self-attention.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)</span></span><br><span class="line"><span class="string">        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)</span></span><br><span class="line"><span class="string">        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k</span></span><br><span class="line"><span class="string">        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> query.shape[<span class="number">-1</span>] == key.shape[<span class="number">-1</span>] == value.shape[<span class="number">-1</span>], <span class="string">"Embedding dimensions of q, k, v aren't all the same"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># Save depth/dimension of the query embedding for scaling down the dot product</span></span><br><span class="line">    depth = query.shape[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate scaled query key dot product according to formula above</span></span><br><span class="line">    dots = jnp.matmul(query, jnp.swapaxes(key, <span class="number">1</span>, <span class="number">2</span>)) / jnp.sqrt(depth)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Apply the mask</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>: <span class="comment"># The 'None' in this line does not need to be replaced</span></span><br><span class="line">        dots = jnp.where(mask, dots, jnp.full_like(dots, <span class="number">-1e9</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Softmax formula implementation</span></span><br><span class="line">    <span class="comment"># Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers</span></span><br><span class="line">    <span class="comment"># Hint: Last axis should be used and keepdims should be True</span></span><br><span class="line">    <span class="comment"># Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)</span></span><br><span class="line">    logsumexp = trax.fastmath.logsumexp(dots,axis=<span class="number">-1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take exponential of dots minus logsumexp to get softmax</span></span><br><span class="line">    <span class="comment"># Use jnp.exp()</span></span><br><span class="line">    dots = jnp.exp( dots - logsumexp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Multiply dots by value to get self-attention</span></span><br><span class="line">    <span class="comment"># Use jnp.matmul()</span></span><br><span class="line">    attention = jnp.matmul(dots, value)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> attention</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)</span><br></pre></td></tr></table></figure><pre><code>DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],              [1.        , 0.        , 1.        ]]], dtype=float32)</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">DeviceArray([[[<span class="number">0.8496746</span> , <span class="number">0.15032545</span>, <span class="number">0.8496746</span> ],</span><br><span class="line">              [<span class="number">1.</span>        , <span class="number">0.</span>        , <span class="number">1.</span>        ]]], dtype=float32)</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">&lt;a name='2.2'&gt;&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">## <span class="number">2.2</span> Causal Attention</span><br><span class="line"></span><br><span class="line">Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. </span><br><span class="line"></span><br><span class="line">&lt;img src = <span class="string">"causal.png"</span>&gt;</span><br><span class="line"></span><br><span class="line">In the image above, a word can see everything that is before it, but <span class="keyword">not</span> what is after it. To implement causal attention, you will have to transform vectors <span class="keyword">and</span> <span class="keyword">do</span> many reshapes. You will need to implement the functions below.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;a name='ex02'&gt;&lt;/a&gt;</span><br><span class="line">### Exercise <span class="number">02</span></span><br><span class="line"></span><br><span class="line">Implement the following functions that will be needed <span class="keyword">for</span> Causal Attention:</span><br><span class="line"></span><br><span class="line">- &lt;span style='color:blue'&gt; compute_attention_heads &lt;/span&gt;: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\times$ n_heads, seqlen, d_head).</span><br><span class="line">- &lt;span style='color:blue'&gt; dot_product_self_attention &lt;/span&gt;: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention.</span><br><span class="line">- &lt;span style='color:blue'&gt; compute_attention_output &lt;/span&gt;: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\times$ d_head). These operations concatenate (stack/merge) the heads. </span><br><span class="line"></span><br><span class="line">Next there are some toy tensors which may serve to give you an idea of the data shapes <span class="keyword">and</span> opperations involved in Causal Attention. They are also useful to test out your functions! </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">tensor2d = create_tensor(q)</span><br><span class="line">display_tensor(tensor2d, 'query matrix (2D tensor)')</span><br><span class="line"></span><br><span class="line">tensor4d2b = create_tensor([[q, q], [q, q]])</span><br><span class="line">display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')</span><br><span class="line"></span><br><span class="line">tensor3dc = create_tensor([jnp.concatenate([q, q], axis = <span class="number">-1</span>)])</span><br><span class="line">display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')</span><br><span class="line"></span><br><span class="line">tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = <span class="number">-1</span>), jnp.concatenate([q, q], axis = <span class="number">-1</span>), jnp.concatenate([q, q], axis = <span class="number">-1</span>)])</span><br><span class="line">display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')</span><br></pre></td></tr></table></figure></p><pre><code>query matrix (2D tensor) shape: (2, 3)[[1 0 0] [0 1 0]]batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)[[[[1 0 0]   [0 1 0]]  [[1 0 0]   [0 1 0]]] [[[1 0 0]   [0 1 0]]  [[1 0 0]   [0 1 0]]]]one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)[[[1 0 0 1 0 0]  [0 1 0 0 1 0]]]three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)[[[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]]]</code></pre><p>It is important to know that the following 3 functions would normally be defined within the <code>CausalAttention</code> function further below. </p><p>However this makes these functions harder to test. Because of this, these functions are shown individually using a <code>closure</code> (when necessary) that simulates them being inside of the <code>CausalAttention</code> function. This is done because they rely on some variables that can be accessed from within <code>CausalAttention</code>.</p><h3 id="Support-Functions"><a href="#Support-Functions" class="headerlink" title="Support Functions"></a>Support Functions</h3><p><span style="color:blue"> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\times$ n_heads, seqlen, d_head).</p><p><strong>For the closures you only have to fill the inner function.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_attention_heads_closure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_attention_heads_closure</span><span class="params">(n_heads, d_head)</span>:</span></span><br><span class="line">    <span class="string">""" Function that simulates environment inside CausalAttention function.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_head (int):  dimensionality of heads.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        function: compute_attention_heads function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_attention_heads</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="string">""" Compute the attention heads.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Size of the x's batch dimension</span></span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Length of the sequence</span></span><br><span class="line">        <span class="comment"># Should be size of x's first dimension without counting the batch dim</span></span><br><span class="line">        seqlen = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape()</span></span><br><span class="line">        <span class="comment"># batch_size, seqlen, n_heads*d_head -&gt; batch_size, seqlen, n_heads, d_head</span></span><br><span class="line">        x = jnp.reshape(x, (batch_size, seqlen,n_heads, d_head))</span><br><span class="line">        <span class="comment"># Transpose x using jnp.transpose()</span></span><br><span class="line">        <span class="comment"># batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head</span></span><br><span class="line">        <span class="comment"># Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them</span></span><br><span class="line">        x = jnp.transpose(x, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape()</span></span><br><span class="line">        <span class="comment"># batch_size, n_heads, seqlen, d_head -&gt; batch_size*n_heads, seqlen, d_head</span></span><br><span class="line">        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compute_attention_heads</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(tensor3dc3b, <span class="string">"input tensor"</span>)</span><br><span class="line">result_cah = compute_attention_heads_closure(<span class="number">2</span>,<span class="number">3</span>)(tensor3dc3b)</span><br><span class="line">display_tensor(result_cah, <span class="string">"output tensor"</span>)</span><br></pre></td></tr></table></figure><pre><code>input tensor shape: (3, 2, 6)[[[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]]]output tensor shape: (6, 2, 3)[[[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">input tensor shape: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">output tensor shape: (<span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure></p><p><span style="color:blue"> dot_product_self_attention </span>: Creates a mask matrix with <code>False</code> values above the diagonal and <code>True</code> values below and calls DotProductAttention which implements dot product self attention.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: dot_product_self_attention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dot_product_self_attention</span><span class="params">(q, k, v)</span>:</span></span><br><span class="line">    <span class="string">""" Masked dot product self attention.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q (jax.interpreters.xla.DeviceArray): queries.</span></span><br><span class="line"><span class="string">        k (jax.interpreters.xla.DeviceArray): keys.</span></span><br><span class="line"><span class="string">        v (jax.interpreters.xla.DeviceArray): values.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)</span></span><br><span class="line">    mask_size = q.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)</span></span><br><span class="line">    <span class="comment"># Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_</span></span><br><span class="line">    <span class="comment"># Use jnp.tril() - Lower triangle of an array and jnp.ones()</span></span><br><span class="line">    mask = jnp.tril(jnp.ones((<span class="number">1</span>, mask_size, mask_size), dtype=jnp.bool_), k=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> DotProductAttention(q, k, v, mask)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)</span><br></pre></td></tr></table></figure><pre><code>DeviceArray([[[0.        , 1.        , 0.        ],              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DeviceArray([[[<span class="number">0.</span>        , <span class="number">1.</span>        , <span class="number">0.</span>        ],</span><br><span class="line">              [<span class="number">0.8496746</span> , <span class="number">0.15032543</span>, <span class="number">0.8496746</span> ]]], dtype=float32)</span><br></pre></td></tr></table></figure></p><p><span style="color:blue"> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\times$ d_head). These operations concatenate (stack/merge) the heads. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_attention_output_closure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_attention_output_closure</span><span class="params">(n_heads, d_head)</span>:</span></span><br><span class="line">    <span class="string">""" Function that simulates environment inside CausalAttention function.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_head (int):  dimensionality of heads.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        function: compute_attention_output function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_attention_output</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="string">""" Compute the attention output.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Length of the sequence</span></span><br><span class="line">        <span class="comment"># Should be size of x's first dimension without counting the batch dim</span></span><br><span class="line">        seqlen = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)</span></span><br><span class="line">        x = jnp.reshape(x,(<span class="number">-1</span>, n_heads, seqlen, d_head))</span><br><span class="line">        <span class="comment"># Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)</span></span><br><span class="line">        x = jnp.transpose(x, (<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Reshape to allow to concatenate the heads</span></span><br><span class="line">        <span class="keyword">return</span> jnp.reshape(x, (<span class="number">-1</span>, seqlen, n_heads * d_head))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compute_attention_output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(result_cah, <span class="string">"input tensor"</span>)</span><br><span class="line">result_cao = compute_attention_output_closure(<span class="number">2</span>,<span class="number">3</span>)(result_cah)</span><br><span class="line">display_tensor(result_cao, <span class="string">"output tensor"</span>)</span><br></pre></td></tr></table></figure><pre><code>input tensor shape: (6, 2, 3)[[[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]]]output tensor shape: (3, 2, 6)[[[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">input tensor shape: (<span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">output tensor shape: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure></p><h3 id="Causal-Attention-Function"><a href="#Causal-Attention-Function" class="headerlink" title="Causal Attention Function"></a>Causal Attention Function</h3><p>Now it is time for you to put everything together within the <code>CausalAttention</code> or Masked multi-head attention function:</p><p><img src="masked-attention.png"> </p><p><strong>Instructions:</strong> Implement the causal attention.<br>Your model returns the causal attention through a $tl.Serial$ with the following:</p><ul><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch" target="_blank" rel="noopener">tl.Branch</a> </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn" target="_blank" rel="noopener">tl.Fn</a></span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn" target="_blank" rel="noopener">tl.Fn</a></span>: Takes in compute_attention_output_closure to allow for parallel computing.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a></span>: Final Dense layer, with dimension <code>d_feature</code>.</li></ul><p>Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn" target="_blank" rel="noopener"><code>tl.Fn()</code></a> function. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: CausalAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CausalAttention</span><span class="params">(d_feature, </span></span></span><br><span class="line"><span class="function"><span class="params">                    n_heads, </span></span></span><br><span class="line"><span class="function"><span class="params">                    compute_attention_heads_closure=compute_attention_heads_closure,</span></span></span><br><span class="line"><span class="function"><span class="params">                    dot_product_self_attention=dot_product_self_attention,</span></span></span><br><span class="line"><span class="function"><span class="params">                    compute_attention_output_closure=compute_attention_output_closure,</span></span></span><br><span class="line"><span class="function"><span class="params">                    mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Transformer-style multi-headed causal attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_feature (int):  dimensionality of feature embedding.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        compute_attention_heads_closure (function): Closure around compute_attention heads.</span></span><br><span class="line"><span class="string">        dot_product_self_attention (function): dot_product_self_attention function. </span></span><br><span class="line"><span class="string">        compute_attention_output_closure (function): Closure around compute_attention_output. </span></span><br><span class="line"><span class="string">        mode (str): 'train' or 'eval'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: Multi-headed self-attention model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> d_feature % n_heads == <span class="number">0</span></span><br><span class="line">    d_head = d_feature // n_heads</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span></span><br><span class="line">    <span class="comment"># Since you are dealing with closures you might need to call the outer </span></span><br><span class="line">    <span class="comment"># function with the correct parameters to get the actual uncalled function.</span></span><br><span class="line">    ComputeAttentionHeads = tl.Fn(<span class="string">'AttnHeads'</span>, compute_attention_heads_closure(n_heads, d_head), n_out=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        tl.Branch( <span class="comment"># creates three towers for one input, takes activations and creates queries keys and values</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># queries</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># keys</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># values</span></span><br><span class="line">        ),</span><br><span class="line">        </span><br><span class="line">        tl.Fn(<span class="string">'DotProductAttn'</span>, dot_product_self_attention, n_out=<span class="number">1</span>), <span class="comment"># takes QKV</span></span><br><span class="line">        <span class="comment"># HINT: The second argument to tl.Fn() is an uncalled function</span></span><br><span class="line">        <span class="comment"># Since you are dealing with closures you might need to call the outer </span></span><br><span class="line">        <span class="comment"># function with the correct parameters to get the actual uncalled function.</span></span><br><span class="line">        tl.Fn(<span class="string">'AttnOutput'</span>, compute_attention_output_closure(n_heads, d_head), n_out=<span class="number">1</span>), <span class="comment"># to allow for parallel</span></span><br><span class="line">        tl.Dense(d_feature) <span class="comment"># Final dense layer</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the causal attention model</span></span><br><span class="line">print(CausalAttention(d_feature=<span class="number">512</span>, n_heads=<span class="number">8</span>))</span><br></pre></td></tr></table></figure><pre><code>Serial[  Branch_out3[    [Dense_512, AttnHeads]    [Dense_512, AttnHeads]    [Dense_512, AttnHeads]  ]  DotProductAttn_in3  AttnOutput  Dense_512]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Branch_out3[</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">  ]</span><br><span class="line">  DotProductAttn_in3</span><br><span class="line">  AttnOutput</span><br><span class="line">  Dense_512</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p><a name="2.3"></a></p><h2 id="2-3-Transformer-decoder-block"><a href="#2-3-Transformer-decoder-block" class="headerlink" title="2.3 Transformer decoder block"></a>2.3 Transformer decoder block</h2><p>Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.</p><p><img src="transformer_decoder_1.png" style="height:300px"> </p><p>To implement this function, you will have to call the <code>CausalAttention</code> or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: </p><ul><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm" target="_blank" rel="noopener">tl.LayerNorm</a> </span>: used to layer normalize</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a> </span>: the dense layer</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu" target="_blank" rel="noopener">ff_activation</a> </span>: feed forward activation (we use ReLu) here.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout" target="_blank" rel="noopener">tl.Dropout</a> </span>: dropout layer</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a> </span>: dense layer</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout" target="_blank" rel="noopener">tl.Dropout</a> </span>: dropout layer</li></ul><p>Finally once you implement the feedforward, you can go ahead and implement the entire block using: </p><ul><li><p><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual" target="_blank" rel="noopener">tl.Residual</a> </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout. </p></li><li><p><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual" target="_blank" rel="noopener">tl.Residual</a> </span>: takes in the feedforward block you will implement. </p></li></ul><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the transformer decoder block. Good luck!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: DecoderBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DecoderBlock</span><span class="params">(d_model, d_ff, n_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, mode, ff_activation)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a list of layers that implements a Transformer decoder block.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input is an activation tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model (int):  depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        mode (str): 'train' or 'eval'.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create masked multi-head attention block using CausalAttention function</span></span><br><span class="line">    causal_attention = CausalAttention( </span><br><span class="line">                        d_model,</span><br><span class="line">                        n_heads=n_heads,</span><br><span class="line">                        mode=mode</span><br><span class="line">                        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create feed-forward block (list) with two dense layers with dropout and input normalized</span></span><br><span class="line">    feed_forward = [ </span><br><span class="line">        <span class="comment"># Normalize layer inputs</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># Add first feed forward (dense) layer (don't forget to set the correct value for n_units)</span></span><br><span class="line">        tl.Dense(d_ff),</span><br><span class="line">        <span class="comment"># Add activation function passed in as a parameter (you need to call it!)</span></span><br><span class="line">        ff_activation(), <span class="comment"># Generally ReLU</span></span><br><span class="line">        <span class="comment"># Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)</span></span><br><span class="line">        tl.Dropout(rate=dropout, mode=mode),</span><br><span class="line">        <span class="comment"># Add second feed forward layer (don't forget to set the correct value for n_units)</span></span><br><span class="line">        tl.Dense(d_model),</span><br><span class="line">        <span class="comment"># Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)</span></span><br><span class="line">        tl.Dropout(rate=dropout, mode=mode)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">      tl.Residual(</span><br><span class="line">          <span class="comment"># Normalize layer input</span></span><br><span class="line">          tl.LayerNorm(),</span><br><span class="line">          <span class="comment"># Add causal attention block previously defined (without parentheses)</span></span><br><span class="line">          causal_attention,</span><br><span class="line">          <span class="comment"># Add dropout with rate and mode specified</span></span><br><span class="line">          tl.Dropout()</span><br><span class="line">        ),</span><br><span class="line">      tl.Residual(</span><br><span class="line">          <span class="comment"># Add feed forward block (without parentheses)</span></span><br><span class="line">          feed_forward</span><br><span class="line">        ),</span><br><span class="line">      ]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the decoder block</span></span><br><span class="line">print(DecoderBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, n_heads=<span class="number">8</span>, dropout=<span class="number">0.1</span>, mode=<span class="string">'train'</span>, ff_activation=tl.Relu))</span><br></pre></td></tr></table></figure><pre><code>[Serial[  Branch_out2[    None    Serial[      LayerNorm      Serial[        Branch_out3[          [Dense_512, AttnHeads]          [Dense_512, AttnHeads]          [Dense_512, AttnHeads]        ]        DotProductAttn_in3        AttnOutput        Dense_512      ]      Dropout    ]  ]  Add_in2], Serial[  Branch_out2[    None    Serial[      LayerNorm      Dense_2048      Relu      Dropout      Dense_512      Dropout    ]  ]  Add_in2]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Serial[</span><br><span class="line">        Branch_out3[</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">        ]</span><br><span class="line">        DotProductAttn_in3</span><br><span class="line">        AttnOutput</span><br><span class="line">        Dense_512</span><br><span class="line">      ]</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">], Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Dense_2048</span><br><span class="line">      Relu</span><br><span class="line">      Dropout</span><br><span class="line">      Dense_512</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">]]</span><br></pre></td></tr></table></figure></p><p><a name="2.4"></a></p><h2 id="2-4-Transformer-Language-Model"><a href="#2-4-Transformer-Language-Model" class="headerlink" title="2.4 Transformer Language Model"></a>2.4 Transformer Language Model</h2><p>You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing.<br><img src="transformer_decoder.png" style="height:400px"></p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need. </p><ul><li><p><span style="color:blue"> positional_enconder </span>- a list containing the following layers:</p><ul><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding" target="_blank" rel="noopener">tl.Embedding</a></span></li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout" target="_blank" rel="noopener">tl.Dropout</a></span></li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding" target="_blank" rel="noopener">tl.PositionalEncoding</a></span></li></ul></li><li><p>A list of <code>n_layers</code> <span style="color:blue"> decoder blocks</span>.</p></li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial" target="_blank" rel="noopener">tl.Serial</a>: </span> takes in the following layers or lists of layers:<ul><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight" target="_blank" rel="noopener">tl.ShiftRight</a>: </span>: shift the tensor to the right by padding on axis 1.</li><li><span style="color:blue"> positional_encoder </span>: encodes the text positions.</li><li><span style="color:blue"> decoder_blocks </span>: the ones you created.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm" target="_blank" rel="noopener">tl.LayerNorm</a> </span>: a layer norm.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a> </span>: takes in the vocab_size.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax" target="_blank" rel="noopener">tl.LogSoftmax</a> </span>: to predict.</li></ul></li></ul><p>Go go go!! You can do it :)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TransformerLM</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TransformerLM</span><span class="params">(vocab_size=<span class="number">33300</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  d_ff=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  n_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  max_len=<span class="number">4096</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  mode=<span class="string">'train'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  ff_activation=tl.Relu)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a Transformer language model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input to the model is a tensor of tokens. (This model uses only the</span></span><br><span class="line"><span class="string">    decoder part of the overall Transformer.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int): vocab size.</span></span><br><span class="line"><span class="string">        d_model (int):  depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        max_len (int): maximum symbol length for positional encoding.</span></span><br><span class="line"><span class="string">        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens</span></span><br><span class="line"><span class="string">        to activations over a vocab set.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Embedding inputs and positional encoder</span></span><br><span class="line">    positional_encoder = [ </span><br><span class="line">        <span class="comment"># Add embedding layer of dimension (vocab_size, d_model)</span></span><br><span class="line">        tl.Embedding(vocab_size,d_model),</span><br><span class="line">        <span class="comment"># Use dropout with rate and mode specified</span></span><br><span class="line">        tl.Dropout(rate = dropout, mode = mode),</span><br><span class="line">        <span class="comment"># Add positional encoding layer with maximum input length and mode specified</span></span><br><span class="line">        tl.PositionalEncoding()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span></span><br><span class="line">    decoder_blocks = [ </span><br><span class="line">        DecoderBlock(d_model, d_ff, n_heads,</span><br><span class="line">                 dropout, mode, ff_activation) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_layers)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the complete model as written in the figure</span></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        <span class="comment"># Use teacher forcing (feed output of previous step to current step)</span></span><br><span class="line">        tl.ShiftRight(), <span class="comment"># Specify the mode!</span></span><br><span class="line">        <span class="comment"># Add positional encoder</span></span><br><span class="line">        positional_encoder,</span><br><span class="line">        <span class="comment"># Add decoder blocks</span></span><br><span class="line">        decoder_blocks,</span><br><span class="line">        <span class="comment"># Normalize layer</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add dense layer of vocab_size (since need to select a word to translate to)</span></span><br><span class="line">        <span class="comment"># (a.k.a., logits layer. Note: activation already set by ff_activation)</span></span><br><span class="line">        tl.Dense(vocab_size),</span><br><span class="line">        <span class="comment"># Get probabilities with Logsoftmax</span></span><br><span class="line">        tl.LogSoftmax()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the Transformer</span></span><br><span class="line">print(TransformerLM(n_layers=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Serial[  ShiftRight(1)  Embedding_33300_512  Dropout  PositionalEncoding  Serial[    Branch_out2[      None      Serial[        LayerNorm        Serial[          Branch_out3[            [Dense_512, AttnHeads]            [Dense_512, AttnHeads]            [Dense_512, AttnHeads]          ]          DotProductAttn_in3          AttnOutput          Dense_512        ]        Dropout      ]    ]    Add_in2  ]  Serial[    Branch_out2[      None      Serial[        LayerNorm        Dense_2048        Relu        Dropout        Dense_512        Dropout      ]    ]    Add_in2  ]  LayerNorm  Dense_33300  LogSoftmax]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  ShiftRight(<span class="number">1</span>)</span><br><span class="line">  Embedding_33300_512</span><br><span class="line">  Dropout</span><br><span class="line">  PositionalEncoding</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Serial[</span><br><span class="line">          Branch_out3[</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">          ]</span><br><span class="line">          DotProductAttn_in3</span><br><span class="line">          AttnOutput</span><br><span class="line">          Dense_512</span><br><span class="line">        ]</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Dense_2048</span><br><span class="line">        Relu</span><br><span class="line">        Dropout</span><br><span class="line">        Dense_512</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  LayerNorm</span><br><span class="line">  Dense_33300</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p><a name="3"></a></p><h1 id="Part-3-Training"><a href="#Part-3-Training" class="headerlink" title="Part 3: Training"></a>Part 3: Training</h1><p>Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a <code>gpu</code> or <code>cpu</code>. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words.</p><p><a name="3.1"></a></p><h3 id="3-1-Training-the-model"><a href="#3-1-Training-the-model" class="headerlink" title="3.1 Training the model"></a>3.1 Training the model</h3><p>You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an <code>epoch</code>. For each epoch, you have to go over all the data, using your training iterator.</p><p><a name="ex05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions:</strong> Implement the <code>train_model</code> program below to train the neural network above. Here is a list of things you should do:</p><ul><li>Create the train task by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask" target="_blank" rel="noopener"><code>trax.supervised.training.TrainTask</code></a> and pass in the following: <ul><li><span style="color:blue"> labeled_data </span> = train_gen</li><li><span style="color:blue"> loss_fn </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss" target="_blank" rel="noopener">tl.CrossEntropyLoss()</a></li><li><span style="color:blue"> optimizer </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam" target="_blank" rel="noopener">trax.optimizers.Adam(0.01)</a></li><li><span style="color:blue"> lr_schedule </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay" target="_blank" rel="noopener">lr_schedule</a></li></ul></li></ul><ul><li>Create the eval task by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask" target="_blank" rel="noopener"><code>trax.supervised.training.EvalTask</code></a> and pass in the following: <ul><li><span style="color:blue"> labeled_data </span> = eval_gen</li><li><span style="color:blue"> metrics </span> = tl.CrossEntropyLoss() and <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy" target="_blank" rel="noopener">tl.Accuracy()</a></li></ul></li></ul><ul><li>Create the training loop by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop" target="_blank" rel="noopener"><code>trax.supervised.Training.Loop</code></a> and pass in the following: <ul><li><span style="color:blue"> TransformerLM </span> </li><li><span style="color:blue"> train_task </span> </li><li><span style="color:blue"> eval_task </span> = [eval_task]</li><li><span style="color:blue"> output_dir</span> = output_dir</li></ul></li></ul><p>You will be using a cross entropy loss, with Adam optimizer. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Trax</a> documentation to get a full understanding. </p><p>The training loop that this function returns can be runned using the <code>run()</code> method by passing in the desired number of steps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"></span><br><span class="line"><span class="comment"># UNQ_C8</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span><span class="params">(TransformerLM, train_gen, eval_gen, output_dir = <span class="string">"~/model"</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        TransformerLM (trax.layers.combinators.Serial): The model you are building.</span></span><br><span class="line"><span class="string">        train_gen (generator): Training stream of data.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Evaluation stream of data.</span></span><br><span class="line"><span class="string">        output_dir (str): folder to save your file.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    output_dir = os.path.expanduser(output_dir)  <span class="comment"># trainer is an object</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    train_task = training.TrainTask( </span><br><span class="line">      labeled_data=train_gen, <span class="comment"># The training generator</span></span><br><span class="line">      loss_layer= tl.CrossEntropyLoss(), <span class="comment"># Loss function </span></span><br><span class="line">      optimizer= trax.optimizers.Adam(<span class="number">0.01</span>), <span class="comment"># Optimizer (Don't forget to set LR to 0.01)</span></span><br><span class="line">      lr_schedule= lr_schedule,</span><br><span class="line">      n_steps_per_checkpoint = <span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask( </span><br><span class="line">      labeled_data=eval_gen, <span class="comment"># The evaluation generator</span></span><br><span class="line">      metrics=[tl.CrossEntropyLoss(),tl.Accuracy()] <span class="comment"># CrossEntropyLoss and Accuracy</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    loop = training.Loop(TransformerLM(d_model=<span class="number">4</span>,</span><br><span class="line">                                       d_ff=<span class="number">16</span>,</span><br><span class="line">                                       n_layers=<span class="number">1</span>,</span><br><span class="line">                                       n_heads=<span class="number">2</span>,</span><br><span class="line">                                       mode=<span class="string">'train'</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure><p>Notice that the model will be trained for only 10 steps. </p><p>Even with this constraint the model with the original default arguments took a very long time to finish. Because of this some parameters are changed when defining the model that is fed into the training loop in the function above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Should take around 1.5 minutes</span></span><br><span class="line">!rm -f ~/model/model.pkl.gz</span><br><span class="line">loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>Step      1: Ran 1 train steps in 9.11 secsStep      1: train CrossEntropyLoss |  10.41297626Step      1: eval  CrossEntropyLoss |  10.41586781Step      1: eval          Accuracy |  0.00000000Step     10: Ran 9 train steps in 58.21 secsStep     10: train CrossEntropyLoss |  10.41278458Step     10: eval  CrossEntropyLoss |  10.41440201Step     10: eval          Accuracy |  0.00000000</code></pre><p> <a name="4"></a></p><h1 id="Part-4-Evaluation"><a href="#Part-4-Evaluation" class="headerlink" title="Part 4:  Evaluation"></a>Part 4:  Evaluation</h1><p><a name="4.1"></a></p><h3 id="4-1-Loading-in-a-trained-model"><a href="#4-1-Loading-in-a-trained-model" class="headerlink" title="4.1 Loading in a trained model"></a>4.1 Loading in a trained model</h3><p>In this part you will evaluate by loading in an almost exact version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model.</p><p>As you may have already noticed the model that you trained and the pretrained model share the same overall architecture but they have different values for some of the parameters:</p><p>   <code>Original (pretrained) model:</code>                                 </p><pre><code>TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8,                dropout=0.1, max_len=4096, ff_activation=tl.Relu)</code></pre><p>   <code>Your model:</code></p><pre><code>TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)</code></pre><p>   <strong>Only the parameters shown for your model were changed. The others stayed the same.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the model architecture</span></span><br><span class="line">model = TransformerLM(mode=<span class="string">'eval'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained weights</span></span><br><span class="line">model.init_from_file(<span class="string">'model.pkl.gz'</span>, weights_only=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><a name="5"></a></p><h1 id="Part-5-Testing-with-your-own-input"><a href="#Part-5-Testing-with-your-own-input" class="headerlink" title="Part 5: Testing with your own input"></a>Part 5: Testing with your own input</h1><p>You will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index. </p><p><a name="ex06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement the next symbol function that takes in the cur_output_tokens and the trained model to return the index of the next word. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_symbol</span><span class="params">(cur_output_tokens, model)</span>:</span></span><br><span class="line">    <span class="string">"""Returns the next symbol for a given sentence.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Serial): The transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        int: tokenized symbol.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current output tokens length</span></span><br><span class="line">    token_length = len(cur_output_tokens)</span><br><span class="line">    <span class="comment"># calculate the minimum power of 2 big enough to store token_length</span></span><br><span class="line">    <span class="comment"># HINT: use np.ceil() and np.log2()</span></span><br><span class="line">    <span class="comment"># add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0</span></span><br><span class="line">    padded_length = <span class="number">2</span>**int(np.ceil(np.log2(token_length + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fill cur_output_tokens with 0's until it reaches padded_length</span></span><br><span class="line">    padded = cur_output_tokens + [<span class="number">0</span>] * (padded_length - token_length)</span><br><span class="line">    padded_with_batch = np.array(padded)[<span class="keyword">None</span>, :] <span class="comment"># Don't replace this 'None'! This is a way of setting the batch dim</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># model expects a tuple containing two padded tensors (with batch)</span></span><br><span class="line">    output, _ = model((padded_with_batch, padded_with_batch)) </span><br><span class="line">    <span class="comment"># HINT: output has shape (1, padded_length, vocab_size)</span></span><br><span class="line">    <span class="comment"># To get log_probs you need to index output with 0 in the first dim</span></span><br><span class="line">    <span class="comment"># token_length in the second dim and all of the entries for the last dim.</span></span><br><span class="line">    log_probs = output[<span class="number">0</span>, token_length, :]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> int(np.argmax(log_probs))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out!</span></span><br><span class="line">sentence_test_nxt_symbl = <span class="string">"I want to fly in the sky."</span></span><br><span class="line">detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[<span class="number">0</span>], model)])</span><br></pre></td></tr></table></figure><pre><code>&#39;The&#39;</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">'The'</span><br></pre></td></tr></table></figure></p><p><a name="5.1"></a></p><h3 id="5-1-Greedy-decoding"><a href="#5-1-Greedy-decoding" class="headerlink" title="5.1 Greedy decoding"></a>5.1 Greedy decoding</h3><p>Now you will implement the greedy_decode algorithm that will call the <code>next_symbol</code> function. It takes in the input_sentence, the trained model and returns the decoded sentence. </p><p><a name="ex07"></a></p><h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p><strong>Instructions</strong>: Implement the greedy_decode algorithm. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10</span></span><br><span class="line"><span class="comment"># Decoding functions.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(input_sentence, model)</span>:</span></span><br><span class="line">    <span class="string">"""Greedy decode function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_sentence (string): a sentence or article.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Serial): Transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: summary of the input.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># Use tokenize()</span></span><br><span class="line">    cur_output_tokens = tokenize(input_sentence) + [<span class="number">0</span>]</span><br><span class="line">    generated_output = [] </span><br><span class="line">    cur_output = <span class="number">0</span> </span><br><span class="line">    EOS = <span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> cur_output != EOS:</span><br><span class="line">        <span class="comment"># Get next symbol</span></span><br><span class="line">        cur_output = next_symbol(cur_output_tokens, model)</span><br><span class="line">        <span class="comment"># Append next symbol to original sentence</span></span><br><span class="line">        cur_output_tokens.append(cur_output)</span><br><span class="line">        <span class="comment"># Append next symbol to generated sentence</span></span><br><span class="line">        generated_output.append(cur_output)</span><br><span class="line">        print(detokenize(generated_output))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> detokenize(generated_output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out on a sentence!</span></span><br><span class="line">test_sentence = <span class="string">"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips."</span></span><br><span class="line">print(wrapper.fill(test_sentence), <span class="string">'\n'</span>)</span><br><span class="line">print(greedy_decode(test_sentence, model))</span><br></pre></td></tr></table></figure><pre><code>It was a sunny day when I went to the market to buy some flowers. ButI only found roses, not tulips. :: I: I just: I just found: I just found ros: I just found roses: I just found roses,: I just found roses, not: I just found roses, not tu: I just found roses, not tulips: I just found roses, not tulips: I just found roses, not tulips.: I just found roses, not tulips.&lt;EOS&gt;: I just found roses, not tulips.&lt;EOS&gt;</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">:</span><br><span class="line">: I</span><br><span class="line">: I just</span><br><span class="line">: I just found</span><br><span class="line">: I just found ros</span><br><span class="line">: I just found roses</span><br><span class="line">: I just found roses,</span><br><span class="line">: I just found roses, <span class="keyword">not</span></span><br><span class="line">: I just found roses, <span class="keyword">not</span> tu</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.&lt;EOS&gt;</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.&lt;EOS&gt;</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out with a whole article!</span></span><br><span class="line">article = <span class="string">"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students."</span></span><br><span class="line">print(wrapper.fill(article), <span class="string">'\n'</span>)</span><br><span class="line">print(greedy_decode(article, model))</span><br></pre></td></tr></table></figure><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Jordan</span><br><span class="line">Jordan Ful</span><br><span class="line">Jordan Fulcol</span><br><span class="line">Jordan Fulcoly</span><br><span class="line">Jordan Fulcoly,</span><br><span class="line">Jordan Fulcoly, Wayne</span><br><span class="line">Jordan Fulcoly, Wayne Dre</span><br><span class="line">Jordan Fulcoly, Wayne Drexe</span><br><span class="line">Jordan Fulcoly, Wayne Drexel</span><br><span class="line">Jordan Fulcoly, Wayne Drexel,</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line"></span><br><span class="line">Final summary:</span><br><span class="line"></span><br><span class="line">Jordan Fulcoly, Wayne Drexel, Tyler Carroll <span class="keyword">and</span> Connor Carroll were</span><br><span class="line">suspended <span class="keyword">for</span> one day. Four students were suspended <span class="keyword">for</span> one day</span><br><span class="line">because they allegedly did not heed to warnings that the 'Tebowing'</span><br><span class="line">craze was blocking the hallway <span class="keyword">and</span> presenting a safety hazard to</span><br><span class="line">students.&lt;EOS&gt;</span><br></pre></td></tr></table></figure></p><p><strong>Congratulations on finishing this week’s assignment!</strong> You did a lot of work and now you should have a better understanding of the encoder part of Transformers and how Transformers can be used for text summarization.</p><p><strong>Keep it up!</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Transformer-Summarizer&quot;&gt;&lt;a href=&quot;#Assignment-2-Transformer-Summarizer&quot; class=&quot;headerlink&quot; title=&quot;Assignment 2: Transfor
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Using RL to Solve Blackjack</title>
    <link href="https://zhangruochi.com/Using-RL-to-Solve-Blackjack/2020/09/17/"/>
    <id>https://zhangruochi.com/Using-RL-to-Solve-Blackjack/2020/09/17/</id>
    <published>2020-09-17T09:44:04.000Z</published>
    <updated>2020-09-17T09:45:27.584Z</updated>
    
    <content type="html"><![CDATA[<p><strong>转载自: <a href="https://github.com/zht007/tensorflow-practice/blob/master/7_Renforcement_Learning_blackjack/强化学习——MC(蒙特卡洛)玩21点扑克游戏.md" target="_blank" rel="noopener">https://github.com/zht007/tensorflow-practice/blob/master/7_Renforcement_Learning_blackjack/强化学习——MC(蒙特卡洛)玩21点扑克游戏.md</a></strong></p><h3 id="1-关于21点游戏"><a href="#1-关于21点游戏" class="headerlink" title="1. 关于21点游戏"></a>1. 关于21点游戏</h3><h4 id="1-1-规则简介"><a href="#1-1-规则简介" class="headerlink" title="1.1 规则简介"></a>1.1 规则简介</h4><p>21点的游戏规则详细很容易就能够找到，这里进行简单的介绍。</p><blockquote><ul><li><p>在这里<strong>智能体(Agent)</strong>扮演<strong>玩家(Player)</strong>，对方是<strong>庄家(Dealer)</strong>。</p></li><li><p><strong>点数(Score)</strong>：2-10的点数为牌面数字；J，Q，K是10点；<strong>A有两种算法</strong>，1或者11，算11总点数不超过21时则必须算成11(<strong>usable</strong>)，否则算作1。</p></li><li><p>庄家需要<strong>亮(Show)</strong>一张牌，玩家根据自己手中的牌和庄家亮的牌决定是<strong>要牌(hits)</strong>还是<strong>停牌(sticks)</strong>。</p></li><li><p>庄家要牌和停牌的规则是固定的，即点数小于17必须要牌，否则停牌。</p></li><li><p><strong>爆牌(goes bust)</strong>：牌总数操过21点，谁爆牌谁输，谁首先凑到21点谁赢，每有爆牌的时候谁大谁赢，同时凑到21为和局。</p></li></ul></blockquote><h4 id="1-2-转换成MDP"><a href="#1-2-转换成MDP" class="headerlink" title="1.2 转换成MDP"></a>1.2 转换成MDP</h4><p>了解规则后，我们将游戏转换成MDP，MDP的几大要素：状态(S: State)，行动(A: Action)，奖励(R: Reward)，策略Policy，状态值函数V(s): State-Value Function，行动值函数Q(s, a)Action-Value Function。</p><blockquote><p><strong>行动A</strong>：<strong>要牌(hits)</strong>还是<strong>停牌(sticks)</strong></p><p><strong>状态S</strong>：状态是由双方目前牌的点数决定的，但是当玩家点数小于等于11时，当然会毫不犹豫选择要牌，所以真正涉及到做选择的状态是12-21点的状态，此时庄家亮牌有A-10种情况，再加上是否有11的A(usable A)，所以21点游戏中所有的状态一<strong>共只有200个</strong>。</p><p><strong>奖励R</strong>：玩家赢牌奖励为1，输牌奖励为-1，和局和其他状态奖励为0。</p><p><strong>策略Policy</strong>：该状态下，要牌和停牌的概率</p></blockquote><h3 id="2-MC策略评估"><a href="#2-MC策略评估" class="headerlink" title="2. MC策略评估"></a>2. MC策略评估</h3><p>在<strong>给定策略</strong>下，为什么我们不用上一篇文章提到的DP方法进行策略评估呢？DP方法需要look one step ahead，假设玩家手里牌点数为14，庄家亮牌为10，你需要计算要牌和停牌之后所有可能性，下一张牌是什么？庄家可能抽到什么？离获得奖励有多远？等等，这几乎是不可能的。</p><p>MC可以通过抽样方式，直接根据策略实践，从而获取奖励和学习V(s)，克服了DP方法的限制。这里采用首次访问MC方法。大致分为三步：</p><p><strong>第一步</strong>：根据策略采样，直到游戏结束，获得一个episode的 (S0, A0, R1), (S1, A1, R2), . . . , (ST-1, AT-1, RT)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">episode = []</span><br><span class="line">state = env.reset()      </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    action = policy(state)</span><br><span class="line">    next_state, reward, done, _ = env.step(action)</span><br><span class="line">    episode.append((state, action, reward))</span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    state = next_state</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p><strong>第二步:</strong>   计算首次出现s状态的Reward，直到这个episode结束总共累积的Reward。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">states_in_episode = set([tuple(x[<span class="number">0</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> episode])</span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> states_in_episode:</span><br><span class="line">            <span class="comment"># Find the first occurance of the state in the episode</span></span><br><span class="line">            first_occurence_idx = next(i <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(episode) <span class="keyword">if</span> x[<span class="number">0</span>] == state)</span><br><span class="line">            <span class="comment"># Sum up all rewards since the first occurance</span></span><br><span class="line">            G = sum([x[<span class="number">2</span>]*(discount_factor**i) <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(episode[first_occurence_idx:])])</span><br><span class="line">            <span class="comment"># Calculate average return for this state over all sampled episodes</span></span><br><span class="line">            returns_sum[state] += G</span><br><span class="line">            returns_count[state] += <span class="number">1.0</span></span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p><strong>第三步</strong>：若干个epsoide之后，将累积的R平均就得到该s下的V(s)了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">V[state] = returns_sum[state] / returns_count[state]</span><br></pre></td></tr></table></figure><p>给定玩家的策略，当分数小于20则要牌，否则停牌</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_policy</span><span class="params">(observation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A policy that sticks if the player score is &gt;= 20 and hits otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    score, dealer_score, usable_ace = observation</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> score &gt;= <span class="number">20</span> <span class="keyword">else</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p>下图为500,000个epsoide之后的V(s)<img src="/Users/hongtao/Library/Application Support/typora-user-images/image-20190424164753631.png" alt="image-20190424164753631"></p><p>V(s)的分布只能告诉我们<strong>当前策略下</strong>每个<strong>状态</strong>(你的点数，庄家亮牌，是否有usable A)的<strong>价值</strong>，我们如何使用V(s)来改进我们的策略，从而获得最大几率获胜的可能性呢？这就是我们下一节要讨论的内容。</p><h3 id="3-MC控制"><a href="#3-MC控制" class="headerlink" title="3. MC控制"></a>3. MC控制</h3><p>当然我们的目的不仅仅是对当前策略进行评估，我们希望改进策略在游戏中获得最大的收益。与DP一样，MC可以采用评估加改进(Policy Evaluation and Policy Improvement)的方式，迭代更新策略，最终可以收敛到一个最佳的策略。</p><p>当然我们在MC控制中采用策略评估的时候，需要加入对行动的评估，即<strong>Q(s, a)行动值函数</strong>的评估。但是如果我们采用DP中Greedy的方式来改进策略会遇到问题。由于MC是用<strong>采样</strong>的方式更新<strong>Q(s, a)</strong>，这就意味着我们很可能错过一些状态和行动，而且永远也无法更新该状态和行动的Q函数了。这就是典型的<strong>探索利用困境(Explore Exploit Delima)</strong>。</p><p>解决探索利用困境，我们可以使用epsilon-greedy 方法，或者将探索和利用的policy分开，采用off-policy的方法更新策略。</p><h4 id="3-1-On-Policy-的-epsilon-greedy采样法"><a href="#3-1-On-Policy-的-epsilon-greedy采样法" class="headerlink" title="3.1 On-Policy 的 epsilon-greedy采样法"></a>3.1 On-Policy 的 epsilon-greedy采样法</h4><p>On-Policy即评估和改进的策略是同一个策略，为避免探索利用困境，我们采用 epsilon-greedy的方法。</p><p><strong>第一步</strong>：对于21点的游戏，我们定义 epsilon-greedy policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(Q, epsilon, nA)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></span><br><span class="line">        A = np.ones(nA, dtype=float) * epsilon / nA </span><br><span class="line">        best_action = np.argmax(Q[observation])</span><br><span class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</span><br><span class="line">        <span class="keyword">return</span> A </span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure><p>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</p><p>其中Q是一个dictionary，为该状态下对应的行动，这样定义epsilon greedy policy 既保证了最优行动的几率最大，同时也让采取其他行动几率为一个非零的小值(epsilon / nA )。这样就保证了智能体在采样的时候能够探索未知的状态和行动。</p><p><strong>第二步</strong>：与MC评估的第一步一致，根据策略采样，直到游戏结束，获得一个episode的 (S0, A0, R1), (S1, A1, R2), . . . , (ST-1, AT-1, RT)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</span><br><span class="line">       episode = []</span><br><span class="line">       state = env.reset()</span><br><span class="line">       <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">           probs = policy(state)</span><br><span class="line">           action = np.random.choice(np.arange(len(probs)), p=probs)</span><br><span class="line">           next_state, reward, done, _ = env.step(action)</span><br><span class="line">           episode.append((state, action, reward))</span><br><span class="line">           <span class="keyword">if</span> done:</span><br><span class="line">               <span class="keyword">break</span></span><br><span class="line">           state = next_state</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Control%20with%20Epsilon-Greedy%20Policies.ipynb" target="_blank" rel="noopener">github</a> MIT license</em></p><p>注意与MC评估不同的是，action无法从policy中直接得出，而是根据概率随机选择的，也就是有可能智能体会”探索”非最优行动。</p><p><strong>第三步</strong>：计算首次出现该s 和 a 的Reward，直到这个episode结束，总共累积的Reward。平均Reward并更新Q表。Q表更新的同时，Policy也就自动更新了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sa_in_episode = set([(tuple(x[<span class="number">0</span>]), x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> episode])</span><br><span class="line">        <span class="keyword">for</span> state, action <span class="keyword">in</span> sa_in_episode:</span><br><span class="line">            sa_pair = (state, action)</span><br><span class="line">            <span class="comment"># Find the first occurance of the (state, action) pair in the episode</span></span><br><span class="line">            first_occurence_idx = next(i <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(episode)</span><br><span class="line">                                       <span class="keyword">if</span> x[<span class="number">0</span>] == state <span class="keyword">and</span> x[<span class="number">1</span>] == action)</span><br><span class="line">            <span class="comment"># Sum up all rewards since the first occurance</span></span><br><span class="line">            G = sum([x[<span class="number">2</span>]*(discount_factor**i) <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(episode[first_occurence_idx:])])</span><br><span class="line">            <span class="comment"># Calculate average return for this state over all sampled episodes</span></span><br><span class="line">            returns_sum[sa_pair] += G</span><br><span class="line">            returns_count[sa_pair] += <span class="number">1.0</span></span><br><span class="line">            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The policy is improved implicitly by changing the Q dictionary</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Q, policy</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Control%20with%20Epsilon-Greedy%20Policies.ipynb" target="_blank" rel="noopener">github</a> MIT license</em></p><p>下图是500,000个episode之后Q表中各个状态对应的Action值，Action只有两个值0(停牌)和1(要牌)，读者就可以尝试用下图的策略指导玩21点的游戏啦。举个例子，比如你现在手上牌是14点，没有可作为11的A，庄家亮牌为8，那么根据左图所示，最好的策略就是要牌。</p><p><img src="/Users/hongtao/Library/Application Support/typora-user-images/image-20190429164937096.png" alt="image-20190429164937096"></p><h4 id="3-2-Off-Policy的-Weighted-Importance采样法"><a href="#3-2-Off-Policy的-Weighted-Importance采样法" class="headerlink" title="3.2 Off-Policy的 Weighted Importance采样法"></a>3.2 Off-Policy的 Weighted Importance采样法</h4><p>Off-Policy就是将最终想要得到的<strong>目标策略(Target Policy)</strong>和用于探索的<strong>行为策略(Behavior Policy)</strong>分离，对目标策略采取Greedy的改进方式，而对实际行动的行为策略采用随机探索的改进方式从而解决了探索利用困境。当然Off-Policy 还有很多其他的优点比如学习历史经验，学习别人的经验等等。</p><p>这部分涉及到的理论比较复杂，可参考<a href="http://incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">[2]</a>中的相关内容。简单解释即首先用Behavior Policy指导智能体进行MC采样，然后用包含<strong>Importance Sampling Ratio</strong> 函数来更新Target Policy。Importance Sampling Ratio是Target Policy和Behavior Policy在同一路径下的概率比值。</p><p>Target Policy的Q(s, a)函数在MC采样下需要平均，这里采用加权平均的方法，包含Importatnce Sampling Ratio的权重简记为 W，最后，我们通过递推的方法更新 W 即可跟新Q(s, a)。</p><p><strong>第一步</strong>：生成两种policy方法，random policy 用于Behavior Policy，greedy policy用于Target Policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_random_policy</span><span class="params">(nA)</span>:</span></span><br><span class="line">    A = np.ones(nA, dtype=float) / nA</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_greedy_policy</span><span class="params">(Q)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(state)</span>:</span></span><br><span class="line">        A = np.zeros_like(Q[state], dtype=float)</span><br><span class="line">        best_action = np.argmax(Q[state])</span><br><span class="line">        A[best_action] = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p><strong>第二步</strong>：用Behavior Policy进行MC采样，这里与On-Policy 的方法类似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">target_policy = create_greedy_policy(Q)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        episode = []</span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            <span class="comment"># Sample an action from our policy</span></span><br><span class="line">            probs = behavior_policy(state)</span><br><span class="line">            action = np.random.choice(np.arange(len(probs)), p=probs)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            episode.append((state, action, reward))</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            state = next_state</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p>第三步**：递推的方法更新W和Q，Target Policy 也就自动更新了。注意由于是采用递推的方法，该episode是从后往前计算的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    G = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># The importance sampling ratio (the weights of the returns)</span></span><br><span class="line">    W = <span class="number">1.0</span></span><br><span class="line">    <span class="comment"># For each step in the episode, backwards</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(len(episode))[::<span class="number">-1</span>]:</span><br><span class="line">        state, action, reward = episode[t]</span><br><span class="line">        <span class="comment"># Update the total reward since step t</span></span><br><span class="line">        G = discount_factor * G + reward</span><br><span class="line">        <span class="comment"># Update weighted importance sampling formula denominator</span></span><br><span class="line">        C[state][action] += W</span><br><span class="line">        <span class="comment"># Update the action-value function using the incremental update formula (5.7)</span></span><br><span class="line">        <span class="comment"># This also improves our target policy which holds a reference to Q</span></span><br><span class="line">        Q[state][action] += (W / C[state][action]) * (G - Q[state][action])</span><br><span class="line">        <span class="comment"># If the action taken by the behavior policy is not the action </span></span><br><span class="line">        <span class="comment"># taken by the target policy the probability will be 0 and we can break</span></span><br><span class="line">        <span class="keyword">if</span> action !=  np.argmax(target_policy(state)):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        W = W * <span class="number">1.</span>/behavior_policy(state)[action]</span><br><span class="line">    </span><br><span class="line"><span class="keyword">return</span> Q, target_policy</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p>最后，经过500,000 个episod我们得到的最佳策略，与上一节采用On-Policy MC 方法的结果稍有差异，但基本一致。</p><p><img src="/Users/hongtao/Library/Application Support/typora-user-images/image-20190426125404672.png" alt="image-20190426125404672"></p><hr><p>参考资料</p><p>[1] <a href="http://incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">Reinforcement Learning: An Introduction (2nd Edition)</a></p><p>[2] <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">David Silver’s Reinforcement Learning Course (UCL, 2015)</a></p><p>[3] <a href="https://github.com/dennybritz/reinforcement-learning" target="_blank" rel="noopener">Github repo: Reinforcement Learning</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;转载自: &lt;a href=&quot;https://github.com/zht007/tensorflow-practice/blob/master/7_Renforcement_Learning_blackjack/强化学习——MC(蒙特卡洛)玩21点扑克游戏.
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Generalized Policy Iteration</title>
    <link href="https://zhangruochi.com/Generalized-Policy-Iteration/2020/09/10/"/>
    <id>https://zhangruochi.com/Generalized-Policy-Iteration/2020/09/10/</id>
    <published>2020-09-10T06:26:25.000Z</published>
    <updated>2020-09-10T07:20:31.481Z</updated>
    
    <content type="html"><![CDATA[<p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies.  As discussed there, we can easily obtain optimal policies once we have found the optimal value functions, $v_{\star}$ or $q_{\star}$, which satisfy the Bellman optimality equations:</p><script type="math/tex; mode=display">V_{\star} = max_a \sum_{s\prime,r} p(s\prime,r | s, a)[ r + \gamma v_{\star} (s\prime)]</script><script type="math/tex; mode=display">q_{\star}(s,a) = \sum_{s\prime,r} p(s\prime,r | s,a)[ r + \gamma max_{a\prime} q_{\star}(s\prime,a\prime)]</script><h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p>We consider how to compute the state-value function $V_{\pi}$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. </p><script type="math/tex; mode=display">V_{\pi}(s) = \sum_a \pi(a | s) \sum_{s\prime,r} p(s\prime, r | s, a) [ r + \gamma v_{\pi} (s\prime)]</script><p>Consider a sequence of approximate value functions $v_0,v_1, \cdots $,  each mapping $S+$ to $\mathbb{R}$ (the real numbers). The initial approximation, $v_0$, is chosen arbitrarily, and each successive approximation is obtained by using the Bellman equation for $V_{\pi}$ as an update rule:</p><script type="math/tex; mode=display">V_{k+1}(s) = \sum_a \pi(a | s) \sum_{s\prime,r} p(s\prime, r | s, a) [ r + \gamma v_{k} (s\prime)]</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>Our reason for computing the value function for a policy is to help find better policies. </p><p>Consider selecting a in s and thereafter following the existing policy, $\pi$,  The value of this way of behaving is</p><script type="math/tex; mode=display">q_{\pi}(s,a) = \sum_{s\prime,r}p(s\prime,r | s, a) [ r + \gamma  v_{\pi}(s\prime)]</script><p>The key criterion is whether this is greater than or less than $V_{\pi}(s)$.  If it is greater—that is, if it is better to select a once in s and thereafter follow ⇡ than it would be to follow $\pi$ all the time—then one would expect it to be better still to select a every time s is encountered, and that the new policy would in fact be a better one overall. </p><p>That this is true is a special case of a general result called the <strong>policy improvement theorem</strong>. Let $\pi$ and $\pi\prime$ be any pair of deterministic policies such that, for all $s\in S$, </p><script type="math/tex; mode=display">q_{\pi}(s, \pi\prime(s)) \geq v_{\pi}(s)</script><p>Then the policy $\pi\prime$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states $s\in S$:</p><script type="math/tex; mode=display">V_{\pi\prime}(s) \geq V_{\pi}(s)</script><h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>Once a policy, $\pi$, has been improved using $v_{\pi}$ to yield a better policy, $\pi\prime$, we can then compute $\pi\prime$ and  improve it again to yield an even better $\pi\prime\prime$. We can thus obtain a sequence of monotonically improving policies and value functions:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%"></center><p>This way of finding an optimal policy is called policy iteration. A complete algorithm is given in the box below.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%"></center><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><script type="math/tex; mode=display">V_{k+1}(s) = max_a \sum_{s\prime,r} p(s\prime,r | s, a)[ r + \gamma v_k (s\prime)]</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%"></center><h2 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h2><p>Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement). In policy iteration, these two processes alternate, each completing before the other begins, but this is not really necessary. In value iteration, for example, only a single iteration of policy evaluation is performed in between each policy improvement. In asynchronous DP methods, the evaluation and improvement processes are interleaved at an even finer grain. In some cases a single state is updated in one process before returning to the other. As long as both processes continue to update all states, the ultimate result is typically the same—convergence to the optimal value function and an optimal policy.</p><p>We use the term generalized policy iteration (GPI) to refer to the general idea of letting policy-evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram to the right. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the Bellman optimality equation holds, and thus that the policy and the value function are optimal.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="50%" height="50%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good 
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimal Policies with Dynamic Programming</title>
    <link href="https://zhangruochi.com/Optimal-Policies-with-Dynamic-Programming/2020/09/10/"/>
    <id>https://zhangruochi.com/Optimal-Policies-with-Dynamic-Programming/2020/09/10/</id>
    <published>2020-09-10T06:17:39.000Z</published>
    <updated>2020-09-10T06:18:14.325Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Optimal-Policies-with-Dynamic-Programming"><a href="#Assignment-2-Optimal-Policies-with-Dynamic-Programming" class="headerlink" title="Assignment 2: Optimal Policies with Dynamic Programming"></a>Assignment 2: Optimal Policies with Dynamic Programming</h1><p>Welcome to Assignment 2. This notebook will help you understand:</p><ul><li>Policy Evaluation and Policy Improvement.</li><li>Value and Policy Iteration.</li><li>Bellman Equations.</li></ul><h2 id="Gridworld-City"><a href="#Gridworld-City" class="headerlink" title="Gridworld City"></a>Gridworld City</h2><p>Gridworld City, a thriving metropolis with a booming technology industry, has recently experienced an influx of grid-loving software engineers. Unfortunately, the city’s street parking system, which charges a fixed rate, is struggling to keep up with the increased demand. To address this, the city council has decided to modify the pricing scheme to better promote social welfare. In general, the city considers social welfare higher when more parking is being used, the exception being that the city prefers that at least one spot is left unoccupied (so that it is available in case someone really needs it). The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. Now the city has hired you &mdash; an expert in dynamic programming &mdash; to help determine an optimal policy.</p><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><p>You’ll need two imports to complete this assigment:</p><ul><li>numpy: The fundamental package for scientific computing with Python.</li><li>tools: A module containing an environment and a plotting function.</li></ul><p>There are also some other lines in the cell below that are used for grading and plotting &mdash; you needn’t worry about them.</p><p>In this notebook, all cells are locked except those that you are explicitly asked to modify. It is up to you to decide how to implement your solution in these cells, <strong>but please do not import other libraries</strong> &mdash; doing so will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tools</span><br><span class="line"><span class="keyword">import</span> grader</span><br></pre></td></tr></table></figure><pre><code>&lt;Figure size 432x288 with 0 Axes&gt;</code></pre><p>In the city council’s parking MDP, states are nonnegative integers indicating how many parking spaces are occupied, actions are nonnegative integers designating the price of street parking, the reward is a real value describing the city’s preference for the situation, and time is discretized by hour. As might be expected, charging a high price is likely to decrease occupancy over the hour, while charging a low price is likely to increase it.</p><p>For now, let’s consider an environment with three parking spaces and three price points. Note that an environment with three parking spaces actually has four states &mdash; zero, one, two, or three spaces could be occupied.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">num_spaces = <span class="number">3</span></span><br><span class="line">num_prices = <span class="number">3</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">pi = np.ones((num_spaces + <span class="number">1</span>, num_prices)) / num_prices</span><br></pre></td></tr></table></figure><p>The value function is a one-dimensional array where the $i$-th entry gives the value of $i$ spaces being occupied.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">V</span><br></pre></td></tr></table></figure><pre><code>array([0., 0., 0., 0.])</code></pre><p>We can represent the policy as a two-dimensional array where the $(i, j)$-th entry gives the probability of taking action $j$ in state $i$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pi</span><br></pre></td></tr></table></figure><pre><code>array([[0.33333333, 0.33333333, 0.33333333],       [0.33333333, 0.33333333, 0.33333333],       [0.33333333, 0.33333333, 0.33333333],       [0.33333333, 0.33333333, 0.33333333]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pi[<span class="number">0</span>] = [<span class="number">0.75</span>, <span class="number">0.11</span>, <span class="number">0.14</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s, pi_s <span class="keyword">in</span> enumerate(pi):</span><br><span class="line">    <span class="keyword">for</span> a, p <span class="keyword">in</span> enumerate(pi_s):</span><br><span class="line">        print(<span class="string">f'pi(A=<span class="subst">&#123;a&#125;</span>|S=<span class="subst">&#123;s&#125;</span>) = <span class="subst">&#123;p.round(<span class="number">2</span>)&#125;</span>    '</span>, end=<span class="string">''</span>)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure><pre><code>pi(A=0|S=0) = 0.75    pi(A=1|S=0) = 0.11    pi(A=2|S=0) = 0.14    pi(A=0|S=1) = 0.33    pi(A=1|S=1) = 0.33    pi(A=2|S=1) = 0.33    pi(A=0|S=2) = 0.33    pi(A=1|S=2) = 0.33    pi(A=2|S=2) = 0.33    pi(A=0|S=3) = 0.33    pi(A=1|S=3) = 0.33    pi(A=2|S=3) = 0.33    </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">V[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><p>We can visualize a value function and policy with the <code>plot</code> function in the <code>tools</code> module. On the left, the value function is displayed as a barplot. State zero has an expected return of ten, while the other states have an expected return of zero. On the right, the policy is displayed on a two-dimensional grid. Each vertical strip gives the policy at the labeled state. In state zero, action zero is the darkest because the agent’s policy makes this choice with the highest probability. In the other states the agent has the equiprobable policy, so the vertical strips are colored uniformly.</p><p>You can access the state space and the action set as attributes of the environment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.S</span><br></pre></td></tr></table></figure><pre><code>[0, 1, 2, 3]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.A</span><br></pre></td></tr></table></figure><pre><code>[0, 1, 2]</code></pre><p>You will need to use the environment’s <code>transitions</code> method to complete this assignment. The method takes a state and an action and returns a 2-dimensional array, where the entry at $(i, 0)$ is the reward for transitioning to state $i$ from the current state and the entry at $(i, 1)$ is the conditional probability of transitioning to state $i$ given the current state and action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">state = <span class="number">3</span></span><br><span class="line">action = <span class="number">1</span></span><br><span class="line">transitions = env.transitions(state, action)</span><br><span class="line">transitions</span><br></pre></td></tr></table></figure><pre><code>array([[1.        , 0.12390437],       [2.        , 0.15133714],       [3.        , 0.1848436 ],       [2.        , 0.53991488]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sp, (r, p) <span class="keyword">in</span> enumerate(transitions):</span><br><span class="line">    print(<span class="string">f'p(S\'=<span class="subst">&#123;sp&#125;</span>, R=<span class="subst">&#123;r&#125;</span> | S=<span class="subst">&#123;state&#125;</span>, A=<span class="subst">&#123;action&#125;</span>) = <span class="subst">&#123;p.round(<span class="number">2</span>)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>p(S&#39;=0, R=1.0 | S=3, A=1) = 0.12p(S&#39;=1, R=2.0 | S=3, A=1) = 0.15p(S&#39;=2, R=3.0 | S=3, A=1) = 0.18p(S&#39;=3, R=2.0 | S=3, A=1) = 0.54</code></pre><h2 id="Section-1-Policy-Evaluation"><a href="#Section-1-Policy-Evaluation" class="headerlink" title="Section 1: Policy Evaluation"></a>Section 1: Policy Evaluation</h2><p>You’re now ready to begin the assignment! First, the city council would like you to evaluate the quality of the existing pricing scheme. Policy evaluation works by iteratively applying the Bellman equation for $v_{\pi}$ to a working value function, as an update rule, as shown below.</p><script type="math/tex; mode=display">\large v(s) \leftarrow \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a)[r + \gamma v(s')]</script><p>This update can either occur “in-place” (i.e. the update rule is sequentially applied to each state) or with “two-arrays” (i.e. the update rule is simultaneously applied to each state). Both versions converge to $v_{\pi}$ but the in-place version usually converges faster. <strong>In this assignment, we will be implementing all update rules in-place</strong>, as is done in the pseudocode of chapter 4 of the textbook. </p><p>We have written an outline of the policy evaluation algorithm described in chapter 4.1 of the textbook. It is left to you to fill in the <code>bellman_update</code> function to complete the algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_policy</span><span class="params">(env, V, pi, gamma, theta)</span>:</span></span><br><span class="line">    delta = float(<span class="string">'inf'</span>)</span><br><span class="line">    <span class="keyword">while</span> delta &gt; theta:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            bellman_update(env, V, pi, s, gamma)</span><br><span class="line">            delta = max(delta, abs(v - V[s]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> V</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bellman_update</span><span class="params">(env, V, pi, s, gamma)</span>:</span></span><br><span class="line">    <span class="string">"""Mutate ``V`` according to the Bellman update equation."""</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    actions = pi[s]</span><br><span class="line">    G = [<span class="number">0</span>] * len(actions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, action)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> s_, (r,p) <span class="keyword">in</span> enumerate(transitions):</span><br><span class="line">            G[action] += p * (r + gamma * V[s_])</span><br><span class="line">            </span><br><span class="line">    V[s] = np.sum(G * actions)</span><br></pre></td></tr></table></figure><p>The cell below uses the policy evaluation algorithm to evaluate the city’s policy, which charges a constant price of one.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set up test environment</span></span><br><span class="line">num_spaces = <span class="number">10</span></span><br><span class="line">num_prices = <span class="number">4</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build test policy</span></span><br><span class="line">city_policy = np.zeros((num_spaces + <span class="number">1</span>, num_prices))</span><br><span class="line">city_policy[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">V = evaluate_policy(env, V, city_policy, gamma, theta)</span><br><span class="line"></span><br><span class="line">print(V)</span><br></pre></td></tr></table></figure><pre><code>[80.04173399 81.65532303 83.37394007 85.12975566 86.87174913 88.55589131 90.14020422 91.58180605 92.81929841 93.78915889 87.77792991]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set up test environment</span></span><br><span class="line">num_spaces = <span class="number">10</span></span><br><span class="line">num_prices = <span class="number">4</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build test policy</span></span><br><span class="line">city_policy = np.zeros((num_spaces + <span class="number">1</span>, num_prices))</span><br><span class="line">city_policy[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">V = evaluate_policy(env, V, city_policy, gamma, theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test the value function</span></span><br><span class="line">answer = [<span class="number">80.04</span>, <span class="number">81.65</span>, <span class="number">83.37</span>, <span class="number">85.12</span>, <span class="number">86.87</span>, <span class="number">88.55</span>, <span class="number">90.14</span>, <span class="number">91.58</span>, <span class="number">92.81</span>, <span class="number">93.78</span>, <span class="number">87.77</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the value function is within 2 decimal places of the correct answer</span></span><br><span class="line"><span class="keyword">assert</span> grader.near(V, answer, <span class="number">1e-2</span>)</span><br></pre></td></tr></table></figure><p>You can use the <code>plot</code> function to visualize the final value function and policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line">tools.plot(V, city_policy)</span><br></pre></td></tr></table></figure><p><img src="output_26_0.png" alt="png"></p><p>Observe that the value function qualitatively resembles the city council’s preferences &mdash; it monotonically increases as more parking is used, until there is no parking left, in which case the value is lower. Because of the relatively simple reward function (more reward is accrued when many but not all parking spots are taken and less reward is accrued when few or all parking spots are taken) and the highly stochastic dynamics function (each state has positive probability of being reached each time step) the value functions of most policies will qualitatively resemble this graph. However, depending on the intelligence of the policy, the scale of the graph will differ. In other words, better policies will increase the expected return at every state rather than changing the relative desirability of the states. Intuitively, the value of a less desirable state can be increased by making it less likely to remain in a less desirable state. Similarly, the value of a more desirable state can be increased by making it more likely to remain in a more desirable state. That is to say, good policies are policies that spend more time in desirable states and less time in undesirable states. As we will see in this assignment, such a steady state distribution is achieved by setting the price to be low in low occupancy states (so that the occupancy will increase) and setting the price high when occupancy is high (so that full occupancy will be avoided).</p><h2 id="Section-2-Policy-Iteration"><a href="#Section-2-Policy-Iteration" class="headerlink" title="Section 2: Policy Iteration"></a>Section 2: Policy Iteration</h2><p>Now the city council would like you to compute a more efficient policy using policy iteration. Policy iteration works by alternating between evaluating the existing policy and making the policy greedy with respect to the existing value function. We have written an outline of the policy iteration algorithm described in chapter 4.3 of the textbook. We will make use of the policy evaluation algorithm you completed in section 1. It is left to you to fill in the <code>q_greedify_policy</code> function, such that it modifies the policy at $s$ to be greedy with respect to the q-values at $s$, to complete the policy improvement algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">improve_policy</span><span class="params">(env, V, pi, gamma)</span>:</span></span><br><span class="line">    policy_stable = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        old = pi[s].copy()</span><br><span class="line">        q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.array_equal(pi[s], old):</span><br><span class="line">            policy_stable = <span class="keyword">False</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> pi, policy_stable</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_iteration</span><span class="params">(env, gamma, theta)</span>:</span></span><br><span class="line">    V = np.zeros(len(env.S))</span><br><span class="line">    pi = np.ones((len(env.S), len(env.A))) / len(env.A)</span><br><span class="line">    policy_stable = <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> policy_stable:</span><br><span class="line">        V = evaluate_policy(env, V, pi, gamma, theta)</span><br><span class="line">        pi, policy_stable = improve_policy(env, V, pi, gamma)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_greedify_policy</span><span class="params">(env, V, pi, s, gamma)</span>:</span></span><br><span class="line">    <span class="string">"""Mutate ``pi`` to be greedy with respect to the q-values induced by ``V``."""</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    G = [<span class="number">0</span>] * len(env.A)</span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, action)</span><br><span class="line">        <span class="keyword">for</span> s_, (r,p) <span class="keyword">in</span> enumerate(transitions):</span><br><span class="line">            G[action] += p * (r + gamma * V[s_])</span><br><span class="line">            </span><br><span class="line">    best_a = np.argmax(G)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,_ <span class="keyword">in</span> enumerate(pi[s]):</span><br><span class="line">        <span class="keyword">if</span> i == best_a:</span><br><span class="line">            pi[s][i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pi[s][i] = <span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">6</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">pi = np.ones((<span class="number">7</span>, <span class="number">4</span>)) / <span class="number">4</span></span><br><span class="line"></span><br><span class="line">new_pi, stable = improve_policy(env, V, pi, gamma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># expect first call to greedify policy</span></span><br><span class="line">expected_pi = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(new_pi == expected_pi)</span><br><span class="line"><span class="keyword">assert</span> stable == <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the value function has not changed, so the greedy policy should not change</span></span><br><span class="line">new_pi, stable = improve_policy(env, V, new_pi, gamma)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(new_pi == expected_pi)</span><br><span class="line"><span class="keyword">assert</span> stable == <span class="keyword">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V, pi = policy_iteration(env, gamma, theta)</span><br><span class="line"></span><br><span class="line">V_answer = [<span class="number">81.60</span>, <span class="number">83.28</span>, <span class="number">85.03</span>, <span class="number">86.79</span>, <span class="number">88.51</span>, <span class="number">90.16</span>, <span class="number">91.70</span>, <span class="number">93.08</span>, <span class="number">94.25</span>, <span class="number">95.25</span>, <span class="number">89.45</span>]</span><br><span class="line">pi_answer = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure value function is within 2 decimal places of answer</span></span><br><span class="line"><span class="keyword">assert</span> grader.near(V, V_answer, <span class="number">1e-2</span>)</span><br><span class="line"><span class="comment"># make sure policy is exactly correct</span></span><br><span class="line"><span class="keyword">assert</span> np.all(pi == pi_answer)</span><br></pre></td></tr></table></figure><p>When you are ready to test the policy iteration algorithm, run the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = policy_iteration(env, gamma, theta)</span><br></pre></td></tr></table></figure><p>You can use the <code>plot</code> function to visualize the final value function and policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure><p><img src="output_36_0.png" alt="png"></p><p>You can check the value function (rounded to one decimal place) and policy against the answer below:<br><br>State $\quad\quad$    Value $\quad\quad$ Action<br><br>0 $\quad\quad\quad\;$        81.6 $\quad\quad\;$ 0<br><br>1 $\quad\quad\quad\;$        83.3 $\quad\quad\;$ 0<br><br>2 $\quad\quad\quad\;$        85.0 $\quad\quad\;$ 0<br><br>3 $\quad\quad\quad\;$        86.8 $\quad\quad\;$ 0<br><br>4 $\quad\quad\quad\;$        88.5 $\quad\quad\;$ 0<br><br>5 $\quad\quad\quad\;$        90.2 $\quad\quad\;$ 0<br><br>6 $\quad\quad\quad\;$        91.7 $\quad\quad\;$ 0<br><br>7 $\quad\quad\quad\;$        93.1 $\quad\quad\;$ 0<br><br>8 $\quad\quad\quad\;$        94.3 $\quad\quad\;$ 0<br><br>9 $\quad\quad\quad\;$        95.3 $\quad\quad\;$ 3<br><br>10 $\quad\quad\;\;\,\,$      89.5 $\quad\quad\;$ 3<br></p><h2 id="Section-3-Value-Iteration"><a href="#Section-3-Value-Iteration" class="headerlink" title="Section 3: Value Iteration"></a>Section 3: Value Iteration</h2><p>The city has also heard about value iteration and would like you to implement it. Value iteration works by iteratively applying the Bellman optimality equation for $v_{\ast}$ to a working value function, as an update rule, as shown below.</p><script type="math/tex; mode=display">\large v(s) \leftarrow \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v(s')]</script><p>We have written an outline of the value iteration algorithm described in chapter 4.4 of the textbook. It is left to you to fill in the <code>bellman_optimality_update</code> function to complete the value iteration algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_iteration</span><span class="params">(env, gamma, theta)</span>:</span></span><br><span class="line">    V = np.zeros(len(env.S))</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            bellman_optimality_update(env, V, s, gamma)</span><br><span class="line">            delta = max(delta, abs(v - V[s]))</span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    pi = np.ones((len(env.S), len(env.A))) / len(env.A)</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bellman_optimality_update</span><span class="params">(env, V, s, gamma)</span>:</span></span><br><span class="line">    <span class="string">"""Mutate ``V`` according to the Bellman optimality update equation."""</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    G = np.zeros(len(env.A))    </span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, a)</span><br><span class="line">        <span class="keyword">for</span> s_, (r, p) <span class="keyword">in</span> enumerate(transitions):</span><br><span class="line">            G[a] += p*(r + gamma*V[s_])</span><br><span class="line">    V[s] = np.max(G)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">6</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># only state 0 updated</span></span><br><span class="line">bellman_optimality_update(env, V, <span class="number">0</span>, gamma)</span><br><span class="line"><span class="keyword">assert</span> list(V) == [<span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># only state 2 updated</span></span><br><span class="line">bellman_optimality_update(env, V, <span class="number">2</span>, gamma)</span><br><span class="line"><span class="keyword">assert</span> list(V) == [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        bellman_optimality_update(env, V, s, gamma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure value function is exactly correct</span></span><br><span class="line">answer = [<span class="number">61</span>, <span class="number">63</span>, <span class="number">65</span>, <span class="number">67</span>, <span class="number">69</span>, <span class="number">71</span>, <span class="number">72</span>, <span class="number">74</span>, <span class="number">75</span>, <span class="number">76</span>, <span class="number">71</span>]</span><br><span class="line"><span class="keyword">assert</span> np.all(V == answer)</span><br></pre></td></tr></table></figure><p>When you are ready to test the value iteration algorithm, run the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = value_iteration(env, gamma, theta)</span><br></pre></td></tr></table></figure><p>You can use the <code>plot</code> function to visualize the final value function and policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure><p><img src="output_46_0.png" alt="png"></p><p>You can check your value function (rounded to one decimal place) and policy against the answer below:<br><br>State $\quad\quad$    Value $\quad\quad$ Action<br><br>0 $\quad\quad\quad\;$        81.6 $\quad\quad\;$ 0<br><br>1 $\quad\quad\quad\;$        83.3 $\quad\quad\;$ 0<br><br>2 $\quad\quad\quad\;$        85.0 $\quad\quad\;$ 0<br><br>3 $\quad\quad\quad\;$        86.8 $\quad\quad\;$ 0<br><br>4 $\quad\quad\quad\;$        88.5 $\quad\quad\;$ 0<br><br>5 $\quad\quad\quad\;$        90.2 $\quad\quad\;$ 0<br><br>6 $\quad\quad\quad\;$        91.7 $\quad\quad\;$ 0<br><br>7 $\quad\quad\quad\;$        93.1 $\quad\quad\;$ 0<br><br>8 $\quad\quad\quad\;$        94.3 $\quad\quad\;$ 0<br><br>9 $\quad\quad\quad\;$        95.3 $\quad\quad\;$ 3<br><br>10 $\quad\quad\;\;\,\,$      89.5 $\quad\quad\;$ 3<br></p><p>In the value iteration algorithm above, a policy is not explicitly maintained until the value function has converged. Below, we have written an identically behaving value iteration algorithm that maintains an updated policy. Writing value iteration in this form makes its relationship to policy iteration more evident. Policy iteration alternates between doing complete greedifications and complete evaluations. On the other hand, value iteration alternates between doing local greedifications and local evaluations. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_iteration2</span><span class="params">(env, gamma, theta)</span>:</span></span><br><span class="line">    V = np.zeros(len(env.S))</span><br><span class="line">    pi = np.ones((len(env.S), len(env.A))) / len(env.A)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">            bellman_update(env, V, pi, s, gamma)</span><br><span class="line">            delta = max(delta, abs(v - V[s]))</span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure><p>You can try the second value iteration algorithm by running the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = value_iteration2(env, gamma, theta)</span><br><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure><p><img src="output_51_0.png" alt="png"></p><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations, you’ve completed assignment 2! In this assignment, we investigated policy evaluation and policy improvement, policy iteration and value iteration, and Bellman updates. Gridworld City thanks you for your service!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Optimal-Policies-with-Dynamic-Programming&quot;&gt;&lt;a href=&quot;#Assignment-2-Optimal-Policies-with-Dynamic-Programming&quot; class=&quot;hea
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Markov Decision Processes II</title>
    <link href="https://zhangruochi.com/Markov-Decision-Processes-II/2020/09/06/"/>
    <id>https://zhangruochi.com/Markov-Decision-Processes-II/2020/09/06/</id>
    <published>2020-09-06T11:36:56.000Z</published>
    <updated>2020-09-08T10:08:41.368Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lesson-1-Policies-and-Value-Functions"><a href="#Lesson-1-Policies-and-Value-Functions" class="headerlink" title="Lesson 1: Policies and Value Functions"></a>Lesson 1: Policies and Value Functions</h2><h3 id="Recognize-that-a-policy-is-a-distribution-over-actions-for-each-possible-state"><a href="#Recognize-that-a-policy-is-a-distribution-over-actions-for-each-possible-state" class="headerlink" title="Recognize that a policy is a distribution over actions for each possible state."></a>Recognize that a policy is a distribution over actions for each possible state.</h3><p>a policy is a mapping from states to probabilities of selecting each possible action.  If the agent is following policy $\pi$ at time $t$, then $\pi(a | s)$ is the probability that $A_t = a$ if $S_t = s$</p><h3 id="Describe-the-similarities-and-differences-between-stochastic-and-deterministic-policies"><a href="#Describe-the-similarities-and-differences-between-stochastic-and-deterministic-policies" class="headerlink" title="Describe the similarities and differences between stochastic and deterministic policies"></a>Describe the similarities and differences between stochastic and deterministic policies</h3><p>Deterministic policies: a policy assigns probabilities to each action in each state.</p><p>Stochastic policy: a policy where multiple actions may be selected with non-zero probability.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="50%" height="50%"></center><h3 id="Identify-the-characteristics-of-a-well-defined-policy"><a href="#Identify-the-characteristics-of-a-well-defined-policy" class="headerlink" title="Identify the characteristics of a well-defined policy"></a>Identify the characteristics of a well-defined policy</h3><ul><li>An agent’s behavior is specified by a policy that maps the state to a probability distribution over actions</li><li>The policy can depend only on the current state, and not other things like time or previous states. See you next time.</li></ul><h3 id="Describe-the-roles-of-state-value-and-action-value-functions-in-reinforcement-learning"><a href="#Describe-the-roles-of-state-value-and-action-value-functions-in-reinforcement-learning" class="headerlink" title="Describe the roles of state-value and action-value functions in reinforcement learning"></a>Describe the roles of state-value and action-value functions in reinforcement learning</h3><p>Similarly, we define the value of taking action a in state s under a policy $\pi$, denoted $q_{\pi}(s,a)$,  as the expected return starting from $s$,  taking the action a, and thereafter following policy $\pi$:</p><script type="math/tex; mode=display">q_{\pi}(s) = \mathbb{E}_{\pi}[ G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi}[ \sum_{k=0}^{\infty} \eta^k R_{t+k+1} | S_t = s, A_t = a ]</script><p>We call $q_{\pi}$ the action-value function for policy $\pi$.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">the roles of state-value and action-value functions</div></center><h3 id="Describe-the-relationship-between-value-functions-and-policies"><a href="#Describe-the-relationship-between-value-functions-and-policies" class="headerlink" title="Describe the relationship between value functions and policies"></a>Describe the relationship between value functions and policies</h3><p>Value function enable us to judge the quality of different policies.</p><p>The value function of a state s under a policy $\pi$, denoted $V_\pi(s)$, is the expected return when starting in s and following $\pi$ thereafter. For MDPs, we can define $V_{\pi}$ formally by</p><script type="math/tex; mode=display">V_{\pi}(s) = \mathbb{E}[ G_t | S_t = s] = \mathbb{E}_{\pi}[ \sum_{k=0}^{\infty} \eta^k R_{t+k+1} | S_t = s ], \text{for all} \quad s \in \mathbb{S}</script><p>where $\mathbb{E}[\dot]$ denotes the expected value of a random variable given that the agent follows policy $\pi$, and t is any time step.</p><h3 id="Create-examples-of-valid-value-functions-for-a-given-MDP"><a href="#Create-examples-of-valid-value-functions-for-a-given-MDP" class="headerlink" title="Create examples of valid value functions for a given MDP"></a>Create examples of valid value functions for a given MDP</h3><h2 id="Lesson-2-Bellman-Equations"><a href="#Lesson-2-Bellman-Equations" class="headerlink" title="Lesson 2: Bellman Equations"></a>Lesson 2: Bellman Equations</h2><h3 id="Derive-the-Bellman-equation-for-state-value-functions"><a href="#Derive-the-Bellman-equation-for-state-value-functions" class="headerlink" title="Derive the Bellman equation for state-value functions"></a>Derive the Bellman equation for state-value functions</h3><script type="math/tex; mode=display">V_{\pi}(s) = \mathbb{E}[ G_t | S_t = s] = \sum_a \pi (a | s)\sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\pi}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}</script><h3 id="Derive-the-Bellman-equation-for-action-value-functions"><a href="#Derive-the-Bellman-equation-for-action-value-functions" class="headerlink" title="Derive the Bellman equation for action-value functions"></a>Derive the Bellman equation for action-value functions</h3><script type="math/tex; mode=display">q_{\pi}(s,a) = \mathbb{E}_{\pi}[ G_t | S_t = s, A_t = a] = \sum_{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta\sum_{a\prime} \pi (a\prime | q_{\pi}(s\prime, a\prime))]</script><h3 id="Understand-how-Bellman-equations-relate-current-and-future-values"><a href="#Understand-how-Bellman-equations-relate-current-and-future-values" class="headerlink" title="Understand how Bellman equations relate current and future values"></a>Understand how Bellman equations relate current and future values</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><p>The current time-step’s state/action values can be written recursivelu in terms of future state/action values</p><p>Bellman equation for $V_{\pi}$,  It expresses a relationship between the value of a state and the values of its successor states. Think of looking ahead from a state to its possible successor states, as suggested by the diagram to the right. Each open circle represents a state and each solid circle represents a state–action pair. Starting from state s, the root node at the top, the agent could take any of some set of actions—three are shown in the diagram—based on its policy $\pi$.  From each of these, the environment could respond with one of several next states, $s\prime$ (two are shown in the figure), along with a reward, r, depending on its dynamics given by the function $p$. The Bellman equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the(discounted) value of the expected next state, plus the reward expected along the way.</p><p>We call diagrams like that above backup diagrams because they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. These operations transfer value information back to a state (or a state–action pair) from its successor states (or state–action pairs)</p><h3 id="Use-the-Bellman-equations-to-compute-value-functions"><a href="#Use-the-Bellman-equations-to-compute-value-functions" class="headerlink" title="Use the Bellman equations to compute value functions"></a>Use the Bellman equations to compute value functions</h3><p>The value function  is the unique solution to its Bellman equation.</p><h2 id="Lesson-3-Optimality-Optimal-Policies-amp-Value-Functions"><a href="#Lesson-3-Optimality-Optimal-Policies-amp-Value-Functions" class="headerlink" title="Lesson 3: Optimality (Optimal Policies &amp; Value Functions)"></a>Lesson 3: Optimality (Optimal Policies &amp; Value Functions)</h2><h3 id="Define-an-optimal-policy-understand-how-a-policy-can-be-at-least-as-good-as-every-other-policy-in-every-state"><a href="#Define-an-optimal-policy-understand-how-a-policy-can-be-at-least-as-good-as-every-other-policy-in-every-state" class="headerlink" title="Define an optimal policy, understand how a policy can be at least as good as every other policy in every state"></a>Define an optimal policy, understand how a policy can be at least as good as every other policy in every state</h3><p>For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies.  A policy $\pi$ is defined to be better than or equal to a policy $\pi\prime$ if its expected return is greater than or equal to that of $\pi\prime$ for all states.  In other words, $\pi \geq \pi\prime$ if and only if $V_{\pi}(s) \geq V_{\pi\prime}(s)$ for all $s \in \mathbb{S}$. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the optimal state-value function, denoted $V_{\star}$, and defined as</p><script type="math/tex; mode=display">V_{\star} = max_{\pi}V_{\pi}(s) \quad \text{for all} \quad s \in \mathbb{S}</script><p>Optimal policies also share the same optimal action-value function, denoted $q_{\star}$, and defined as</p><script type="math/tex; mode=display">q_{\star}(s,a) = max_{\pi} q_{\pi}(s,a) \quad \text{for all} \quad s \in \mathbb{S} \, \text{and} \, a \in \mathbb{A}(s)</script><p>For the state–action pair (s, a), this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_{\star}$ in terms of $V_{\star}$ as follows:</p><script type="math/tex; mode=display">q_{\star}(s,a) = \mathbb{E}[ R_{t+1} + \eta v_{\star}(S_{t+1}) | S_t = s, A_t = a]</script><h3 id="Derive-the-Bellman-optimality-equation-for-state-value-functions"><a href="#Derive-the-Bellman-optimality-equation-for-state-value-functions" class="headerlink" title="Derive the Bellman optimality equation for state-value functions"></a>Derive the Bellman optimality equation for state-value functions</h3><script type="math/tex; mode=display">V_{\star}(s) = \sum_a \pi_{\star} (a | s)\sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\star}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}</script><script type="math/tex; mode=display">V_{\star}(s) = max_a \sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\star}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}</script><h3 id="Derive-the-Bellman-optimality-equation-for-action-value-functions"><a href="#Derive-the-Bellman-optimality-equation-for-action-value-functions" class="headerlink" title="Derive the Bellman optimality equation for action-value functions"></a>Derive the Bellman optimality equation for action-value functions</h3><script type="math/tex; mode=display">q_{\star}(s,a) = \sum_{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta\sum_{a\prime} \pi_{\star} (a\prime | q_{\star}(s\prime, a\prime))]</script><script type="math/tex; mode=display">q_{\star}(s,a) = \sum_{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta max_{a\prime}q_{\star}(s\prime, a\prime))]</script><h3 id="Understand-the-connection-between-the-optimal-value-function-and-optimal-policies"><a href="#Understand-the-connection-between-the-optimal-value-function-and-optimal-policies" class="headerlink" title="Understand the connection between the optimal value function and optimal policies"></a>Understand the connection between the optimal value function and optimal policies</h3><p>Once we had the optimal state-value function, it’s relatively easy to work out the optimal policy. If we have the optimal action-value function, working out the optimal policy is even easier. This correspondence between optimal-value functions and optimal-policies will help us to derive many of the reinforced learning algorithms we will explore later in this specialization.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lesson-1-Policies-and-Value-Functions&quot;&gt;&lt;a href=&quot;#Lesson-1-Policies-and-Value-Functions&quot; class=&quot;headerlink&quot; title=&quot;Lesson 1: Policies
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Markov Decision Processes I</title>
    <link href="https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/"/>
    <id>https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/</id>
    <published>2020-09-04T03:37:10.000Z</published>
    <updated>2020-09-07T05:55:54.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lesson-1-Introduction-to-Markov-Decision-Processes"><a href="#Lesson-1-Introduction-to-Markov-Decision-Processes" class="headerlink" title="Lesson 1: Introduction to Markov Decision Processes"></a>Lesson 1: Introduction to Markov Decision Processes</h2><h3 id="Understand-Markov-Decision-Processes-or-MDPs"><a href="#Understand-Markov-Decision-Processes-or-MDPs" class="headerlink" title="Understand Markov Decision Processes, or MDPs"></a>Understand Markov Decision Processes, or MDPs</h3><p>MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards.</p><h3 id="Understand-the-graphical-representation-of-a-Markov-Decision-Process"><a href="#Understand-the-graphical-representation-of-a-Markov-Decision-Process" class="headerlink" title="Understand the graphical representation of a Markov Decision Process"></a>Understand the graphical representation of a Markov Decision Process</h3><p>MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent.1 The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time<br>through its choice of actions.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">The agent–environment interaction in a Markov decision process.</div></center><h3 id="Describe-how-the-dynamics-of-an-MDP-are-defined"><a href="#Describe-how-the-dynamics-of-an-MDP-are-defined" class="headerlink" title="Describe how the dynamics of an MDP are defined"></a>Describe how the dynamics of an MDP are defined</h3><p>In a finite MDP, the sets of states, actions, and rewards (S, A and R) all  have a finite number of elements. In this case, the random variables $R_t$ and $S_t$ have well defined<br>discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s\prime \in S$ and $r \in R$, there is a probability of those values occurring at time t, given particular values of the preceding state and action:</p><script type="math/tex; mode=display">p(s\prime, r | s, a) = Pr\{S_t = s\prime, R_t = r | S_{t-1} = s, A_{t-1} = a \}</script><script type="math/tex; mode=display">\sum_{s\prime \in s}\sum_{r \in R} p(s\prime, r | s, a) = 1, \text{for all} \, s\in S, a \in A_{(s)}</script><p>for all $s\prime, s \in S, r \in R \text{and} a \in A_{(s)}$.  The function p defines the dynamics of the MDP. The dot over the equals sign in the equation reminds us that it is a definition (in this case of the function p) rather than a fact that follows from previous definitions. The dynamics function $p: S x R x S x A \rightarrow [0, 1]$ is an ordinary deterministic function of four arguments.</p><p>In a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for $S_t$ and $A_t$, and, given them, not at all on earlier states and actions. This is best viewed a restriction not on the decision process, but on the state.  The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property.</p><p>From the four-argument dynamics function, p, one can compute anything else one might want to know about the environment, such as the <strong>state-transition probabilities</strong> (which we denote, with a slight abuse of notation, as a three-argument function<br>$p: S x S x A \rightarrow [0, 1]$)</p><script type="math/tex; mode=display">p(s\prime | s, a) = Pr\{S_t = s\prime | S_{t-1} = s, A_{t-1} = a \} = \sum_{r \in R}p(s\prime, r | s, a)</script><p>We can also compute the expected rewards for state–action pairs as a two-argument function $r : S x A \rightarrow \mathbb{R}$</p><script type="math/tex; mode=display">r(s,a) = \mathbb{R} [R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R} r \sum_{s\prime \in S} p(s\prime, r | s, a)</script><p>and the expected rewards for <strong>state–action–next-state</strong> triples as a three-argument function $r: S x A x S \rightarrow \mathbb{R}$</p><script type="math/tex; mode=display">r(s,a,s\prime) = \mathbb{E} [R_t | S_{t-1} = s, A_{t-1} = a, S_t = s\prime] = \sum_{r \in R} r \frac{ p(s\prime, r | s, a) }{p(s\prime | s, a)}</script><h3 id="Explain-how-many-diverse-processes-can-be-written-in-terms-of-the-MDP-framework"><a href="#Explain-how-many-diverse-processes-can-be-written-in-terms-of-the-MDP-framework" class="headerlink" title="Explain how many diverse processes can be written in terms of the MDP framework"></a>Explain how many diverse processes can be written in terms of the MDP framework</h3><p>The MDP framework is abstract and flexible and can be applied to many di↵erent problems in many di↵erent ways.  </p><ul><li>In general, actions can be any decisions we want to learn how to make, and<br>the states can be anything we can know that might be useful in making them. </li><li>The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.  </li><li>The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.</li></ul><p>The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the <strong>actions</strong>), one signal to represent the basis on which the choices are made (the <strong>states</strong>), and one signal to define the agent’s goal (the <strong>rewards</strong>).</p><h2 id="Lesson-2-Goal-of-Reinforcement-Learning"><a href="#Lesson-2-Goal-of-Reinforcement-Learning" class="headerlink" title="Lesson 2: Goal of Reinforcement Learning"></a>Lesson 2: Goal of Reinforcement Learning</h2><h3 id="Describe-how-rewards-relate-to-the-goal-of-an-agent"><a href="#Describe-how-rewards-relate-to-the-goal-of-an-agent" class="headerlink" title="Describe how rewards relate to the goal of an agent"></a>Describe how rewards relate to the goal of an agent</h3><p>Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis:</p><blockquote><p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p></blockquote><h3 id="Understand-episodes-and-identify-episodic-tasks"><a href="#Understand-episodes-and-identify-episodic-tasks" class="headerlink" title="Understand episodes and identify episodic tasks"></a>Understand episodes and identify episodic tasks</h3><p>when the agent–environment interaction breaks naturally into <strong>subsequences</strong>, which we call episodes,7 such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the <strong>terminal state</strong>, followed by a <strong>reset</strong> to a standard starting state or to a sample from a standard distribution of starting states.</p><p>In general, we seek to maximize the expected return, where the return, denoted $G_t$,  is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:</p><script type="math/tex; mode=display">G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_{T}</script><p>where T is a final time step.</p><h2 id="Lesson-3-Continuing-Tasks"><a href="#Lesson-3-Continuing-Tasks" class="headerlink" title="Lesson 3: Continuing Tasks"></a>Lesson 3: Continuing Tasks</h2><h3 id="Formulate-returns-for-continuing-tasks-using-discounting"><a href="#Formulate-returns-for-continuing-tasks-using-discounting" class="headerlink" title="Formulate returns for continuing tasks using discounting"></a>Formulate returns for continuing tasks using discounting</h3><p>In many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit.  For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation is problematic for continuing tasks because the final time step would be $T = \infty$ and the return, which is what we are trying to maximize, could itself easily be infinite.</p><p>The additional concept that we need is that of <strong>discounting</strong>. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_t$ to maximize the expected discounted return:</p><script type="math/tex; mode=display">G_t = R_{t+1} + \eta R_{t+2} + \eta^2 R_{t+3} + \cdots  = \sum_{k=0}^{\infty} \eta^k R_{t+k+1}</script><p>where \eta is a parameter, $ 0 \leq \eta \leq 1$, called the discount rate.</p><p>The discount rate determines the present value of future rewards: a reward received k time steps in the future is worth only $\eta^{k-1}$ times what it would be worth if it were received immediately. If $\eta &lt; 1$, the infinite sum has a finite value as long as the reward sequence $\{ R_k \}$ is <strong>bounded</strong>.</p><p>Returns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning:</p><script type="math/tex; mode=display">G_t =  R_{t+1} + \eta (R_{t+2} + \eta R_{t+3} + \eta^2 R_{t+4} + \cdots) = R_{t+1} + \eta G_{t+1}</script><h3 id="Describe-how-returns-at-successive-time-steps-are-related-to-each-other"><a href="#Describe-how-returns-at-successive-time-steps-are-related-to-each-other" class="headerlink" title="Describe how returns at successive time steps are related to each other"></a>Describe how returns at successive time steps are related to each other</h3><p>If $\eta = 0$, the agent is “myopic” in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose At so as to maximize only $R_{t+1}$. If each of the agent’s actions happened to influence only the immediate reward, not future rewards as well, then a myopic agent could maximize by separately maximizing each immediate reward. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced. As $\eta$ approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lesson-1-Introduction-to-Markov-Decision-Processes&quot;&gt;&lt;a href=&quot;#Lesson-1-Introduction-to-Markov-Decision-Processes&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>The K-Armed Bandit Problem</title>
    <link href="https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/"/>
    <id>https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/</id>
    <published>2020-09-03T10:55:55.000Z</published>
    <updated>2020-09-03T11:10:47.100Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lesson-1-The-K-Armed-Bandit-Problem"><a href="#Lesson-1-The-K-Armed-Bandit-Problem" class="headerlink" title="Lesson 1: The K-Armed Bandit Problem"></a>Lesson 1: The K-Armed Bandit Problem</h2><h3 id="Define-reward"><a href="#Define-reward" class="headerlink" title="Define reward"></a>Define reward</h3><p>In the k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the <strong>value</strong> of that action. We denote the action selected on time step t as $A_t$, and the corresponding reward as $R_t$. The value then<br>of an arbitrary action a, denoted $q\ast(a)$, is the expected reward given that a is selected:</p><script type="math/tex; mode=display">q\ast(a) = \mathbb{E} [R_t | A_t = a ]</script><p>We denote the estimated value of action a at time step t as $Q_t(a)$. We would like $Q_t(a)$ to be close<br>to $q\ast(a)$.</p><h3 id="Understand-the-temporal-nature-of-the-bandit-problem"><a href="#Understand-the-temporal-nature-of-the-bandit-problem" class="headerlink" title="Understand the temporal nature of the bandit problem"></a>Understand the temporal nature of the bandit problem</h3><h3 id="Define-k-armed-bandit"><a href="#Define-k-armed-bandit" class="headerlink" title="Define k-armed bandit"></a>Define k-armed bandit</h3><p>Consider the following learning problem. You are faced repeatedly with a choice among k di↵erent options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p><h3 id="Define-action-values"><a href="#Define-action-values" class="headerlink" title="Define action-values"></a>Define action-values</h3><p>We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods.</p><h2 id="Lesson-2-What-to-Learn-Estimating-Action-Values"><a href="#Lesson-2-What-to-Learn-Estimating-Action-Values" class="headerlink" title="Lesson 2: What to Learn? Estimating Action Values"></a>Lesson 2: What to Learn? Estimating Action Values</h2><h3 id="Define-action-value-estimation-methods"><a href="#Define-action-value-estimation-methods" class="headerlink" title="Define action-value estimation methods"></a>Define action-value estimation methods</h3><p>One natural way to estimate this is by averaging the rewards<br>actually received:</p><script type="math/tex; mode=display">Q_t(a) = \frac{\text{sum of rewards when a taken prior to t} }{\text{ number of times a taken prior to t} }</script><h3 id="Define-exploration-and-exploitation"><a href="#Define-exploration-and-exploitation" class="headerlink" title="Define exploration and exploitation"></a>Define exploration and exploitation</h3><p>If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are <strong>exploiting</strong> your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are <strong>exploring</strong>, because this enables you to improve your estimate of the nongreedy action’s value.</p><p>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</p><h3 id="Select-actions-greedily-using-an-action-value-function"><a href="#Select-actions-greedily-using-an-action-value-function" class="headerlink" title="Select actions greedily using an action-value function"></a>Select actions greedily using an action-value function</h3><script type="math/tex; mode=display">A_t = argmax_a Q_t(a)</script><h3 id="Define-online-learning"><a href="#Define-online-learning" class="headerlink" title="Define online learning"></a>Define online learning</h3><script type="math/tex; mode=display">Q_n = \frac{ R_1 + R_2 + \cdots + R_{n-1} }{ n - 1 }</script><p>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if this is done, then the memory and computational requirements would grow over time as more rewards are seen. Each additional reward would require additional memory to store it and additional computation to compute the sum in the numerator.</p><p>As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward.</p><script type="math/tex; mode=display">Q_(n+1) = Q_n + \frac{1}{n}[ R_n - Q_n ]</script><h3 id="Define-the-general-online-update-equation"><a href="#Define-the-general-online-update-equation" class="headerlink" title="Define the general online update equation"></a>Define the general online update equation</h3><script type="math/tex; mode=display">\text{NewEstimate} = \text{OldEstimate} + \text{StepSize} [ \text{target} - \text{OldEstimate} ]</script><h3 id="Understand-why-we-might-use-a-constant-stepsize-in-the-case-of-non-stationarity"><a href="#Understand-why-we-might-use-a-constant-stepsize-in-the-case-of-non-stationarity" class="headerlink" title="Understand why we might use a constant stepsize in the case of non-stationarity"></a>Understand why we might use a constant stepsize in the case of non-stationarity</h3><p>The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter.</p><script type="math/tex; mode=display">Q_(n+1) = Q_n + \alpha [ R_n - Q_n ]</script><p>where the step-size parameter $ \alpha \in (0, 1]$ is constant.</p><script type="math/tex; mode=display">Q_{n+1} = (1 - \alpha)^n Q_1 + \sum^{n}_{i=1}\alpha(1 - \alpha)^{n-i}R_i</script><p>Note that the weight, $\alpha(1 - \alpha)^{n-i}$ given the reward $R_i$ depends on how many rewards ago, $n-1$ it was observed. The quantity $1-\alpha$<br>is less than 1, and thus the weight given to $R_i$ decreases as the number of intervening rewards increases.</p><h2 id="Lesson-3-Exploration-vs-Exploitation-Tradeoff"><a href="#Lesson-3-Exploration-vs-Exploitation-Tradeoff" class="headerlink" title="Lesson 3: Exploration vs. Exploitation Tradeoff"></a>Lesson 3: Exploration vs. Exploitation Tradeoff</h2><h3 id="Define-epsilon-greedy"><a href="#Define-epsilon-greedy" class="headerlink" title="Define epsilon-greedy"></a>Define epsilon-greedy</h3><p>Greedy action selection always exploits current<br>knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability $\alpha$ instead select randomly from among all the actions with equal probability, independently of the action-value estimates.  We call methods using this near-greedy action selection rule $\alpha$-greedy methods.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,len(self.q_values))</span><br></pre></td></tr></table></figure><h3 id="Understand-optimistic-initial-values-Describe-the-benefits-of-optimistic-initial-values-for-early-exploration"><a href="#Understand-optimistic-initial-values-Describe-the-benefits-of-optimistic-initial-values-for-early-exploration" class="headerlink" title="Understand optimistic initial values, Describe the benefits of optimistic initial values for early exploration"></a>Understand optimistic initial values, Describe the benefits of optimistic initial values for early exploration</h3><p>Initial action values can also be used as a simple way to encourage exploration. Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed,we set them all to +5. Recall that the $q\ast(a)$ in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism encourages action-value methods to explore. Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being <strong>disappointed</strong> with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time.</p><p><img src="initial_values.png" alt></p><h3 id="Explain-the-criticisms-of-optimistic-initial-values"><a href="#Explain-the-criticisms-of-optimistic-initial-values" class="headerlink" title="Explain the criticisms of optimistic initial values"></a>Explain the criticisms of optimistic initial values</h3><p>We call this technique for encouraging exploration optimistic initial values. We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial conditions in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much. This criticism applies as well to the sample-average methods, which also treat the beginning of time as a special event, averaging all subsequent rewards with equal weights.</p><h3 id="Describe-the-upper-confidence-bound-action-selection-method"><a href="#Describe-the-upper-confidence-bound-action-selection-method" class="headerlink" title="Describe the upper confidence bound action selection method"></a>Describe the upper confidence bound action selection method</h3><p>It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.One effective way of doing this is to select actions according to</p><script type="math/tex; mode=display">A_t = argmax_a [ Q_t(a) + c\sqrt{\frac{\ln t}{ N_t(a)}} ]</script><h3 id="Define-optimism-in-the-face-of-uncertainty"><a href="#Define-optimism-in-the-face-of-uncertainty" class="headerlink" title="Define optimism in the face of uncertainty"></a>Define optimism in the face of uncertainty</h3><p>The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value  The quantity being max’ed over is thus a sort of upper bound on the possible true value of action $a$, with $c$ determining the confidence level. Each time $a$ is selected the uncertainty is presumably reduced: $N_t(a)$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, $t$ increases but $N_t(a)$ does not; because $t$ appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates,or that have already been selected frequently, will be selected with decreasing frequency over time.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lesson-1-The-K-Armed-Bandit-Problem&quot;&gt;&lt;a href=&quot;#Lesson-1-The-K-Armed-Bandit-Problem&quot; class=&quot;headerlink&quot; title=&quot;Lesson 1: The K-Armed 
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Bandits and Exploration/Exploitation</title>
    <link href="https://zhangruochi.com/Bandits-and-Exploration-Exploitation/2020/09/03/"/>
    <id>https://zhangruochi.com/Bandits-and-Exploration-Exploitation/2020/09/03/</id>
    <published>2020-09-03T09:59:35.000Z</published>
    <updated>2020-09-03T10:00:48.172Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-1-Bandits-and-Exploration-Exploitation"><a href="#Assignment-1-Bandits-and-Exploration-Exploitation" class="headerlink" title="Assignment 1: Bandits and Exploration/Exploitation"></a>Assignment 1: Bandits and Exploration/Exploitation</h1><p>Welcome to Assignment 1. This notebook will:</p><ul><li>Help you create your first bandit algorithm</li><li>Help you understand the effect of epsilon on exploration and learn about the exploration/exploitation tradeoff</li><li>Introduce you to some of the reinforcement learning software we are going to use for this specialization</li></ul><p>This class uses RL-Glue to implement most of our experiments. It was originally designed by Adam White, Brian Tanner, and Rich Sutton. This library will give you a solid framework to understand how reinforcement learning experiments work and how to run your own. If it feels a little confusing at first, don’t worry - we are going to walk you through it slowly and introduce you to more and more parts as you progress through the specialization.</p><p>We are assuming that you have used a Jupyter notebook before. But if not, it is quite simple. Simply press the run button, or shift+enter to run each of the cells. The places in the code that you need to fill in will be clearly marked for you.</p><h2 id="Section-0-Preliminaries"><a href="#Section-0-Preliminaries" class="headerlink" title="Section 0: Preliminaries"></a>Section 0: Preliminaries</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rlglue.rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">import</span> main_agent</span><br><span class="line"><span class="keyword">import</span> ten_arm_env</span><br><span class="line"><span class="keyword">import</span> test_env</span><br></pre></td></tr></table></figure><p>In the above cell, we import the libraries we need for this assignment. We use numpy throughout the course and occasionally provide hints for which methods to use in numpy. Other than that we mostly use vanilla python and the occasional other library, such as matplotlib for making plots.</p><p>You might have noticed that we import ten_arm_env. This is the <strong>10-armed Testbed</strong> introduced in <a href="http://www.incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">section 2.3</a> of the textbook. We use this throughout this notebook to test our bandit agents. It has 10 arms, which are the actions the agent can take. Pulling an arm generates a stochastic reward from a Gaussian distribution with unit-variance. For each action, the expected value of that action is randomly sampled from a normal distribution, at the start of each run. If you are unfamiliar with the 10-armed Testbed please review it in the textbook before continuing.</p><p><strong>DO NOT IMPORT OTHER LIBRARIES as this will break the autograder.</strong></p><p><strong>DO NOT SET A RANDOM SEED as this will break the autograder.</strong></p><h2 id="Section-1-Greedy-Agent"><a href="#Section-1-Greedy-Agent" class="headerlink" title="Section 1: Greedy Agent"></a>Section 1: Greedy Agent</h2><p>We want to create an agent that will find the action with the highest expected reward. One way an agent could operate is to always choose the action with  the highest value based on the agent’s current estimates. This is called a greedy agent as it greedily chooses the action that it thinks has the highest value. Let’s look at what happens in this case.</p><p>First we are going to implement the argmax function, which takes in a list of action values and returns an action with the highest value. Why are we implementing our own instead of using the argmax function that numpy uses? Numpy’s argmax function returns the first instance of the highest value. We do not want that to happen as it biases the agent to choose a specific action in the case of ties. Instead we want to break ties between the highest values randomly. So we are going to implement our own argmax function. You may want to look at <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html" target="_blank" rel="noopener">np.random.choice</a> to randomly select from a list of values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(q_values)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Takes in a list of q_values and returns the index of the item </span></span><br><span class="line"><span class="string">    with the highest value. Breaks ties randomly.</span></span><br><span class="line"><span class="string">    returns: int - the index of the highest value in q_values</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    top_value = float(<span class="string">"-inf"</span>)</span><br><span class="line">    ties = []</span><br><span class="line">    </span><br><span class="line">    top_value = max(q_values)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">        <span class="comment"># if a value in q_values is greater than the highest value update top and reset ties to zero</span></span><br><span class="line">        <span class="comment"># if a value is equal to top value add the index to ties</span></span><br><span class="line">        <span class="comment"># return a random selection from ties.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top_value:</span><br><span class="line">            ties.append(i)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.random.choice(ties)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">test_array = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">8</span>, <span class="string">"Check your argmax implementation returns the index of the largest value"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure np.random.choice is called correctly</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">test_array = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">test_array = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">8</span>, <span class="string">"Check your argmax implementation returns the index of the largest value"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seed so results are deterministic</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">test_array = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">counts = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    a = argmax(test_array)</span><br><span class="line">    counts[a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure argmax does not always choose first entry</span></span><br><span class="line"><span class="keyword">assert</span> counts[<span class="number">0</span>] != <span class="number">100</span>, <span class="string">"Make sure your argmax implementation randomly choooses among the largest values."</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure argmax does not always choose last entry</span></span><br><span class="line"><span class="keyword">assert</span> counts[<span class="number">3</span>] != <span class="number">100</span>, <span class="string">"Make sure your argmax implementation randomly choooses among the largest values."</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the random number generator is called exactly once whenver `argmax` is called</span></span><br><span class="line">expected = [<span class="number">44</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">56</span>] <span class="comment"># &lt;-- notice not perfectly uniform due to randomness</span></span><br><span class="line"><span class="keyword">assert</span> counts == expected</span><br></pre></td></tr></table></figure><p>Now we introduce the first part of an RL-Glue agent that you will implement. Here we are going to create a GreedyAgent and implement the agent_step method. This method gets called each time the agent takes a step. The method has to return the action selected by the agent. This method also ensures the agent’s estimates are updated based on the signals it gets from the environment.</p><p>Fill in the code below to implement a greedy agent.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GreedyAgent</span><span class="params">(main_agent.Agent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : The action that the agent took on the previous time step</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update Q values Hint: Look at the algorithm in section 2.4 of the textbook.</span></span><br><span class="line">        <span class="comment"># increment the counter in self.arm_count for the action from the previous time step</span></span><br><span class="line">        <span class="comment"># update the step size using self.arm_count</span></span><br><span class="line">        <span class="comment"># update self.q_values for the action from the previous time step</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] + (<span class="number">1.0</span> / self.arm_count[self.last_action]) * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># current action = ? # Use the argmax function you created above</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        current_action = argmax(self.q_values)</span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">greedy_agent = GreedyAgent()</span><br><span class="line">greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.last_action = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the q_values were updated correctly</span></span><br><span class="line"><span class="keyword">assert</span> greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the agent is using the argmax that breaks ties randomly</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">greedy_agent = GreedyAgent()</span><br><span class="line">greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.last_action = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># take a fake agent step</span></span><br><span class="line">action = greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure agent took greedy action</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure q_values were updated correctly</span></span><br><span class="line"><span class="keyword">assert</span> greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>Let’s visualize the result. Here we run an experiment using RL-Glue to test our agent. For now, we will set up the experiment code; in future lessons, we will walk you through running experiments so that you can create your own.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">num_runs = <span class="number">200</span>                    <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">num_steps = <span class="number">1000</span>                  <span class="comment"># The number of pulls of each arm the agent takes</span></span><br><span class="line">env = ten_arm_env.Environment     <span class="comment"># We set what environment we want to use to test</span></span><br><span class="line">agent = GreedyAgent               <span class="comment"># We choose what agent we want to use</span></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>&#125;  <span class="comment"># We pass the agent the information it needs. Here how many arms there are.</span></span><br><span class="line">env_info = &#123;&#125;                     <span class="comment"># We pass the environment the information it needs. In this case nothing.</span></span><br><span class="line"></span><br><span class="line">all_averages = []</span><br><span class="line"></span><br><span class="line">average_best = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):           <span class="comment"># tqdm is what creates the progress bar below</span></span><br><span class="line">    np.random.seed(run)</span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(env, agent)          <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">    rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line">    rl_glue.rl_start()                    <span class="comment"># We start the experiment</span></span><br><span class="line"></span><br><span class="line">    average_best += np.max(rl_glue.environment.arms)</span><br><span class="line">    </span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    averages = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        reward, _, action, _ = rl_glue.rl_step() <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                                                 <span class="comment"># the reward, and action taken.</span></span><br><span class="line">        scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">        averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    all_averages.append(averages)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot([average_best / num_runs <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend([<span class="string">"Best Possible"</span>, <span class="string">"Greedy"</span>])</span><br><span class="line">plt.title(<span class="string">"Average Reward of Greedy Agent"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">greedy_scores = np.mean(all_averages, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 63.68it/s]</code></pre><p><img src="output_15_1.png" alt="png"></p><p>How did our agent do? Is it possible for it to do better?</p><h2 id="Section-2-Epsilon-Greedy-Agent"><a href="#Section-2-Epsilon-Greedy-Agent" class="headerlink" title="Section 2: Epsilon-Greedy Agent"></a>Section 2: Epsilon-Greedy Agent</h2><p>We learned about <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/tHDck/what-is-the-trade-off" target="_blank" rel="noopener">another way for an agent to operate</a>, where it does not always take the greedy action. Instead, sometimes it takes an exploratory action. It does this so that it can find out what the best action really is. If we always choose what we think is the current best action is, we may miss out on taking the true best action, because we haven’t explored enough times to find that best action.</p><p>Implement an epsilon-greedy agent below. Hint: we are implementing the algorithm from <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=52" target="_blank" rel="noopener">section 2.4</a> of the textbook. You may want to use your greedy code from above and look at <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random.html" target="_blank" rel="noopener">np.random.random</a>, as well as <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html" target="_blank" rel="noopener">np.random.randint</a>, to help you select random actions. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedyAgent</span><span class="params">(main_agent.Agent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : The action that the agent took on the previous time step</span></span><br><span class="line">        <span class="comment"># self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update Q values - this should be the same update as your greedy agent above</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] + (<span class="number">1.0</span> / self.arm_count[self.last_action]) * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy</span></span><br><span class="line">        <span class="comment"># Randomly choose a number between 0 and 1 and see if it's less than self.epsilon</span></span><br><span class="line">        <span class="comment"># (hint: look at np.random.random()). If it is, set current_action to a random action.</span></span><br><span class="line">        <span class="comment"># otherwise choose current_action greedily as you did above.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line"></span><br><span class="line">        _ = np.random.random()</span><br><span class="line">        </span><br><span class="line">        current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,len(self.q_values))</span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0.0</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># given this random seed, we should see a greedy action (action 2) here</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"><span class="comment"># we'll try to guess a few of the trickier places</span></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure to update for the *last_action* not the current action</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values != [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">"A"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the stepsize is based on the *last_action* not the current action</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values != [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">"B"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the agent is using the argmax that breaks ties randomly</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span>, <span class="string">"C"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># let's see what happens for another action</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># given this random seed, we should see a random action (action 4) here</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The agent saw a reward of 1, so should increase the value for *last_action*</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">"D"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the agent should have picked a random action for this particular random seed</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">4</span>, <span class="string">"E"</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># manipulate the random seed so the agent takes a random action</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">0</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check to make sure we update value for action 4</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.0</span>, <span class="number">0</span>, <span class="number">1.0</span>]</span><br></pre></td></tr></table></figure><p>Now that we have our epsilon greedy agent created. Let’s compare it against the greedy agent with epsilon of 0.1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot Epsilon greedy results and greedy results</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">agent = EpsilonGreedyAgent</span><br><span class="line">env = ten_arm_env.Environment</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: epsilon&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">all_averages = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">    np.random.seed(run)</span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">    rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    averages = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        reward, _, action, _ = rl_glue.rl_step() <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                                                 <span class="comment"># the reward, and action taken.</span></span><br><span class="line">        scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">        averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    all_averages.append(averages)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.plot(greedy_scores)</span><br><span class="line">plt.title(<span class="string">"Average Reward of Greedy Agent vs. E-Greedy Agent"</span>)</span><br><span class="line">plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend((<span class="string">"Best Possible"</span>, <span class="string">"Greedy"</span>, <span class="string">"Epsilon: 0.1"</span>))</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 62.89it/s]</code></pre><p><img src="output_23_1.png" alt="png"></p><p>Notice how much better the epsilon-greedy agent did. Because we occasionally choose a random action we were able to find a better long term policy. By acting greedily before our value estimates are accurate, we risk settling on a suboptimal action.</p><h2 id="Section-2-1-Averaging-Multiple-Runs"><a href="#Section-2-1-Averaging-Multiple-Runs" class="headerlink" title="Section 2.1 Averaging Multiple Runs"></a>Section 2.1 Averaging Multiple Runs</h2><p>Did you notice that we averaged over 2000 runs? Why did we do that?</p><p>To get some insight, let’s look at the results of two individual runs by the same agent.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot runs of e-greedy agent</span></span><br><span class="line">agent = EpsilonGreedyAgent</span><br><span class="line">env = ten_arm_env.Environment</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">all_averages = []</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> (<span class="number">0</span>, <span class="number">1</span>):</span><br><span class="line">    np.random.seed(run) <span class="comment"># Here we set the seed so that we can compare two different runs</span></span><br><span class="line">    averages = []</span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">    rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">        scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">        averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.plot(averages)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"Comparing two independent runs"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_27_0.png" alt="png"></p><p>Notice how the two runs were different? But, if this is the exact same algorithm, why does it behave differently in these two runs?</p><p>The answer is that it is due to randomness in the environment and in the agent. Depending on what action the agent randomly starts with, or when it randomly chooses to explore, it can change the results of the runs. And even if the agent chooses the same action, the reward from the environment is randomly sampled from a Gaussian. The agent could get lucky, and see larger rewards for the best action early on and so settle on the best action faster. Or, it could get unlucky and see smaller rewards for best action early on and so take longer to recognize that it is in fact the best action.</p><p>To be more concrete, let’s look at how many times an exploratory action is taken, for different seeds. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">print(<span class="string">"Random Seed 1"</span>)</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    <span class="keyword">if</span> np.random.random() &lt; <span class="number">0.1</span>:</span><br><span class="line">        print(<span class="string">"Exploratory Action"</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">print()</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Random Seed 2"</span>)</span><br><span class="line">np.random.seed(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    <span class="keyword">if</span> np.random.random() &lt; <span class="number">0.1</span>:</span><br><span class="line">        print(<span class="string">"Exploratory Action"</span>)</span><br></pre></td></tr></table></figure><pre><code>Random Seed 1Exploratory ActionExploratory ActionExploratory ActionRandom Seed 2Exploratory Action</code></pre><p>With the first seed, we take an exploratory action three times out of 15, but with the second, we only take an exploratory action once. This can significantly affect the performance of our agent because the amount of exploration has changed significantly.</p><p>To compare algorithms, we therefore report performance averaged across many runs. We do this to ensure that we are not simply reporting a result that is due to stochasticity, as explained <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/PtVBs/sequential-decision-making-with-evaluative-feedback" target="_blank" rel="noopener">in the lectures</a>. Rather, we want statistically significant outcomes. We will not use statistical significance tests in this course. Instead, because we have access to simulators for our experiments, we use the simpler strategy of running for a large number of runs and ensuring that the confidence intervals do not overlap. </p><h2 id="Section-3-Comparing-values-of-epsilon"><a href="#Section-3-Comparing-values-of-epsilon" class="headerlink" title="Section 3: Comparing values of epsilon"></a>Section 3: Comparing values of epsilon</h2><p>Can we do better than an epsilon of 0.1? Let’s try several different values for epsilon and see how they perform. We try different settings of key performance parameters to understand how the agent might perform under different conditions.</p><p>Below we run an experiment where we sweep over different values for epsilon:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment code for different e-greedy</span></span><br><span class="line">epsilons = [<span class="number">0.0</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.4</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line"></span><br><span class="line">n_q_values = []</span><br><span class="line">n_averages = []</span><br><span class="line">n_best_actions = []</span><br><span class="line"></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epsilon <span class="keyword">in</span> epsilons:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        agent = EpsilonGreedyAgent</span><br><span class="line">        agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: epsilon&#125;</span><br><span class="line">        env_info = &#123;<span class="string">"random_seed"</span>: run&#125;</span><br><span class="line"></span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line">        </span><br><span class="line">        best_arm = np.argmax(rl_glue.environment.arms)</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        best_action_chosen = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">            averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> action == best_arm:</span><br><span class="line">                best_action_chosen.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                best_action_chosen.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> epsilon == <span class="number">0.1</span> <span class="keyword">and</span> run == <span class="number">0</span>:</span><br><span class="line">                n_q_values.append(np.copy(rl_glue.agent.q_values))</span><br><span class="line">        <span class="keyword">if</span> epsilon == <span class="number">0.1</span>:</span><br><span class="line">            n_averages.append(averages)</span><br><span class="line">            n_best_actions.append(best_action_chosen)</span><br><span class="line">        all_averages.append(averages)</span><br><span class="line">        </span><br><span class="line">    plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.legend([<span class="string">"Best Possible"</span>] + epsilons)</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 58.80it/s]100%|██████████| 200/200 [00:03&lt;00:00, 59.62it/s]100%|██████████| 200/200 [00:03&lt;00:00, 61.00it/s]100%|██████████| 200/200 [00:02&lt;00:00, 70.13it/s]</code></pre><p><img src="output_33_1.png" alt="png"></p><p>Why did 0.1 perform better than 0.01?</p><p>If exploration helps why did 0.4 perform worse that 0.0 (the greedy agent)?</p><p>Think about these and how you would answer these questions. They are questions in the practice quiz. If you still have questions about it, retake the practice quiz.</p><h2 id="Section-4-The-Effect-of-Step-Size"><a href="#Section-4-The-Effect-of-Step-Size" class="headerlink" title="Section 4: The Effect of Step Size"></a>Section 4: The Effect of Step Size</h2><p>In Section 1 of this assignment, we decayed the step size over time based on action-selection counts. The step-size was 1/N(A), where N(A) is the number of times action A was selected. This is the same as computing a sample average. We could also set the step size to be a constant value, such as 0.1. What would be the effect of doing that? And is it better to use a constant or the sample average method? </p><p>To investigate this question, let’s start by creating a new agent that has a constant step size. This will be nearly identical to the agent created above. You will use the same code to select the epsilon-greedy action. You will change the update to have a constant step size instead of using the 1/N(A) update.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedyAgentConstantStepsize</span><span class="params">(main_agent.Agent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : An int of the action that the agent took on the previous time step.</span></span><br><span class="line">        <span class="comment"># self.step_size : A float which is the current step size for the agent.</span></span><br><span class="line">        <span class="comment"># self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update q_values for action taken at previous time step </span></span><br><span class="line">        <span class="comment"># using self.step_size intead of using self.arm_count</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] +  self.step_size * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy. This is the same as you implemented above.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        _ = np.random.random()</span><br><span class="line">        current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,len(self.q_values))</span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>]:</span><br><span class="line">    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()</span><br><span class="line">    e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">    e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">    e_greedy_agent.epsilon = <span class="number">0.0</span></span><br><span class="line">    e_greedy_agent.step_size = step_size</span><br><span class="line">    action = e_greedy_agent.agent_step(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, step_size, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">"Check that you are updating q_values correctly using the stepsize."</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Check Epsilon Greedy with Different Constant Stepsizes</span></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>]:</span><br><span class="line">    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()</span><br><span class="line">    e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">    e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">    e_greedy_agent.epsilon = <span class="number">0.0</span></span><br><span class="line">    e_greedy_agent.step_size = step_size</span><br><span class="line">    </span><br><span class="line">    action = e_greedy_agent.agent_step(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, step_size, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment code for different step sizes</span></span><br><span class="line">step_sizes = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="string">'1/N(A)'</span>]</span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line"></span><br><span class="line">q_values = &#123;step_size: [] <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line">true_values = &#123;step_size: <span class="keyword">None</span> <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line">best_actions = &#123;step_size: [] <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        np.random.seed(run)</span><br><span class="line">        agent = EpsilonGreedyAgentConstantStepsize <span class="keyword">if</span> step_size != <span class="string">'1/N(A)'</span> <span class="keyword">else</span> EpsilonGreedyAgent</span><br><span class="line">        agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: epsilon, <span class="string">"step_size"</span>: step_size, <span class="string">"initial_value"</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">        env_info = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line">        </span><br><span class="line">        best_arm = np.argmax(rl_glue.environment.arms)</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> run == <span class="number">0</span>:</span><br><span class="line">            true_values[step_size] = np.copy(rl_glue.environment.arms)</span><br><span class="line">            </span><br><span class="line">        best_action_chosen = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">            averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> action == best_arm:</span><br><span class="line">                best_action_chosen.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                best_action_chosen.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> run == <span class="number">0</span>:</span><br><span class="line">                q_values[step_size].append(np.copy(rl_glue.agent.q_values))</span><br><span class="line">        best_actions[step_size].append(best_action_chosen)</span><br><span class="line">    ax.plot(np.mean(best_actions[step_size], axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.legend(step_sizes)</span><br><span class="line">plt.title(<span class="string">"% Best Arm Pulled"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"% Best Arm Pulled"</span>)</span><br><span class="line">vals = ax.get_yticks()</span><br><span class="line">ax.set_yticklabels([<span class="string">'&#123;:,.2%&#125;'</span>.format(x) <span class="keyword">for</span> x <span class="keyword">in</span> vals])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 63.14it/s]100%|██████████| 200/200 [00:03&lt;00:00, 62.84it/s]100%|██████████| 200/200 [00:03&lt;00:00, 62.57it/s]100%|██████████| 200/200 [00:03&lt;00:00, 62.70it/s]100%|██████████| 200/200 [00:03&lt;00:00, 61.82it/s]</code></pre><p><img src="output_40_1.png" alt="png"></p><p>Notice first that we are now plotting the amount of time that the best action is taken rather than the average reward. To better  understand the performance of an agent, it can be useful to measure specific behaviors, beyond just how much reward is accumulated. This measure indicates how close the agent’s behaviour is to optimal.</p><p>It seems as though 1/N(A) performed better than the others, in that it reaches a solution where it takes the best action most frequently. Now why might this be? Why did a step size of 0.5 start out better but end up performing worse? Why did a step size of 0.01 perform so poorly?</p><p>Let’s dig into this further below. Let’s plot how well each agent tracks the true value, where each agent has a different step size method. You do not have to enter any code here, just follow along.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">largest = <span class="number">0</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes:</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">    largest = np.argmax(true_values[step_size])</span><br><span class="line">    plt.plot([true_values[step_size][largest] <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line">    plt.title(<span class="string">"Step Size: &#123;&#125;"</span>.format(step_size))</span><br><span class="line">    plt.plot(np.array(q_values[step_size])[:, largest])</span><br><span class="line">    plt.legend([<span class="string">"True Expected Value"</span>, <span class="string">"Estimated Value"</span>])</span><br><span class="line">    plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Value"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_42_0.png" alt="png"></p><p><img src="output_42_1.png" alt="png"></p><p><img src="output_42_2.png" alt="png"></p><p><img src="output_42_3.png" alt="png"></p><p><img src="output_42_4.png" alt="png"></p><p>These plots help clarify the performance differences between the different step sizes. A step size of 0.01 makes such small updates that the agent’s value estimate of the best action does not get close to the actual value. Step sizes of 0.5 and 1.0 both get close to the true value quickly, but are very susceptible to stochasticity in the rewards. The updates overcorrect too much towards recent rewards, and so oscillate around the true value. This means that on many steps, the action that pulls the best arm may seem worse than it actually is.  A step size of 0.1 updates fairly quickly to the true value, and does not oscillate as widely around the true values as 0.5 and 1.0. This is one of the reasons that 0.1 performs quite well. Finally we see why 1/N(A) performed well. Early on while the step size is still reasonably high it moves quickly to the true expected value, but as it gets pulled more its step size is reduced which makes it less susceptible to the stochasticity of the rewards.</p><p>Does this mean that 1/N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment.</p><p>Let’s look at how a sudden change in the reward distributions affects a step size like 1/N(A). This time we will run the environment for 2000 steps, and after 1000 steps we will randomly change the expected value of all of the arms. We compare two agents, both using epsilon-greedy with epsilon = 0.1. One uses a constant step size of 0.1, the other a step size of 1/N(A) that reduces over time. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">num_steps = <span class="number">2000</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> agent <span class="keyword">in</span> [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: epsilon, <span class="string">"step_size"</span>: step_size&#125;</span><br><span class="line">        np.random.seed(run)</span><br><span class="line">        </span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">            averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">1000</span>:</span><br><span class="line">                rl_glue.environment.arms = np.random.randn(<span class="number">10</span>)</span><br><span class="line">        all_averages.append(averages)</span><br><span class="line">        </span><br><span class="line">    plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend([<span class="string">"Best Possible"</span>, <span class="string">"1/N(A)"</span>, <span class="string">"0.1"</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:06&lt;00:00, 29.43it/s]100%|██████████| 200/200 [00:06&lt;00:00, 30.10it/s]</code></pre><p><img src="output_44_1.png" alt="png"></p><p>Now the agent with a step size of 1/N(A) performed better at the start but then performed worse when the environment changed! What happened?</p><p>Think about what the step size would be after 1000 steps. Let’s say the best action gets chosen 500 times. That means the step size for that action is 1/500 or 0.002. At each step when we update the value of the action and the value is going to move only 0.002 * the error. That is a very tiny adjustment and it will take a long time for it to get to the true value.</p><p>The agent with step size 0.1, however, will always update in 1/10th of the direction of the error. This means that on average it will take ten steps for it to update its value to the sample mean.</p><p>These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarity—-and the related concept of partial observability—-is a common feature of reinforcement learning problems and when learning online.  </p><h2 id="Section-5-Conclusion"><a href="#Section-5-Conclusion" class="headerlink" title="Section 5: Conclusion"></a>Section 5: Conclusion</h2><p>Great work! You have:</p><ul><li>Implemented your first agent</li><li>Learned about the effect of epsilon, an exploration parameter, on the performance of an agent</li><li>Learned about the effect of step size on the performance of the agent</li><li>Learned about a good experiment practice of averaging across multiple runs</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-1-Bandits-and-Exploration-Exploitation&quot;&gt;&lt;a href=&quot;#Assignment-1-Bandits-and-Exploration-Exploitation&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Question duplicates</title>
    <link href="https://zhangruochi.com/Question-duplicates/2020/08/23/"/>
    <id>https://zhangruochi.com/Question-duplicates/2020/08/23/</id>
    <published>2020-08-22T18:47:25.000Z</published>
    <updated>2020-08-22T18:47:47.800Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-4-Question-duplicates"><a href="#Assignment-4-Question-duplicates" class="headerlink" title="Assignment 4:  Question duplicates"></a>Assignment 4:  Question duplicates</h1><p>Welcome to the fourth assignment of course 3. In this assignment you will explore Siamese networks applied to natural language processing. You will further explore the fundamentals of Trax and you will be able to implement a more complicated structure using it. By completing this assignment, you will learn how to implement models with different architectures. </p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">Overview</a></li><li><a href="#1">Part 1: Importing the Data</a><ul><li><a href="#1.1">1.1 Loading in the data</a></li><li><a href="#1.2">1.2 Converting a question to a tensor</a></li><li><a href="#1.3">1.3 Understanding the iterator</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li></ul></li><li><a href="#2">Part 2: Defining the Siamese model</a><ul><li><a href="#2.1">2.1 Understanding Siamese Network</a><ul><li><a href="#ex02">Exercise 02</a></li></ul></li><li><a href="#2.2">2.2 Hard  Negative Mining</a><ul><li><a href="#ex03">Exercise 03</a></li></ul></li></ul></li><li><a href="#3">Part 3: Training</a><ul><li><a href="#3.1">3.1 Training the model</a><ul><li><a href="#ex04">Exercise 04</a></li></ul></li></ul></li><li><a href="#4">Part 4: Evaluation</a><ul><li><a href="#4.1">4.1 Evaluating your siamese network</a></li><li><a href="#4.2">4.2 Classify</a><ul><li><a href="#ex05">Exercise 05</a></li></ul></li></ul></li><li><a href="#5">Part 5: Testing with your own questions</a><ul><li><a href="#ex06">Exercise 06</a></li></ul></li><li><a href="#6">On Siamese networks</a></li></ul><p><a name="0"></a></p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>In this assignment, concretely you will: </p><ul><li>Learn about Siamese networks</li><li>Understand how the triplet loss works</li><li>Understand how to evaluate accuracy</li><li>Use cosine similarity between the model’s outputted vectors</li><li>Use the data generator to get batches of questions</li><li>Predict using your own model</li></ul><p>By now, you are familiar with trax and know how to make use of classes to define your model. We will start this homework by asking you to preprocess the data the same way you did in the previous assignments. After processing the data you will build a classifier that will allow you to identify whether to questions are the same or not.<br><img src="meme.png" style="width:550px;height:300px;"></p><p>You will process the data first and then pad in a similar way you have done in the previous assignment. Your model will take in the two question embeddings, run them through an LSTM, and then compare the outputs of the two sub networks using cosine similarity. Before taking a deep dive into the model, start by importing the data set.</p><p><a name="1"></a></p><h1 id="Part-1-Importing-the-Data"><a href="#Part-1-Importing-the-Data" class="headerlink" title="Part 1: Importing the Data"></a>Part 1: Importing the Data</h1><p><a name="1.1"></a></p><h3 id="1-1-Loading-in-the-data"><a href="#1-1-Loading-in-the-data" class="headerlink" title="1.1 Loading in the data"></a>1.1 Loading in the data</h3><p>You will be using the Quora question answer dataset to build a model that could identify similar questions. This is a useful task because you don’t want to have several versions of the same question posted. Several times when teaching I end up responding to similar questions on piazza, or on other community forums. This data set has been labeled for you. Run the cell below to import some of the packages you will be using. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"><span class="keyword">from</span> trax.fastmath <span class="keyword">import</span> numpy <span class="keyword">as</span> fastnp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rnd</span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seeds</span></span><br><span class="line">trax.supervised.trainer_lib.init_random_number_generators(<span class="number">34</span>)</span><br><span class="line">rnd.seed(<span class="number">34</span>)</span><br></pre></td></tr></table></figure><p><strong>Notice that for this assignment Trax’s numpy is referred to as <code>fastnp</code>, while regular numpy is referred to as <code>np</code>.</strong></p><p>You will now load in the data set. We have done some preprocessing for you. If you have taken the deeplearning specialization, this is a slightly different training method than the one you have seen there. If you have not, then don’t worry about it, we will explain everything. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"questions.csv"</span>)</span><br><span class="line">N=len(data)</span><br><span class="line">print(<span class="string">'Number of question pairs: '</span>, N)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><pre><code>Number of question pairs:  404351</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>qid1</th>      <th>qid2</th>      <th>question1</th>      <th>question2</th>      <th>is_duplicate</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>1</td>      <td>2</td>      <td>What is the step by step guide to invest in sh...</td>      <td>What is the step by step guide to invest in sh...</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>3</td>      <td>4</td>      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>      <td>What would happen if the Indian government sto...</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>5</td>      <td>6</td>      <td>How can I increase the speed of my internet co...</td>      <td>How can Internet speed be increased by hacking...</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>7</td>      <td>8</td>      <td>Why am I mentally very lonely? How can I solve...</td>      <td>Find the remainder when [math]23^{24}[/math] i...</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>9</td>      <td>10</td>      <td>Which one dissolve in water quikly sugar, salt...</td>      <td>Which fish would survive in salt water?</td>      <td>0</td>    </tr>  </tbody></table></div><p>We first split the data into a train and test set. The test set will be used later to evaluate our model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">N_train = <span class="number">300000</span></span><br><span class="line">N_test  = <span class="number">10</span>*<span class="number">1024</span></span><br><span class="line">data_train = data[:N_train]</span><br><span class="line">data_test  = data[N_train:N_train+N_test]</span><br><span class="line">print(<span class="string">"Train set:"</span>, len(data_train), <span class="string">"Test set:"</span>, len(data_test))</span><br><span class="line"><span class="keyword">del</span>(data) <span class="comment"># remove to free memory</span></span><br></pre></td></tr></table></figure><pre><code>Train set: 300000 Test set: 10240</code></pre><p>As explained in the lectures, we select only the question pairs that are duplicate to train the model. <br><br>We build two batches as input for the Siamese network and we assume that question $q1_i$ (question $i$ in the first batch) is a duplicate of $q2_i$ (question $i$ in the second batch), but all other questions in the second batch are not duplicates of $q1_i$.<br>The test set uses the original pairs of questions and the status describing if the questions are duplicates.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">td_index = (data_train[<span class="string">'is_duplicate'</span>] == <span class="number">1</span>).to_numpy()</span><br><span class="line">td_index = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(td_index) <span class="keyword">if</span> x] </span><br><span class="line">print(<span class="string">'number of duplicate questions: '</span>, len(td_index))</span><br><span class="line">print(<span class="string">'indexes of first ten duplicate questions:'</span>, td_index[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>number of duplicate questions:  111486indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(data_train[<span class="string">'question1'</span>][<span class="number">5</span>])  <span class="comment">#  Example of question duplicates (first one in data)</span></span><br><span class="line">print(data_train[<span class="string">'question2'</span>][<span class="number">5</span>])</span><br><span class="line">print(<span class="string">'is_duplicate: '</span>, data_train[<span class="string">'is_duplicate'</span>][<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?I&#39;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?is_duplicate:  1</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Q1_train_words = np.array(data_train[<span class="string">'question1'</span>][td_index])</span><br><span class="line">Q2_train_words = np.array(data_train[<span class="string">'question2'</span>][td_index])</span><br><span class="line"></span><br><span class="line">Q1_test_words = np.array(data_test[<span class="string">'question1'</span>])</span><br><span class="line">Q2_test_words = np.array(data_test[<span class="string">'question2'</span>])</span><br><span class="line">y_test  = np.array(data_test[<span class="string">'is_duplicate'</span>])</span><br></pre></td></tr></table></figure><p>Above, you have seen that you only took the duplicated questions for training our model. <br>You did so on purpose, because the data generator will produce batches $([q1_1, q1_2, q1_3, …]$, $[q2_1, q2_2,q2_3, …])$  where $q1_i$ and $q2_k$ are duplicate if and only if $i = k$.</p><p><br>Let’s print to see what your data looks like.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'TRAINING QUESTIONS:\n'</span>)</span><br><span class="line">print(<span class="string">'Question 1: '</span>, Q1_train_words[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Question 2: '</span>, Q2_train_words[<span class="number">0</span>], <span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">'Question 1: '</span>, Q1_train_words[<span class="number">5</span>])</span><br><span class="line">print(<span class="string">'Question 2: '</span>, Q2_train_words[<span class="number">5</span>], <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'TESTING QUESTIONS:\n'</span>)</span><br><span class="line">print(<span class="string">'Question 1: '</span>, Q1_test_words[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Question 2: '</span>, Q2_test_words[<span class="number">0</span>], <span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">'is_duplicate ='</span>, y_test[<span class="number">0</span>], <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><pre><code>TRAINING QUESTIONS:Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?Question 2:  I&#39;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? TESTING QUESTIONS:Question 1:  How do I prepare for interviews for cse?Question 2:  What is the best way to prepare for cse? is_duplicate = 0 </code></pre><p>You will now encode each word of the selected duplicate pairs with an index. <br> Given a question, you can then just encode it as a list of numbers.  </p><p>First you tokenize the questions using <code>nltk.word_tokenize</code>. <br><br>You need a python default dictionary which later, during inference, assigns the values $0$ to all Out Of Vocabulary (OOV) words.<br><br>Then you encode each word of the selected duplicate pairs with an index. Given a question, you can then just encode it as a list of numbers. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#create arrays</span></span><br><span class="line">Q1_train = np.empty_like(Q1_train_words)</span><br><span class="line">Q2_train = np.empty_like(Q2_train_words)</span><br><span class="line"></span><br><span class="line">Q1_test = np.empty_like(Q1_test_words)</span><br><span class="line">Q2_test = np.empty_like(Q2_test_words)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Building the vocabulary with the train set         (this might take a minute)</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">vocab = defaultdict(<span class="keyword">lambda</span>: <span class="number">0</span>)</span><br><span class="line">vocab[<span class="string">'&lt;PAD&gt;'</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(len(Q1_train_words)):</span><br><span class="line">    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])</span><br><span class="line">    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])</span><br><span class="line">    q = Q1_train[idx] + Q2_train[idx]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            vocab[word] = len(vocab) + <span class="number">1</span></span><br><span class="line">print(<span class="string">'The length of the vocabulary is: '</span>, len(vocab))</span><br></pre></td></tr></table></figure><pre><code>The length of the vocabulary is:  36268</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(vocab[<span class="string">'&lt;PAD&gt;'</span>])</span><br><span class="line">print(vocab[<span class="string">'Astrology'</span>])</span><br><span class="line">print(vocab[<span class="string">'Astronomy'</span>])  <span class="comment">#not in vocabulary, returns 0</span></span><br></pre></td></tr></table></figure><pre><code>120</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(len(Q1_test_words)): </span><br><span class="line">    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])</span><br><span class="line">    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Train set has reduced to: '</span>, len(Q1_train) ) </span><br><span class="line">print(<span class="string">'Test set length: '</span>, len(Q1_test) )</span><br></pre></td></tr></table></figure><pre><code>Train set has reduced to:  111486Test set length:  10240</code></pre><p><a name="1.2"></a></p><h3 id="1-2-Converting-a-question-to-a-tensor"><a href="#1-2-Converting-a-question-to-a-tensor" class="headerlink" title="1.2 Converting a question to a tensor"></a>1.2 Converting a question to a tensor</h3><p>You will now convert every question to a tensor, or an array of numbers, using your vocabulary built above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Converting questions to array of integers</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(Q1_train)):</span><br><span class="line">    Q1_train[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q1_train[i]]</span><br><span class="line">    Q2_train[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q2_train[i]]</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(Q1_test)):</span><br><span class="line">    Q1_test[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q1_test[i]]</span><br><span class="line">    Q2_test[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q2_test[i]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'first question in the train set:\n'</span>)</span><br><span class="line">print(Q1_train_words[<span class="number">0</span>], <span class="string">'\n'</span>) </span><br><span class="line">print(<span class="string">'encoded version:'</span>)</span><br><span class="line">print(Q1_train[<span class="number">0</span>],<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'first question in the test set:\n'</span>)</span><br><span class="line">print(Q1_test_words[<span class="number">0</span>], <span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">'encoded version:'</span>)</span><br><span class="line">print(Q1_test[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>first question in the train set:Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? encoded version:[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] first question in the test set:How do I prepare for interviews for cse? encoded version:[32, 38, 4, 107, 65, 1015, 65, 11509, 21]</code></pre><p>You will now split your train set into a training/validation set so that you can use it to train and evaluate your Siamese model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splitting the data</span></span><br><span class="line">cut_off = int(len(Q1_train)*<span class="number">.8</span>)</span><br><span class="line">train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]</span><br><span class="line">val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]</span><br><span class="line">print(<span class="string">'Number of duplicate questions: '</span>, len(Q1_train))</span><br><span class="line">print(<span class="string">"The length of the training set is:  "</span>, len(train_Q1))</span><br><span class="line">print(<span class="string">"The length of the validation set is: "</span>, len(val_Q1))</span><br></pre></td></tr></table></figure><pre><code>Number of duplicate questions:  111486The length of the training set is:   89188The length of the validation set is:  22298</code></pre><p><a name="1.3"></a></p><h3 id="1-3-Understanding-the-iterator"><a href="#1-3-Understanding-the-iterator" class="headerlink" title="1.3 Understanding the iterator"></a>1.3 Understanding the iterator</h3><p>Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. If you were to use stochastic gradient descent with one example at a time, it will take you forever to build a model. In this example, we show you how you can build a data generator that takes in $Q1$ and $Q2$ and returns a batch of size <code>batch_size</code>  in the following format $([q1_1, q1_2, q1_3, …]$, $[q2_1, q2_2,q2_3, …])$. The tuple consists of two arrays and each array has <code>batch_size</code> questions. Again, $q1_i$ and $q2_i$ are duplicates, but they are not duplicates with any other elements in the batch. </p><p><br></p><p>The command <figure class="highlight plain"><figcaption><span>the next batch. This iterator returns the data in a format that you could directly use in your model when computing the feed-forward of your algorithm. This iterator returns a pair of arrays of questions. </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;a name=&apos;ex01&apos;&gt;&lt;/a&gt;</span><br><span class="line">### Exercise 01</span><br><span class="line"></span><br><span class="line">**Instructions:**  </span><br><span class="line">Implement the data generator below. Here are some things you will need. </span><br><span class="line"></span><br><span class="line">- While true loop.</span><br><span class="line">- if `index &gt;= len_Q1`, set the `idx` to $0$.</span><br><span class="line">- The generator should return shuffled batches of data. To achieve this without modifying the actual question lists, a list containing the indexes of the questions is created. This list can be shuffled and used to get random batches everytime the index is reset.</span><br><span class="line">- Append elements of $Q1$ and $Q2$ to `input1` and `input2` respectively.</span><br><span class="line">- if `len(input1) == batch_size`, determine `max_len` as the longest question in `input1` and `input2`. Ceil `max_len` to a power of $2$ (for computation purposes) using the following command:  `max_len = 2**int(np.ceil(np.log2(max_len)))`.</span><br><span class="line">- Pad every question by `vocab[&apos;&lt;PAD&gt;&apos;]` until you get the length `max_len`.</span><br><span class="line">- Use yield to return `input1, input2`. </span><br><span class="line">- Don&apos;t forget to reset `input1, input2`  to empty arrays at the end (data generator resumes from where it last left).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span><br><span class="line"># GRADED FUNCTION: data_generator</span><br><span class="line">def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):</span><br><span class="line">    &quot;&quot;&quot;Generator function that yields batches of data</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        Q1 (list): List of transformed (to tensor) questions.</span><br><span class="line">        Q2 (list): List of transformed (to tensor) questions.</span><br><span class="line">        batch_size (int): Number of elements per batch.</span><br><span class="line">        pad (int, optional): Pad character from the vocab. Defaults to 1.</span><br><span class="line">        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.</span><br><span class="line">    Yields:</span><br><span class="line">        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)</span><br><span class="line">        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates</span><br><span class="line">              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    input1 = []</span><br><span class="line">    input2 = []</span><br><span class="line">    idx = 0</span><br><span class="line">    len_q = len(Q1)</span><br><span class="line">    question_indexes = [*range(len_q)]</span><br><span class="line">    </span><br><span class="line">    if shuffle:</span><br><span class="line">        rnd.shuffle(question_indexes)</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE (Replace instances of &apos;None&apos; with your code) ###</span><br><span class="line">    while True:</span><br><span class="line">        if idx &gt;= len_q:</span><br><span class="line">            # if idx is greater than or equal to len_q, set idx accordingly </span><br><span class="line">            # (Hint: look at the instructions above)</span><br><span class="line">            idx = 0</span><br><span class="line">            # shuffle to get random batches if shuffle is set to True</span><br><span class="line">            if shuffle:</span><br><span class="line">                rnd.shuffle(question_indexes)</span><br><span class="line">        </span><br><span class="line">        # get questions at the `question_indexes[idx]` position in Q1 and Q2</span><br><span class="line">        q1 = Q1[question_indexes[idx]]</span><br><span class="line">        q2 = Q2[question_indexes[idx]]</span><br><span class="line">        </span><br><span class="line">        # increment idx by 1</span><br><span class="line">        idx += 1</span><br><span class="line">        # append q1</span><br><span class="line">        input1.append(q1)</span><br><span class="line">        # append q2</span><br><span class="line">        input2.append(q2)</span><br><span class="line">        </span><br><span class="line">        if len(input1) == batch_size:</span><br><span class="line">            # determine max_len as the longest question in input1 &amp; input 2</span><br><span class="line">            # Hint: use the `max` function. </span><br><span class="line">            # take max of input1 &amp; input2 and then max out of the two of them.</span><br><span class="line">            max_len = max(max([len(_) for _ in input1]),max([len(_) for _ in input2]))</span><br><span class="line">            # pad to power-of-2 (Hint: look at the instructions above)</span><br><span class="line">            max_len = 2**int(np.ceil(np.log2(max_len)))</span><br><span class="line">            b1 = []</span><br><span class="line">            b2 = []</span><br><span class="line">            for q1, q2 in zip(input1, input2):</span><br><span class="line">                # add [pad] to q1 until it reaches max_len</span><br><span class="line">                q1 = q1 + [pad] * (max_len - len(q1))</span><br><span class="line">                # add [pad] to q2 until it reaches max_len</span><br><span class="line">                q2 = q2 + [pad] * (max_len - len(q2))</span><br><span class="line">                # append q1</span><br><span class="line">                b1.append(q1)</span><br><span class="line">                # append q2</span><br><span class="line">                b2.append(q2)</span><br><span class="line">            # use b1 and b2</span><br><span class="line">            yield np.array(b1), np.array(b2)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">            # reset the batches</span><br><span class="line">            input1, input2 = [], []  # reset the batches</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))</span><br><span class="line">print(<span class="string">"First questions  : "</span>,<span class="string">'\n'</span>, res1, <span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">"Second questions : "</span>,<span class="string">'\n'</span>, res2)</span><br></pre></td></tr></table></figure><pre><code>First questions  :   [[  30   87   78  134 2132 1981   28   78  594   21    1    1    1    1     1    1] [  30   55   78 3541 1460   28   56  253   21    1    1    1    1    1     1    1]] Second questions :   [[  30  156   78  134 2132 9508   21    1    1    1    1    1    1    1     1    1] [  30  156   78 3541 1460  131   56  253   21    1    1    1    1    1     1    1]]</code></pre><p><strong>Note</strong>: The following expected output is valid only if you run the above test cell <strong>_once_</strong> (first time). The output will change on each execution.</p><p>If you think your implementation is correct and it is not matching the output, make sure to restart the kernel and run all the cells from the top again. </p><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">First questions  :  </span><br><span class="line"> [[  <span class="number">30</span>   <span class="number">87</span>   <span class="number">78</span>  <span class="number">134</span> <span class="number">2132</span> <span class="number">1981</span>   <span class="number">28</span>   <span class="number">78</span>  <span class="number">594</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]</span><br><span class="line"> [  <span class="number">30</span>   <span class="number">55</span>   <span class="number">78</span> <span class="number">3541</span> <span class="number">1460</span>   <span class="number">28</span>   <span class="number">56</span>  <span class="number">253</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]] </span><br><span class="line"></span><br><span class="line">Second questions :  </span><br><span class="line"> [[  <span class="number">30</span>  <span class="number">156</span>   <span class="number">78</span>  <span class="number">134</span> <span class="number">2132</span> <span class="number">9508</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]</span><br><span class="line"> [  <span class="number">30</span>  <span class="number">156</span>   <span class="number">78</span> <span class="number">3541</span> <span class="number">1460</span>  <span class="number">131</span>   <span class="number">56</span>  <span class="number">253</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]]</span><br></pre></td></tr></table></figure></p><p>Now that you have your generator, you can just call it and it will return tensors which correspond to your questions in the Quora data set.<br>Now you can go ahead and start building your neural network. </p><p><a name="2"></a></p><h1 id="Part-2-Defining-the-Siamese-model"><a href="#Part-2-Defining-the-Siamese-model" class="headerlink" title="Part 2: Defining the Siamese model"></a>Part 2: Defining the Siamese model</h1><p><a name="2.1"></a></p><h3 id="2-1-Understanding-Siamese-Network"><a href="#2-1-Understanding-Siamese-Network" class="headerlink" title="2.1 Understanding Siamese Network"></a>2.1 Understanding Siamese Network</h3><p>A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.The Siamese network you are about to implement looks like this:</p><p><img src="siamese.png" style="width:600px;height:300px;"></p><p>You get the question embedding, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally use a triplet loss (explained below) to get the corresponding cosine similarity for each pair of questions. As usual, you will start by importing the data set. The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. In math equations, you are trying to maximize the following.</p><script type="math/tex; mode=display">\mathcal{L}(A, P, N)=\max \left(\|\mathrm{f}(A)-\mathrm{f}(P)\|^{2}-\|\mathrm{f}(A)-\mathrm{f}(N)\|^{2}+\alpha, 0\right)</script><p>$A$ is the anchor input, for example $q1_1$, $P$ the duplicate input, for example, $q2_1$, and $N$ the negative input (the non duplicate question), for example $q2_2$.<br><br>$\alpha$ is a margin; you can think about it as a safety net, or by how much you want to push the duplicates from the non duplicates.<br><br></p><p><a name="ex02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement the <code>Siamese</code> function below. You should be using all the objects explained below. </p><p>To implement this model, you will be using <code>trax</code>. Concretely, you will be using the following functions.</p><ul><li><code>tl.Serial</code>: Combinator that applies layers serially (by function composition) allows you set up the overall structure of the feedforward. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26" target="_blank" rel="noopener">source code</a><ul><li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas. </li><li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code> </li></ul></li></ul><ul><li><code>tl.Embedding</code>: Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113" target="_blank" rel="noopener">source code</a><ul><li><code>tl.Embedding(vocab_size, d_feature)</code>.</li><li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li><li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li></ul></li></ul><ul><li><code>tl.LSTM</code> The LSTM layer. It leverages another Trax layer called <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTMCell" target="_blank" rel="noopener"><code>LSTMCell</code></a>. The number of units should be specified and should match the number of elements in the word embedding. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87" target="_blank" rel="noopener">source code</a><ul><li><code>tl.LSTM(n_units)</code> Builds an LSTM layer of n_units.</li></ul></li></ul><ul><li><code>tl.Mean</code>: Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276" target="_blank" rel="noopener">source code</a><ul><li><code>tl.Mean(axis=1)</code> mean over columns.</li></ul></li></ul><ul><li><code>tl.Fn</code> Layer with no weights that applies the function f, which should be specified using a lambda syntax. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/70f5364dcaf6ec11aabbd918e5f5e4b0f5bfb995/trax/layers/base.py#L576" target="_blank" rel="noopener">source doce</a><ul><li>$x$ -&gt; This is used for cosine similarity.</li><li><code>tl.Fn(&#39;Normalize&#39;, lambda x: normalize(x))</code> Returns a layer with no weights that applies the function <code>f</code></li></ul></li></ul><ul><li><code>tl.parallel</code>: It is a combinator layer (like <code>Serial</code>) that applies a list of layers in parallel to its inputs. <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel" target="_blank" rel="noopener">docs</a> / <a href="https://github.com/google/trax/blob/37aba571a89a8ad86be76a569d0ec4a46bdd8642/trax/layers/combinators.py#L152" target="_blank" rel="noopener">source code</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Siamese</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Siamese</span><span class="params">(vocab_size=len<span class="params">(vocab)</span>, d_model=<span class="number">128</span>, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a Siamese model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int, optional): Length of the vocabulary. Defaults to len(vocab).</span></span><br><span class="line"><span class="string">        d_model (int, optional): Depth of the model. Defaults to 128.</span></span><br><span class="line"><span class="string">        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to 'train'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Parallel: A Siamese model. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(x)</span>:</span>  <span class="comment"># normalizes the vectors to have L2 norm 1</span></span><br><span class="line">        <span class="keyword">return</span> x / fastnp.sqrt(fastnp.sum(x * x, axis=<span class="number">-1</span>, keepdims=<span class="keyword">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    q_processor = tl.Serial(  <span class="comment"># Processor will run on Q1 and Q2.</span></span><br><span class="line">        tl.Embedding(vocab_size, d_model), <span class="comment"># Embedding layer</span></span><br><span class="line">        tl.LSTM(d_model), <span class="comment"># LSTM layer</span></span><br><span class="line">        tl.Mean(axis=<span class="number">1</span>), <span class="comment"># Mean over columns</span></span><br><span class="line">        tl.Fn(<span class="string">'Normalize'</span>, <span class="keyword">lambda</span> x: normalize(x))  <span class="comment"># Apply normalize function</span></span><br><span class="line">    )  <span class="comment"># Returns one vector of shape [batch_size, d_model].</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run on Q1 and Q2 in parallel.</span></span><br><span class="line">    model = tl.Parallel(q_processor, q_processor)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>Setup the Siamese network model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check your model</span></span><br><span class="line">model = Siamese()</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><pre><code>Parallel_in2_out2[  Serial[    Embedding_41699_128    LSTM_128    Mean    Normalize  ]  Serial[    Embedding_41699_128    LSTM_128    Mean    Normalize  ]]</code></pre><p><strong>Expected output:</strong>  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Parallel_in2_out2[</span><br><span class="line">  Serial[</span><br><span class="line">    Embedding_41699_128</span><br><span class="line">    LSTM_128</span><br><span class="line">    Mean</span><br><span class="line">    Normalize</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Embedding_41699_128</span><br><span class="line">    LSTM_128</span><br><span class="line">    Mean</span><br><span class="line">    Normalize</span><br><span class="line">  ]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p><a name="2.2"></a></p><h3 id="2-2-Hard-Negative-Mining"><a href="#2-2-Hard-Negative-Mining" class="headerlink" title="2.2 Hard  Negative Mining"></a>2.2 Hard  Negative Mining</h3><p>You will now implement the <code>TripletLoss</code>.<br><br>As explained in the lecture, loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the <em>closest negative</em>. Our loss expression is then:</p><p>\begin{align}<br> \mathcal{Loss_1(A,P,N)} &amp;=\max \left( -cos(A,P)  + mean_{neg} +\alpha, 0\right) \\<br> \mathcal{Loss_2(A,P,N)} &amp;=\max \left( -cos(A,P)  + closest_{neg} +\alpha, 0\right) \\<br>\mathcal{Loss(A,P,N)} &amp;= mean(Loss_1 + Loss_2) \\<br>\end{align}</p><p>Further, two sets of instructions are provided. The first set provides a brief description of the task. If that set proves insufficient, a more detailed set can be displayed.  </p><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions (Brief):</strong> Here is a list of things you should do: <br></p><ul><li>As this will be run inside trax, use <code>fastnp.xyz</code> when using any <code>xyz</code> numpy function</li><li>Use <code>fastnp.dot</code> to calculate the similarity matrix $v_1v_2^T$ of dimension <code>batch_size</code> x <code>batch_size</code></li><li>Take the score of the duplicates on the diagonal <code>fastnp.diagonal</code></li><li>Use the <code>trax</code> functions <code>fastnp.eye</code> and <code>fastnp.maximum</code> for the identity matrix and the maximum.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>More Detailed Instructions </b></font></summary><br>We’ll describe the algorithm using a detailed example. Below, V1, V2 are the output of the normalization blocks in our model. Here we will use a batch_size of 4 and a d_model of 3. As explained in lecture, the inputs, Q1, Q2 are arranged so that corresponding inputs are duplicates while non-corresponding entries are not. The outputs will have the same pattern.<br><img src="C3_W4_triploss1.png" style="width:1021px;height:229px;"><br>This testcase arranges the outputs, v1,v2, to highlight different scenarios. Here, the first two outputs V1[0], V2[0] match exactly - so the model is generating the same vector for Q1[0] and Q2[0] inputs. The second outputs differ, circled in orange, we set, V2[1] is set to match V2[<strong>2</strong>], simulating a model which is generating very poor results. V1[3] and V2[3] match exactly again while V1[4] and V2[4] are set to be exactly wrong - 180 degrees from each other, circled in blue. </p><p>The first step is to compute the cosine similarity matrix or <code>score</code> in the code. As explained in lecture, this is <script type="math/tex">V_1 V_2^T</script> This is generated with <code>fastnp.dot</code>.<br><img src="C3_W4_triploss2.png" style="width:959px;height:236px;"><br>The clever arrangement of inputs creates the data needed for positive <em>and</em> negative examples without having to run all pair-wise combinations. Because Q1[n] is a duplicate of only Q2[n], other combinations are explicitly created negative examples or <em>Hard Negative</em> examples. The matrix multiplication efficiently produces the cosine similarity of all positive/negative combinations as shown above on the left side of the diagram. ‘Positive’ are the results of duplicate examples and ‘negative’ are the results of explicitly created negative examples. The results for our test case are as expected, V1[0]V2[0] match producing ‘1’ while our other ‘positive’ cases (in green) don’t match well, as was arranged. The V2[2] was set to match V1[3] producing a poor match at <code>score[2,2]</code> and an undesired ‘negative’ case of a ‘1’ shown in grey. </p><p>With the similarity matrix (<code>score</code>) we can begin to implement the loss equations. First, we can extract <script type="math/tex">cos(A,P)</script> by utilizing <code>fastnp.diagonal</code>. The goal is to grab all the green entries in the diagram above. This is <code>positive</code> in the code.</p><p>Next, we will create the <em>closest_negative</em>. This is the nonduplicate entry in V2 that is closest (has largest cosine similarity) to an entry in V1. Each row, n, of <code>score</code> represents all comparisons of the results of Q1[n] vs Q2[x] within a batch. A specific example in our testcase is row <code>score[2,:]</code>. It has the cosine similarity of V1[2] and V2[x]. The <em>closest_negative</em>, as was arranged, is V2[2] which has a score of 1. This is the maximum value of the ‘negative’ entries (blue entries in the diagram).</p><p>To implement this, we need to pick the maximum entry on a row of <code>score</code>, ignoring the ‘positive’/green entries. To avoid selecting the ‘positive’/green entries, we can make them larger negative numbers. Multiply <code>fastnp.eye(batch_size)</code> with 2.0 and subtract it out of <code>scores</code>. The result is <code>negative_without_positive</code>. Now we can use <code>fastnp.max</code>, row by row (axis=1), to select the maximum which is <code>closest_negative</code>.</p><p>Next, we’ll create <em>mean_negative</em>. As the name suggests, this is the mean of all the ‘negative’/blue values in <code>score</code> on a row by row basis. We can use <code>fastnp.eye(batch_size)</code> and a constant, this time to create a mask with zeros on the diagonal. Element-wise multiply this with <code>score</code> to get just the ‘negative values. This is <code>negative_zero_on_duplicate</code> in the code. Compute the mean by using <code>fastnp.sum</code> on <code>negative_zero_on_duplicate</code> for <code>axis=1</code> and divide it by <code>(batch_size - 1)</code> . This is <code>mean_negative</code>.</p><p>Now, we can compute loss using the two equations above and <code>fastnp.maximum</code>. This will form <code>triplet_loss1</code> and <code>triplet_loss2</code>. </p><p><code>triple_loss</code> is the <code>fastnp.mean</code> of the sum of the two individual losses.</p><p>Once you have this code matching the expected results, you can clip out the section between ### START CODE HERE and ### END CODE HERE it out and insert it into TripletLoss below.</p><p>&lt;\details&gt;  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TripletLossFn</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TripletLossFn</span><span class="params">(v1, v2, margin=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Custom Loss function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.</span></span><br><span class="line"><span class="string">        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.</span></span><br><span class="line"><span class="string">        margin (float, optional): Desired margin. Defaults to 0.25.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: Triplet Loss.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use fastnp to take the dot product of the two batches (don't forget to transpose the second argument)</span></span><br><span class="line">    scores = fastnp.dot(v1,v2.T)  <span class="comment"># pairwise cosine sim</span></span><br><span class="line">    <span class="comment"># calculate new batch size</span></span><br><span class="line">    batch_size = len(scores)</span><br><span class="line">    <span class="comment"># use fastnp to grab all postive `diagonal` entries in `scores`</span></span><br><span class="line">    positive = fastnp.diagonal(scores)  <span class="comment"># the positive ones (duplicates)</span></span><br><span class="line">    <span class="comment"># multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`</span></span><br><span class="line">    negative_without_positive = scores - fastnp.eye(batch_size) * <span class="number">2.0</span> </span><br><span class="line">    <span class="comment"># take the row by row `max` of `negative_without_positive`. </span></span><br><span class="line">    <span class="comment"># Hint: negative_without_positive.max(axis = [?])  </span></span><br><span class="line">    closest_negative = negative_without_positive.max(axis = <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`</span></span><br><span class="line">    negative_zero_on_duplicate = (<span class="number">1.0</span> - fastnp.eye(batch_size)) * scores</span><br><span class="line">    <span class="comment"># use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` </span></span><br><span class="line">    mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=<span class="number">1</span>) / (batch_size - <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># compute `fastnp.maximum` among 0.0 and `A`</span></span><br><span class="line">    <span class="comment"># A = subtract `positive` from `margin` and add `closest_negative` </span></span><br><span class="line">    triplet_loss1 = fastnp.maximum(margin - positive + closest_negative, <span class="number">0</span> )</span><br><span class="line">    <span class="comment"># compute `fastnp.maximum` among 0.0 and `B`</span></span><br><span class="line">    <span class="comment"># B = subtract `positive` from `margin` and add `mean_negative`</span></span><br><span class="line">    triplet_loss2 = fastnp.maximum(margin - positive + mean_negative, <span class="number">0</span> )</span><br><span class="line">    <span class="comment"># add the two losses together and take the `fastnp.mean` of it</span></span><br><span class="line">    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> triplet_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v1 = np.array([[<span class="number">0.26726124</span>, <span class="number">0.53452248</span>, <span class="number">0.80178373</span>],[<span class="number">0.5178918</span> , <span class="number">0.57543534</span>, <span class="number">0.63297887</span>]])</span><br><span class="line">v2 = np.array([[ <span class="number">0.26726124</span>,  <span class="number">0.53452248</span>,  <span class="number">0.80178373</span>],[<span class="number">-0.5178918</span> , <span class="number">-0.57543534</span>, <span class="number">-0.63297887</span>]])</span><br><span class="line">TripletLossFn(v2,v1)</span><br><span class="line">print(<span class="string">"Triplet Loss:"</span>, TripletLossFn(v2,v1))</span><br></pre></td></tr></table></figure><pre><code>Triplet Loss: 0.5</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Triplet Loss: <span class="number">0.5</span></span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line">To make a layer out of a function with no trainable variables, use `tl.Fn`.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">from functools <span class="keyword">import</span> partial</span><br><span class="line">def TripletLoss(margin=0.25):</span><br><span class="line">    triplet_loss_fn = partial(TripletLossFn, margin=margin)</span><br><span class="line">    return tl.Fn('TripletLoss', triplet_loss_fn)</span><br></pre></td></tr></table></figure></p><p><a name="3"></a></p><h1 id="Part-3-Training"><a href="#Part-3-Training" class="headerlink" title="Part 3: Training"></a>Part 3: Training</h1><p>Now you are going to train your model. As usual, you have to define the cost function and the optimizer. You also have to feed in the built model. Before, going into the training, we will use a special data set up. We will define the inputs using the data generator we built above. The lambda function acts as a seed to remember the last batch that was given. Run the cell below to get the question pairs inputs. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab[<span class="string">'&lt;PAD&gt;'</span>])</span><br><span class="line">val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab[<span class="string">'&lt;PAD&gt;'</span>])</span><br><span class="line">print(<span class="string">'train_Q1.shape '</span>, train_Q1.shape)</span><br><span class="line">print(<span class="string">'val_Q1.shape   '</span>, val_Q1.shape)</span><br></pre></td></tr></table></figure><pre><code>train_Q1.shape  (89188,)val_Q1.shape    (22298,)</code></pre><p><a name="3.1"></a></p><h3 id="3-1-Training-the-model"><a href="#3-1-Training-the-model" class="headerlink" title="3.1 Training the model"></a>3.1 Training the model</h3><p>You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set; each iteration is defined as an <code>epoch</code>. For each epoch, you have to go over all the data, using your training iterator.</p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement the <code>train_model</code> below to train the neural network above. Here is a list of things you should do, as already shown in lecture 7: </p><ul><li>Create <code>TrainTask</code> and <code>EvalTask</code></li><li>Create the training loop <code>trax.supervised.training.Loop</code></li><li>Pass in the following depending on the context (train_task or eval_task):<ul><li><code>labeled_data=generator</code></li><li><code>metrics=[TripletLoss()]</code>,</li><li><code>loss_layer=TripletLoss()</code></li><li><code>optimizer=trax.optimizers.Adam</code> with learning rate of 0.01</li><li><code>lr_schedule=lr_schedule</code>,</li><li><code>output_dir=output_dir</code></li></ul></li></ul><p>You will be using your triplet loss function with Adam optimizer. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam" target="_blank" rel="noopener">trax</a> documentation to get a full understanding. </p><p>This function should return a <code>training.Loop</code> object. To read more about this check the <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop" target="_blank" rel="noopener">docs</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">lr_schedule = trax.lr.warmup_and_rsqrt_decay(<span class="number">400</span>, <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir=<span class="string">'model/'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Training the Siamese Model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Siamese (function): Function that returns the Siamese model.</span></span><br><span class="line"><span class="string">        TripletLoss (function): Function that defines the TripletLoss loss function.</span></span><br><span class="line"><span class="string">        lr_schedule (function): Trax multifactor schedule function.</span></span><br><span class="line"><span class="string">        train_generator (generator, optional): Training generator. Defaults to train_generator.</span></span><br><span class="line"><span class="string">        val_generator (generator, optional): Validation generator. Defaults to val_generator.</span></span><br><span class="line"><span class="string">        output_dir (str, optional): Path to save model to. Defaults to 'model/'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop for the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    output_dir = os.path.expanduser(output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    train_task = training.TrainTask(</span><br><span class="line">        labeled_data=train_generator,       <span class="comment"># Use generator (train)</span></span><br><span class="line">        loss_layer=TripletLoss(),         <span class="comment"># Use triplet loss. Don't forget to instantiate this object</span></span><br><span class="line">        optimizer=trax.optimizers.Adam(learning_rate = <span class="number">0.01</span>),          <span class="comment"># Don't forget to add the learning rate parameter</span></span><br><span class="line">        lr_schedule=lr_schedule, <span class="comment"># Use Trax multifactor schedule function</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask(</span><br><span class="line">        labeled_data=val_generator,       <span class="comment"># Use generator (val)</span></span><br><span class="line">        metrics=[TripletLoss()],          <span class="comment"># Use triplet loss. Don't forget to instantiate this object</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    training_loop = training.Loop(Siamese(),</span><br><span class="line">                                  train_task,</span><br><span class="line">                                  eval_task=eval_task,</span><br><span class="line">                                  output_dir=output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> training_loop</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_steps = <span class="number">5</span></span><br><span class="line">training_loop = train_model(Siamese, TripletLoss, lr_schedule)</span><br><span class="line">training_loop.run(train_steps)</span><br></pre></td></tr></table></figure><pre><code>Step      1: train TripletLoss |  0.49954823Step      1: eval  TripletLoss |  0.49950948</code></pre><p>The model was only trained for 5 steps due to the constraints of this environment. For the rest of the assignment you will be using a pretrained model but now you should understand how the training can be done using Trax.</p><p><a name="4"></a></p><h1 id="Part-4-Evaluation"><a href="#Part-4-Evaluation" class="headerlink" title="Part 4:  Evaluation"></a>Part 4:  Evaluation</h1><p><a name="4.1"></a></p><h3 id="4-1-Evaluating-your-siamese-network"><a href="#4-1-Evaluating-your-siamese-network" class="headerlink" title="4.1 Evaluating your siamese network"></a>4.1 Evaluating your siamese network</h3><p>In this section you will learn how to evaluate a Siamese network. You will first start by loading a pretrained model and then you will use it to predict. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading in the saved model</span></span><br><span class="line">model = Siamese()</span><br><span class="line">model.init_from_file(<span class="string">'model.pkl.gz'</span>)</span><br></pre></td></tr></table></figure><p><a name="4.2"></a></p><h3 id="4-2-Classify"><a href="#4-2-Classify" class="headerlink" title="4.2 Classify"></a>4.2 Classify</h3><p>To determine the accuracy of the model, we will utilize the test set that was configured earlier. While in training we used only positive examples, the test data, Q1_test, Q2_test and y_test, is setup as pairs of questions, some of which are duplicates some are not.<br>This routine will run all the test question pairs through the model, compute the cosine simlarity of each pair, threshold it and compare the result to  y_test - the correct response from the data set. The results are accumulated to produce an accuracy.</p><p><a name="ex05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions</strong>  </p><ul><li>Loop through the incoming data in batch_size chunks</li><li>Use the data generator to load q1, q2 a batch at a time. <strong>Don’t forget to set shuffle=False!</strong></li><li>copy a batch_size chunk of y into y_test</li><li>compute v1, v2 using the model</li><li>for each element of the batch<pre><code> - compute the cos similarity of each pair of entries, v1[j],v2[j] - determine if d &gt; threshold - increment accuracy if that result matches the expected results (y_test[j])</code></pre></li><li>compute the final accuracy and return</li></ul><p>Due to some limitations of this environment, running classify multiple times may result in the kernel failing. If that happens <em>Restart Kernal &amp; clear output</em> and then run from the top. During development, consider using a smaller set of data to reduce the number of calls to model(). </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: classify</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=<span class="number">64</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Function to test the accuracy of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        test_Q1 (numpy.ndarray): Array of Q1 questions.</span></span><br><span class="line"><span class="string">        test_Q2 (numpy.ndarray): Array of Q2 questions.</span></span><br><span class="line"><span class="string">        y (numpy.ndarray): Array of actual target.</span></span><br><span class="line"><span class="string">        threshold (float): Desired threshold.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Parallel): The Siamese model.</span></span><br><span class="line"><span class="string">        vocab (collections.defaultdict): The vocabulary used.</span></span><br><span class="line"><span class="string">        data_generator (function): Data generator function. Defaults to data_generator.</span></span><br><span class="line"><span class="string">        batch_size (int, optional): Size of the batches. Defaults to 64.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float: Accuracy of the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    accuracy = <span class="number">0</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(test_Q1), batch_size):</span><br><span class="line">        <span class="comment"># Call the data generator (built in Ex 01) with shuffle=False using next()</span></span><br><span class="line">        <span class="comment"># use batch size chuncks of questions as Q1 &amp; Q2 arguments of the data generator. e.g x[i:i + batch_size]</span></span><br><span class="line">        <span class="comment"># Hint: use `vocab['&lt;PAD&gt;']` for the `pad` argument of the data generator</span></span><br><span class="line">        q1, q2 = next(data_generator(test_Q1[i: i + batch_size], test_Q2[i: i+batch_size], batch_size, pad=vocab[<span class="string">'&lt;PAD&gt;'</span>], shuffle=<span class="keyword">False</span>))</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(q1.shape)</span></span><br><span class="line"><span class="comment">#         print(q2.shape)</span></span><br><span class="line"><span class="comment">#         (512, 64)</span></span><br><span class="line"><span class="comment">#         (512, 64)</span></span><br><span class="line">        <span class="comment"># use batch size chuncks of actual output targets (same syntax as example above)</span></span><br><span class="line">        y_test = y[i: i + batch_size]</span><br><span class="line">        <span class="comment"># Call the model</span></span><br><span class="line">        v1, v2 = model((q1,q2))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="comment"># take dot product to compute cos similarity of each pair of entries, v1[j], v2[j]</span></span><br><span class="line">            <span class="comment"># don't forget to transpose the second argument</span></span><br><span class="line">            d = fastnp.dot(v1[j],v2[j].T)</span><br><span class="line">            <span class="comment"># is d greater than the threshold?</span></span><br><span class="line">            res = d &gt; threshold</span><br><span class="line">            <span class="comment"># increment accurancy if y_test is equal `res`</span></span><br><span class="line">            accuracy += (y_test[j] == res)</span><br><span class="line">    <span class="comment"># compute accuracy using accuracy and total length of test questions</span></span><br><span class="line">    accuracy = accuracy / len(test_Q1)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this takes around 1 minute</span></span><br><span class="line">accuracy = classify(Q1_test,Q2_test, y_test, <span class="number">0.7</span>, model, vocab, batch_size = <span class="number">512</span>) </span><br><span class="line">print(<span class="string">"Accuracy"</span>, accuracy)</span><br></pre></td></tr></table></figure><pre><code>Accuracy 0.69091797</code></pre><p><strong>Expected Result</strong><br>Accuracy ~0.69</p><p><a name="5"></a></p><h1 id="Part-5-Testing-with-your-own-questions"><a href="#Part-5-Testing-with-your-own-questions" class="headerlink" title="Part 5: Testing with your own questions"></a>Part 5: Testing with your own questions</h1><p>In this section you will test the model with your own questions. You will write a function <code>predict</code> which takes two questions as input and returns $1$ or $0$ depending on whether the question pair is a duplicate or not.   </p><p>But first, we build a reverse vocabulary that allows to map encoded questions back to words: </p><p>Write a function <code>predict</code>that takes in two questions, the model, and the vocabulary and returns whether the questions are duplicates ($1$) or not duplicates ($0$) given a similarity threshold. </p><p><a name="ex06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> </p><ul><li>Tokenize your question using <code>nltk.word_tokenize</code> </li><li>Create Q1,Q2 by encoding your questions as a list of numbers using vocab</li><li>pad Q1,Q2 with next(data_generator([Q1], [Q2],1,vocab[‘<pad>‘]))</pad></li><li>use model() to create v1, v2</li><li>compute the cosine similarity (dot product) of v1, v2</li><li>compute res by comparing d to the threshold</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">"""Function for predicting if two questions are duplicates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        question1 (str): First question.</span></span><br><span class="line"><span class="string">        question2 (str): Second question.</span></span><br><span class="line"><span class="string">        threshold (float): Desired threshold.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Parallel): The Siamese model.</span></span><br><span class="line"><span class="string">        vocab (collections.defaultdict): The vocabulary used.</span></span><br><span class="line"><span class="string">        data_generator (function): Data generator function. Defaults to data_generator.</span></span><br><span class="line"><span class="string">        verbose (bool, optional): If the results should be printed out. Defaults to False.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bool: True if the questions are duplicates, False otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># use `nltk` word tokenize function to tokenize</span></span><br><span class="line">    q1 = nltk.word_tokenize(question1)  <span class="comment"># tokenize</span></span><br><span class="line">    q2 = nltk.word_tokenize(question2)  <span class="comment"># tokenize</span></span><br><span class="line">    Q1, Q2 = [], []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q1:  <span class="comment"># encode q1</span></span><br><span class="line">        <span class="comment"># increment by checking the 'word' index in `vocab`</span></span><br><span class="line">        Q1 += [vocab[word]]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q2:  <span class="comment"># encode q2</span></span><br><span class="line">        <span class="comment"># increment by checking the 'word' index in `vocab`</span></span><br><span class="line">        Q2 += [vocab[word]]</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Call the data generator (built in Ex 01) using next()</span></span><br><span class="line">    <span class="comment"># pass [Q1] &amp; [Q2] as Q1 &amp; Q2 arguments of the data generator. Set batch size as 1</span></span><br><span class="line">    <span class="comment"># Hint: use `vocab['&lt;PAD&gt;']` for the `pad` argument of the data generator</span></span><br><span class="line">    Q1, Q2 = next(data_generator([Q1], [Q2], <span class="number">1</span>, vocab[<span class="string">'&lt;PAD&gt;'</span>]))</span><br><span class="line">    <span class="comment"># Call the model</span></span><br><span class="line">    v1, v2 = model((Q1,Q2))</span><br><span class="line">    <span class="comment"># take dot product to compute cos similarity of each pair of entries, v1, v2</span></span><br><span class="line">    <span class="comment"># don't forget to transpose the second argument</span></span><br><span class="line">    d = fastnp.dot(v1, v2.T)</span><br><span class="line">    <span class="comment"># is d greater than the threshold?</span></span><br><span class="line">    res = d &gt; threshold</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(verbose):</span><br><span class="line">        print(<span class="string">"Q1  = "</span>, Q1, <span class="string">"\nQ2  = "</span>, Q2)</span><br><span class="line">        print(<span class="string">"d   = "</span>, d)</span><br><span class="line">        print(<span class="string">"res = "</span>, res)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feel free to try with your own questions</span></span><br><span class="line">question1 = <span class="string">"When will I see you?"</span></span><br><span class="line">question2 = <span class="string">"When can I see you again?"</span></span><br><span class="line"><span class="comment"># 1 means it is duplicated, 0 otherwise</span></span><br><span class="line">predict(question1 , question2, <span class="number">0.7</span>, model, vocab, verbose = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Q1  =  [[585  76   4  46  53  21   1   1]] Q2  =  [[ 585   33    4   46   53 7280   21    1]]d   =  [[0.8811324]]res =  [[ True]]DeviceArray([[ True]], dtype=bool)</code></pre><h5 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h5><p>If input is:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">question1 = <span class="string">"When will I see you?"</span></span><br><span class="line">question2 = <span class="string">"When can I see you again?"</span></span><br></pre></td></tr></table></figure></p><p>Output is (d may vary a bit):<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q1  =  [[<span class="number">585</span>  <span class="number">76</span>   <span class="number">4</span>  <span class="number">46</span>  <span class="number">53</span>  <span class="number">21</span>   <span class="number">1</span>   <span class="number">1</span>]] </span><br><span class="line">Q2  =  [[ <span class="number">585</span>   <span class="number">33</span>    <span class="number">4</span>   <span class="number">46</span>   <span class="number">53</span> <span class="number">7280</span>   <span class="number">21</span>    <span class="number">1</span>]]</span><br><span class="line">d   =  <span class="number">0.88113236</span></span><br><span class="line">res =  True</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feel free to try with your own questions</span></span><br><span class="line">question1 = <span class="string">"Do they enjoy eating the dessert?"</span></span><br><span class="line">question2 = <span class="string">"Do they like hiking in the desert?"</span></span><br><span class="line"><span class="comment"># 1 means it is duplicated, 0 otherwise</span></span><br><span class="line">predict(question1 , question2, <span class="number">0.7</span>, model, vocab, verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Q1  =  [[  443  1145  3159  1169    78 29017    21     1]] Q2  =  [[  443  1145    60 15302    28    78  7431    21]]d   =  [[0.477536]]res =  [[False]]DeviceArray([[False]], dtype=bool)</code></pre><h5 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h5><p>If input is:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">question1 = <span class="string">"Do they enjoy eating the dessert?"</span></span><br><span class="line">question2 = <span class="string">"Do they like hiking in the desert?"</span></span><br></pre></td></tr></table></figure></p><p>Output  (d may vary a bit):</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q1  =  [[  <span class="number">443</span>  <span class="number">1145</span>  <span class="number">3159</span>  <span class="number">1169</span>    <span class="number">78</span> <span class="number">29017</span>    <span class="number">21</span>     <span class="number">1</span>]] </span><br><span class="line">Q2  =  [[  <span class="number">443</span>  <span class="number">1145</span>    <span class="number">60</span> <span class="number">15302</span>    <span class="number">28</span>    <span class="number">78</span>  <span class="number">7431</span>    <span class="number">21</span>]]</span><br><span class="line">d   =  <span class="number">0.477536</span></span><br><span class="line">res =  False</span><br><span class="line">False</span><br></pre></td></tr></table></figure><p>You can see that the Siamese network is capable of catching complicated structures. Concretely it can identify question duplicates although the questions do not have many words in common. </p><p><a name="6"></a></p><h3 id="On-Siamese-networks"><a href="#On-Siamese-networks" class="headerlink" title=" On Siamese networks "></a><span style="color:blue"> On Siamese networks </span></h3><p>Siamese networks are important and useful. Many times there are several questions that are already asked in quora, or other platforms and you can use Siamese networks to avoid question duplicates. </p><p>Congratulations, you have now built a powerful system that can recognize question duplicates. In the next course we will use transformers for machine translation, summarization, question answering, and chatbots. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-4-Question-duplicates&quot;&gt;&lt;a href=&quot;#Assignment-4-Question-duplicates&quot; class=&quot;headerlink&quot; title=&quot;Assignment 4:  Question dupl
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Named Entity Recognition (NER)</title>
    <link href="https://zhangruochi.com/Named-Entity-Recognition-NER/2020/08/23/"/>
    <id>https://zhangruochi.com/Named-Entity-Recognition-NER/2020/08/23/</id>
    <published>2020-08-22T17:01:04.000Z</published>
    <updated>2020-08-22T17:01:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-3-Named-Entity-Recognition-NER"><a href="#Assignment-3-Named-Entity-Recognition-NER" class="headerlink" title="Assignment 3 - Named Entity Recognition (NER)"></a>Assignment 3 - Named Entity Recognition (NER)</h1><p>Welcome to the third programming assignment of Course 3. In this assignment, you will learn to build more complicated models with Trax. By completing this assignment, you will be able to: </p><ul><li>Design the architecture of a neural network, train it, and test it. </li><li>Process features and represents them</li><li>Understand word padding</li><li>Implement LSTMs</li><li>Test with your own sentence</li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">Introduction</a></li><li><a href="#1">Part 1:  Exploring the data</a><ul><li><a href="#1.1">1.1  Importing the Data</a></li><li><a href="#1.2">1.2  Data generator</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li></ul></li><li><a href="#2">Part 2:  Building the model</a><ul><li><a href="#ex02">Exercise 02</a></li></ul></li><li><a href="#3">Part 3:  Train the Model </a><ul><li><a href="#ex03">Exercise 03</a></li></ul></li><li><a href="#4">Part 4:  Compute Accuracy</a><ul><li><a href="#ex04">Exercise 04</a></li></ul></li><li><a href="#5">Part 5:  Testing with your own sentence</a></li></ul><p><a name="0"></a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>We first start by defining named entity recognition (NER). NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. </p><p>For example:</p><p><img src="ner.png" width="width" height="height" style="width:600px;height:150px;"></p><p>Is labeled as follows: </p><ul><li>French: geopolitical entity</li><li>Morocco: geographic entity </li><li>Christmas: time indicator</li></ul><p>Everything else that is labeled with an <code>O</code> is not considered to be a named entity. In this assignment, you will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. Then, you will load in the exact version of your model, which was trained for a longer period of time. You could then evaluate the trained version of your model to get 96% accuracy! Finally, you will be able to test your named entity recognition system with your own sentence.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!pip -q install trax==1.3.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> get_params, get_vocab</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rnd</span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seeds to make this notebook easier to replicate</span></span><br><span class="line">trax.supervised.trainer_lib.init_random_number_generators(<span class="number">33</span>)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 DeviceArray([ 0, 33], dtype=uint32)</code></pre><p><a name="1"></a></p><h1 id="Part-1-Exploring-the-data"><a href="#Part-1-Exploring-the-data" class="headerlink" title="Part 1:  Exploring the data"></a>Part 1:  Exploring the data</h1><p>We will be using a dataset from Kaggle, which we will preprocess for you. The original data consists of four columns, the sentence number, the word, the part of speech of the word, and the tags.  A few tags you might expect to see are: </p><ul><li>geo: geographical entity</li><li>org: organization</li><li>per: person </li><li>gpe: geopolitical entity</li><li>tim: time indicator</li><li>art: artifact</li><li>eve: event</li><li>nat: natural phenomenon</li><li>O: filler word</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># display original kaggle data</span></span><br><span class="line">data = pd.read_csv(<span class="string">"ner_dataset.csv"</span>, encoding = <span class="string">"ISO-8859-1"</span>) </span><br><span class="line">train_sents = open(<span class="string">'data/small/train/sentences.txt'</span>, <span class="string">'r'</span>).readline()</span><br><span class="line">train_labels = open(<span class="string">'data/small/train/labels.txt'</span>, <span class="string">'r'</span>).readline()</span><br><span class="line">print(<span class="string">'SENTENCE:'</span>, train_sents)</span><br><span class="line">print(<span class="string">'SENTENCE LABEL:'</span>, train_labels)</span><br><span class="line">print(<span class="string">'ORIGINAL DATA:\n'</span>, data.head(<span class="number">5</span>))</span><br><span class="line"><span class="keyword">del</span>(data, train_sents, train_labels)</span><br></pre></td></tr></table></figure><pre><code>SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O OORIGINAL DATA:     Sentence #           Word  POS Tag0  Sentence: 1      Thousands  NNS   O1          NaN             of   IN   O2          NaN  demonstrators  NNS   O3          NaN           have  VBP   O4          NaN        marched  VBN   O</code></pre><p><a name="1.1"></a></p><h2 id="1-1-Importing-the-Data"><a href="#1-1-Importing-the-Data" class="headerlink" title="1.1  Importing the Data"></a>1.1  Importing the Data</h2><p>In this part, we will import the preprocessed data and explore it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vocab, tag_map = get_vocab(<span class="string">'data/large/words.txt'</span>, <span class="string">'data/large/tags.txt'</span>)</span><br><span class="line">t_sentences, t_labels, t_size = get_params(vocab, tag_map, <span class="string">'data/large/train/sentences.txt'</span>, <span class="string">'data/large/train/labels.txt'</span>)</span><br><span class="line">v_sentences, v_labels, v_size = get_params(vocab, tag_map, <span class="string">'data/large/val/sentences.txt'</span>, <span class="string">'data/large/val/labels.txt'</span>)</span><br><span class="line">test_sentences, test_labels, test_size = get_params(vocab, tag_map, <span class="string">'data/large/test/sentences.txt'</span>, <span class="string">'data/large/test/labels.txt'</span>)</span><br></pre></td></tr></table></figure><p><code>vocab</code> is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a <code>&lt;PAD&gt;</code> token. </p><p>When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic <code>&lt;PAD&gt;</code> token to fill all the empty spaces. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocab translates from a word to a unique number</span></span><br><span class="line">print(<span class="string">'vocab["the"]:'</span>, vocab[<span class="string">"the"</span>])</span><br><span class="line"><span class="comment"># Pad token</span></span><br><span class="line">print(<span class="string">'padded token:'</span>, vocab[<span class="string">'&lt;PAD&gt;'</span>])</span><br></pre></td></tr></table></figure><pre><code>vocab[&quot;the&quot;]: 9padded token: 35180</code></pre><p>The tag_map corresponds to one of the possible tags a word can have. Run the cell below to see the possible classes you will be predicting. The prepositions in the tags mean:</p><ul><li>I: Token is inside an entity.</li><li>B: Token begins an entity.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(tag_map)</span><br></pre></td></tr></table></figure><pre><code>{&#39;O&#39;: 0, &#39;B-geo&#39;: 1, &#39;B-gpe&#39;: 2, &#39;B-per&#39;: 3, &#39;I-geo&#39;: 4, &#39;B-org&#39;: 5, &#39;I-org&#39;: 6, &#39;B-tim&#39;: 7, &#39;B-art&#39;: 8, &#39;I-art&#39;: 9, &#39;I-per&#39;: 10, &#39;I-gpe&#39;: 11, &#39;I-tim&#39;: 12, &#39;B-nat&#39;: 13, &#39;B-eve&#39;: 14, &#39;I-eve&#39;: 15, &#39;I-nat&#39;: 16}</code></pre><p>So the coding scheme that tags the entities is a minimal one where B- indicates the first token in a multi-token entity, and I- indicates one in the middle of a multi-token entity. If you had the sentence </p><p><strong>“Sharon flew to Miami on Friday”</strong></p><p>the outputs would look like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Sharon B-per</span><br><span class="line">flew   O</span><br><span class="line">to     O</span><br><span class="line">Miami  B-geo</span><br><span class="line">on     O</span><br><span class="line">Friday B-tim</span><br></pre></td></tr></table></figure><p>your tags would reflect three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon’s last name to the sentence: </p><p><strong>“Sharon Floyd flew to Miami on Friday”</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Sharon B-per</span><br><span class="line">Floyd  I-per</span><br><span class="line">flew   O</span><br><span class="line">to     O</span><br><span class="line">Miami  B-geo</span><br><span class="line">on     O</span><br><span class="line">Friday B-tim</span><br></pre></td></tr></table></figure><p>then your tags would change to show first “Sharon” as B-per, and “Floyd” as I-per, where I- indicates an inner token in a multi-token sequence.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Exploring information about the data</span></span><br><span class="line">print(<span class="string">'The number of outputs is tag_map'</span>, len(tag_map))</span><br><span class="line"><span class="comment"># The number of vocabulary tokens (including &lt;PAD&gt;)</span></span><br><span class="line">g_vocab_size = len(vocab)</span><br><span class="line">print(<span class="string">f"Num of vocabulary words: <span class="subst">&#123;g_vocab_size&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">'The vocab size is'</span>, len(vocab))</span><br><span class="line">print(<span class="string">'The training size is'</span>, t_size)</span><br><span class="line">print(<span class="string">'The validation size is'</span>, v_size)</span><br><span class="line">print(<span class="string">'An example of the first sentence is'</span>, t_sentences[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'An example of its corresponding label is'</span>, t_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>The number of outputs is tag_map 17Num of vocabulary words: 35181The vocab size is 35181The training size is 33570The validation size is 7194An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]</code></pre><p>So you can see that we have already encoded each sentence into a tensor by converting it into a number. We also have 16 possible classes, as shown in the tag map.</p><p><a name="1.2"></a></p><h2 id="1-2-Data-generator"><a href="#1-2-Data-generator" class="headerlink" title="1.2  Data generator"></a>1.2  Data generator</h2><p>In python, a generator is a function that behaves like an iterator. It will return the next item. Here is a <a href="https://wiki.python.org/moin/Generators" target="_blank" rel="noopener">link</a> to review python generators. </p><p>In many AI applications it is very useful to have a data generator. You will now implement a data generator for our NER application.</p><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Implement a data generator function that takes in <code>batch_size, x, y, pad, shuffle</code> where x is a large list of sentences, and y is a list of the tags associated with those sentences and pad is a pad value. Return a subset of those inputs in a tuple of two arrays <code>(X,Y)</code>. Each is an array of dimension (<code>batch_size, max_len</code>), where <code>max_len</code> is the length of the longest sentence <em>in that batch</em>. You will pad the X and Y examples with the pad argument. If <code>shuffle=True</code>, the data will be traversed in a random form.</p><p><strong>Details:</strong></p><p>This code as an outer loop<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while True:  </span><br><span class="line">...  </span><br><span class="line">yield((X,Y))</span><br></pre></td></tr></table></figure></p><p>Which runs continuously in the fashion of generators, pausing when yielding the next values. We will generate a batch_size output on each pass of this loop.    </p><p>It has two inner loops. </p><ol><li><p>The first stores in temporal lists the data samples to be included in the next batch, and finds the maximum length of the sentences contained in it. By adjusting the length to include only the size of the longest sentence in each batch, overall computation is reduced. </p></li><li><p>The second loop moves those inputs from the temporal list into NumPy arrays pre-filled with pad values.</p></li></ol><p>There are three slightly out of the ordinary features. </p><ol><li><p>The first is the use of the NumPy <code>full</code> function to fill the NumPy arrays with a pad value. See <a href="https://numpy.org/doc/1.18/reference/generated/numpy.full.html" target="_blank" rel="noopener">full function documentation</a>.</p></li><li><p>The second is tracking the current location in the incoming lists of sentences. Generators variables hold their values between invocations, so we create an <code>index</code> variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the <code>index</code> to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.  </p></li><li><p>The third also relates to wrapping. Because <code>batch_size</code> and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the <code>index</code> to 0. We can re-shuffle the list of indexes to produce different batches each time.</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: data_generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span><span class="params">(batch_size, x, y, pad, shuffle=False, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">      Input: </span></span><br><span class="line"><span class="string">        batch_size - integer describing the batch size</span></span><br><span class="line"><span class="string">        x - list containing sentences where words are represented as integers</span></span><br><span class="line"><span class="string">        y - list containing tags associated with the sentences</span></span><br><span class="line"><span class="string">        shuffle - Shuffle the data order</span></span><br><span class="line"><span class="string">        pad - an integer representing a pad character</span></span><br><span class="line"><span class="string">        verbose - Print information during runtime</span></span><br><span class="line"><span class="string">      Output:</span></span><br><span class="line"><span class="string">        a tuple containing 2 elements:</span></span><br><span class="line"><span class="string">        X - np.ndarray of dim (batch_size, max_len) of padded sentences</span></span><br><span class="line"><span class="string">        Y - np.ndarray of dim (batch_size, max_len) of tags associated with the sentences in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># count the number of lines in data_lines</span></span><br><span class="line">    num_lines = len(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># create an array with the indexes of data_lines that can be shuffled</span></span><br><span class="line">    lines_index = [*range(num_lines)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># shuffle the indexes if shuffle is set to True</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        rnd.shuffle(lines_index)</span><br><span class="line">    </span><br><span class="line">    index = <span class="number">0</span> <span class="comment"># tracks current location in x, y</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        buffer_x = [<span class="number">0</span>] * batch_size <span class="comment"># Temporal array to store the raw x data for this batch</span></span><br><span class="line">        buffer_y = [<span class="number">0</span>] * batch_size <span class="comment"># Temporal array to store the raw y data for this batch</span></span><br><span class="line">                </span><br><span class="line">  <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Copy into the temporal buffers the sentences in x[index : index + batch_size] </span></span><br><span class="line">        <span class="comment"># along with their corresponding labels y[index : index + batch_size]</span></span><br><span class="line">        <span class="comment"># Find maximum length of sentences in x[index : index + batch_size] for this batch. </span></span><br><span class="line">        <span class="comment"># Reset the index if we reach the end of the data set, and shuffle the indexes if needed.</span></span><br><span class="line">        max_len = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">             <span class="comment"># if the index is greater than or equal to the number of lines in x</span></span><br><span class="line">            <span class="keyword">if</span> index &gt;= num_lines:</span><br><span class="line">                <span class="comment"># then reset the index to 0</span></span><br><span class="line">                index = <span class="number">0</span></span><br><span class="line">                <span class="comment"># re-shuffle the indexes if shuffle is set to True</span></span><br><span class="line">                <span class="keyword">if</span> shuffle:</span><br><span class="line">                    rnd.shuffle(lines_index)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># The current position is obtained using `lines_index[index]`</span></span><br><span class="line">            <span class="comment"># Store the x value at the current position into the buffer_x</span></span><br><span class="line">            buffer_x[i] = x[lines_index[index]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Store the y value at the current position into the buffer_y</span></span><br><span class="line">            buffer_y[i] = y[lines_index[index]]</span><br><span class="line">            </span><br><span class="line">            lenx = len(x[lines_index[index]])    <span class="comment">#length of current x[]</span></span><br><span class="line">            <span class="keyword">if</span> lenx &gt; max_len:</span><br><span class="line">                max_len = lenx                   <span class="comment">#max_len tracks longest x[]</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># increment index by one</span></span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value</span></span><br><span class="line">        X = np.full((batch_size, max_len), pad)</span><br><span class="line">        Y = np.full((batch_size, max_len), pad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># copy values from lists to NumPy arrays. Use the buffered values</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="comment"># get the example (sentence as a tensor)</span></span><br><span class="line">            <span class="comment"># in `buffer_x` at the `i` index</span></span><br><span class="line">            x_i = buffer_x[i]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># similarly, get the example's labels</span></span><br><span class="line">            <span class="comment"># in `buffer_y` at the `i` index</span></span><br><span class="line">            y_i = buffer_y[i]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Walk through each word in x_i</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_i)):</span><br><span class="line">                <span class="comment"># store the word in x_i at position j into X</span></span><br><span class="line">                X[i, j] = x_i[j]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># store the label in y_i at position j into Y</span></span><br><span class="line">                Y[i, j] = y_i[j]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        <span class="keyword">if</span> verbose: print(<span class="string">"index="</span>, index)</span><br><span class="line">        <span class="keyword">yield</span>((X,Y))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">5</span></span><br><span class="line">mini_sentences = t_sentences[<span class="number">0</span>: <span class="number">8</span>]</span><br><span class="line">mini_labels = t_labels[<span class="number">0</span>: <span class="number">8</span>]</span><br><span class="line">dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[<span class="string">"&lt;PAD&gt;"</span>], shuffle=<span class="keyword">False</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line">X1, Y1 = next(dg)</span><br><span class="line">X2, Y2 = next(dg)</span><br><span class="line">print(Y1.shape, X1.shape, Y2.shape, X2.shape)</span><br><span class="line">print(X1[<span class="number">0</span>][:], <span class="string">"\n"</span>, Y1[<span class="number">0</span>][:])</span><br></pre></td></tr></table></figure><pre><code>index= 5index= 2(5, 30) (5, 30) (5, 30) (5, 30)[    0     1     2     3     4     5     6     7     8     9    10    11    12    13    14     9    15     1    16    17    18    19    20    21 35180 35180 35180 35180 35180 35180]  [    0     0     0     0     0     0     1     0     0     0     0     0     1     0     0     0     0     0     2     0     0     0     0     0 35180 35180 35180 35180 35180 35180]</code></pre><p><strong>Expected output:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">index= 5</span><br><span class="line">index= 2</span><br><span class="line">(5, 30) (5, 30) (5, 30) (5, 30)</span><br><span class="line">[    0     1     2     3     4     5     6     7     8     9    10    11</span><br><span class="line">    12    13    14     9    15     1    16    17    18    19    20    21</span><br><span class="line"> 35180 35180 35180 35180 35180 35180] </span><br><span class="line"> [    0     0     0     0     0     0     1     0     0     0     0     0</span><br><span class="line">     1     0     0     0     0     0     2     0     0     0     0     0</span><br><span class="line"> 35180 35180 35180 35180 35180 35180]</span><br></pre></td></tr></table></figure></p><p><a name="2"></a></p><h1 id="Part-2-Building-the-model"><a href="#Part-2-Building-the-model" class="headerlink" title="Part 2:  Building the model"></a>Part 2:  Building the model</h1><p>You will now implement the model. You will be using Google’s TensorFlow. Your model will be able to distinguish the following:</p><table>    <tr>        <td><img src="ner1.png" width="width" height="height" style="width:500px;height:150px;">        </td>    </tr></table><p>The model architecture will be as follows: </p><p><img src="ner2.png" width="width" height="height" style="width:600px;height:250px;"></p><p>Concretely: </p><ul><li>Use the input tensors you built in your data generator</li><li>Feed it into an Embedding layer, to produce more semantic entries</li><li>Feed it into an LSTM layer</li><li>Run the output through a linear layer</li><li>Run the result through a log softmax layer to get the predicted class for each word.</li></ul><p>Good news! We won’t make you implement the LSTM unit drawn above. However, we will ask you to build the model. </p><p><a name="ex02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement the initialization step and the forward function of your Named Entity Recognition system.<br>Please utilize help function e.g. <code>help(tl.Dense)</code> for more information on a layer</p><ul><li><a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26" target="_blank" rel="noopener">tl.Serial</a>: Combinator that applies layers serially (by function composition).<ul><li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas. </li><li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code> </li></ul></li></ul><ul><li><a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113" target="_blank" rel="noopener">tl.Embedding</a>: Initializes the embedding. In this case it is the dimension of the model by the size of the vocabulary. <ul><li><code>tl.Embedding(vocab_size, d_feature)</code>.</li><li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li><li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li></ul></li></ul><ul><li><a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87" target="_blank" rel="noopener">tl.LSTM</a>:<code>Trax</code> LSTM layer of size d_model. <ul><li><code>LSTM(n_units)</code> Builds an LSTM layer of n_cells.</li></ul></li></ul><ul><li><a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L28" target="_blank" rel="noopener">tl.Dense</a>:  A dense layer.<ul><li><code>tl.Dense(n_units)</code>: The parameter <code>n_units</code> is the number of units chosen for this dense layer.  </li></ul></li></ul><ul><li><a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242" target="_blank" rel="noopener">tl.LogSoftmax</a>: Log of the output probabilities.<ul><li>Here, you don’t need to set any parameters for <code>LogSoftMax()</code>.</li></ul></li></ul><p><strong>Online documentation</strong></p><ul><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators" target="_blank" rel="noopener">tl.Serial</a></p></li><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding" target="_blank" rel="noopener">tl.Embedding</a></p></li><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM" target="_blank" rel="noopener">tl.LSTM</a></p></li><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a></p></li><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax" target="_blank" rel="noopener">tl.LogSoftmax</a>    </p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: NER</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">NER</span><span class="params">(vocab_size=<span class="number">35181</span>, d_model=<span class="number">50</span>, tags=tag_map)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">      Input: </span></span><br><span class="line"><span class="string">        vocab_size - integer containing the size of the vocabulary</span></span><br><span class="line"><span class="string">        d_model - integer describing the embedding size</span></span><br><span class="line"><span class="string">      Output:</span></span><br><span class="line"><span class="string">        model - a trax serial model</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    model = tl.Serial(</span><br><span class="line">      tl.Embedding(vocab_size,d_model), <span class="comment"># Embedding layer</span></span><br><span class="line">      tl.LSTM(d_model), <span class="comment"># LSTM layer</span></span><br><span class="line">      tl.Dense(len(tags)), <span class="comment"># Dense layer with len(tags) units</span></span><br><span class="line">      tl.LogSoftmax()  <span class="comment"># LogSoftmax layer</span></span><br><span class="line">      )</span><br><span class="line">      <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initializing your model</span></span><br><span class="line">model = NER()</span><br><span class="line"><span class="comment"># display your model</span></span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><pre><code>Serial[  Embedding_35181_50  LSTM_50  Dense_17  LogSoftmax]</code></pre><p><strong>Expected output:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Embedding_35181_50</span><br><span class="line">  LSTM_50</span><br><span class="line">  Dense_17</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;a name=&quot;3&quot;&gt;&lt;/a&gt;</span><br><span class="line"># Part 3:  Train the Model </span><br><span class="line"></span><br><span class="line">This section will train your model.</span><br><span class="line"></span><br><span class="line">Before you start, you need to create the data generators for training and validation data. It is important that you mask padding in the loss weights of your data, which can be done using the `id_to_mask` argument of `trax.supervised.inputs.add_loss_weights`.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">from trax.supervised import training</span><br><span class="line"></span><br><span class="line">rnd.seed(33)</span><br><span class="line"></span><br><span class="line">batch_size = 64</span><br><span class="line"></span><br><span class="line"># Create training data, mask pad id=35180 for training.</span><br><span class="line">train_generator = trax.supervised.inputs.add_loss_weights(</span><br><span class="line">    data_generator(batch_size, t_sentences, t_labels, vocab[&apos;&lt;PAD&gt;&apos;], True),</span><br><span class="line">    id_to_mask=vocab[&apos;&lt;PAD&gt;&apos;])</span><br><span class="line"></span><br><span class="line"># Create validation data, mask pad id=35180 for training.</span><br><span class="line">eval_generator = trax.supervised.inputs.add_loss_weights(</span><br><span class="line">    data_generator(batch_size, v_sentences, v_labels, vocab[&apos;&lt;PAD&gt;&apos;], True),</span><br><span class="line">    id_to_mask=vocab[&apos;&lt;PAD&gt;&apos;])</span><br></pre></td></tr></table></figure></p><p><a name="3.1"></a></p><h3 id="3-1-Training-the-model"><a href="#3-1-Training-the-model" class="headerlink" title="3.1 Training the model"></a>3.1 Training the model</h3><p>You will now write a function that takes in your model and trains it.</p><p>As you’ve seen in the previous assignments, you will first create the <a href="https://trax-ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.TrainTask" target="_blank" rel="noopener">TrainTask</a> and <a href="https://trax-ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.EvalTask" target="_blank" rel="noopener">EvalTask</a> using your data generator. Then you will use the <code>training.Loop</code> to train your model.</p><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the <code>train_model</code> program below to train the neural network above. Here is a list of things you should do: </p><ul><li><p>Create the trainer object by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop" target="_blank" rel="noopener"><code>trax.supervised.training.Loop</code></a> and pass in the following:</p><ul><li>model = <a href="#ex02">NER</a></li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask" target="_blank" rel="noopener">training task</a> that uses the train data generator defined in the cell above<ul><li>loss_layer = <a href="https://github.com/google/trax/blob/22765bb18608d376d8cd660f9865760e4ff489cd/trax/layers/metrics.py#L71" target="_blank" rel="noopener">tl.CrossEntropyLoss()</a></li><li>optimizer = <a href="https://github.com/google/trax/blob/03cb32995e83fc1455b0c8d1c81a14e894d0b7e3/trax/optimizers/adam.py#L23" target="_blank" rel="noopener">trax.optimizers.Adam(0.01)</a></li></ul></li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask" target="_blank" rel="noopener">evaluation task</a> that uses the validation data generator defined in the cell above<ul><li>metrics for <code>EvalTask</code>: <code>tl.CrossEntropyLoss()</code> and <code>tl.Accuracy()</code></li><li>in <code>EvalTask</code> set <code>n_eval_batches=10</code> for better evaluation accuracy</li></ul></li><li>output_dir = output_dir</li></ul></li></ul><p>You’ll be using a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss" target="_blank" rel="noopener">cross entropy loss</a>, with an <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam" target="_blank" rel="noopener">Adam optimizer</a>. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/trax.html" target="_blank" rel="noopener">trax</a> documentation to get a full understanding. The <a href="https://github.com/google/trax" target="_blank" rel="noopener">trax GitHub</a> also contains some useful information and a link to a colab notebook.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(NER, train_generator, eval_generator, train_steps=<span class="number">1</span>, output_dir=<span class="string">'model'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        NER - the model you are building</span></span><br><span class="line"><span class="string">        train_generator - The data generator for training examples</span></span><br><span class="line"><span class="string">        eval_generator - The data generator for validation examples,</span></span><br><span class="line"><span class="string">        train_steps - number of training steps</span></span><br><span class="line"><span class="string">        output_dir - folder to save your model</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        training_loop - a trax supervised training Loop</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    train_task = training.TrainTask(</span><br><span class="line">      train_generator, <span class="comment"># A train data generator</span></span><br><span class="line">      loss_layer = tl.CrossEntropyLoss(), <span class="comment"># A cross-entropy loss function</span></span><br><span class="line">      optimizer =  trax.optimizers.Adam(<span class="number">0.01</span>),  <span class="comment"># The adam optimizer</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask(</span><br><span class="line">      labeled_data = eval_generator, <span class="comment"># A labeled data generator</span></span><br><span class="line">      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], <span class="comment"># Evaluate with cross-entropy loss and accuracy</span></span><br><span class="line">      n_eval_batches = <span class="number">10</span> <span class="comment"># Number of batches to use on each evaluation</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    training_loop = training.Loop(</span><br><span class="line">        NER, <span class="comment"># A model to train</span></span><br><span class="line">        train_task, <span class="comment"># A train task</span></span><br><span class="line">        eval_task = eval_task, <span class="comment"># The evaluation task</span></span><br><span class="line">        output_dir = output_dir) <span class="comment"># The output directory</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train with train_steps</span></span><br><span class="line">    training_loop.run(n_steps = train_steps)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> training_loop</span><br></pre></td></tr></table></figure><p>On your local machine, you can run this training for 1000 train_steps and get your own model. This training takes about 5 to 10 minutes to run.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_steps = <span class="number">100</span>            <span class="comment"># In coursera we can only train 100 steps</span></span><br><span class="line">!rm -f <span class="string">'model/model.pkl.gz'</span>  <span class="comment"># Remove old model.pkl if it exists</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">training_loop = train_model(NER(), train_generator, eval_generator, train_steps)</span><br></pre></td></tr></table></figure><pre><code>Step      1: train CrossEntropyLoss |  3.29933977Step      1: eval  CrossEntropyLoss |  2.27930465Step      1: eval          Accuracy |  0.22279498Step    100: train CrossEntropyLoss |  0.61237383Step    100: eval  CrossEntropyLoss |  0.37608672Step    100: eval          Accuracy |  0.90983244</code></pre><p><strong>Expected output (Approximately)</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Step      1: train CrossEntropyLoss |  2.94375849</span><br><span class="line">Step      1: eval  CrossEntropyLoss |  1.93172036</span><br><span class="line">Step      1: eval          Accuracy |  0.78727312</span><br><span class="line">Step    100: train CrossEntropyLoss |  0.57727730</span><br><span class="line">Step    100: eval  CrossEntropyLoss |  0.36356260</span><br><span class="line">Step    100: eval          Accuracy |  0.90943187</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>This value may change between executions, but it must be around 90% of accuracy on train and validations sets, after 100 training steps.</p><p>We have trained the model longer, and we give you such a trained model. In that way, we ensure you can continue with the rest of the assignment even if you had some troubles up to here, and also we are sure that everybody will get the same outputs for the last example. However, you are free to try your model, as well. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># loading in a pretrained model..</span></span><br><span class="line">model = NER()</span><br><span class="line">model.init(trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pretrained model</span></span><br><span class="line">model.init_from_file(<span class="string">'model.pkl.gz'</span>, weights_only=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><a name="4"></a></p><h1 id="Part-4-Compute-Accuracy"><a href="#Part-4-Compute-Accuracy" class="headerlink" title="Part 4:  Compute Accuracy"></a>Part 4:  Compute Accuracy</h1><p>You will now evaluate in the test set. Previously, you have seen the accuracy on the training set and the validation (noted as eval) set. You will now evaluate on your test set. To get a good evaluation, you will need to create a mask to avoid counting the padding tokens when computing the accuracy. </p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Write a program that takes in your model and uses it to evaluate on the test set. You should be able to get an accuracy of 95%.  </p><p><details>    </details></p><summary>    <font size="3" color="darkgreen"><b>More Detailed Instructions </b></font></summary><ul><li><p><em>Step 1</em>: model(sentences) will give you the predicted output. </p></li><li><p><em>Step 2</em>: Prediction will produce an output with an added dimension. For each sentence, for each word, there will be a vector of probabilities for each tag type. For each sentence,word, you need to pick the maximum valued tag. This will require <code>np.argmax</code> and careful use of the <code>axis</code> argument.</p></li><li><em>Step 3</em>: Create a mask to prevent counting pad characters. It has the same dimension as output. An example below on matrix comparison provides a hint.</li><li><em>Step 4</em>: Compute the accuracy metric by comparing your outputs against your test labels. Take the sum of that and divide by the total number of <strong>unpadded</strong> tokens. Use your mask value to mask the padded tokens. Return the accuracy.<br>&lt;/detail&gt;</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Example of a comparision on a matrix </span></span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">a == <span class="number">2</span></span><br></pre></td></tr></table></figure><pre><code>array([False,  True, False, False])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create the evaluation inputs</span></span><br><span class="line">x, y = next(data_generator(len(test_sentences), test_sentences, test_labels, vocab[<span class="string">'&lt;PAD&gt;'</span>]))</span><br><span class="line">print(<span class="string">"input shapes"</span>, x.shape, y.shape)</span><br></pre></td></tr></table></figure><pre><code>input shapes (7194, 70) (7194, 70)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sample prediction</span></span><br><span class="line">tmp_pred = model(x)</span><br><span class="line">print(type(tmp_pred))</span><br><span class="line">print(<span class="string">f"tmp_pred has shape: <span class="subst">&#123;tmp_pred.shape&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;jax.interpreters.xla.DeviceArray&#39;&gt;tmp_pred has shape: (7194, 70, 17)</code></pre><p>Note that the model’s prediction has 3 axes: </p><ul><li>the number of examples</li><li>the number of words in each example (padded to be as long as the longest sentence in the batch)</li><li>the number of possible targets (the 17 named entity tags).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: evaluate_prediction</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_prediction</span><span class="params">(pred, labels, pad)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">        pred: prediction array with shape </span></span><br><span class="line"><span class="string">            (num examples, max sentence length in batch, num of classes)</span></span><br><span class="line"><span class="string">        labels: array of size (batch_size, seq_len)</span></span><br><span class="line"><span class="string">        pad: integer representing pad character</span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        accuracy: float</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"><span class="comment">## step 1 ##</span></span><br><span class="line">    outputs = np.argmax(pred, axis=<span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">"outputs shape:"</span>, outputs.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">## step 2 ##</span></span><br><span class="line">    mask = ~(labels == pad)</span><br><span class="line">    print(<span class="string">"mask shape:"</span>, mask.shape, <span class="string">"mask[0][20:30]:"</span>, mask[<span class="number">0</span>][<span class="number">20</span>:<span class="number">30</span>])</span><br><span class="line"><span class="comment">## step 3 ##</span></span><br><span class="line">    accuracy = np.sum(outputs == labels) / np.sum(mask)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy = evaluate_prediction(model(x), y, vocab[<span class="string">'&lt;PAD&gt;'</span>])</span><br><span class="line">print(<span class="string">"accuracy: "</span>, accuracy)</span><br></pre></td></tr></table></figure><pre><code>outputs shape: (7194, 70)mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]accuracy:  0.9543761281155191</code></pre><p><strong>Expected output (Approximately)</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">outputs shape: (7194, 70)</span><br><span class="line">mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]</span><br><span class="line">accuracy:  0.9543761281155191</span><br></pre></td></tr></table></figure></p><p><a name="5"></a></p><h1 id="Part-5-Testing-with-your-own-sentence"><a href="#Part-5-Testing-with-your-own-sentence" class="headerlink" title="Part 5:  Testing with your own sentence"></a>Part 5:  Testing with your own sentence</h1><p>Below, you can test it out with your own sentence! </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the function you will be using to test your own sentence.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(sentence, model, vocab, tag_map)</span>:</span></span><br><span class="line">    s = [vocab[token] <span class="keyword">if</span> token <span class="keyword">in</span> vocab <span class="keyword">else</span> vocab[<span class="string">'UNK'</span>] <span class="keyword">for</span> token <span class="keyword">in</span> sentence.split(<span class="string">' '</span>)]</span><br><span class="line">    batch_data = np.ones((<span class="number">1</span>, len(s)))</span><br><span class="line">    batch_data[<span class="number">0</span>][:] = s</span><br><span class="line">    sentence = np.array(batch_data).astype(int)</span><br><span class="line">    output = model(sentence)</span><br><span class="line">    outputs = np.argmax(output, axis=<span class="number">2</span>)</span><br><span class="line">    labels = list(tag_map.keys())</span><br><span class="line">    pred = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(outputs[<span class="number">0</span>])):</span><br><span class="line">        idx = outputs[<span class="number">0</span>][i] </span><br><span class="line">        pred_label = labels[idx]</span><br><span class="line">        pred.append(pred_label)</span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Try the output for the introduction example</span></span><br><span class="line"><span class="comment">#sentence = "Many French citizens are goin to visit Morocco for summer"</span></span><br><span class="line"><span class="comment">#sentence = "Sharon Floyd flew to Miami last Friday"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># New york times news:</span></span><br><span class="line">sentence = <span class="string">"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come"</span></span><br><span class="line">s = [vocab[token] <span class="keyword">if</span> token <span class="keyword">in</span> vocab <span class="keyword">else</span> vocab[<span class="string">'UNK'</span>] <span class="keyword">for</span> token <span class="keyword">in</span> sentence.split(<span class="string">' '</span>)]</span><br><span class="line">predictions = predict(sentence, model, vocab, tag_map)</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(sentence.split(<span class="string">' '</span>), predictions):</span><br><span class="line">    <span class="keyword">if</span> y != <span class="string">'O'</span>:</span><br><span class="line">        print(x,y)</span><br></pre></td></tr></table></figure><pre><code>Peter B-perNavarro, I-perWhite B-orgHouse I-orgSunday B-timmorning I-timWhite B-orgHouse I-orgcoronavirus B-timfall, B-tim</code></pre><p><strong> Expected Results </strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Peter B-per</span><br><span class="line">Navarro, I-per</span><br><span class="line">White B-org</span><br><span class="line">House I-org</span><br><span class="line">Sunday B-tim</span><br><span class="line">morning I-tim</span><br><span class="line">White B-org</span><br><span class="line">House I-org</span><br><span class="line">coronavirus B-tim</span><br><span class="line">fall, B-tim</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-3-Named-Entity-Recognition-NER&quot;&gt;&lt;a href=&quot;#Assignment-3-Named-Entity-Recognition-NER&quot; class=&quot;headerlink&quot; title=&quot;Assignment
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Sentiment with Deep Neural Networks</title>
    <link href="https://zhangruochi.com/Sentiment-with-Deep-Neural-Networks/2020/08/22/"/>
    <id>https://zhangruochi.com/Sentiment-with-Deep-Neural-Networks/2020/08/22/</id>
    <published>2020-08-22T02:42:14.000Z</published>
    <updated>2020-09-02T03:16:46.955Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-1-Sentiment-with-Deep-Neural-Networks"><a href="#Assignment-1-Sentiment-with-Deep-Neural-Networks" class="headerlink" title="Assignment 1:  Sentiment with Deep Neural Networks"></a>Assignment 1:  Sentiment with Deep Neural Networks</h1><p>Welcome to the first assignment of course 3. In this assignment, you will explore sentiment analysis using deep neural networks. </p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">Part 1:  Import libraries and try out Trax</a></li><li><a href="#2">Part 2:  Importing the data</a><ul><li><a href="#2.1">2.1  Loading in the data</a></li><li><a href="#2.2">2.2  Building the vocabulary</a></li><li><a href="#2.3">2.3  Converting a tweet to a tensor</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li><li><a href="#2.4">2.4  Creating a batch generator</a><ul><li><a href="#ex02">Exercise 02</a></li></ul></li></ul></li><li><a href="#3">Part 3:  Defining classes</a><ul><li><a href="#3.1">3.1  ReLU class</a><ul><li><a href="#ex03">Exercise 03</a></li></ul></li><li><a href="#3.2">3.2  Dense class </a><ul><li><a href="#ex04">Exercise 04</a></li></ul></li><li><a href="#3.3">3.3  Model</a><ul><li><a href="#ex05">Exercise 05</a></li></ul></li></ul></li><li><a href="#4">Part 4:  Training</a><ul><li><a href="#4.1">4.1  Training the model</a><ul><li><a href="#ex06">Exercise 06</a></li></ul></li><li><a href="#4.2">4.2  Practice Making a prediction</a></li></ul></li><li><a href="#5">Part 5:  Evaluation  </a><ul><li><a href="#5.1">5.1  Computing the accuracy on a batch</a><ul><li><a href="#ex07">Exercise 07</a></li></ul></li><li><a href="#5.2">5.2  Testing your model on Validation Data</a><ul><li><a href="#ex08">Exercise 08</a></li></ul></li></ul></li><li><a href="#6">Part 6:  Testing with your own input</a></li></ul><p>In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:</p><center> <span style="color:blue"> <b>This movie was almost good.</b> </span> </center><p>Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will: </p><ul><li>Understand how you can build/design a model using layers</li><li>Train a model using a training loop</li><li>Use a binary cross-entropy loss function</li><li>Compute the accuracy of your model</li><li>Predict using your own input</li></ul><p>As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. </p><ul><li>Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library <code>trax</code> that we use for building and training models.</li></ul><p>Now we will show you how to compute the gradient of a certain function <code>f</code> by just using <code>.grad(f)</code>. </p><ul><li>Trax source code can be found on Github: <a href="https://github.com/google/trax" target="_blank" rel="noopener">Trax</a></li><li>The Trax code also uses the JAX library: <a href="https://jax.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">JAX</a></li></ul><p><a name="1"></a></p><h1 id="Part-1-Import-libraries-and-try-out-Trax"><a href="#Part-1-Import-libraries-and-try-out-Trax" class="headerlink" title="Part 1:  Import libraries and try out Trax"></a>Part 1:  Import libraries and try out Trax</h1><ul><li>Let’s import libraries and look at an example of using the Trax library.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rnd</span><br><span class="line"></span><br><span class="line"><span class="comment"># import relevant libraries</span></span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seeds to make this notebook easier to replicate</span></span><br><span class="line">trax.supervised.trainer_lib.init_random_number_generators(<span class="number">31</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import trax.fastmath.numpy</span></span><br><span class="line"><span class="keyword">import</span> trax.fastmath.numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># import trax.layers</span></span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"></span><br><span class="line"><span class="comment"># import Layer from the utils.py file</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> Layer, load_tweets, process_tweet</span><br><span class="line"><span class="comment">#from utils import</span></span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 [nltk_data] Downloading package twitter_samples to[nltk_data]     /home/jovyan/nltk_data...[nltk_data]   Package twitter_samples is already up-to-date![nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...[nltk_data]   Package stopwords is already up-to-date!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create an array using trax.fastmath.numpy</span></span><br><span class="line">a = np.array(<span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the returned array</span></span><br><span class="line">display(a)</span><br><span class="line"></span><br><span class="line">print(type(a))</span><br></pre></td></tr></table></figure><pre><code>DeviceArray(5., dtype=float32)&lt;class &#39;jax.interpreters.xla.DeviceArray&#39;&gt;</code></pre><p>Notice that trax.fastmath.numpy returns a DeviceArray from the jax library.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a function that will use the trax.fastmath.numpy array</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># f = x^2</span></span><br><span class="line">    <span class="keyword">return</span> (x**<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Call the function</span></span><br><span class="line">print(<span class="string">f"f(a) for a=<span class="subst">&#123;a&#125;</span> is <span class="subst">&#123;f(a)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>f(a) for a=5.0 is 25.0</code></pre><p>The gradient (derivative) of function <code>f</code> with respect to its input <code>x</code> is the derivative of $x^2$.</p><ul><li>The derivative of $x^2$ is $2x$.  </li><li>When x is 5, then $2x=10$.</li></ul><p>You can calculate the gradient of a function by using <code>trax.fastmath.grad(fun=)</code> and passing in the name of the function.</p><ul><li>In this case the function you want to take the gradient of is <code>f</code>.</li><li>The object returned (saved in <code>grad_f</code> in this example) is a function that can calculate the gradient of f for a given trax.fastmath.numpy array.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Directly use trax.fastmath.grad to calculate the gradient (derivative) of the function</span></span><br><span class="line">grad_f = trax.fastmath.grad(fun=f)  <span class="comment"># df / dx - Gradient of function f(x) with respect to x</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># View the type of the retuned object (it's a function)</span></span><br><span class="line">type(grad_f)</span><br></pre></td></tr></table></figure><pre><code>function</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Call the newly created function and pass in a value for x (the DeviceArray stored in 'a')</span></span><br><span class="line">grad_calculation = grad_f(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the result of calling the grad_f function</span></span><br><span class="line">display(grad_calculation)</span><br></pre></td></tr></table></figure><pre><code>DeviceArray(10., dtype=float32)</code></pre><p>The function returned by trax.fastmath.grad takes in x=5 and calculates the gradient of f, which is 2*x, which is 10. The value is also stored as a DeviceArray from the jax library.</p><p><a name="2"></a></p><h1 id="Part-2-Importing-the-data"><a href="#Part-2-Importing-the-data" class="headerlink" title="Part 2:  Importing the data"></a>Part 2:  Importing the data</h1><p><a name="2.1"></a></p><h2 id="2-1-Loading-in-the-data"><a href="#2-1-Loading-in-the-data" class="headerlink" title="2.1  Loading in the data"></a>2.1  Loading in the data</h2><p>Import the data set.  </p><ul><li>You may recognize this from earlier assignments in the specialization.</li><li>Details of process_tweet function are available in utils.py file</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## DO NOT EDIT THIS CELL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import functions from the utils.py file</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load positive and negative tweets</span></span><br><span class="line">all_positive_tweets, all_negative_tweets = load_tweets()</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the total number of positive and negative tweets.</span></span><br><span class="line">print(<span class="string">f"The number of positive tweets: <span class="subst">&#123;len(all_positive_tweets)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"The number of negative tweets: <span class="subst">&#123;len(all_negative_tweets)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split positive set into validation and training</span></span><br><span class="line">val_pos   = all_positive_tweets[<span class="number">4000</span>:] <span class="comment"># generating validation set for positive tweets</span></span><br><span class="line">train_pos  = all_positive_tweets[:<span class="number">4000</span>]<span class="comment"># generating training set for positive tweets</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split negative set into validation and training</span></span><br><span class="line">val_neg   = all_negative_tweets[<span class="number">4000</span>:] <span class="comment"># generating validation set for negative tweets</span></span><br><span class="line">train_neg  = all_negative_tweets[:<span class="number">4000</span>] <span class="comment"># generating training set for nagative tweets</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine training data into one set</span></span><br><span class="line">train_x = train_pos + train_neg </span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine validation data into one set</span></span><br><span class="line">val_x  = val_pos + val_neg</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the labels for the training set (1 for positive, 0 for negative)</span></span><br><span class="line">train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the labels for the validation set (1 for positive, 0 for negative)</span></span><br><span class="line">val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"length of train_x <span class="subst">&#123;len(train_x)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"length of val_x <span class="subst">&#123;len(val_x)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>The number of positive tweets: 5000The number of negative tweets: 5000length of train_x 8000length of val_x 2000</code></pre><p>Now import a function that processes tweets (we’ve provided this in the utils.py file).</p><ul><li>`process_tweets’ removes unwanted characters e.g. hashtag, hyperlinks, stock tickers from tweet.</li><li>It also returns a list of words (it tokenizes the original string).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import a function that processes the tweets</span></span><br><span class="line"><span class="comment"># from utils import process_tweet</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Try out function that processes tweets</span></span><br><span class="line">print(<span class="string">"original tweet at training position 0"</span>)</span><br><span class="line">print(train_pos[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Tweet at training position 0 after processing:"</span>)</span><br><span class="line">process_tweet(train_pos[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>original tweet at training position 0#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)Tweet at training position 0 after processing:[&#39;followfriday&#39;, &#39;top&#39;, &#39;engag&#39;, &#39;member&#39;, &#39;commun&#39;, &#39;week&#39;, &#39;:)&#39;]</code></pre><p>Notice that the function <code>process_tweet</code> keeps key words, removes the hash # symbol, and ignores usernames (words that begin with ‘@’).  It also returns a list of the words.</p><p><a name="2.2"></a></p><h2 id="2-2-Building-the-vocabulary"><a href="#2-2-Building-the-vocabulary" class="headerlink" title="2.2  Building the vocabulary"></a>2.2  Building the vocabulary</h2><p>Now build the vocabulary.</p><ul><li>Map each word in each tweet to an integer (an “index”). </li><li>The following code does this for you, but please read it and understand what it’s doing.</li><li>Note that you will build the vocabulary based on the training data. </li><li>To do so, you will assign an index to everyword by iterating over your training set.</li></ul><p>The vocabulary will also include some special tokens</p><ul><li><code>__PAD__</code>: padding</li><li><code>&lt;/e&gt;</code>: end of line</li><li><code>__UNK__</code>: a token representing any word that is not in the vocabulary.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the vocabulary</span></span><br><span class="line"><span class="comment"># Unit Test Note - There is no test set here only train/val</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Include special tokens </span></span><br><span class="line"><span class="comment"># started with pad, end of line and unk tokens</span></span><br><span class="line">Vocab = &#123;<span class="string">'__PAD__'</span>: <span class="number">0</span>, <span class="string">'__&lt;/e&gt;__'</span>: <span class="number">1</span>, <span class="string">'__UNK__'</span>: <span class="number">2</span>&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that we build vocab using training data</span></span><br><span class="line"><span class="keyword">for</span> tweet <span class="keyword">in</span> train_x: </span><br><span class="line">    processed_tweet = process_tweet(tweet)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> processed_tweet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> Vocab: </span><br><span class="line">            Vocab[word] = len(Vocab)</span><br><span class="line">    </span><br><span class="line">print(<span class="string">"Total words in vocab are"</span>,len(Vocab))</span><br><span class="line">display(Vocab)</span><br></pre></td></tr></table></figure><pre><code>Total words in vocab are 9088{&#39;__PAD__&#39;: 0, &#39;__&lt;/e&gt;__&#39;: 1, &#39;__UNK__&#39;: 2, &#39;followfriday&#39;: 3, &#39;top&#39;: 4, &#39;engag&#39;: 5, &#39;member&#39;: 6, &#39;commun&#39;: 7, &#39;week&#39;: 8, &#39;:)&#39;: 9, &#39;hey&#39;: 10, &#39;jame&#39;: 11, ...}</code></pre><p>The dictionary <code>Vocab</code> will look like this:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;'__PAD__': 0,</span><br><span class="line"> '__&lt;/e&gt;__': 1,</span><br><span class="line"> '__UNK__': 2,</span><br><span class="line"> 'followfriday': 3,</span><br><span class="line"> 'top': 4,</span><br><span class="line"> 'engag': 5,</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure></p><ul><li>Each unique word has a unique integer associated with it.</li><li>The total number of words in Vocab: 9088</li></ul><p><a name="2.3"></a></p><h2 id="2-3-Converting-a-tweet-to-a-tensor"><a href="#2-3-Converting-a-tweet-to-a-tensor" class="headerlink" title="2.3  Converting a tweet to a tensor"></a>2.3  Converting a tweet to a tensor</h2><p>Write a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet).</p><ul><li>Note, the returned data type will be a <strong>regular Python <code>list()</code></strong><ul><li>You won’t use TensorFlow in this function</li><li>You also won’t use a numpy array</li><li>You also won’t use trax.fastmath.numpy array</li></ul></li><li>For words in the tweet that are not in the vocabulary, set them to the unique ID for the token <code>__UNK__</code>.</li></ul><h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><p>Input a tweet:<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">'@happypuppy, is Maria happy?'</span><br></pre></td></tr></table></figure></p><p>The tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">['maria', 'happi']</span><br></pre></td></tr></table></figure></p><p>Then it will convert each word into its unique integer</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">2</span>, <span class="number">56</span>]</span><br></pre></td></tr></table></figure><ul><li>Notice that the word “maria” is not in the vocabulary, so it is assigned the unique integer associated with the <code>__UNK__</code> token, because it is considered “unknown.”</li></ul><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Write a program <code>tweet_to_tensor</code> that takes in a tweet and converts it to an array of numbers. You can use the <code>Vocab</code> dictionary you just found to help create the tensor. </p><ul><li>Use the vocab_dict parameter and not a global variable.</li><li>Do not hard code the integer value for the <code>__UNK__</code> token.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Map each word in tweet to corresponding token in 'Vocab'</li>    <li>Use Python's Dictionary.get(key,value) so that the function returns a default value if the key is not found in the dictionary.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: tweet_to_tensor</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tweet_to_tensor</span><span class="params">(tweet, vocab_dict, unk_token=<span class="string">'__UNK__'</span>, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        tweet - A string containing a tweet</span></span><br><span class="line"><span class="string">        vocab_dict - The words dictionary</span></span><br><span class="line"><span class="string">        unk_token - The special string for unknown tokens</span></span><br><span class="line"><span class="string">        verbose - Print info durign runtime</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        tensor_l - A python list with</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    '''</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># Process the tweet into a list of words</span></span><br><span class="line">    <span class="comment"># where only important words are kept (stop words removed)</span></span><br><span class="line">    word_l = process_tweet(tweet)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">        print(<span class="string">"List of words from the processed tweet:"</span>)</span><br><span class="line">        print(word_l)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Initialize the list that will contain the unique integer IDs of each word</span></span><br><span class="line">    tensor_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the unique integer ID of the __UNK__ token</span></span><br><span class="line">    unk_ID = vocab_dict.get(unk_token, <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">        print(<span class="string">f"The unique integer ID for the unk_token is <span class="subst">&#123;unk_ID&#125;</span>"</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># for each word in the list:</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_l:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the unique integer ID.</span></span><br><span class="line">        <span class="comment"># If the word doesn't exist in the vocab dictionary,</span></span><br><span class="line">        <span class="comment"># use the unique ID for __UNK__ instead.</span></span><br><span class="line">        word_ID = vocab_dict.get(word, unk_ID)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Append the unique integer ID to the tensor list.</span></span><br><span class="line">        tensor_l.append(word_ID) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tensor_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Actual tweet is\n"</span>, val_pos[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"\nTensor of tweet:\n"</span>, tweet_to_tensor(val_pos[<span class="number">0</span>], vocab_dict=Vocab))</span><br></pre></td></tr></table></figure><pre><code>Actual tweet is Bro:U wan cut hair anot,ur hair long Liao boMe:since ord liao,take it easy lor treat as save $ leave it longer :)Bro:LOL Sibei xialanTensor of tweet: [1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]</code></pre><h5 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Actual tweet is</span><br><span class="line"> Bro:U wan cut hair anot,ur hair <span class="keyword">long</span> Liao bo</span><br><span class="line">Me:since ord liao,take it easy lor treat as save $ leave it longer :)</span><br><span class="line">Bro:LOL Sibei xialan</span><br><span class="line"></span><br><span class="line">Tensor of tweet:</span><br><span class="line"> [<span class="number">1065</span>, <span class="number">136</span>, <span class="number">479</span>, <span class="number">2351</span>, <span class="number">745</span>, <span class="number">8148</span>, <span class="number">1123</span>, <span class="number">745</span>, <span class="number">53</span>, <span class="number">2</span>, <span class="number">2672</span>, <span class="number">791</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">349</span>, <span class="number">601</span>, <span class="number">2</span>, <span class="number">3489</span>, <span class="number">1017</span>, <span class="number">597</span>, <span class="number">4559</span>, <span class="number">9</span>, <span class="number">1065</span>, <span class="number">157</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test tweet_to_tensor</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_tweet_to_tensor</span><span class="params">()</span>:</span></span><br><span class="line">    test_cases = [</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"name"</span>:<span class="string">"simple_test_check"</span>,</span><br><span class="line">            <span class="string">"input"</span>: [val_pos[<span class="number">1</span>], Vocab],</span><br><span class="line">            <span class="string">"expected"</span>:[<span class="number">444</span>, <span class="number">2</span>, <span class="number">304</span>, <span class="number">567</span>, <span class="number">56</span>, <span class="number">9</span>],</span><br><span class="line">            <span class="string">"error"</span>:<span class="string">"The function gives bad output for val_pos[1]. Test failed"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"name"</span>:<span class="string">"datatype_check"</span>,</span><br><span class="line">            <span class="string">"input"</span>:[val_pos[<span class="number">1</span>], Vocab],</span><br><span class="line">            <span class="string">"expected"</span>:type([]),</span><br><span class="line">            <span class="string">"error"</span>:<span class="string">"Datatype mismatch. Need only list not np.array"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"name"</span>:<span class="string">"without_unk_check"</span>,</span><br><span class="line">            <span class="string">"input"</span>:[val_pos[<span class="number">1</span>], Vocab],</span><br><span class="line">            <span class="string">"expected"</span>:<span class="number">6</span>,</span><br><span class="line">            <span class="string">"error"</span>:<span class="string">"Unk word check not done- Please check if you included mapping for unknown word"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> test_case <span class="keyword">in</span> test_cases:</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> test_case[<span class="string">'name'</span>] == <span class="string">"simple_test_check"</span>:</span><br><span class="line">                <span class="keyword">assert</span> test_case[<span class="string">"expected"</span>] == tweet_to_tensor(*test_case[<span class="string">'input'</span>])</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> test_case[<span class="string">'name'</span>] == <span class="string">"datatype_check"</span>:</span><br><span class="line">                <span class="keyword">assert</span> isinstance(tweet_to_tensor(*test_case[<span class="string">'input'</span>]), test_case[<span class="string">"expected"</span>])</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> test_case[<span class="string">'name'</span>] == <span class="string">"without_unk_check"</span>:</span><br><span class="line">                <span class="keyword">assert</span> <span class="keyword">None</span> <span class="keyword">not</span> <span class="keyword">in</span> tweet_to_tensor(*test_case[<span class="string">'input'</span>])</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(test_case[<span class="string">'error'</span>])</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">3</span>:</span><br><span class="line">        print(<span class="string">"\033[92m All tests passed"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(count,<span class="string">" Tests passed out of 3"</span>)</span><br><span class="line">test_tweet_to_tensor()</span><br></pre></td></tr></table></figure><p><a name="2.4"></a></p><h2 id="2-4-Creating-a-batch-generator"><a href="#2-4-Creating-a-batch-generator" class="headerlink" title="2.4  Creating a batch generator"></a>2.4  Creating a batch generator</h2><p>Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. </p><ul><li>If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. </li><li>You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0). </li></ul><p>Once you create the generator, you could include it in a for loop</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_inputs, batch_targets, batch_example_weights in data_generator:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>You can also get a single batch like this:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_inputs, batch_targets, batch_example_weights = next(data_generator)</span><br></pre></td></tr></table></figure><p>The generator returns the next batch each time it’s called. </p><ul><li>This generator returns the data in a format (tensors) that you could directly use in your model.</li><li>It returns a triple: the inputs, targets, and loss weights:<br>— Inputs is a tensor that contains the batch of tweets we put into the model.<br>— Targets is the corresponding batch of labels that we train to generate.<br>— Loss weights here are just 1s with same shape as targets. Next week, you will use it to mask input padding.</li></ul><p><a name="ex02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p>Implement <code>data_generator</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED: Data generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span><span class="params">(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        data_pos - Set of positive examples</span></span><br><span class="line"><span class="string">        data_neg - Set of negative examples</span></span><br><span class="line"><span class="string">        batch_size - number of samples per batch. Must be even</span></span><br><span class="line"><span class="string">        loop - True or False</span></span><br><span class="line"><span class="string">        vocab_dict - The words dictionary</span></span><br><span class="line"><span class="string">        shuffle - Shuffle the data order</span></span><br><span class="line"><span class="string">    Yield:</span></span><br><span class="line"><span class="string">        inputs - Subset of positive and negative examples</span></span><br><span class="line"><span class="string">        targets - The corresponding labels for the subset</span></span><br><span class="line"><span class="string">        example_weights - An array specifying the importance of each example</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    '''</span>     </span><br><span class="line"><span class="comment">### START GIVEN CODE ###</span></span><br><span class="line">    <span class="comment"># make sure the batch size is an even number</span></span><br><span class="line">    <span class="comment"># to allow an equal number of positive and negative samples</span></span><br><span class="line">    <span class="keyword">assert</span> batch_size % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Number of positive examples in each batch is half of the batch size</span></span><br><span class="line">    <span class="comment"># same with number of negative examples in each batch</span></span><br><span class="line">    n_to_take = batch_size // <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use pos_index to walk through the data_pos array</span></span><br><span class="line">    <span class="comment"># same with neg_index and data_neg</span></span><br><span class="line">    pos_index = <span class="number">0</span></span><br><span class="line">    neg_index = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    len_data_pos = len(data_pos)</span><br><span class="line">    len_data_neg = len(data_neg)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get and array with the data indexes</span></span><br><span class="line">    pos_index_lines = list(range(len_data_pos))</span><br><span class="line">    neg_index_lines = list(range(len_data_neg))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># shuffle lines if shuffle is set to True</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        rnd.shuffle(pos_index_lines)</span><br><span class="line">        rnd.shuffle(neg_index_lines)</span><br><span class="line">        </span><br><span class="line">    stop = <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop indefinitely</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> stop:  </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># create a batch with positive and negative examples</span></span><br><span class="line">        batch = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># First part: Pack n_to_take positive examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Start from pos_index and increment i up to n_to_take</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_to_take):</span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># If the positive index goes past the positive dataset lenght,</span></span><br><span class="line">            <span class="keyword">if</span> pos_index &gt;= len_data_pos: </span><br><span class="line">                </span><br><span class="line">                <span class="comment"># If loop is set to False, break once we reach the end of the dataset</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> loop:</span><br><span class="line">                    stop = <span class="keyword">True</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># If user wants to keep re-using the data, reset the index</span></span><br><span class="line">                pos_index = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> shuffle:</span><br><span class="line">                    <span class="comment"># Shuffle the index of the positive sample</span></span><br><span class="line">                    rnd.shuffle(pos_index_lines)</span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># get the tweet as pos_index</span></span><br><span class="line">            tweet = data_pos[pos_index_lines[pos_index]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># convert the tweet into tensors of integers representing the processed words</span></span><br><span class="line">            tensor = tweet_to_tensor(tweet, vocab_dict)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the tensor to the batch list</span></span><br><span class="line">            batch.append(tensor)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Increment pos_index by one</span></span><br><span class="line">            pos_index = pos_index + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### END GIVEN CODE ###</span></span><br><span class="line">            </span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Second part: Pack n_to_take negative examples</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Using the same batch list, start from neg_index and increment i up to n_to_take</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_to_take):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the negative index goes past the negative dataset length,</span></span><br><span class="line">            <span class="keyword">if</span> neg_index &gt;= len_data_neg:</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># If loop is set to False, break once we reach the end of the dataset</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> loop:</span><br><span class="line">                    stop = <span class="keyword">True</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                    </span><br><span class="line">                <span class="comment"># If user wants to keep re-using the data, reset the index</span></span><br><span class="line">                neg_index = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> shuffle:</span><br><span class="line">                    <span class="comment"># Shuffle the index of the negative sample</span></span><br><span class="line">                    rnd.shuffle(neg_index_lines)</span><br><span class="line">            <span class="comment"># get the tweet as neg_index</span></span><br><span class="line">            tweet = data_neg[neg_index_lines[neg_index]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># convert the tweet into tensors of integers representing the processed words</span></span><br><span class="line">            tensor = tweet_to_tensor(tweet, vocab_dict)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the tensor to the batch list</span></span><br><span class="line">            batch.append(tensor)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Increment neg_index by one</span></span><br><span class="line">            neg_index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###        </span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START GIVEN CODE ###</span></span><br><span class="line">        <span class="keyword">if</span> stop:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update the start index for positive data </span></span><br><span class="line">        <span class="comment"># so that it's n_to_take positions after the current pos_index</span></span><br><span class="line">        pos_index += n_to_take</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the start index for negative data </span></span><br><span class="line">        <span class="comment"># so that it's n_to_take positions after the current neg_index</span></span><br><span class="line">        neg_index += n_to_take</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the max tweet length (the length of the longest tweet) </span></span><br><span class="line">        <span class="comment"># (you will pad all shorter tweets to have this length)</span></span><br><span class="line">        max_len = max([len(t) <span class="keyword">for</span> t <span class="keyword">in</span> batch]) </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize the input_l, which will </span></span><br><span class="line">        <span class="comment"># store the padded versions of the tensors</span></span><br><span class="line">        tensor_pad_l = []</span><br><span class="line">        <span class="comment"># Pad shorter tweets with zeros</span></span><br><span class="line">        <span class="keyword">for</span> tensor <span class="keyword">in</span> batch:</span><br><span class="line"><span class="comment">### END GIVEN CODE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">            <span class="comment"># Get the number of positions to pad for this tensor so that it will be max_len long</span></span><br><span class="line">            n_pad = max_len - len(tensor)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Generate a list of zeros, with length n_pad</span></span><br><span class="line">            pad_l = [<span class="number">0</span>] * n_pad</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># concatenate the tensor and the list of padded zeros</span></span><br><span class="line">            tensor_pad = tensor + pad_l</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the padded tensor to the list of padded tensors</span></span><br><span class="line">            tensor_pad_l.append(tensor_pad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># convert the list of padded tensors to a numpy array</span></span><br><span class="line">        <span class="comment"># and store this as the model inputs</span></span><br><span class="line">        inputs = np.array(tensor_pad_l)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Generate the list of targets for the positive examples (a list of ones)</span></span><br><span class="line">        <span class="comment"># The length is the number of positive examples in the batch</span></span><br><span class="line">        target_pos = [<span class="number">1</span>] * n_to_take</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate the list of targets for the negative examples (a list of zeros)</span></span><br><span class="line">        <span class="comment"># The length is the number of negative examples in the batch</span></span><br><span class="line">        target_neg = [<span class="number">0</span>] * n_to_take</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Concatenate the positve and negative targets</span></span><br><span class="line">        target_l = target_pos + target_neg</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the target list into a numpy array</span></span><br><span class="line">        targets = np.array(target_l)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()</span></span><br><span class="line">        example_weights = np.ones_like(targets)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### GIVEN CODE ###</span></span><br><span class="line">        <span class="comment"># note we use yield and not return</span></span><br><span class="line">        <span class="keyword">yield</span> inputs, targets, example_weights</span><br></pre></td></tr></table></figure><p>Now you can use your data generator to create a data generator for the training data, and another data generator for the validation data.</p><p>We will create a third data generator that does not loop, for testing the final accuracy of the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set the random number generator for the shuffle procedure</span></span><br><span class="line">rnd.seed(<span class="number">30</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the training data generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_generator</span><span class="params">(batch_size, shuffle = False)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data_generator(train_pos, train_neg, batch_size, <span class="keyword">True</span>, Vocab, shuffle)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the validation data generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_generator</span><span class="params">(batch_size, shuffle = False)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data_generator(val_pos, val_neg, batch_size, <span class="keyword">True</span>, Vocab, shuffle)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the validation data generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_generator</span><span class="params">(batch_size, shuffle = False)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data_generator(val_pos, val_neg, batch_size, <span class="keyword">False</span>, Vocab, shuffle)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch from the train_generator and inspect.</span></span><br><span class="line">inputs, targets, example_weights = next(train_generator(<span class="number">4</span>, shuffle=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># this will print a list of 4 tensors padded with zeros</span></span><br><span class="line">print(<span class="string">f'Inputs: <span class="subst">&#123;inputs&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Targets: <span class="subst">&#123;targets&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Example Weights: <span class="subst">&#123;example_weights&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>Inputs: [[2005 4451 3201    9    0    0    0    0    0    0    0] [4954  567 2000 1454 5174 3499  141 3499  130  459    9] [3761  109  136  583 2930 3969    0    0    0    0    0] [ 250 3761    0    0    0    0    0    0    0    0    0]]Targets: [1 1 0 0]Example Weights: [1 1 1 1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the train_generator</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a data generator for training data,</span></span><br><span class="line"><span class="comment"># which produces batches of size 4 (for tensors and their respective targets)</span></span><br><span class="line">tmp_data_gen = train_generator(batch_size = <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call the data generator to get one batch and its targets</span></span><br><span class="line">tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The inputs shape is <span class="subst">&#123;tmp_inputs.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"The targets shape is <span class="subst">&#123;tmp_targets.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"The example weights shape is <span class="subst">&#123;tmp_example_weights.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,t <span class="keyword">in</span> enumerate(tmp_inputs):</span><br><span class="line">    print(<span class="string">f"input tensor: <span class="subst">&#123;t&#125;</span>; target <span class="subst">&#123;tmp_targets[i]&#125;</span>; example weights <span class="subst">&#123;tmp_example_weights[i]&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>The inputs shape is (4, 14)The targets shape is (4,)The example weights shape is (4,)input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1input tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1input tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0; example weights 1</code></pre><h5 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">The inputs shape <span class="title">is</span> <span class="params">(<span class="number">4</span>, <span class="number">14</span>)</span></span></span><br><span class="line"><span class="function">The targets shape <span class="title">is</span> <span class="params">(<span class="number">4</span>,)</span></span></span><br><span class="line"><span class="function">The example weights shape <span class="title">is</span> <span class="params">(<span class="number">4</span>,)</span></span></span><br><span class="line">input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1</span><br><span class="line">input tensor: [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span> <span class="number">18</span> <span class="number">19</span> <span class="number">20</span>  <span class="number">9</span> <span class="number">21</span> <span class="number">22</span>]; target <span class="number">1</span>; example weights <span class="number">1</span></span><br><span class="line">input tensor: [<span class="number">5738</span> <span class="number">2901</span> <span class="number">3761</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>]; target <span class="number">0</span>; example weights <span class="number">1</span></span><br><span class="line">input tensor: [ <span class="number">858</span>  <span class="number">256</span> <span class="number">3652</span> <span class="number">5739</span>  <span class="number">307</span> <span class="number">4458</span>  <span class="number">567</span> <span class="number">1230</span> <span class="number">2767</span>  <span class="number">328</span> <span class="number">1202</span> <span class="number">3761</span>    <span class="number">0</span>    <span class="number">0</span>]; target <span class="number">0</span>; example weights <span class="number">1</span></span><br></pre></td></tr></table></figure><p>Now that you have your train/val generators, you can just call them and they will return tensors which correspond to your tweets in the first column and their corresponding labels in the second column. Now you can go ahead and start building your neural network. </p><p><a name="3"></a></p><h1 id="Part-3-Defining-classes"><a href="#Part-3-Defining-classes" class="headerlink" title="Part 3:  Defining classes"></a>Part 3:  Defining classes</h1><p>In this part, you will write your own library of layers. It will be very similar<br>to the one used in Trax and also in Keras and PyTorch. Writing your own small<br>framework will help you understand how they all work and use them effectively<br>in the future.</p><p>Your framework will be based on the following <code>Layer</code> class from utils.py.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class Layer(object):</span><br><span class="line">    <span class="string">""</span><span class="string">" Base class for layers.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">      </span><br><span class="line">    # Constructor</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="meta"># set weights to None</span></span><br><span class="line">        self.weights = None</span><br><span class="line"></span><br><span class="line">    # The forward propagation should be implemented</span><br><span class="line">    # by subclasses of <span class="keyword">this</span> Layer class</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    # This function initializes the weights</span><br><span class="line">    # based on the input signature <span class="keyword">and</span> random key,</span><br><span class="line">    # should be implemented by subclasses of <span class="keyword">this</span> Layer class</span><br><span class="line">    def init_weights_and_state(self, input_signature, random_key):</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    # This initializes <span class="keyword">and</span> returns the weights, <span class="keyword">do</span> <span class="keyword">not</span> override.</span><br><span class="line">    def init(self, input_signature, random_key):</span><br><span class="line">        self.init_weights_and_state(input_signature, random_key)</span><br><span class="line">        <span class="keyword">return</span> self.weights</span><br><span class="line"> </span><br><span class="line">    # __call__ allows an object of <span class="keyword">this</span> class</span><br><span class="line">    # to be called like it's a function.</span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        # When <span class="keyword">this</span> layer object is called, </span><br><span class="line">        # it calls its forward propagation function</span><br><span class="line">        <span class="keyword">return</span> self.forward(x)</span><br></pre></td></tr></table></figure><p><a name="3.1"></a></p><h2 id="3-1-ReLU-class"><a href="#3-1-ReLU-class" class="headerlink" title="3.1  ReLU class"></a>3.1  ReLU class</h2><p>You will now implement the ReLU activation function in a class below. The ReLU function looks as follows:<br><img src="relu.jpg" style="width:300px;height:150px;"></p><script type="math/tex; mode=display">\mathrm{ReLU}(x) = \mathrm{max}(0,x)</script><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the ReLU activation function below. Your function should take in a matrix or vector and it should transform all the negative numbers into 0 while keeping all the positive numbers intact. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Please use numpy.maximum(A,k) to find the maximum between each element in A and a scalar k</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Relu</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Relu</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="string">"""Relu activation function implementation"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Input: </span></span><br><span class="line"><span class="string">            - x (a numpy array): the input</span></span><br><span class="line"><span class="string">        Output:</span></span><br><span class="line"><span class="string">            - activation (numpy array): all positive or 0 version of x</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        activation = np.maximum(x, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> activation</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test your relu function</span></span><br><span class="line">x = np.array([[<span class="number">-2.0</span>, <span class="number">-1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>]], dtype=float)</span><br><span class="line">relu_layer = Relu()</span><br><span class="line">print(<span class="string">"Test data is:"</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(<span class="string">"Output of Relu is:"</span>)</span><br><span class="line">print(relu_layer(x))</span><br></pre></td></tr></table></figure><pre><code>Test data is:[[-2. -1.  0.] [ 0.  1.  2.]]Output of Relu is:[[0. 0. 0.] [0. 1. 2.]]</code></pre><h5 id="Expected-Outout"><a href="#Expected-Outout" class="headerlink" title="Expected Outout"></a>Expected Outout</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Test data is:</span><br><span class="line">[[<span class="number">-2.</span> <span class="number">-1.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">2.</span>]]</span><br><span class="line">Output of Relu is:</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">2.</span>]]</span><br></pre></td></tr></table></figure><p><a name="3.2"></a></p><h2 id="3-2-Dense-class"><a href="#3-2-Dense-class" class="headerlink" title="3.2  Dense class"></a>3.2  Dense class</h2><h3 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h3><p>Implement the forward function of the Dense class. </p><ul><li>The forward function multiplies the input to the layer (<code>x</code>) by the weight matrix (<code>W</code>)</li></ul><script type="math/tex; mode=display">\mathrm{forward}(\mathbf{x},\mathbf{W}) = \mathbf{xW}</script><ul><li>You can use <code>numpy.dot</code> to perform the matrix multiplication.</li></ul><p>Note that for more efficient code execution, you will use the trax version of <code>math</code>, which includes a trax version of <code>numpy</code> and also <code>random</code>.</p><p>Implement the weight initializer <code>new_weights</code> function</p><ul><li>Weights are initialized with a random key.</li><li>The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols)</li><li>The num of rows for weights should equal the number of columns in x, because for forward propagation, you will multiply x times weights.</li></ul><p>Please use <code>trax.fastmath.random.normal(key, shape, dtype=tf.float32)</code> to generate random values for the weight matrix. The key difference between this function<br>and the standard <code>numpy</code> randomness is the explicit use of random keys, which<br>need to be passed. While it can look tedious at the first sight to pass the random key everywhere, you will learn in Course 4 why this is very helpful when<br>implementing some advanced models.</p><ul><li><code>key</code> can be generated by calling <code>random.get_prng(seed=)</code> and passing in a number for the <code>seed</code>.</li><li><code>shape</code> is a tuple with the desired shape of the weight matrix.<ul><li>The number of rows in the weight matrix should equal the number of columns in the variable <code>x</code>.  Since <code>x</code> may have 2 dimensions if it reprsents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x.</li><li>The number of columns in the weight matrix is the number of units chosen for that dense layer.  Look at the <code>__init__</code> function to see which variable stores the number of units.</li></ul></li><li><code>dtype</code> is the data type of the values in the generated matrix; keep the default of <code>tf.float32</code>. In this case, don’t explicitly set the dtype (just let it use the default value).</li></ul><p>Set the standard deviation of the random values to 0.1</p><ul><li>The values generated have a mean of 0 and standard deviation of 1.</li><li>Set the default standard deviation <code>stdev</code> to be 0.1 by multiplying the standard deviation to each of the values in the weight matrix.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use the fastmath module within trax</span></span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> fastmath</span><br><span class="line"></span><br><span class="line"><span class="comment"># use the numpy module from trax</span></span><br><span class="line">np = fastmath.numpy</span><br><span class="line"></span><br><span class="line"><span class="comment"># use the fastmath.random module from trax</span></span><br><span class="line">random = fastmath.random</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># See how the fastmath.trax.random.normal function works</span></span><br><span class="line">tmp_key = random.get_prng(seed=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"The random seed generated by random.get_prng"</span>)</span><br><span class="line">display(tmp_key)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"choose a matrix with 2 rows and 3 columns"</span>)</span><br><span class="line">tmp_shape=(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">display(tmp_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate a weight matrix</span></span><br><span class="line"><span class="comment"># Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow</span></span><br><span class="line"><span class="comment"># Just avoid setting the dtype and allow it to use the default data type</span></span><br><span class="line">tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Weight matrix generated with a normal distribution with mean 0 and stdev of 1"</span>)</span><br><span class="line">display(tmp_weight)</span><br></pre></td></tr></table></figure><pre><code>The random seed generated by random.get_prngDeviceArray([0, 1], dtype=uint32)choose a matrix with 2 rows and 3 columns(2, 3)Weight matrix generated with a normal distribution with mean 0 and stdev of 1DeviceArray([[ 0.95730704, -0.96992904,  1.0070664 ],             [ 0.36619025,  0.17294823,  0.29092228]], dtype=float32)</code></pre><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p>Implement the <code>Dense</code> class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Dense</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dense</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A dense (fully-connected) layer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># __init__ is implemented for you</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_units, init_stdev=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the number of units in this layer</span></span><br><span class="line">        self._n_units = n_units</span><br><span class="line">        self._init_stdev = init_stdev</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Please implement 'forward()'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Matrix multiply x and the weight matrix</span></span><br><span class="line">        dense = np.dot(x, self.weights)</span><br><span class="line">        </span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line">        <span class="keyword">return</span> dense</span><br><span class="line"></span><br><span class="line">    <span class="comment"># init_weights</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights_and_state</span><span class="params">(self, input_signature, random_key)</span>:</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># The input_signature has a .shape attribute that gives the shape as a tuple    </span></span><br><span class="line">        input_shape = (input_signature.shape[<span class="number">-1</span>], self._n_units)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate the weight matrix from a normal distribution, </span></span><br><span class="line">        <span class="comment"># and standard deviation of 'stdev'        </span></span><br><span class="line">        w = random.normal(key = random_key, shape = input_shape) * self._init_stdev</span><br><span class="line">        </span><br><span class="line"><span class="comment">### END CODE HERE ###     </span></span><br><span class="line">        self.weights = w</span><br><span class="line">        <span class="keyword">return</span> self.weights</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Testing your Dense layer </span></span><br><span class="line">dense_layer = Dense(n_units=<span class="number">10</span>)  <span class="comment">#sets  number of units in dense layer</span></span><br><span class="line">random_key = random.get_prng(seed=<span class="number">0</span>)  <span class="comment"># sets random seed</span></span><br><span class="line">z = np.array([[<span class="number">2.0</span>, <span class="number">7.0</span>, <span class="number">25.0</span>]]) <span class="comment"># input array </span></span><br><span class="line"></span><br><span class="line">dense_layer.init(z, random_key)</span><br><span class="line">print(<span class="string">"Weights are\n "</span>,dense_layer.weights) <span class="comment">#Returns randomly generated weights</span></span><br><span class="line">print(<span class="string">"Foward function output is "</span>, dense_layer(z)) <span class="comment"># Returns multiplied values of units and weights</span></span><br></pre></td></tr></table></figure><pre><code>Weights are  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126  -0.04265672  0.0986188  -0.05575325  0.00153249] [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617  -0.03237354  0.16234995  0.02450038 -0.13809784] [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]Foward function output is  [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209  -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]</code></pre><h5 id="Expected-Outout-1"><a href="#Expected-Outout-1" class="headerlink" title="Expected Outout"></a>Expected Outout</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Weights are</span><br><span class="line">  [[<span class="number">-0.02837108</span>  <span class="number">0.09368162</span> <span class="number">-0.10050076</span>  <span class="number">0.14165013</span>  <span class="number">0.10543301</span>  <span class="number">0.09108126</span></span><br><span class="line">  <span class="number">-0.04265672</span>  <span class="number">0.0986188</span>  <span class="number">-0.05575325</span>  <span class="number">0.00153249</span>]</span><br><span class="line"> [<span class="number">-0.20785688</span>  <span class="number">0.0554837</span>   <span class="number">0.09142365</span>  <span class="number">0.05744595</span>  <span class="number">0.07227863</span>  <span class="number">0.01210617</span></span><br><span class="line">  <span class="number">-0.03237354</span>  <span class="number">0.16234995</span>  <span class="number">0.02450038</span> <span class="number">-0.13809784</span>]</span><br><span class="line"> [<span class="number">-0.06111237</span>  <span class="number">0.01403724</span>  <span class="number">0.08410042</span> <span class="number">-0.1094358</span>  <span class="number">-0.10775021</span> <span class="number">-0.11396459</span></span><br><span class="line">  <span class="number">-0.05933381</span> <span class="number">-0.01557652</span> <span class="number">-0.03832145</span> <span class="number">-0.11144515</span>]]</span><br><span class="line">Foward function output is  [[<span class="number">-3.0395496</span>   <span class="number">0.9266802</span>   <span class="number">2.5414743</span>  <span class="number">-2.050473</span>   <span class="number">-1.9769388</span>  <span class="number">-2.582209</span></span><br><span class="line">  <span class="number">-1.7952735</span>   <span class="number">0.94427425</span> <span class="number">-0.8980402</span>  <span class="number">-3.7497487</span> ]]</span><br></pre></td></tr></table></figure><p><a name="3.3"></a></p><h2 id="3-3-Model"><a href="#3-3-Model" class="headerlink" title="3.3  Model"></a>3.3  Model</h2><p>Now you will implement a classifier using neural networks. Here is the model architecture you will be implementing. </p><p><img src="nn.jpg" style="width:400px;height:250px;"></p><p>For the model implementation, you will use the Trax layers library <code>tl</code>.<br>Note that the second character of <code>tl</code> is the lowercase of letter <code>L</code>, not the number 1. Trax layers are very similar to the ones you implemented above,<br>but in addition to trainable weights also have a non-trainable state.<br>State is used in layers like batch normalization and for inference, you will learn more about it in course 4.</p><p>First, look at the code of the Trax Dense layer and compare to your implementation above.</p><ul><li><a href="https://github.com/google/trax/blob/master/trax/layers/core.py#L29" target="_blank" rel="noopener">tl.Dense</a>: Trax Dense layer implementation</li></ul><p>One other important layer that you will use a lot is one that allows to execute one layer after another in sequence.</p><ul><li><a href="https://github.com/google/trax/blob/master/trax/layers/combinators.py#L26" target="_blank" rel="noopener">tl.Serial</a>: Combinator that applies layers serially.  <ul><li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas. </li><li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code></li></ul></li></ul><p>Please use the <code>help</code> function to view documentation for each layer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation on tl.Dense</span></span><br><span class="line">help(tl.Dense)</span><br></pre></td></tr></table></figure><pre><code>Help on class Dense in module trax.layers.core:class Dense(trax.layers.base.Layer) |  Dense(n_units, kernel_initializer=&lt;function ScaledInitializer.&lt;locals&gt;.Init at 0x7fb32d622620&gt;, bias_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb32d6226a8&gt;, use_bias=True) |   |  A dense (a.k.a. fully-connected, affine) layer. |   |  Dense layers are the prototypical example of a trainable layer, i.e., a layer |  with trainable weights. Each node in a dense layer computes a weighted sum of |  all node values from the preceding layer and adds to that sum a node-specific |  bias term. The full layer computation is expressed compactly in linear |  algebra as an affine map `y = Wx + b`, where `W` is a matrix and `y`, `x`, |  and `b` are vectors. The layer is trained, or &quot;learns&quot;, by updating the |  values in `W` and `b`. |   |  Less commonly, a dense layer can omit the bias term and be a pure linear map: |  `y = Wx`. |   |  Method resolution order: |      Dense |      trax.layers.base.Layer |      builtins.object |   |  Methods defined here: |   |  __init__(self, n_units, kernel_initializer=&lt;function ScaledInitializer.&lt;locals&gt;.Init at 0x7fb32d622620&gt;, bias_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb32d6226a8&gt;, use_bias=True) |      Returns a dense (fully connected) layer of width `n_units`. |       |      A dense layer maps collections of `R^m` vectors to `R^n`, where `n` |      (`= n_units`) is fixed at layer creation time, and `m` is set at layer |      initialization time. |       |      Args: |        n_units: Number of nodes in the layer, also known as the width of the |            layer. |        kernel_initializer: Function that creates a matrix of (random) initial |            connection weights `W` for the layer. |        bias_initializer: Function that creates a vector of (random) initial |            bias weights `b` for the layer. |        use_bias: If `True`, compute an affine map `y = Wx + b`; else compute |            a linear map `y = Wx`. |   |  forward(self, x) |      Executes this layer as part of a forward pass through the model. |       |      Args: |        x: Tensor of same shape and dtype as the input signature used to |            initialize this layer. |       |      Returns: |        Tensor of same shape and dtype as the input, except the final dimension |        is the layer&#39;s `n_units` value. |   |  init_weights_and_state(self, input_signature) |      Returns newly initialized weights for this layer. |       |      Weights are a `(w, b)` tuple for layers created with `use_bias=True` (the |      default case), or a `w` tensor for layers created with `use_bias=False`. |       |      Args: |        input_signature: `ShapeDtype` instance characterizing the input this layer |            should compute on. |   |  ---------------------------------------------------------------------- |  Methods inherited from trax.layers.base.Layer: |   |  __call__(self, x, weights=None, state=None, rng=None) |      Makes layers callable; for use in tests or interactive settings. |       |      This convenience method helps library users play with, test, or otherwise |      probe the behavior of layers outside of a full training environment. It |      presents the layer as callable function from inputs to outputs, with the |      option of manually specifying weights and non-parameter state per individual |      call. For convenience, weights and non-parameter state are cached per layer |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`, |      and acquiring non-empty values either by initialization or from values |      explicitly provided via the weights and state keyword arguments. |       |      Args: |        x: Zero or more input tensors, packaged as described in the `Layer` class |            docstring. |        weights: Weights or `None`; if `None`, use self&#39;s cached weights value. |        state: State or `None`; if `None`, use self&#39;s cached state value. |        rng: Single-use random number generator (JAX PRNG key), or `None`; |            if `None`, use a default computed from an integer 0 seed. |       |      Returns: |        Zero or more output tensors, packaged as described in the `Layer` class |        docstring. |   |  __repr__(self) |      Return repr(self). |   |  backward(self, inputs, output, grad, weights, state, new_state, rng) |      Custom backward pass to propagate gradients in a custom way. |       |      Args: |        inputs: Input tensors; can be a (possibly nested) tuple. |        output: The result of running this layer on inputs. |        grad: Gradient signal computed based on subsequent layers; its structure |            and shape must match output. |        weights: This layer&#39;s weights. |        state: This layer&#39;s state prior to the current forward pass. |        new_state: This layer&#39;s state after the current forward pass. |        rng: Single-use random number generator (JAX PRNG key). |       |      Returns: |        The custom gradient signal for the input. Note that we need to return |        a gradient for each argument of forward, so it will usually be a tuple |        of signals: the gradient for inputs and weights. |   |  init(self, input_signature, rng=None, use_cache=False) |      Initializes weights/state of this layer and its sublayers recursively. |       |      Initialization creates layer weights and state, for layers that use them. |      It derives the necessary array shapes and data types from the layer&#39;s input |      signature, which is itself just shape and data type information. |       |      For layers without weights or state, this method safely does nothing. |       |      This method is designed to create weights/state only once for each layer |      instance, even if the same layer instance occurs in multiple places in the |      network. This enables weight sharing to be implemented as layer sharing. |       |      Args: |        input_signature: `ShapeDtype` instance (if this layer takes one input) |            or list/tuple of `ShapeDtype` instances. |        rng: Single-use random number generator (JAX PRNG key), or `None`; |            if `None`, use a default computed from an integer 0 seed. |        use_cache: If `True`, and if this layer instance has already been |            initialized elsewhere in the network, then return special marker |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`. |            Else return this layer&#39;s newly initialized weights and state. |       |      Returns: |        A `(weights, state)` tuple. |   |  init_from_file(self, file_name, weights_only=False, input_signature=None) |      Initializes this layer and its sublayers from a pickled checkpoint. |       |      In the common case (`weights_only=False`), the file must be a gziped pickled |      dictionary containing items with keys `&#39;flat_weights&#39;, `&#39;flat_state&#39;` and |      `&#39;input_signature&#39;`, which are used to initialize this layer. |      If `input_signature` is specified, it&#39;s used instead of the one in the file. |      If `weights_only` is `True`, the dictionary does not need to have the |      `&#39;flat_state&#39;` item and the state it not restored either. |       |      Args: |        file_name: Name/path of the pickeled weights/state file. |        weights_only: If `True`, initialize only the layer&#39;s weights. Else |            initialize both weights and state. |        input_signature: Input signature to be used instead of the one from file. |   |  output_signature(self, input_signature) |      Returns output signature this layer would give for `input_signature`. |   |  pure_fn(self, x, weights, state, rng, use_cache=False) |      Applies this layer as a pure function with no optional args. |       |      This method exposes the layer&#39;s computation as a pure function. This is |      especially useful for JIT compilation. Do not override, use `forward` |      instead. |       |      Args: |        x: Zero or more input tensors, packaged as described in the `Layer` class |            docstring. |        weights: A tuple or list of trainable weights, with one element for this |            layer if this layer has no sublayers, or one for each sublayer if |            this layer has sublayers. If a layer (or sublayer) has no trainable |            weights, the corresponding weights element is an empty tuple. |        state: Layer-specific non-parameter state that can update between batches. |        rng: Single-use random number generator (JAX PRNG key). |        use_cache: if `True`, cache weights and state in the layer object; used |          to implement layer sharing in combinators. |       |      Returns: |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`) |        promised by this layer, and are packaged as described in the `Layer` |        class docstring. |   |  weights_and_state_signature(self, input_signature) |      Return a pair containing the signatures of weights and state. |   |  ---------------------------------------------------------------------- |  Data descriptors inherited from trax.layers.base.Layer: |   |  __dict__ |      dictionary for instance variables (if defined) |   |  __weakref__ |      list of weak references to the object (if defined) |   |  has_backward |      Returns `True` if this layer provides its own custom backward pass code. |       |      A layer subclass that provides custom backward pass code (for custom |      gradients) must override this method to return `True`. |   |  n_in |      Returns how many tensors this layer expects as input. |   |  n_out |      Returns how many tensors this layer promises as output. |   |  name |      Returns the name of this layer. |   |  rng |      Returns a single-use random number generator without advancing it. |   |  state |      Returns a tuple containing this layer&#39;s state; may be empty. |   |  sublayers |      Returns a tuple containing this layer&#39;s sublayers; may be empty. |   |  weights |      Returns this layer&#39;s weights. |       |      Depending on the layer, the weights can be in the form of: |       |        - an empty tuple |        - a tensor (ndarray) |        - a nested structure of tuples and tensors</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation on tl.Serial</span></span><br><span class="line">help(tl.Serial)</span><br></pre></td></tr></table></figure><pre><code>Help on class Serial in module trax.layers.combinators:class Serial(trax.layers.base.Layer) |  Serial(*sublayers, name=None, sublayers_to_print=None) |   |  Combinator that applies layers serially (by function composition). |   |  This combinator is commonly used to construct deep networks, e.g., like this:: |   |      mlp = tl.Serial( |        tl.Dense(128), |        tl.Relu(), |        tl.Dense(10), |        tl.LogSoftmax() |      ) |   |  A Serial combinator uses stack semantics to manage data for its sublayers. |  Each sublayer sees only the inputs it needs and returns only the outputs it |  has generated. The sublayers interact via the data stack. For instance, a |  sublayer k, following sublayer j, gets called with the data stack in the |  state left after layer j has applied. The Serial combinator then: |   |    - takes n_in items off the top of the stack (n_in = k.n_in) and calls |      layer k, passing those items as arguments; and |   |    - takes layer k&#39;s n_out return values (n_out = k.n_out) and pushes |      them onto the data stack. |   |  A Serial instance with no sublayers acts as a special-case (but useful) |  1-input 1-output no-op. |   |  Method resolution order: |      Serial |      trax.layers.base.Layer |      builtins.object |   |  Methods defined here: |   |  __init__(self, *sublayers, name=None, sublayers_to_print=None) |      Creates a partially initialized, unconnected layer instance. |       |      Args: |        n_in: Number of inputs expected by this layer. |        n_out: Number of outputs promised by this layer. |        name: Class-like name for this layer; for use when printing this layer. |        sublayers_to_print: Sublayers to display when printing out this layer; |          By default (when None) we display all sublayers. |   |  forward(self, xs) |      Computes this layer&#39;s output as part of a forward pass through the model. |       |      Authors of new layer subclasses should override this method to define the |      forward computation that their layer performs. Use `self.weights` to access |      trainable weights of this layer. If you need to use local non-trainable |      state or randomness, use `self.rng` for the random seed (no need to set it) |      and use `self.state` for non-trainable state (and set it to the new value). |       |      Args: |        inputs: Zero or more input tensors, packaged as described in the `Layer` |            class docstring. |       |      Returns: |        Zero or more output tensors, packaged as described in the `Layer` class |        docstring. |   |  init_weights_and_state(self, input_signature) |      Initializes weights and state for inputs with the given signature. |       |      Authors of new layer subclasses should override this method if their layer |      uses trainable weights or non-trainable state. To initialize trainable |      weights, set `self.weights` and to initialize non-trainable state, |      set `self.state` to the intended value. |       |      Args: |        input_signature: A `ShapeDtype` instance (if this layer takes one input) |            or a list/tuple of `ShapeDtype` instances; signatures of inputs. |   |  ---------------------------------------------------------------------- |  Data descriptors defined here: |   |  state |      Returns a tuple containing this layer&#39;s state; may be empty. |   |  weights |      Returns this layer&#39;s weights. |       |      Depending on the layer, the weights can be in the form of: |       |        - an empty tuple |        - a tensor (ndarray) |        - a nested structure of tuples and tensors |   |  ---------------------------------------------------------------------- |  Methods inherited from trax.layers.base.Layer: |   |  __call__(self, x, weights=None, state=None, rng=None) |      Makes layers callable; for use in tests or interactive settings. |       |      This convenience method helps library users play with, test, or otherwise |      probe the behavior of layers outside of a full training environment. It |      presents the layer as callable function from inputs to outputs, with the |      option of manually specifying weights and non-parameter state per individual |      call. For convenience, weights and non-parameter state are cached per layer |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`, |      and acquiring non-empty values either by initialization or from values |      explicitly provided via the weights and state keyword arguments. |       |      Args: |        x: Zero or more input tensors, packaged as described in the `Layer` class |            docstring. |        weights: Weights or `None`; if `None`, use self&#39;s cached weights value. |        state: State or `None`; if `None`, use self&#39;s cached state value. |        rng: Single-use random number generator (JAX PRNG key), or `None`; |            if `None`, use a default computed from an integer 0 seed. |       |      Returns: |        Zero or more output tensors, packaged as described in the `Layer` class |        docstring. |   |  __repr__(self) |      Return repr(self). |   |  backward(self, inputs, output, grad, weights, state, new_state, rng) |      Custom backward pass to propagate gradients in a custom way. |       |      Args: |        inputs: Input tensors; can be a (possibly nested) tuple. |        output: The result of running this layer on inputs. |        grad: Gradient signal computed based on subsequent layers; its structure |            and shape must match output. |        weights: This layer&#39;s weights. |        state: This layer&#39;s state prior to the current forward pass. |        new_state: This layer&#39;s state after the current forward pass. |        rng: Single-use random number generator (JAX PRNG key). |       |      Returns: |        The custom gradient signal for the input. Note that we need to return |        a gradient for each argument of forward, so it will usually be a tuple |        of signals: the gradient for inputs and weights. |   |  init(self, input_signature, rng=None, use_cache=False) |      Initializes weights/state of this layer and its sublayers recursively. |       |      Initialization creates layer weights and state, for layers that use them. |      It derives the necessary array shapes and data types from the layer&#39;s input |      signature, which is itself just shape and data type information. |       |      For layers without weights or state, this method safely does nothing. |       |      This method is designed to create weights/state only once for each layer |      instance, even if the same layer instance occurs in multiple places in the |      network. This enables weight sharing to be implemented as layer sharing. |       |      Args: |        input_signature: `ShapeDtype` instance (if this layer takes one input) |            or list/tuple of `ShapeDtype` instances. |        rng: Single-use random number generator (JAX PRNG key), or `None`; |            if `None`, use a default computed from an integer 0 seed. |        use_cache: If `True`, and if this layer instance has already been |            initialized elsewhere in the network, then return special marker |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`. |            Else return this layer&#39;s newly initialized weights and state. |       |      Returns: |        A `(weights, state)` tuple. |   |  init_from_file(self, file_name, weights_only=False, input_signature=None) |      Initializes this layer and its sublayers from a pickled checkpoint. |       |      In the common case (`weights_only=False`), the file must be a gziped pickled |      dictionary containing items with keys `&#39;flat_weights&#39;, `&#39;flat_state&#39;` and |      `&#39;input_signature&#39;`, which are used to initialize this layer. |      If `input_signature` is specified, it&#39;s used instead of the one in the file. |      If `weights_only` is `True`, the dictionary does not need to have the |      `&#39;flat_state&#39;` item and the state it not restored either. |       |      Args: |        file_name: Name/path of the pickeled weights/state file. |        weights_only: If `True`, initialize only the layer&#39;s weights. Else |            initialize both weights and state. |        input_signature: Input signature to be used instead of the one from file. |   |  output_signature(self, input_signature) |      Returns output signature this layer would give for `input_signature`. |   |  pure_fn(self, x, weights, state, rng, use_cache=False) |      Applies this layer as a pure function with no optional args. |       |      This method exposes the layer&#39;s computation as a pure function. This is |      especially useful for JIT compilation. Do not override, use `forward` |      instead. |       |      Args: |        x: Zero or more input tensors, packaged as described in the `Layer` class |            docstring. |        weights: A tuple or list of trainable weights, with one element for this |            layer if this layer has no sublayers, or one for each sublayer if |            this layer has sublayers. If a layer (or sublayer) has no trainable |            weights, the corresponding weights element is an empty tuple. |        state: Layer-specific non-parameter state that can update between batches. |        rng: Single-use random number generator (JAX PRNG key). |        use_cache: if `True`, cache weights and state in the layer object; used |          to implement layer sharing in combinators. |       |      Returns: |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`) |        promised by this layer, and are packaged as described in the `Layer` |        class docstring. |   |  weights_and_state_signature(self, input_signature) |      Return a pair containing the signatures of weights and state. |   |  ---------------------------------------------------------------------- |  Data descriptors inherited from trax.layers.base.Layer: |   |  __dict__ |      dictionary for instance variables (if defined) |   |  __weakref__ |      list of weak references to the object (if defined) |   |  has_backward |      Returns `True` if this layer provides its own custom backward pass code. |       |      A layer subclass that provides custom backward pass code (for custom |      gradients) must override this method to return `True`. |   |  n_in |      Returns how many tensors this layer expects as input. |   |  n_out |      Returns how many tensors this layer promises as output. |   |  name |      Returns the name of this layer. |   |  rng |      Returns a single-use random number generator without advancing it. |   |  sublayers |      Returns a tuple containing this layer&#39;s sublayers; may be empty.</code></pre><ul><li><a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113" target="_blank" rel="noopener">tl.Embedding</a>: Layer constructor function for an embedding layer.  <ul><li><code>tl.Embedding(vocab_size, d_feature)</code>.</li><li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li><li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation for tl.Embedding</span></span><br><span class="line">help(tl.Embedding)</span><br></pre></td></tr></table></figure><pre><code>Help on class Embedding in module trax.layers.core:class Embedding(trax.layers.base.Layer) |  Embedding(vocab_size, d_feature, kernel_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb32d6228c8&gt;) |   |  Trainable layer that maps discrete tokens/ids to vectors. |   |  Method resolution order: |      Embedding |      trax.layers.base.Layer |      builtins.object |   |  Methods defined here: |   |  __init__(self, vocab_size, d_feature, kernel_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb32d6228c8&gt;) |      Returns an embedding layer with given vocabulary size and vector size. |       |      The layer clips input values (token ids) to the range `[0, vocab_size)`. |      That is, negative token ids all clip to `0` before being mapped to a |      vector, and token ids with value `vocab_size` or greater all clip to |      `vocab_size - 1` before being mapped to a vector. |       |      Args: |        vocab_size: Size of the input vocabulary. The layer will assign a unique |            vector to each id in `range(vocab_size)`. |        d_feature: Dimensionality/depth of the output vectors. |        kernel_initializer: Function that creates (random) initial vectors for |            the embedding. |   |  forward(self, x) |      Returns embedding vectors corresponding to input token id&#39;s. |       |      Args: |        x: Tensor of token id&#39;s. |       |      Returns: |        Tensor of embedding vectors. |   |  init_weights_and_state(self, input_signature) |      Returns tensor of newly initialized embedding vectors. |   |  ---------------------------------------------------------------------- |  Methods inherited from trax.layers.base.Layer: |   |  __call__(self, x, weights=None, state=None, rng=None) |      Makes layers callable; for use in tests or interactive settings. |       |      This convenience method helps library users play with, test, or otherwise |      probe the behavior of layers outside of a full training environment. It |      presents the layer as callable function from inputs to outputs, with the |      option of manually specifying weights and non-parameter state per individual |      call. For convenience, weights and non-parameter state are cached per layer |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`, |      and acquiring non-empty values either by initialization or from values |      explicitly provided via the weights and state keyword arguments. |       |      Args: |        x: Zero or more input tensors, packaged as described in the `Layer` class |            docstring. |        weights: Weights or `None`; if `None`, use self&#39;s cached weights value. |        state: State or `None`; if `None`, use self&#39;s cached state value. |        rng: Single-use random number generator (JAX PRNG key), or `None`; |            if `None`, use a default computed from an integer 0 seed. |       |      Returns: |        Zero or more output tensors, packaged as described in the `Layer` class |        docstring. |   |  __repr__(self) |      Return repr(self). |   |  backward(self, inputs, output, grad, weights, state, new_state, rng) |      Custom backward pass to propagate gradients in a custom way. |       |      Args: |        inputs: Input tensors; can be a (possibly nested) tuple. |        output: The result of running this layer on inputs. |        grad: Gradient signal computed based on subsequent layers; its structure |            and shape must match output. |        weights: This layer&#39;s weights. |        state: This layer&#39;s state prior to the current forward pass. |        new_state: This layer&#39;s state after the current forward pass. |        rng: Single-use random number generator (JAX PRNG key). |       |      Returns: |        The custom gradient signal for the input. Note that we need to return |        a gradient for each argument of forward, so it will usually be a tuple |        of signals: the gradient for inputs and weights. |   |  init(self, input_signature, rng=None, use_cache=False) |      Initializes weights/state of this layer and its sublayers recursively. |       |      Initialization creates layer weights and state, for layers that use them. |      It derives the necessary array shapes and data types from the layer&#39;s input |      signature, which is itself just shape and data type information. |       |      For layers without weights or state, this method safely does nothing. |       |      This method is designed to create weights/state only once for each layer |      instance, even if the same layer instance occurs in multiple places in the |      network. This enables weight sharing to be implemented as layer sharing. |       |      Args: |        input_signature: `ShapeDtype` instance (if this layer takes one input) |            or list/tuple of `ShapeDtype` instances. |        rng: Single-use random number generator (JAX PRNG key), or `None`; |            if `None`, use a default computed from an integer 0 seed. |        use_cache: If `True`, and if this layer instance has already been |            initialized elsewhere in the network, then return special marker |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`. |            Else return this layer&#39;s newly initialized weights and state. |       |      Returns: |        A `(weights, state)` tuple. |   |  init_from_file(self, file_name, weights_only=False, input_signature=None) |      Initializes this layer and its sublayers from a pickled checkpoint. |       |      In the common case (`weights_only=False`), the file must be a gziped pickled |      dictionary containing items with keys `&#39;flat_weights&#39;, `&#39;flat_state&#39;` and |      `&#39;input_signature&#39;`, which are used to initialize this layer. |      If `input_signature` is specified, it&#39;s used instead of the one in the file. |      If `weights_only` is `True`, the dictionary does not need to have the |      `&#39;flat_state&#39;` item and the state it not restored either. |       |      Args: |        file_name: Name/path of the pickeled weights/state file. |        weights_only: If `True`, initialize only the layer&#39;s weights. Else |            initialize both weights and state. |        input_signature: Input signature to be used instead of the one from file. |   |  output_signature(self, input_signature) |      Returns output signature this layer would give for `input_signature`. |   |  pure_fn(self, x, weights, state, rng, use_cache=False) |      Applies this layer as a pure function with no optional args. |       |      This method exposes the layer&#39;s computation as a pure function. This is |      especially useful for JIT compilation. Do not override, use `forward` |      instead. |       |      Args: |        x: Zero or more input tensors, packaged as described in the `Layer` class |            docstring. |        weights: A tuple or list of trainable weights, with one element for this |            layer if this layer has no sublayers, or one for each sublayer if |            this layer has sublayers. If a layer (or sublayer) has no trainable |            weights, the corresponding weights element is an empty tuple. |        state: Layer-specific non-parameter state that can update between batches. |        rng: Single-use random number generator (JAX PRNG key). |        use_cache: if `True`, cache weights and state in the layer object; used |          to implement layer sharing in combinators. |       |      Returns: |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`) |        promised by this layer, and are packaged as described in the `Layer` |        class docstring. |   |  weights_and_state_signature(self, input_signature) |      Return a pair containing the signatures of weights and state. |   |  ---------------------------------------------------------------------- |  Data descriptors inherited from trax.layers.base.Layer: |   |  __dict__ |      dictionary for instance variables (if defined) |   |  __weakref__ |      list of weak references to the object (if defined) |   |  has_backward |      Returns `True` if this layer provides its own custom backward pass code. |       |      A layer subclass that provides custom backward pass code (for custom |      gradients) must override this method to return `True`. |   |  n_in |      Returns how many tensors this layer expects as input. |   |  n_out |      Returns how many tensors this layer promises as output. |   |  name |      Returns the name of this layer. |   |  rng |      Returns a single-use random number generator without advancing it. |   |  state |      Returns a tuple containing this layer&#39;s state; may be empty. |   |  sublayers |      Returns a tuple containing this layer&#39;s sublayers; may be empty. |   |  weights |      Returns this layer&#39;s weights. |       |      Depending on the layer, the weights can be in the form of: |       |        - an empty tuple |        - a tensor (ndarray) |        - a nested structure of tuples and tensors</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmp_embed = tl.Embedding(vocab_size=<span class="number">3</span>, d_feature=<span class="number">2</span>)</span><br><span class="line">display(tmp_embed)</span><br></pre></td></tr></table></figure><pre><code>Embedding_3_2</code></pre><ul><li><a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276" target="_blank" rel="noopener">tl.Mean</a>: Calculates means across an axis.  In this case, please choose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).  </li><li>For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># view the documentation for tl.mean</span></span><br><span class="line">help(tl.Mean)</span><br></pre></td></tr></table></figure><pre><code>Help on function Mean in module trax.layers.core:Mean(axis=-1, keepdims=False)    Returns a layer that computes mean values using one tensor axis.    `Mean` uses one tensor axis to form groups of values and replaces each group    with the mean value of that group. The resulting values can either remain    in their own size 1 axis (`keepdims=True`), or that axis can be removed from    the overall tensor (default `keepdims=False`), lowering the rank of the    tensor by one.    Args:      axis: Axis along which values are grouped for computing a mean.      keepdims: If `True`, keep the resulting size 1 axis as a separate tensor          axis; else, remove that axis.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pretend the embedding matrix uses </span></span><br><span class="line"><span class="comment"># 2 elements for embedding the meaning of a word</span></span><br><span class="line"><span class="comment"># and has a vocabulary size of 3</span></span><br><span class="line"><span class="comment"># So it has shape (2,3)</span></span><br><span class="line">tmp_embed = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,],</span><br><span class="line">                    [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">                   ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># take the mean along axis 0</span></span><br><span class="line">print(<span class="string">"The mean along axis 0 creates a vector whose length equals the vocabulary size"</span>)</span><br><span class="line">display(np.mean(tmp_embed,axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding"</span>)</span><br><span class="line">display(np.mean(tmp_embed,axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>The mean along axis 0 creates a vector whose length equals the vocabulary sizeDeviceArray([2.5, 3.5, 4.5], dtype=float32)The mean along axis 1 creates a vector whose length equals the number of elements in a word embeddingDeviceArray([2., 5.], dtype=float32)</code></pre><ul><li><a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242" target="_blank" rel="noopener">tl.LogSoftmax</a>: Implements log softmax function</li><li>Here, you don’t need to set any parameters for <code>LogSoftMax()</code>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">help(tl.LogSoftmax)</span><br></pre></td></tr></table></figure><pre><code>Help on function LogSoftmax in module trax.layers.core:LogSoftmax(axis=-1)    Returns a layer that applies log softmax along one tensor axis.    `LogSoftmax` acts on a group of values and normalizes them to look like a set    of log probability values. (Probability values must be non-negative, and as    a set must sum to 1. A group of log probability values can be seen as the    natural logarithm function applied to a set of probability values.)    Args:      axis: Axis along which values are grouped for computing log softmax.</code></pre><p><strong>Online documentation</strong></p><ul><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a></p></li><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators" target="_blank" rel="noopener">tl.Serial</a></p></li><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding" target="_blank" rel="noopener">tl.Embedding</a></p></li><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean" target="_blank" rel="noopener">tl.Mean</a></p></li><li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax" target="_blank" rel="noopener">tl.LogSoftmax</a></p></li></ul><p><a name="ex05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>Implement the classifier function. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: classifier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifier</span><span class="params">(vocab_size=len<span class="params">(Vocab)</span>, embedding_dim=<span class="number">256</span>, output_dim=<span class="number">2</span>, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># create embedding layer</span></span><br><span class="line">    embed_layer = tl.Embedding(</span><br><span class="line">        vocab_size=vocab_size, <span class="comment"># Size of the vocabulary</span></span><br><span class="line">        d_feature=embedding_dim)  <span class="comment"># Embedding dimension</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a mean layer, to create an "average" word embedding</span></span><br><span class="line">    mean_layer = tl.Mean(axis = <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a dense layer, one unit for each output</span></span><br><span class="line">    dense_output_layer = tl.Dense(n_units = output_dim)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the log softmax layer (no parameters needed)</span></span><br><span class="line">    log_softmax_layer = tl.LogSoftmax()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use tl.Serial to combine all layers</span></span><br><span class="line">    <span class="comment"># and create the classifier</span></span><br><span class="line">    <span class="comment"># of type trax.layers.combinators.Serial</span></span><br><span class="line">    model = tl.Serial(</span><br><span class="line">      embed_layer, <span class="comment"># embedding layer</span></span><br><span class="line">      mean_layer, <span class="comment"># mean layer</span></span><br><span class="line">      dense_output_layer, <span class="comment"># dense output layer </span></span><br><span class="line">      log_softmax_layer <span class="comment"># log softmax layer</span></span><br><span class="line">    )</span><br><span class="line"><span class="comment">### END CODE HERE ###     </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># return the model of type</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmp_model = classifier()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(type(tmp_model))</span><br><span class="line">display(tmp_model)</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;trax.layers.combinators.Serial&#39;&gt;Serial[  Embedding_9088_256  Mean  Dense_2  LogSoftmax]</code></pre><h5 id="Expected-Outout-2"><a href="#Expected-Outout-2" class="headerlink" title="Expected Outout"></a>Expected Outout</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">trax</span>.<span class="title">layers</span>.<span class="title">combinators</span>.<span class="title">Serial</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Serial</span>[</span></span><br><span class="line"><span class="class">  <span class="title">Embedding_9088_256</span></span></span><br><span class="line"><span class="class">  <span class="title">Mean</span></span></span><br><span class="line"><span class="class">  <span class="title">Dense_2</span></span></span><br><span class="line"><span class="class">  <span class="title">LogSoftmax</span></span></span><br><span class="line"><span class="class">]</span></span><br></pre></td></tr></table></figure><p><a name="4"></a></p><h1 id="Part-4-Training"><a href="#Part-4-Training" class="headerlink" title="Part 4:  Training"></a>Part 4:  Training</h1><p>To train a model on a task, Trax defines an abstraction <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask" target="_blank" rel="noopener"><code>trax.supervised.training.TrainTask</code></a> which packages the train data, loss and optimizer (among other things) together into an object.</p><p>Similarly to evaluate a model, Trax defines an abstraction <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask" target="_blank" rel="noopener"><code>trax.supervised.training.EvalTask</code></a> which packages the eval data and metrics (among other things) into another object.</p><p>The final piece tying things together is the <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop" target="_blank" rel="noopener"><code>trax.supervised.training.Loop</code></a> abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.<br>Using <code>Loop</code> will save you a lot of code compared to always writing the training loop by hand, like you did in courses 1 and 2. More importantly, you are less likely to have a bug in that code that would ruin your training.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># View documentation for trax.supervised.training.TrainTask</span></span><br><span class="line">help(trax.supervised.training.TrainTask)</span><br></pre></td></tr></table></figure><pre><code>Help on class TrainTask in module trax.supervised.training:class TrainTask(builtins.object) |  TrainTask(labeled_data, loss_layer, optimizer, lr_schedule=None, n_steps_per_checkpoint=100) |   |  A supervised task (labeled data + feedback mechanism) for training. |   |  Methods defined here: |   |  __init__(self, labeled_data, loss_layer, optimizer, lr_schedule=None, n_steps_per_checkpoint=100) |      Configures a training task. |       |      Args: |        labeled_data: Iterator of batches of labeled data tuples. Each tuple has |            1+ data (input value) tensors followed by 1 label (target value) |            tensor.  All tensors are NumPy ndarrays or their JAX counterparts. |        loss_layer: Layer that computes a scalar value (the &quot;loss&quot;) by comparing |            model output :math:`\hat{y}=f(x)` to the target :math:`y`. |        optimizer: Optimizer object that computes model weight updates from |            loss-function gradients. |        lr_schedule: Learning rate schedule, a function step -&gt; learning_rate. |        n_steps_per_checkpoint: How many steps to run between checkpoints. |   |  learning_rate(self, step) |      Return the learning rate for the given step. |   |  next_batch(self) |      Returns one batch of labeled data: a tuple of input(s) plus label. |   |  ---------------------------------------------------------------------- |  Data descriptors defined here: |   |  __dict__ |      dictionary for instance variables (if defined) |   |  __weakref__ |      list of weak references to the object (if defined) |   |  labeled_data |   |  loss_layer |   |  n_steps_per_checkpoint |   |  optimizer |   |  sample_batch</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation for trax.supervised.training.EvalTask</span></span><br><span class="line">help(trax.supervised.training.EvalTask)</span><br></pre></td></tr></table></figure><pre><code>Help on class EvalTask in module trax.supervised.training:class EvalTask(builtins.object) |  EvalTask(labeled_data, metrics, metric_names=None, n_eval_batches=1) |   |  Labeled data plus scalar functions for (periodically) measuring a model. |   |  An eval task specifies how (`labeled_data` + `metrics`) and with what |  precision (`n_eval_batches`) to measure a model as it is training. |  The variance of each scalar output is reduced by measuring over multiple |  (`n_eval_batches`) batches and reporting the average from those measurements. |   |  Methods defined here: |   |  __init__(self, labeled_data, metrics, metric_names=None, n_eval_batches=1) |      Configures an eval task: named metrics run with a given data source. |       |      Args: |        labeled_data: Iterator of batches of labeled data tuples. Each tuple has |            1+ data tensors (NumPy ndarrays) followed by 1 label (target value) |            tensor. |        metrics: List of layers; each computes a scalar value per batch by |            comparing model output :math:`\hat{y}=f(x)` to the target :math:`y`. |        metric_names: List of names, one for each item in `metrics`, in matching |             order, to be used when recording/reporting eval output. If None, |             generate default names using layer names from metrics. |        n_eval_batches: Integer N that specifies how many eval batches to run; |            the output is then the average of the outputs from the N batches. |   |  next_batch(self) |      Returns one batch of labeled data: a tuple of input(s) plus label. |   |  ---------------------------------------------------------------------- |  Data descriptors defined here: |   |  __dict__ |      dictionary for instance variables (if defined) |   |  __weakref__ |      list of weak references to the object (if defined) |   |  labeled_data |   |  metric_names |   |  metrics |   |  n_eval_batches |   |  sample_batch</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation for trax.supervised.training.Loop</span></span><br><span class="line">help(trax.supervised.training.Loop)</span><br></pre></td></tr></table></figure><pre><code>Help on class Loop in module trax.supervised.training:class Loop(builtins.object) |  Loop(model, task, eval_model=None, eval_task=None, output_dir=None, checkpoint_at=None, eval_at=None) |   |  Loop that can run for a given number of steps to train a supervised model. |   |  The typical supervised training process randomly initializes a model and |  updates its weights via feedback (loss-derived gradients) from a training |  task, by looping through batches of labeled data. A training loop can also |  be configured to run periodic evals and save intermediate checkpoints. |   |  For speed, the implementation takes advantage of JAX&#39;s composable function |  transformations (specifically, `jit` and `grad`). It creates JIT-compiled |  pure functions derived from variants of the core model; schematically: |   |    - training variant: jit(grad(pure_function(model+loss))) |    - evals variant: jit(pure_function(model+evals)) |   |  In training or during evals, these variants are called with explicit |  arguments for all relevant input data, model weights/state, optimizer slots, |  and random number seeds: |   |    - batch: labeled data |    - model weights/state: trainable weights and input-related state (e.g., as |      used by batch norm) |    - optimizer slots: weights in the optimizer that evolve during the training |      process |    - random number seeds: JAX PRNG keys that enable high-quality, distributed, |      repeatable generation of pseudo-random numbers |   |  Methods defined here: |   |  __init__(self, model, task, eval_model=None, eval_task=None, output_dir=None, checkpoint_at=None, eval_at=None) |      Configures a training `Loop`, including a random initialization. |       |      Args: |        model: Trax layer, representing the core model to be trained. Loss |            functions and eval functions (a.k.a. metrics) are considered to be |            outside the core model, taking core model output and data labels as |            their two inputs. |        task: TrainTask instance, which defines the training data, loss function, |            and optimizer to be used in this training loop. |        eval_model: Optional Trax layer, representing model used for evaluation, |          e.g., with dropout turned off. If None, the training model (model) |          will be used. |        eval_task: EvalTask instance or None. If None, don&#39;t do any evals. |        output_dir: Path telling where to save outputs (evals and checkpoints). |            Can be None if both `eval_task` and `checkpoint_at` are None. |        checkpoint_at: Function (integer --&gt; boolean) telling, for step n, whether |            that step should have its checkpoint saved. If None, the default is |            periodic checkpointing at `task.n_steps_per_checkpoint`. |        eval_at: Function (integer --&gt; boolean) that says, for training step n, |            whether that step should run evals. If None, run when checkpointing. |   |  new_rng(self) |      Returns a new single-use random number generator (JAX PRNG key). |   |  run(self, n_steps=1) |      Runs this training loop for n steps. |       |      Optionally runs evals and saves checkpoints at specified points. |       |      Args: |        n_steps: Stop training after completing n steps. |   |  run_evals(self, weights=None, state=None) |      Runs and records evals for this training session. |       |      Args: |        weights: Current weights from model in training. |        state: Current state from model in training. |   |  save_checkpoint(self, weights=None, state=None, slots=None) |      Saves checkpoint to disk for the current training step. |       |      Args: |        weights: Weights from model being trained. |        state: State (non-weight parameters) from model being trained. |        slots: Updatable weights for the optimizer in this training loop. |   |  ---------------------------------------------------------------------- |  Data descriptors defined here: |   |  __dict__ |      dictionary for instance variables (if defined) |   |  __weakref__ |      list of weak references to the object (if defined) |   |  current_step |      Returns current step number in this training session. |   |  eval_model |      Returns the model used for evaluation. |   |  model |      Returns the model that is training.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View optimizers that you could choose from</span></span><br><span class="line">help(trax.optimizers)</span><br></pre></td></tr></table></figure><pre><code>Help on package trax.optimizers in trax:NAME    trax.optimizers - Optimizers for use with Trax layers.PACKAGE CONTENTS    adafactor    adam    base    momentum    optimizers_test    rms_prop    sm3FUNCTIONS    opt_configure(*args, **kwargs)FILE    /opt/conda/lib/python3.7/site-packages/trax/optimizers/__init__.py</code></pre><p>Notice some available optimizers include:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">adafactor</span><br><span class="line">adam</span><br><span class="line">momentum</span><br><span class="line">rms_prop</span><br><span class="line">sm3</span><br></pre></td></tr></table></figure></p><p><a name="4.1"></a></p><h2 id="4-1-Training-the-model"><a href="#4-1-Training-the-model" class="headerlink" title="4.1  Training the model"></a>4.1  Training the model</h2><p>Now you are going to train your model. </p><p>Let’s define the <code>TrainTask</code>, <code>EvalTask</code> and <code>Loop</code> in preparation to train the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">rnd.seed(<span class="number">271</span>)</span><br><span class="line"></span><br><span class="line">train_task = training.TrainTask(</span><br><span class="line">    labeled_data=train_generator(batch_size=batch_size, shuffle=<span class="keyword">True</span>),</span><br><span class="line">    loss_layer=tl.CrossEntropyLoss(),</span><br><span class="line">    optimizer=trax.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">    n_steps_per_checkpoint=<span class="number">10</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">eval_task = training.EvalTask(</span><br><span class="line">    labeled_data=val_generator(batch_size=batch_size, shuffle=<span class="keyword">True</span>),</span><br><span class="line">    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = classifier()</span><br></pre></td></tr></table></figure><p>This defines a model trained using <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss" target="_blank" rel="noopener"><code>tl.CrossEntropyLoss</code></a> optimized with the <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam" target="_blank" rel="noopener"><code>trax.optimizers.Adam</code></a> optimizer, all the while tracking the accuracy using <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy" target="_blank" rel="noopener"><code>tl.Accuracy</code></a> metric. We also track <code>tl.CrossEntropyLoss</code> on the validation set.</p><p>Now let’s make an output directory and train the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output_dir = <span class="string">'~/model/'</span></span><br><span class="line">output_dir_expand = os.path.expanduser(output_dir)</span><br><span class="line">print(output_dir_expand)</span><br></pre></td></tr></table></figure><pre><code>/home/jovyan/model/</code></pre><p><a name="ex06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement <code>train_model</code> to train the model (<code>classifier</code> that you wrote earlier) for the given number of training steps (<code>n_steps</code>) using <code>TrainTask</code>, <code>EvalTask</code> and <code>Loop</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(classifier, train_task, eval_task, n_steps, output_dir)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        classifier - the model you are building</span></span><br><span class="line"><span class="string">        train_task - Training task</span></span><br><span class="line"><span class="string">        eval_task - Evaluation task</span></span><br><span class="line"><span class="string">        n_steps - the evaluation steps</span></span><br><span class="line"><span class="string">        output_dir - folder to save your files</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        trainer -  trax trainer</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    training_loop = training.Loop(</span><br><span class="line">                                classifier, <span class="comment"># The learning model</span></span><br><span class="line">                                train_task, <span class="comment"># The training task</span></span><br><span class="line">                                eval_task = eval_task, <span class="comment"># The evaluation task</span></span><br><span class="line">                                output_dir = output_dir) <span class="comment"># The output directory</span></span><br><span class="line"></span><br><span class="line">    training_loop.run(n_steps = n_steps)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the training_loop, since it has the model.</span></span><br><span class="line">    <span class="keyword">return</span> training_loop</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">training_loop = train_model(model, train_task, eval_task, <span class="number">100</span>, output_dir_expand)</span><br></pre></td></tr></table></figure><pre><code>Step      1: train CrossEntropyLoss |  0.88939196Step      1: eval  CrossEntropyLoss |  0.68833977Step      1: eval          Accuracy |  0.50000000Step     10: train CrossEntropyLoss |  0.61036736Step     10: eval  CrossEntropyLoss |  0.52182281Step     10: eval          Accuracy |  0.68750000Step     20: train CrossEntropyLoss |  0.34137666Step     20: eval  CrossEntropyLoss |  0.20654774Step     20: eval          Accuracy |  1.00000000Step     30: train CrossEntropyLoss |  0.20208922Step     30: eval  CrossEntropyLoss |  0.21594886Step     30: eval          Accuracy |  0.93750000Step     40: train CrossEntropyLoss |  0.19611198Step     40: eval  CrossEntropyLoss |  0.17582777Step     40: eval          Accuracy |  1.00000000Step     50: train CrossEntropyLoss |  0.11203773Step     50: eval  CrossEntropyLoss |  0.07589275Step     50: eval          Accuracy |  1.00000000Step     60: train CrossEntropyLoss |  0.09375446Step     60: eval  CrossEntropyLoss |  0.09290724Step     60: eval          Accuracy |  1.00000000Step     70: train CrossEntropyLoss |  0.08785903Step     70: eval  CrossEntropyLoss |  0.09610598Step     70: eval          Accuracy |  1.00000000Step     80: train CrossEntropyLoss |  0.08858261Step     80: eval  CrossEntropyLoss |  0.02319432Step     80: eval          Accuracy |  1.00000000Step     90: train CrossEntropyLoss |  0.05699894Step     90: eval  CrossEntropyLoss |  0.01778970Step     90: eval          Accuracy |  1.00000000Step    100: train CrossEntropyLoss |  0.03663783Step    100: eval  CrossEntropyLoss |  0.00210550Step    100: eval          Accuracy |  1.00000000</code></pre><h5 id="Expected-output-Approximately"><a href="#Expected-output-Approximately" class="headerlink" title="Expected output (Approximately)"></a>Expected output (Approximately)</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Step      <span class="number">1</span>: train CrossEntropyLoss |  <span class="number">0.88939196</span></span><br><span class="line">Step      <span class="number">1</span>: eval  CrossEntropyLoss |  <span class="number">0.68833977</span></span><br><span class="line">Step      <span class="number">1</span>: eval          Accuracy |  <span class="number">0.50000000</span></span><br><span class="line">Step     <span class="number">10</span>: train CrossEntropyLoss |  <span class="number">0.61036736</span></span><br><span class="line">Step     <span class="number">10</span>: eval  CrossEntropyLoss |  <span class="number">0.52182281</span></span><br><span class="line">Step     <span class="number">10</span>: eval          Accuracy |  <span class="number">0.68750000</span></span><br><span class="line">Step     <span class="number">20</span>: train CrossEntropyLoss |  <span class="number">0.34137666</span></span><br><span class="line">Step     <span class="number">20</span>: eval  CrossEntropyLoss |  <span class="number">0.20654774</span></span><br><span class="line">Step     <span class="number">20</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">30</span>: train CrossEntropyLoss |  <span class="number">0.20208922</span></span><br><span class="line">Step     <span class="number">30</span>: eval  CrossEntropyLoss |  <span class="number">0.21594886</span></span><br><span class="line">Step     <span class="number">30</span>: eval          Accuracy |  <span class="number">0.93750000</span></span><br><span class="line">Step     <span class="number">40</span>: train CrossEntropyLoss |  <span class="number">0.19611198</span></span><br><span class="line">Step     <span class="number">40</span>: eval  CrossEntropyLoss |  <span class="number">0.17582777</span></span><br><span class="line">Step     <span class="number">40</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">50</span>: train CrossEntropyLoss |  <span class="number">0.11203773</span></span><br><span class="line">Step     <span class="number">50</span>: eval  CrossEntropyLoss |  <span class="number">0.07589275</span></span><br><span class="line">Step     <span class="number">50</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">60</span>: train CrossEntropyLoss |  <span class="number">0.09375446</span></span><br><span class="line">Step     <span class="number">60</span>: eval  CrossEntropyLoss |  <span class="number">0.09290724</span></span><br><span class="line">Step     <span class="number">60</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">70</span>: train CrossEntropyLoss |  <span class="number">0.08785903</span></span><br><span class="line">Step     <span class="number">70</span>: eval  CrossEntropyLoss |  <span class="number">0.09610598</span></span><br><span class="line">Step     <span class="number">70</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">80</span>: train CrossEntropyLoss |  <span class="number">0.08858261</span></span><br><span class="line">Step     <span class="number">80</span>: eval  CrossEntropyLoss |  <span class="number">0.02319432</span></span><br><span class="line">Step     <span class="number">80</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">90</span>: train CrossEntropyLoss |  <span class="number">0.05699894</span></span><br><span class="line">Step     <span class="number">90</span>: eval  CrossEntropyLoss |  <span class="number">0.01778970</span></span><br><span class="line">Step     <span class="number">90</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step    <span class="number">100</span>: train CrossEntropyLoss |  <span class="number">0.03663783</span></span><br><span class="line">Step    <span class="number">100</span>: eval  CrossEntropyLoss |  <span class="number">0.00210550</span></span><br><span class="line">Step    <span class="number">100</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br></pre></td></tr></table></figure><p><a name="4.2"></a></p><h2 id="4-2-Practice-Making-a-prediction"><a href="#4-2-Practice-Making-a-prediction" class="headerlink" title="4.2  Practice Making a prediction"></a>4.2  Practice Making a prediction</h2><p>Now that you have trained a model, you can access it as <code>training_loop.model</code> object. We will actually use <code>training_loop.eval_model</code> and in the next weeks you will learn why we sometimes use a different model for evaluation, e.g., one without dropout. For now, make predictions with your model.</p><p>Use the training data just to see how the prediction process works.  </p><ul><li>Later, you will use validation data to evaluate your model’s performance.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a generator object</span></span><br><span class="line">tmp_train_generator = train_generator(<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get one batch</span></span><br><span class="line">tmp_batch = next(tmp_train_generator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Position 0 has the model inputs (tweets as tensors)</span></span><br><span class="line"><span class="comment"># position 1 has the targets (the actual labels)</span></span><br><span class="line">tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The batch is a tuple of length <span class="subst">&#123;len(tmp_batch)&#125;</span> because position 0 contains the tweets, and position 1 contains the targets."</span>) </span><br><span class="line">print(<span class="string">f"The shape of the tweet tensors is <span class="subst">&#123;tmp_inputs.shape&#125;</span> (num of examples, length of tweet tensors)"</span>)</span><br><span class="line">print(<span class="string">f"The shape of the labels is <span class="subst">&#123;tmp_targets.shape&#125;</span>, which is the batch size."</span>)</span><br><span class="line">print(<span class="string">f"The shape of the example_weights is <span class="subst">&#123;tmp_example_weights.shape&#125;</span>, which is the same as inputs/targets size."</span>)</span><br></pre></td></tr></table></figure><pre><code>The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.The shape of the tweet tensors is (16, 15) (num of examples, length of tweet tensors)The shape of the labels is (16,), which is the batch size.The shape of the example_weights is (16,), which is the same as inputs/targets size.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feed the tweet tensors into the model to get a prediction</span></span><br><span class="line">tmp_pred = training_loop.eval_model(tmp_inputs)</span><br><span class="line">print(<span class="string">f"The prediction shape is <span class="subst">&#123;tmp_pred.shape&#125;</span>, num of tensor_tweets as rows"</span>)</span><br><span class="line">print(<span class="string">"Column 0 is the probability of a negative sentiment (class 0)"</span>)</span><br><span class="line">print(<span class="string">"Column 1 is the probability of a positive sentiment (class 1)"</span>)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"View the prediction array"</span>)</span><br><span class="line">tmp_pred</span><br></pre></td></tr></table></figure><pre><code>The prediction shape is (16, 2), num of tensor_tweets as rowsColumn 0 is the probability of a negative sentiment (class 0)Column 1 is the probability of a positive sentiment (class 1)View the prediction arrayDeviceArray([[-4.9417334e+00, -7.1678162e-03],             [-6.5846415e+00, -1.3823509e-03],             [-5.4463043e+00, -4.3215752e-03],             [-4.3487482e+00, -1.3007164e-02],             [-4.9131694e+00, -7.3764324e-03],             [-4.7097692e+00, -9.0477467e-03],             [-5.2801600e+00, -5.1045418e-03],             [-4.1103225e+00, -1.6538620e-02],             [-1.8327236e-03, -6.3028107e+00],             [-4.7376156e-03, -5.3545618e+00],             [-3.4697056e-03, -5.6654320e+00],             [-1.1444092e-05, -1.1379558e+01],             [-1.0051131e-02, -4.6050973e+00],             [-1.0130405e-03, -6.8951964e+00],             [-6.1047077e-03, -5.1017356e+00],             [-7.4422359e-03, -4.9043016e+00]], dtype=float32)</code></pre><p>To turn these probabilities into categories (negative or positive sentiment prediction), for each row:</p><ul><li>Compare the probabilities in each column.</li><li>If column 1 has a value greater than column 0, classify that as a positive tweet.</li><li>Otherwise if column 1 is less than or equal to column 0, classify that example as a negative tweet.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># turn probabilites into category predictions</span></span><br><span class="line">tmp_is_positive = tmp_pred[:,<span class="number">1</span>] &gt; tmp_pred[:,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i, p <span class="keyword">in</span> enumerate(tmp_is_positive):</span><br><span class="line">    print(<span class="string">f"Neg log prob <span class="subst">&#123;tmp_pred[i,<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>\tPos log prob <span class="subst">&#123;tmp_pred[i,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>\t is positive? <span class="subst">&#123;p&#125;</span>\t actual <span class="subst">&#123;tmp_targets[i]&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Neg log prob -4.9417    Pos log prob -0.0072     is positive? True   actual 1Neg log prob -6.5846    Pos log prob -0.0014     is positive? True   actual 1Neg log prob -5.4463    Pos log prob -0.0043     is positive? True   actual 1Neg log prob -4.3487    Pos log prob -0.0130     is positive? True   actual 1Neg log prob -4.9132    Pos log prob -0.0074     is positive? True   actual 1Neg log prob -4.7098    Pos log prob -0.0090     is positive? True   actual 1Neg log prob -5.2802    Pos log prob -0.0051     is positive? True   actual 1Neg log prob -4.1103    Pos log prob -0.0165     is positive? True   actual 1Neg log prob -0.0018    Pos log prob -6.3028     is positive? False  actual 0Neg log prob -0.0047    Pos log prob -5.3546     is positive? False  actual 0Neg log prob -0.0035    Pos log prob -5.6654     is positive? False  actual 0Neg log prob -0.0000    Pos log prob -11.3796    is positive? False  actual 0Neg log prob -0.0101    Pos log prob -4.6051     is positive? False  actual 0Neg log prob -0.0010    Pos log prob -6.8952     is positive? False  actual 0Neg log prob -0.0061    Pos log prob -5.1017     is positive? False  actual 0Neg log prob -0.0074    Pos log prob -4.9043     is positive? False  actual 0</code></pre><p>Notice that since you are making a prediction using a training batch, it’s more likely that the model’s predictions match the actual targets (labels).  </p><ul><li>Every prediction that the tweet is positive is also matching the actual target of 1 (positive sentiment).</li><li>Similarly, all predictions that the sentiment is not positive matches the actual target of 0 (negative sentiment)</li></ul><p>One more useful thing to know is how to compare if the prediction is matching the actual target (label).  </p><ul><li>The result of calculation <code>is_positive</code> is a boolean.</li><li>The target is a type trax.fastmath.numpy.int32</li><li>If you expect to be doing division, you may prefer to work with decimal numbers with the data type type trax.fastmath.numpy.int32</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View the array of booleans</span></span><br><span class="line">print(<span class="string">"Array of booleans"</span>)</span><br><span class="line">display(tmp_is_positive)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert boolean to type int32</span></span><br><span class="line"><span class="comment"># True is converted to 1</span></span><br><span class="line"><span class="comment"># False is converted to 0</span></span><br><span class="line">tmp_is_positive_int = tmp_is_positive.astype(np.int32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># View the array of integers</span></span><br><span class="line">print(<span class="string">"Array of integers"</span>)</span><br><span class="line">display(tmp_is_positive_int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert boolean to type float32</span></span><br><span class="line">tmp_is_positive_float = tmp_is_positive.astype(np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the array of floats</span></span><br><span class="line">print(<span class="string">"Array of floats"</span>)</span><br><span class="line">display(tmp_is_positive_float)</span><br></pre></td></tr></table></figure><pre><code>Array of booleansDeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,             False, False, False, False, False, False, False, False],            dtype=bool)Array of integersDeviceArray([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)Array of floatsDeviceArray([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,             0.], dtype=float32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmp_pred.shape</span><br></pre></td></tr></table></figure><pre><code>(16, 2)</code></pre><p>Note that Python usually does type conversion for you when you compare a boolean to an integer</p><ul><li>True compared to 1 is True, otherwise any other integer is False.</li><li>False compared to 0 is True, otherwise any ohter integer is False.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"True == 1: <span class="subst">&#123;<span class="keyword">True</span> == <span class="number">1</span>&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"True == 2: <span class="subst">&#123;<span class="keyword">True</span> == <span class="number">2</span>&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"False == 0: <span class="subst">&#123;<span class="keyword">False</span> == <span class="number">0</span>&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"False == 2: <span class="subst">&#123;<span class="keyword">False</span> == <span class="number">2</span>&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>True == 1: TrueTrue == 2: FalseFalse == 0: TrueFalse == 2: False</code></pre><p>However, we recommend that you keep track of the data type of your variables to avoid unexpected outcomes.  So it helps to convert the booleans into integers</p><ul><li>Compare 1 to 1 rather than comparing True to 1.</li></ul><p>Hopefully you are now familiar with what kinds of inputs and outputs the model uses when making a prediction.</p><ul><li>This will help you implement a function that estimates the accuracy of the model’s predictions.</li></ul><p><a name="5"></a></p><h1 id="Part-5-Evaluation"><a href="#Part-5-Evaluation" class="headerlink" title="Part 5:  Evaluation"></a>Part 5:  Evaluation</h1><p><a name="5.1"></a></p><h2 id="5-1-Computing-the-accuracy-on-a-batch"><a href="#5-1-Computing-the-accuracy-on-a-batch" class="headerlink" title="5.1  Computing the accuracy on a batch"></a>5.1  Computing the accuracy on a batch</h2><p>You will now write a function that evaluates your model on the validation set and returns the accuracy. </p><ul><li><code>preds</code> contains the predictions.<ul><li>Its dimensions are <code>(batch_size, output_dim)</code>.  <code>output_dim</code> is two in this case.  Column 0 contains the probability that the tweet belongs to class 0 (negative sentiment). Column 1 contains probability that it belongs to class 1 (positive sentiment).</li><li>If the probability in column 1 is greater than the probability in column 0, then interpret this as the model’s prediction that the example has label 1 (positive sentiment).  </li><li>Otherwise, if the probabilities are equal or the probability in column 0 is higher, the model’s prediction is 0 (negative sentiment).</li></ul></li><li><code>y</code> contains the actual labels.</li><li><code>y_weights</code> contains the weights to give to predictions.</li></ul><p><a name="ex07"></a></p><h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Implement <code>compute_accuracy</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(preds, y, y_weights)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        preds: a tensor of shape (dim_batch, output_dim) </span></span><br><span class="line"><span class="string">        y: a tensor of shape (dim_batch, output_dim) with the true labels</span></span><br><span class="line"><span class="string">        y_weights: a n.ndarray with the a weight for each example</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: a float between 0-1 </span></span><br><span class="line"><span class="string">        weighted_num_correct (np.float32): Sum of the weighted correct predictions</span></span><br><span class="line"><span class="string">        sum_weights (np.float32): Sum of the weights</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># Create an array of booleans, </span></span><br><span class="line">    <span class="comment"># True if the probability of positive sentiment is greater than</span></span><br><span class="line">    <span class="comment"># the probability of negative sentiment</span></span><br><span class="line">    <span class="comment"># else False</span></span><br><span class="line">    is_pos =  preds[:,<span class="number">1</span>] &gt; preds[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert the array of booleans into an array of np.int32</span></span><br><span class="line">    is_pos_int = is_pos.astype(np.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compare the array of predictions (as int32) with the target (labels) of type int32</span></span><br><span class="line">    correct = is_pos_int == y</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Count the sum of the weights.</span></span><br><span class="line">    sum_weights = np.sum(y_weights)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert the array of correct predictions (boolean) into an arrayof np.float32</span></span><br><span class="line">    correct_float = correct.astype(np.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Multiply each prediction with its corresponding weight.</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    weighted_correct_float = correct_float * y_weights</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sum up the weighted correct predictions (of type np.float32), to go in the</span></span><br><span class="line">    <span class="comment"># denominator.</span></span><br><span class="line">    weighted_num_correct = np.sum(weighted_correct_float)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Divide the number of weighted correct predictions by the sum of the</span></span><br><span class="line">    <span class="comment"># weights.</span></span><br><span class="line">    accuracy = weighted_num_correct / sum_weights</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> accuracy, weighted_num_correct, sum_weights</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your function</span></span><br><span class="line">tmp_val_generator = val_generator(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get one batch</span></span><br><span class="line">tmp_batch = next(tmp_val_generator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Position 0 has the model inputs (tweets as tensors)</span></span><br><span class="line"><span class="comment"># position 1 has the targets (the actual labels)</span></span><br><span class="line">tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch</span><br><span class="line"></span><br><span class="line"><span class="comment"># feed the tweet tensors into the model to get a prediction</span></span><br><span class="line">tmp_pred = training_loop.eval_model(tmp_inputs)</span><br><span class="line"></span><br><span class="line">tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Model's prediction accuracy on a single training batch is: <span class="subst">&#123;<span class="number">100</span> * tmp_acc&#125;</span>%"</span>)</span><br><span class="line">print(<span class="string">f"Weighted number of correct predictions <span class="subst">&#123;tmp_num_correct&#125;</span>; weighted number of total observations predicted <span class="subst">&#123;tmp_num_predictions&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Model&#39;s prediction accuracy on a single training batch is: 100.0%Weighted number of correct predictions 64.0; weighted number of total observations predicted 64</code></pre><h5 id="Expected-output-Approximately-1"><a href="#Expected-output-Approximately-1" class="headerlink" title="Expected output (Approximately)"></a>Expected output (Approximately)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Model&apos;s prediction accuracy on a single training batch is: 100.0%</span><br><span class="line">Weighted number of correct predictions 64.0; weighted number of total observations predicted 64</span><br></pre></td></tr></table></figure><p><a name="5.2"></a></p><h2 id="5-2-Testing-your-model-on-Validation-Data"><a href="#5-2-Testing-your-model-on-Validation-Data" class="headerlink" title="5.2  Testing your model on Validation Data"></a>5.2  Testing your model on Validation Data</h2><p>Now you will write test your model’s prediction accuracy on validation data. </p><p>This program will take in a data generator and your model. </p><ul><li>The generator allows you to get batches of data. You can use it with a <code>for</code> loop:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for batch in iterator: </span><br><span class="line">   # do something with that batch</span><br></pre></td></tr></table></figure><p><code>batch</code> has dimensions <code>(X, Y, weights)</code>. </p><ul><li>Column 0 corresponds to the tweet as a tensor (input).</li><li>Column 1 corresponds to its target (actual label, positive or negative sentiment).</li><li>Column 2 corresponds to the weights associated (example weights)</li><li>You can feed the tweet into model and it will return the predictions for the batch. </li></ul><p><a name="ex08"></a></p><h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p><strong>Instructions:</strong> </p><ul><li>Compute the accuracy over all the batches in the validation iterator. </li><li>Make use of <code>compute_accuracy</code>, which you recently implemented, and return the overall accuracy.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: test_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_model</span><span class="params">(generator, model)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        generator: an iterator instance that provides batches of inputs and targets</span></span><br><span class="line"><span class="string">        model: a model instance </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: float corresponding to the accuracy</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    accuracy = <span class="number">0.</span></span><br><span class="line">    total_num_correct = <span class="number">0</span></span><br><span class="line">    total_num_pred = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> generator: </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the inputs from the batch</span></span><br><span class="line">        inputs = batch[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the targets (actual labels) from the batch</span></span><br><span class="line">        targets = batch[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the example weight.</span></span><br><span class="line">        example_weight = batch[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Make predictions using the inputs</span></span><br><span class="line">        pred = model(inputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate accuracy for the batch by comparing its predictions and targets</span></span><br><span class="line">        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the total number of correct predictions</span></span><br><span class="line">        <span class="comment"># by adding the number of correct predictions from this batch</span></span><br><span class="line">        total_num_correct += batch_num_correct</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the total number of predictions </span></span><br><span class="line">        <span class="comment"># by adding the number of predictions made for the batch</span></span><br><span class="line">        total_num_pred +=batch_num_pred</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate accuracy over all examples</span></span><br><span class="line">    accuracy = total_num_correct / total_num_pred</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DO NOT EDIT THIS CELL</span></span><br><span class="line"><span class="comment"># testing the accuracy of your model: this takes around 20 seconds</span></span><br><span class="line">model = training_loop.eval_model</span><br><span class="line">accuracy = test_model(test_generator(<span class="number">16</span>), model)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The accuracy of your model on the validation set is <span class="subst">&#123;accuracy:<span class="number">.4</span>f&#125;</span>'</span>, )</span><br></pre></td></tr></table></figure><pre><code>The accuracy of your model on the validation set is 0.9931</code></pre><h5 id="Expected-Output-Approximately"><a href="#Expected-Output-Approximately" class="headerlink" title="Expected Output (Approximately)"></a>Expected Output (Approximately)</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The accuracy of your model on the validation <span class="built_in">set</span> is <span class="number">0.9931</span></span><br></pre></td></tr></table></figure><p><a name="6"></a></p><h1 id="Part-6-Testing-with-your-own-input"><a href="#Part-6-Testing-with-your-own-input" class="headerlink" title="Part 6:  Testing with your own input"></a>Part 6:  Testing with your own input</h1><p>Finally you will test with your own input. You will see that deepnets are more powerful than the older methods you have used before. Although you go close to 100% accuracy on the first two assignments, the task was way easier. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is used to predict on your own sentnece</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Batch size 1, add dimension for batch, to work with the model</span></span><br><span class="line">    inputs = inputs[<span class="keyword">None</span>, :]  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># predict with the model</span></span><br><span class="line">    preds_probs = model(inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Turn probabilities into categories</span></span><br><span class="line">    preds = int(preds_probs[<span class="number">0</span>, <span class="number">1</span>] &gt; preds_probs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    sentiment = <span class="string">"negative"</span></span><br><span class="line">    <span class="keyword">if</span> preds == <span class="number">1</span>:</span><br><span class="line">        sentiment = <span class="string">'positive'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> preds, sentiment</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># try a positive sentence</span></span><br><span class="line">sentence = <span class="string">"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe"</span></span><br><span class="line">tmp_pred, tmp_sentiment = predict(sentence)</span><br><span class="line">print(<span class="string">f"The sentiment of the sentence \n***\n\"<span class="subst">&#123;sentence&#125;</span>\"\n***\nis <span class="subst">&#123;tmp_sentiment&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">print()</span><br><span class="line"><span class="comment"># try a negative sentence</span></span><br><span class="line">sentence = <span class="string">"I hated my day, it was the worst, I'm so sad."</span></span><br><span class="line">tmp_pred, tmp_sentiment = predict(sentence)</span><br><span class="line">print(<span class="string">f"The sentiment of the sentence \n***\n\"<span class="subst">&#123;sentence&#125;</span>\"\n***\nis <span class="subst">&#123;tmp_sentiment&#125;</span>."</span>)</span><br></pre></td></tr></table></figure><pre><code>The sentiment of the sentence ***&quot;It&#39;s such a nice day, think i&#39;ll be taking Sid to Ramsgate fish and chips for lunch at Peter&#39;s fish factory and then the beach maybe&quot;***is positive.The sentiment of the sentence ***&quot;I hated my day, it was the worst, I&#39;m so sad.&quot;***is negative.</code></pre><p>Notice that the model works well even for complex sentences.</p><h3 id="On-Deep-Nets"><a href="#On-Deep-Nets" class="headerlink" title="On Deep Nets"></a>On Deep Nets</h3><p>Deep nets allow you to understand and capture dependencies that you would have not been able to capture with a simple linear regression, or logistic regression. </p><ul><li>It also allows you to better use pre-trained embeddings for classification and tends to generalize better.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-1-Sentiment-with-Deep-Neural-Networks&quot;&gt;&lt;a href=&quot;#Assignment-1-Sentiment-with-Deep-Neural-Networks&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Create a Siamese Network with Triplet Loss in Keras</title>
    <link href="https://zhangruochi.com/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/"/>
    <id>https://zhangruochi.com/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/</id>
    <published>2020-08-11T11:04:16.000Z</published>
    <updated>2020-08-12T02:56:29.882Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Create-a-Siamese-Network-with-Triplet-Loss-in-Keras"><a href="#Create-a-Siamese-Network-with-Triplet-Loss-in-Keras" class="headerlink" title="Create a Siamese Network with Triplet Loss in Keras"></a>Create a Siamese Network with Triplet Loss in Keras</h1><h1 id="Task-1-Understanding-the-Approach"><a href="#Task-1-Understanding-the-Approach" class="headerlink" title="Task 1: Understanding the Approach"></a>Task 1: Understanding the Approach</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pca_plotter <span class="keyword">import</span> PCAPlotter</span><br><span class="line"></span><br><span class="line">print(<span class="string">'TensorFlow version:'</span>, tf.__version__)</span><br></pre></td></tr></table></figure><pre><code>TensorFlow version: 2.1.0</code></pre><h2 id="Understanding-the-Approach"><a href="#Understanding-the-Approach" class="headerlink" title="Understanding the Approach"></a>Understanding the Approach</h2><p>This appraoch is taken from the popular <a href="https://arxiv.org/abs/1503.03832" target="_blank" rel="noopener">FaceNet</a> paper.</p><p>We have a CNN model called <code>EmbeddingModel</code>:</p><p><img src="CNN.png" alt="CNN"></p><p>We use three images for each training example:</p><ol><li><code>person1_image1.jpg</code> (Anchor Example, represented below in green)</li><li><code>person1_image2.jpg</code> (Positive Example, in blue)</li><li><code>person2_image1.jpg</code> (Negative Example, in red).</li></ol><p><img src="embeddings.png" alt="Embeddings"></p><h2 id="Siamese-Network"><a href="#Siamese-Network" class="headerlink" title="Siamese Network"></a>Siamese Network</h2><p>All the three images of an example pass through the model, and we get the three Embeddings: One for the Anchor Example, one for the Positive Example, and one for the Negative Example.</p><p><img src="siamese.png" alt="Siamese Network"></p><p>The three instances of the <code>EmbeddingModel</code> shown above are not different instances. It’s the same, shared model instance - i.e. the parameters are shared, and are updated for all the three paths simultaneously.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCAPlotter</span><span class="params">(tf.keras.callbacks.Callback)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, plt, embedding_model, x_test, y_test)</span>:</span></span><br><span class="line">        super(PCAPlotter, self).__init__()</span><br><span class="line">        self.embedding_model = embedding_model</span><br><span class="line">        self.x_test = x_test</span><br><span class="line">        self.y_test = y_test</span><br><span class="line">        self.fig = plt.figure(figsize=(<span class="number">9</span>, <span class="number">4</span>))</span><br><span class="line">        self.ax1 = plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.ax2 = plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        plt.ion()</span><br><span class="line">        </span><br><span class="line">        self.losses = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(self, epoch=None, plot_loss=False)</span>:</span></span><br><span class="line">        x_test_embeddings = self.embedding_model.predict(self.x_test)</span><br><span class="line">        pca_out = PCA(n_components=<span class="number">2</span>).fit_transform(x_test_embeddings)</span><br><span class="line">        self.ax1.clear()</span><br><span class="line">        self.ax1.scatter(pca_out[:, <span class="number">0</span>], pca_out[:, <span class="number">1</span>], c=self.y_test, cmap=<span class="string">'seismic'</span>)</span><br><span class="line">        <span class="keyword">if</span> plot_loss:</span><br><span class="line">            self.ax2.clear()</span><br><span class="line">            self.ax2.plot(range(epoch), self.losses)</span><br><span class="line">            self.ax2.set_xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">            self.ax2.set_ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">        self.fig.canvas.draw()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span><span class="params">(self, logs=None)</span>:</span></span><br><span class="line">        self.losses = []</span><br><span class="line">        self.fig.show()</span><br><span class="line">        self.fig.canvas.draw()</span><br><span class="line">        self.plot()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        self.losses.append(logs.get(<span class="string">'loss'</span>))</span><br><span class="line">        self.plot(epoch+<span class="number">1</span>, plot_loss=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h1 id="Task-2-Importing-the-Data"><a href="#Task-2-Importing-the-Data" class="headerlink" title="Task 2: Importing the Data"></a>Task 2: Importing the Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure><pre><code>(60000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train = np.reshape(x_train, (x_train.shape[<span class="number">0</span>], <span class="number">784</span>))/<span class="number">255.</span></span><br><span class="line">x_test = np.reshape(x_test, (x_test.shape[<span class="number">0</span>], <span class="number">784</span>))/<span class="number">255.</span></span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure><pre><code>(60000, 784)</code></pre><h1 id="Task-3-Plotting-Examples"><a href="#Task-3-Plotting-Examples" class="headerlink" title="Task 3: Plotting Examples"></a>Task 3: Plotting Examples</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_triplets</span><span class="params">(examples)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">6</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span> + i)</span><br><span class="line">        plt.imshow(np.reshape(examples[i], (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">'binary'</span>)</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_triplets([x_train[<span class="number">0</span>], x_train[<span class="number">1</span>], x_train[<span class="number">2</span>]])</span><br></pre></td></tr></table></figure><p><img src="plot1.png" alt="plot1"></p><h1 id="Task-4-A-Batch-of-Triplets"><a href="#Task-4-A-Batch-of-Triplets" class="headerlink" title="Task 4: A Batch of Triplets"></a>Task 4: A Batch of Triplets</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_batch</span><span class="params">(batch_size=<span class="number">256</span>)</span>:</span></span><br><span class="line">    x_anchors = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    x_positives = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    x_negatives = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, batch_size):</span><br><span class="line">        <span class="comment"># We need to find an anchor, a positive example and a negative example</span></span><br><span class="line">        random_index = random.randint(<span class="number">0</span>, x_train.shape[<span class="number">0</span>] - <span class="number">1</span>)</span><br><span class="line">        x_anchor = x_train[random_index]</span><br><span class="line">        y = y_train[random_index]</span><br><span class="line">        </span><br><span class="line">        indices_for_pos = np.squeeze(np.where(y_train == y))</span><br><span class="line">        indices_for_neg = np.squeeze(np.where(y_train != y))</span><br><span class="line">        </span><br><span class="line">        x_positive = x_train[indices_for_pos[random.randint(<span class="number">0</span>, len(indices_for_pos) - <span class="number">1</span>)]]</span><br><span class="line">        x_negative = x_train[indices_for_neg[random.randint(<span class="number">0</span>, len(indices_for_neg) - <span class="number">1</span>)]]</span><br><span class="line">        </span><br><span class="line">        x_anchors[i] = x_anchor</span><br><span class="line">        x_positives[i] = x_positive</span><br><span class="line">        x_negatives[i] = x_negative</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> [x_anchors, x_positives, x_negatives]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">examples = create_batch(<span class="number">1</span>)</span><br><span class="line">plot_triplets(examples)</span><br></pre></td></tr></table></figure><p><img src="plot2.png" alt="plot2"></p><h1 id="Task-5-Embedding-Model"><a href="#Task-5-Embedding-Model" class="headerlink" title="Task 5: Embedding Model"></a>Task 5: Embedding Model</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">emb_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">embedding_model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">784</span>,)),</span><br><span class="line">    tf.keras.layers.Dense(emb_size, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">embedding_model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense (Dense)                (None, 64)                50240     _________________________________________________________________dense_1 (Dense)              (None, 64)                4160      =================================================================Total params: 54,400Trainable params: 54,400Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">example = np.expand_dims(x_train[<span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">example_emb = embedding_model.predict(example)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">print(example_emb)</span><br></pre></td></tr></table></figure><pre><code>[0.42349347 0.43482512 0.5846526  0.5047948  0.4264534  0.48105526 0.37568194 0.5898737  0.61923265 0.38126072 0.51810735 0.6918024 0.42151055 0.31393877 0.550636   0.4718757  0.72107047 0.5304595 0.60560906 0.54731256 0.47088197 0.57321566 0.38795182 0.3528969 0.5260858  0.5058847  0.60069776 0.5351782  0.45879558 0.49318898 0.52481294 0.48127335 0.41399142 0.53644794 0.596148   0.35952103 0.4660656  0.51290053 0.34802675 0.28829136 0.49941048 0.41946915 0.5193161  0.59598917 0.42652634 0.7554737  0.51301926 0.3393702 0.61319596 0.3912717  0.58737236 0.5881264  0.5892425  0.62002826 0.47996673 0.44889334 0.47385594 0.4038328  0.60131633 0.57539546 0.47411144 0.5514124  0.6192302  0.60763264]</code></pre><h1 id="Task-6-Siamese-Network"><a href="#Task-6-Siamese-Network" class="headerlink" title="Task 6: Siamese Network"></a>Task 6: Siamese Network</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input_anchor = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">input_positive = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">input_negative = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line"></span><br><span class="line">embedding_anchor = embedding_model(input_anchor)</span><br><span class="line">embedding_positive = embedding_model(input_positive)</span><br><span class="line">embedding_negative = embedding_model(input_negative)</span><br><span class="line"></span><br><span class="line">output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)</span><br><span class="line">net.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            [(None, 784)]        0                                            __________________________________________________________________________________________________input_2 (InputLayer)            [(None, 784)]        0                                            __________________________________________________________________________________________________input_3 (InputLayer)            [(None, 784)]        0                                            __________________________________________________________________________________________________sequential (Sequential)         (None, 64)           54400       input_1[0][0]                                                                                     input_2[0][0]                                                                                     input_3[0][0]                    __________________________________________________________________________________________________concatenate (Concatenate)       (None, 192)          0           sequential[1][0]                                                                                  sequential[2][0]                                                                                  sequential[3][0]                 ==================================================================================================Total params: 54,400Trainable params: 54,400Non-trainable params: 0__________________________________________________________________________________________________</code></pre><h1 id="Task-7-Triplet-Loss"><a href="#Task-7-Triplet-Loss" class="headerlink" title="Task 7: Triplet Loss"></a>Task 7: Triplet Loss</h1><p>A loss function that tries to pull the Embeddings of Anchor and Positive Examples closer, and tries to push the Embeddings of Anchor and Negative Examples away from each other.</p><p>Root mean square difference between Anchor and Positive examples in a batch of N images is:<br>$<br>\begin{equation}<br>d_p = \sqrt{\frac{\sum_{i=0}^{N-1}(f(a_i) - f(p_i))^2}{N}}<br>\end{equation}<br>$</p><p>Root mean square difference between Anchor and Negative examples in a batch of N images is:<br>$<br>\begin{equation}<br>d_n = \sqrt{\frac{\sum_{i=0}^{N-1}(f(a_i) - f(n_i))^2}{N}}<br>\end{equation}<br>$</p><p>For each example, we want:<br>$<br>\begin{equation}<br>d_p \leq d_n<br>\end{equation}<br>$</p><p>Therefore,<br>$<br>\begin{equation}<br>d_p - d_n \leq 0<br>\end{equation}<br>$</p><p>This condition is quite easily satisfied during the training.</p><p>We will make it non-trivial by adding a margin (alpha):<br>$<br>\begin{equation}<br>d_p - d_n + \alpha \leq 0<br>\end{equation}<br>$</p><p>Given the condition above, the Triplet Loss L is defined as:<br>$<br>\begin{equation}<br>L = max(d_p - d_n + \alpha, 0)<br>\end{equation}<br>$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:<span class="number">2</span>*emb_size], y_pred[:,<span class="number">2</span>*emb_size:]</span><br><span class="line">    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=<span class="number">1</span>)</span><br><span class="line">    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.maximum(positive_dist - negative_dist + alpha, <span class="number">0.</span>)</span><br></pre></td></tr></table></figure><h1 id="Task-8-Data-Generator"><a href="#Task-8-Data-Generator" class="headerlink" title="Task 8: Data Generator"></a>Task 8: Data Generator</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span><span class="params">(batch_size=<span class="number">256</span>)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        x = create_batch(batch_size)</span><br><span class="line">        y = np.zeros((batch_size, <span class="number">3</span>*emb_size))</span><br><span class="line">        <span class="keyword">yield</span> x, y</span><br></pre></td></tr></table></figure><h1 id="Task-9-Model-Training"><a href="#Task-9-Model-Training" class="headerlink" title="Task 9: Model Training"></a>Task 9: Model Training</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">2048</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">steps_per_epoch = int(x_train.shape[<span class="number">0</span>]/batch_size)</span><br><span class="line"></span><br><span class="line">net.compile(loss=triplet_loss, optimizer=<span class="string">'adam'</span>)</span><br><span class="line"></span><br><span class="line">_ = net.fit(</span><br><span class="line">    data_generator(batch_size),</span><br><span class="line">    steps_per_epoch=steps_per_epoch,</span><br><span class="line">    epochs=epochs, verbose=<span class="keyword">False</span>,</span><br><span class="line">    callbacks=[</span><br><span class="line">        PCAPlotter(</span><br><span class="line">            plt, embedding_model,</span><br><span class="line">            x_test[:<span class="number">1000</span>], y_test[:<span class="number">1000</span>]</span><br><span class="line">        )]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><img src="plot3.png" alt="plot3"></p><pre><code>WARNING:tensorflow:sample_weight modes were coerced from  ...    to    [&#39;...&#39;]</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Create-a-Siamese-Network-with-Triplet-Loss-in-Keras&quot;&gt;&lt;a href=&quot;#Create-a-Siamese-Network-with-Triplet-Loss-in-Keras&quot; class=&quot;headerlin
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
  </entry>
  
  <entry>
    <title>Image Super Resolution</title>
    <link href="https://zhangruochi.com/Image-Super-Resolution/2020/08/11/"/>
    <id>https://zhangruochi.com/Image-Super-Resolution/2020/08/11/</id>
    <published>2020-08-11T10:34:27.000Z</published>
    <updated>2020-08-12T00:12:40.337Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.rhyme.com" target="_blank" rel="noopener"> <img src="https://www.rhyme.com/assets/img/logo-dark.png" alt="Header" style="width: 150px;"> </a></p><h1 align="center"> Image Super Resolution using Autoencoders</h1><!-- <img src="images/high_res_v_low_res.jpg" width=550px> --><h2 id="Task-1-Project-Overview-and-Import-Libraries"><a href="#Task-1-Project-Overview-and-Import-Libraries" class="headerlink" title="Task 1: Project Overview and Import Libraries"></a>Task 1: Project Overview and Import Libraries</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br></pre></td></tr></table></figure><h2 id="Task-2-Build-the-Encoder"><a href="#Task-2-Build-the-Encoder" class="headerlink" title="Task 2: Build the Encoder"></a>Task 2: Build the Encoder</h2><!-- <img src="images/autoencoder.jpg"> --><p>Credit: Autoencoder Schema by <a href="https://blog.keras.io/img/ae/autoencoder_schema.jpg" target="_blank" rel="noopener">Francois Chollet, 2016</a>.</p><!-- <h4 align=center>Encoder Architecture</h4><img src="images/encoder.png" width=450px align=center> --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">input_img = Input(shape=(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">l1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(input_img)</span><br><span class="line">l2 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l1)</span><br><span class="line">l3 = MaxPooling2D(padding=<span class="string">'same'</span>)(l2)</span><br><span class="line">l3 = Dropout(<span class="number">0.3</span>)(l3)</span><br><span class="line">l4 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>),  padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l3)</span><br><span class="line">l5 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l4)</span><br><span class="line">l6 = MaxPooling2D(padding=<span class="string">'same'</span>)(l5)</span><br><span class="line">l7 = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l6)</span><br><span class="line">encoder = Model(input_img, l7)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         [(None, 256, 256, 3)]     0         _________________________________________________________________conv2d (Conv2D)              (None, 256, 256, 64)      1792      _________________________________________________________________conv2d_1 (Conv2D)            (None, 256, 256, 64)      36928     _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         _________________________________________________________________dropout (Dropout)            (None, 128, 128, 64)      0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     _________________________________________________________________conv2d_3 (Conv2D)            (None, 128, 128, 128)     147584    _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    =================================================================Total params: 555,328Trainable params: 555,328Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="Task-3-Build-the-Decoder-to-Complete-the-Network"><a href="#Task-3-Build-the-Decoder-to-Complete-the-Network" class="headerlink" title="Task 3: Build the Decoder to Complete the Network"></a>Task 3: Build the Decoder to Complete the Network</h2><!-- <img src="images/decoder.png" width=450px> --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input_img = Input(shape=(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br><span class="line">l1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(input_img)</span><br><span class="line">l2 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l1)</span><br><span class="line"></span><br><span class="line">l3 = MaxPooling2D(padding=<span class="string">'same'</span>)(l2)</span><br><span class="line">l3 = Dropout(<span class="number">0.3</span>)(l3)</span><br><span class="line">l4 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>),  padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l3)</span><br><span class="line">l5 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l4)</span><br><span class="line"></span><br><span class="line">l6 = MaxPooling2D(padding=<span class="string">'same'</span>)(l5)</span><br><span class="line">l7 = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l6)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decoder</span></span><br><span class="line"></span><br><span class="line">l8 = UpSampling2D()(l7)</span><br><span class="line"></span><br><span class="line">l9 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">            activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l8)</span><br><span class="line">l10 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l9)</span><br><span class="line"></span><br><span class="line">l11 = add([l5, l10])</span><br><span class="line">l12 = UpSampling2D()(l11)</span><br><span class="line">l13 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l12)</span><br><span class="line">l14 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l13)</span><br><span class="line"></span><br><span class="line">l15 = add([l14, l2])</span><br><span class="line"></span><br><span class="line"><span class="comment"># chan = 3, for RGB</span></span><br><span class="line">decoded = Conv2D(<span class="number">3</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l15)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create our network</span></span><br><span class="line">autoencoder = Model(input_img, decoded)</span><br><span class="line"><span class="comment"># You'll understand later what this is</span></span><br><span class="line">autoencoder_hfenn = Model(input_img, decoded)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model_1&quot;__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            __________________________________________________________________________________________________conv2d_5 (Conv2D)               (None, 256, 256, 64) 1792        input_2[0][0]                    __________________________________________________________________________________________________conv2d_6 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_5[0][0]                   __________________________________________________________________________________________________max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_6[0][0]                   __________________________________________________________________________________________________dropout_1 (Dropout)             (None, 128, 128, 64) 0           max_pooling2d_2[0][0]            __________________________________________________________________________________________________conv2d_7 (Conv2D)               (None, 128, 128, 128 73856       dropout_1[0][0]                  __________________________________________________________________________________________________conv2d_8 (Conv2D)               (None, 128, 128, 128 147584      conv2d_7[0][0]                   __________________________________________________________________________________________________max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_8[0][0]                   __________________________________________________________________________________________________conv2d_9 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_3[0][0]            __________________________________________________________________________________________________up_sampling2d (UpSampling2D)    (None, 128, 128, 256 0           conv2d_9[0][0]                   __________________________________________________________________________________________________conv2d_10 (Conv2D)              (None, 128, 128, 128 295040      up_sampling2d[0][0]              __________________________________________________________________________________________________conv2d_11 (Conv2D)              (None, 128, 128, 128 147584      conv2d_10[0][0]                  __________________________________________________________________________________________________add (Add)                       (None, 128, 128, 128 0           conv2d_8[0][0]                                                                                    conv2d_11[0][0]                  __________________________________________________________________________________________________up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 128 0           add[0][0]                        __________________________________________________________________________________________________conv2d_12 (Conv2D)              (None, 256, 256, 64) 73792       up_sampling2d_1[0][0]            __________________________________________________________________________________________________conv2d_13 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_12[0][0]                  __________________________________________________________________________________________________add_1 (Add)                     (None, 256, 256, 64) 0           conv2d_13[0][0]                                                                                   conv2d_6[0][0]                   __________________________________________________________________________________________________conv2d_14 (Conv2D)              (None, 256, 256, 3)  1731        add_1[0][0]                      ==================================================================================================Total params: 1,110,403Trainable params: 1,110,403Non-trainable params: 0__________________________________________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.compile(optimizer=<span class="string">'adadelta'</span>, loss=<span class="string">'mean_squared_error'</span>)</span><br></pre></td></tr></table></figure><h2 id="Task-4-Create-Dataset-and-Specify-Training-Routine"><a href="#Task-4-Create-Dataset-and-Specify-Training-Routine" class="headerlink" title="Task 4: Create Dataset and Specify Training Routine"></a>Task 4: Create Dataset and Specify Training Routine</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage, misc</span><br><span class="line"><span class="keyword">from</span> skimage.transform <span class="keyword">import</span> resize, rescale</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_batches</span><span class="params">(just_load_dataset=False)</span>:</span></span><br><span class="line"></span><br><span class="line">    batches = <span class="number">256</span> <span class="comment"># Number of images to have at the same time in a batch</span></span><br><span class="line"></span><br><span class="line">    batch = <span class="number">0</span> <span class="comment"># Number if images in the current batch (grows over time and then resets for each batch)</span></span><br><span class="line">    batch_nb = <span class="number">0</span> <span class="comment"># Batch current index</span></span><br><span class="line"></span><br><span class="line">    max_batches = <span class="number">-1</span> <span class="comment"># If you want to train only on a limited number of images to finish the training even faster.</span></span><br><span class="line">    </span><br><span class="line">    ep = <span class="number">4</span> <span class="comment"># Number of epochs</span></span><br><span class="line"></span><br><span class="line">    images = []</span><br><span class="line">    x_train_n = []</span><br><span class="line">    x_train_down = []</span><br><span class="line">    </span><br><span class="line">    x_train_n2 = [] <span class="comment"># Resulting high res dataset</span></span><br><span class="line">    x_train_down2 = [] <span class="comment"># Resulting low res dataset</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> root, dirnames, filenames <span class="keyword">in</span> os.walk(<span class="string">"/home/rhyme/Desktop/Project/data/cars_train"</span>):</span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">            <span class="keyword">if</span> re.search(<span class="string">"\.(jpg|jpeg|JPEG|png|bmp|tiff)$"</span>, filename):</span><br><span class="line">                <span class="keyword">if</span> batch_nb == max_batches: <span class="comment"># If we limit the number of batches, just return earlier</span></span><br><span class="line">                    <span class="keyword">return</span> x_train_n2, x_train_down2</span><br><span class="line">                filepath = os.path.join(root, filename)</span><br><span class="line">                image = pyplot.imread(filepath)</span><br><span class="line">                <span class="keyword">if</span> len(image.shape) &gt; <span class="number">2</span>:</span><br><span class="line">                        </span><br><span class="line">                    image_resized = resize(image, (<span class="number">256</span>, <span class="number">256</span>)) <span class="comment"># Resize the image so that every image is the same size</span></span><br><span class="line">                    x_train_n.append(image_resized) <span class="comment"># Add this image to the high res dataset</span></span><br><span class="line">                    x_train_down.append(rescale(rescale(image_resized, <span class="number">0.5</span>), <span class="number">2.0</span>)) <span class="comment"># Rescale it 0.5x and 2x so that it is a low res image but still has 256x256 resolution</span></span><br><span class="line">                    batch += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> batch == batches:</span><br><span class="line">                        batch_nb += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                        x_train_n2 = np.array(x_train_n)</span><br><span class="line">                        x_train_down2 = np.array(x_train_down)</span><br><span class="line">                        </span><br><span class="line">                        <span class="keyword">if</span> just_load_dataset:</span><br><span class="line">                            <span class="keyword">return</span> x_train_n2, x_train_down2</span><br><span class="line">                        </span><br><span class="line">                        print(<span class="string">'Training batch'</span>, batch_nb, <span class="string">'('</span>, batches, <span class="string">')'</span>)</span><br><span class="line"></span><br><span class="line">                        autoencoder.fit(x_train_down2, x_train_n2,</span><br><span class="line">                            epochs=ep,</span><br><span class="line">                            batch_size=<span class="number">10</span>,</span><br><span class="line">                            shuffle=<span class="keyword">True</span>,</span><br><span class="line">                            validation_split=<span class="number">0.15</span>)</span><br><span class="line">                    </span><br><span class="line">                        x_train_n = []</span><br><span class="line">                        x_train_down = []</span><br><span class="line">                    </span><br><span class="line">                        batch = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train_n2, x_train_down2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Task-5-Load-the-Dataset-and-Pre-trained-Model"><a href="#Task-5-Load-the-Dataset-and-Pre-trained-Model" class="headerlink" title="Task 5: Load the Dataset and Pre-trained Model"></a>Task 5: Load the Dataset and Pre-trained Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train_n, x_train_down = train_batches(just_load_dataset=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &#39;constant&#39;, will be changed to &#39;reflect&#39; in skimage 0.15.  warn(&quot;The default mode, &#39;constant&#39;, will be changed to &#39;reflect&#39; in &quot;/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:24: UserWarning: The default multichannel argument (None) is deprecated.  Please specify either True or False explicitly.  multichannel will default to False starting with release 0.16.  warn(&#39;The default multichannel argument (None) is deprecated.  Please &#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.load_weights(<span class="string">"/home/rhyme/Desktop/Project/data/sr.img_net.mse.final_model5.no_patch.weights.best.hdf5"</span>)</span><br></pre></td></tr></table></figure><h3 id="Task-6-Model-Predictions-and-Visualizing-the-Results"><a href="#Task-6-Model-Predictions-and-Visualizing-the-Results" class="headerlink" title="Task 6: Model Predictions and Visualizing the Results"></a>Task 6: Model Predictions and Visualizing the Results</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder.load_weights(<span class="string">'/home/rhyme/Desktop/Project/data/encoder_weights.hdf5'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoded_imgs = encoder.predict(x_train_down)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoded_imgs.shape</span><br></pre></td></tr></table></figure><pre><code>(256, 64, 64, 256)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We clip the output so that it doesn't produce weird colors</span></span><br><span class="line">sr1 = np.clip(autoencoder.predict(x_train_down), <span class="number">0.0</span>, <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image_index = <span class="number">251</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">i = <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_down[image_index])</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_down[image_index], interpolation=<span class="string">"bicubic"</span>)</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(encoded_imgs[image_index].reshape((<span class="number">64</span>*<span class="number">64</span>, <span class="number">256</span>)))</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(sr1[image_index])</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_n[image_index])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_28_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.rhyme.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; &lt;img src=&quot;https://www.rhyme.com/assets/img/logo-dark.png&quot; alt=&quot;Header&quot; st
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Deepfakes with Keras</title>
    <link href="https://zhangruochi.com/Understanding-Deepfakes-with-Keras/2020/07/30/"/>
    <id>https://zhangruochi.com/Understanding-Deepfakes-with-Keras/2020/07/30/</id>
    <published>2020-07-30T07:00:32.000Z</published>
    <updated>2020-07-30T07:03:40.057Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Understanding-Deepfakes-with-Keras"><a href="#Understanding-Deepfakes-with-Keras" class="headerlink" title="Understanding Deepfakes with Keras"></a>Understanding Deepfakes with Keras</h1><p><img src="DCGAN.png" alt="DCGAN"></p><h1 id="Task-1-Importing-Libraries-and-Helper-Functions"><a href="#Task-1-Importing-Libraries-and-Helper-Functions" class="headerlink" title="Task 1: Importing Libraries and Helper Functions"></a>Task 1: Importing Libraries and Helper Functions</h1><p>Please note: If you haven’t already, please install the required packages by executing the code cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip3 install tensorflow==2.1.0 pillow matplotlib</span></span><br><span class="line"><span class="comment"># !pip3 install git+https://github.com/am1tyadav/tfutils.git</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tfutils</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Flatten, Conv2D, BatchNormalization</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Conv2DTranspose, Reshape, LeakyReLU</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">print(<span class="string">'TensorFlow version:'</span>, tf.__version__)</span><br></pre></td></tr></table></figure><pre><code>TensorFlow version: 2.1.0</code></pre><h1 id="Task-2-Importing-and-Plotting-the-Data"><a href="#Task-2-Importing-and-Plotting-the-Data" class="headerlink" title="Task 2: Importing and Plotting the Data"></a>Task 2: Importing and Plotting the Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = tfutils.datasets.mnist.load_data(one_hot=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">x_train = tfutils.datasets.mnist.load_subset([<span class="number">0</span>], x_train, y_train)</span><br><span class="line">x_test = tfutils.datasets.mnist.load_subset([<span class="number">0</span>], x_test, y_test)</span><br><span class="line"></span><br><span class="line">x = np.concatenate([x_train, x_test], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><pre><code>(6903, 784)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tfutils.datasets.mnist.plot_ten_random_examples(plt, x, np.zeros((x.shape[<span class="number">0</span>], <span class="number">1</span>))).show()</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,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" width="1000"></p><h1 id="Task-3-Discriminator"><a href="#Task-3-Discriminator" class="headerlink" title="Task 3: Discriminator"></a>Task 3: Discriminator</h1><p><img src="artist_critic.png" alt="Artist and Critic"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">size = <span class="number">28</span></span><br><span class="line">noise_dim = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">discriminator = Sequential([</span><br><span class="line">    Conv2D(<span class="number">64</span>, <span class="number">3</span>, strides=<span class="number">2</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">128</span>, <span class="number">5</span>, strides=<span class="number">2</span>),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">256</span>, <span class="number">5</span>, strides=<span class="number">2</span>),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Flatten(),</span><br><span class="line">    Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">opt = tf.keras.optimizers.Adam(lr=<span class="number">2e-4</span>, beta_1=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">discriminator.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=opt, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">discriminator.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d (Conv2D)              (None, 13, 13, 64)        640       _________________________________________________________________leaky_re_lu (LeakyReLU)      (None, 13, 13, 64)        0         _________________________________________________________________batch_normalization (BatchNo (None, 13, 13, 64)        256       _________________________________________________________________conv2d_1 (Conv2D)            (None, 5, 5, 128)         204928    _________________________________________________________________leaky_re_lu_1 (LeakyReLU)    (None, 5, 5, 128)         0         _________________________________________________________________batch_normalization_1 (Batch (None, 5, 5, 128)         512       _________________________________________________________________conv2d_2 (Conv2D)            (None, 1, 1, 256)         819456    _________________________________________________________________leaky_re_lu_2 (LeakyReLU)    (None, 1, 1, 256)         0         _________________________________________________________________batch_normalization_2 (Batch (None, 1, 1, 256)         1024      _________________________________________________________________flatten (Flatten)            (None, 256)               0         _________________________________________________________________dense (Dense)                (None, 1)                 257       =================================================================Total params: 1,027,073Trainable params: 1,026,177Non-trainable params: 896_________________________________________________________________</code></pre><h1 id="Task-4-Generator"><a href="#Task-4-Generator" class="headerlink" title="Task 4: Generator"></a>Task 4: Generator</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">generator = Sequential([</span><br><span class="line">    Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>, input_shape=(noise_dim,)),</span><br><span class="line">    Reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>)),</span><br><span class="line">    </span><br><span class="line">    Conv2DTranspose(<span class="number">256</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    Conv2DTranspose(<span class="number">128</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2DTranspose(<span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    Conv2DTranspose(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2DTranspose(<span class="number">1</span>, <span class="number">4</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line"></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">generator.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential_1&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_1 (Dense)              (None, 256)               512       _________________________________________________________________reshape (Reshape)            (None, 1, 1, 256)         0         _________________________________________________________________conv2d_transpose (Conv2DTran (None, 5, 5, 256)         1638656   _________________________________________________________________batch_normalization_3 (Batch (None, 5, 5, 256)         1024      _________________________________________________________________conv2d_transpose_1 (Conv2DTr (None, 9, 9, 128)         819328    _________________________________________________________________batch_normalization_4 (Batch (None, 9, 9, 128)         512       _________________________________________________________________conv2d_transpose_2 (Conv2DTr (None, 21, 21, 64)        204864    _________________________________________________________________batch_normalization_5 (Batch (None, 21, 21, 64)        256       _________________________________________________________________conv2d_transpose_3 (Conv2DTr (None, 25, 25, 32)        51232     _________________________________________________________________batch_normalization_6 (Batch (None, 25, 25, 32)        128       _________________________________________________________________conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         513       =================================================================Total params: 2,717,025Trainable params: 2,716,065Non-trainable params: 960_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">noise = np.random.randn(<span class="number">1</span>, noise_dim)</span><br><span class="line">gen_image = generator.predict(noise)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(np.reshape(gen_image, (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">'binary'</span>);</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,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" width="640"></p><h1 id="Task-5-Generative-Adversarial-Network-GAN"><a href="#Task-5-Generative-Adversarial-Network-GAN" class="headerlink" title="Task 5: Generative Adversarial Network (GAN)"></a>Task 5: Generative Adversarial Network (GAN)</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input_layer = tf.keras.layers.Input(shape=(noise_dim,))</span><br><span class="line">gen_out = generator(input_layer)</span><br><span class="line">disc_out = discriminator(gen_out)</span><br><span class="line"></span><br><span class="line">gan = Model(</span><br><span class="line">    input_layer,</span><br><span class="line">    disc_out</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">discriminator.trainable = <span class="keyword">False</span></span><br><span class="line">gan.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=opt, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">gan.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         [(None, 1)]               0         _________________________________________________________________sequential_1 (Sequential)    (None, 28, 28, 1)         2717025   _________________________________________________________________sequential (Sequential)      (None, 1)                 1027073   =================================================================Total params: 3,744,098Trainable params: 2,716,065Non-trainable params: 1,028,033_________________________________________________________________</code></pre><h1 id="Tasks-6-and-7-Training-the-GAN"><a href="#Tasks-6-and-7-Training-the-GAN" class="headerlink" title="Tasks 6 and 7: Training the GAN"></a>Tasks 6 and 7: Training the GAN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">steps_per_epoch = int(<span class="number">2</span> * x.shape[<span class="number">0</span>]/batch_size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Steps per epoch='</span>, steps_per_epoch)</span><br><span class="line"></span><br><span class="line">dp = tfutils.plotting.DynamicPlot(plt, <span class="number">5</span>, <span class="number">5</span>, (<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">0</span>, epochs):</span><br><span class="line">    </span><br><span class="line">    dp.start_of_epoch(e)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">0</span>, steps_per_epoch):</span><br><span class="line">        true_examples = x[int(batch_size/<span class="number">2</span>)*step: int(batch_size/<span class="number">2</span>)*(step + <span class="number">1</span>)]</span><br><span class="line">        true_examples = np.reshape(true_examples, (true_examples.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        noise = np.random.randn(int(batch_size/<span class="number">2</span>), noise_dim)</span><br><span class="line">        generated_examples = generator.predict(noise)</span><br><span class="line"></span><br><span class="line">        x_batch = np.concatenate([generated_examples, true_examples], axis=<span class="number">0</span>)</span><br><span class="line">        y_batch = np.array([<span class="number">0</span>] * int(batch_size/<span class="number">2</span>) + [<span class="number">1</span>] * int(batch_size/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        indices = np.random.choice(range(batch_size), batch_size, replace=<span class="keyword">False</span>)</span><br><span class="line">        x_batch = x_batch[indices]</span><br><span class="line">        y_batch = y_batch[indices]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train the discriminator</span></span><br><span class="line">        discriminator.trainable = <span class="keyword">True</span></span><br><span class="line">        discriminator.train_on_batch(x_batch, y_batch)</span><br><span class="line">        discriminator.trainable = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># train the generator</span></span><br><span class="line">        loss, _ = gan.train_on_batch(noise, np.ones((int(batch_size/<span class="number">2</span>), <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        _, acc = discriminator.evaluate(x_batch, y_batch, verbose=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    noise = np.random.randn(<span class="number">1</span>, noise_dim)</span><br><span class="line">    generated_example = generator.predict(noise)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    dp.end_of_epoch(np.reshape(generated_example, (<span class="number">28</span>, <span class="number">28</span>)), <span class="string">'binary'</span>,</span><br><span class="line">                   <span class="string">'DiscAcc:&#123;:.2f&#125;'</span>.format(acc), <span class="string">'GANLoss:&#123;:.2f&#125;'</span>.format(loss))</span><br></pre></td></tr></table></figure><pre><code>Steps per epoch= 107&lt;IPython.core.display.Javascript object&gt;</code></pre><p><img src="data:image/png;base64,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" width="800"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Understanding-Deepfakes-with-Keras&quot;&gt;&lt;a href=&quot;#Understanding-Deepfakes-with-Keras&quot; class=&quot;headerlink&quot; title=&quot;Understanding Deepfakes 
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
</feed>
