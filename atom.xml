<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-10-13T01:55:10.267Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Components of StyleGAN </title>
    <link href="https://zhangruochi.com/Components-of-StyleGAN/2020/10/13/"/>
    <id>https://zhangruochi.com/Components-of-StyleGAN/2020/10/13/</id>
    <published>2020-10-13T01:54:33.000Z</published>
    <updated>2020-10-13T01:55:10.267Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Components-of-StyleGAN"><a href="#Components-of-StyleGAN" class="headerlink" title="Components of StyleGAN"></a>Components of StyleGAN</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to implement various components of StyleGAN, including the truncation trick, the mapping layer, noise injection, adaptive instance normalization (AdaIN), and progressive growing. </p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Understand the components of StyleGAN that differ from the traditional GAN.</li><li>Implement the components of StyleGAN.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will begin by importing some packages from PyTorch and defining a visualization function which will be useful later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">16</span>, size=<span class="params">(<span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>)</span>, nrow=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images,</span></span><br><span class="line"><span class="string">    size per image, and images per row, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu().clamp_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=nrow, padding=<span class="number">0</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="Truncation-Trick"><a href="#Truncation-Trick" class="headerlink" title="Truncation Trick"></a>Truncation Trick</h2><p>The first component you will implement is the truncation trick. Remember that this is done after the model is trained and when you are sampling beautiful outputs. The truncation trick resamples the noise vector $z$ from a truncated normal distribution which allows you to tune the generator’s fidelity/diversity. The truncation value is at least 0, where 1 means there is little truncation (high diversity) and 0 means the distribution is all truncated except for the mean (high quality/fidelity). This trick is not exclusive to StyleGAN. In fact, you may recall playing with it in an earlier GAN notebook.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: get_truncated_noise</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> truncnorm</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_truncated_noise</span><span class="params">(n_samples, z_dim, truncation)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating truncated noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    and truncation value, creates a tensor of that shape filled with random</span></span><br><span class="line"><span class="string">    numbers from the truncated normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        truncation: the truncation value, a non-negative scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    truncated_noise = truncnorm.rvs(-truncation, truncation, size=(n_samples, z_dim))</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(truncated_noise)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the truncation sample</span></span><br><span class="line"><span class="keyword">assert</span> tuple(get_truncated_noise(n_samples=<span class="number">10</span>, z_dim=<span class="number">5</span>, truncation=<span class="number">0.7</span>).shape) == (<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">simple_noise = get_truncated_noise(n_samples=<span class="number">1000</span>, z_dim=<span class="number">10</span>, truncation=<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">assert</span> simple_noise.max() &gt; <span class="number">0.199</span> <span class="keyword">and</span> simple_noise.max() &lt; <span class="number">2</span></span><br><span class="line"><span class="keyword">assert</span> simple_noise.min() &lt; <span class="number">-0.199</span> <span class="keyword">and</span> simple_noise.min() &gt; <span class="number">-0.2</span></span><br><span class="line"><span class="keyword">assert</span> simple_noise.std() &gt; <span class="number">0.113</span> <span class="keyword">and</span> simple_noise.std() &lt; <span class="number">0.117</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Mapping-z-→-w"><a href="#Mapping-z-→-w" class="headerlink" title="Mapping $z$ → $w$"></a>Mapping $z$ → $w$</h2><p>The next component you need to implement is the mapping network. It takes the noise vector, $z$, and maps it to an intermediate noise vector, $w$. This makes it so $z$ can be represented in a more disentangled space which makes the features easier to control later.</p><p>The mapping network in StyleGAN is composed of 8 layers, but for your implementation, you will use a neural network with 3 layers. This is to save time training later.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">MappingLayers</font></code></b></font></summary>1.   This code should be five lines.2.   You need 3 linear layers and should use ReLU activations.3.   Your linear layers should be input -> hidden_dim -> hidden_dim -> output.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: MappingLayers</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MappingLayers</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Mapping Layers Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">        w_dim: the dimension of the intermediate noise vector, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim, hidden_dim, w_dim)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.mapping = nn.Sequential(</span><br><span class="line">            <span class="comment"># Please write a neural network which takes in tensors of </span></span><br><span class="line">            <span class="comment"># shape (n_samples, z_dim) and outputs (n_samples, w_dim)</span></span><br><span class="line">            <span class="comment"># with a hidden layer with hidden_dim neurons</span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            nn.Linear(z_dim,hidden_dim,bias=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Linear(hidden_dim,hidden_dim,bias=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Linear(hidden_dim,w_dim,bias=<span class="keyword">True</span>)</span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of MappingLayers: </span></span><br><span class="line"><span class="string">        Given an initial noise tensor, returns the intermediate noise tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.mapping(noise)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_mapping</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.mapping</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the mapping function</span></span><br><span class="line">map_fn = MappingLayers(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>)</span><br><span class="line"><span class="keyword">assert</span> tuple(map_fn(torch.randn(<span class="number">2</span>, <span class="number">10</span>)).shape) == (<span class="number">2</span>, <span class="number">30</span>)</span><br><span class="line"><span class="keyword">assert</span> len(map_fn.mapping) &gt; <span class="number">4</span></span><br><span class="line">outputs = map_fn(torch.randn(<span class="number">1000</span>, <span class="number">10</span>))</span><br><span class="line"><span class="keyword">assert</span> outputs.std() &gt; <span class="number">0.05</span> <span class="keyword">and</span> outputs.std() &lt; <span class="number">0.3</span></span><br><span class="line"><span class="keyword">assert</span> outputs.min() &gt; <span class="number">-2</span> <span class="keyword">and</span> outputs.min() &lt; <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> outputs.max() &lt; <span class="number">2</span> <span class="keyword">and</span> outputs.max() &gt; <span class="number">0</span></span><br><span class="line">layers = [str(x).replace(<span class="string">' '</span>, <span class="string">''</span>).replace(<span class="string">'inplace=True'</span>, <span class="string">''</span>) <span class="keyword">for</span> x <span class="keyword">in</span> map_fn.get_mapping()]</span><br><span class="line"><span class="keyword">assert</span> layers == [<span class="string">'Linear(in_features=10,out_features=20,bias=True)'</span>, </span><br><span class="line">                  <span class="string">'ReLU()'</span>, </span><br><span class="line">                  <span class="string">'Linear(in_features=20,out_features=20,bias=True)'</span>, </span><br><span class="line">                  <span class="string">'ReLU()'</span>, </span><br><span class="line">                  <span class="string">'Linear(in_features=20,out_features=30,bias=True)'</span>]</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Random-Noise-Injection"><a href="#Random-Noise-Injection" class="headerlink" title="Random Noise Injection"></a>Random Noise Injection</h2><p>Next, you will implement the random noise injection that occurs before every AdaIN block. To do this, you need to create a noise tensor that is the same size as the current feature map (image).</p><p>The noise tensor is not entirely random; it is initialized as one random channel that is then multiplied by learned weights for each channel in the image. For example, imagine an image has 512 channels and its height and width are (4 x 4). You would first create a random (4 x 4) noise matrix with one channel. Then, your model would create 512 values—one for each channel. Next, you multiply the (4 x 4) matrix by each one of these values. This creates a “random” tensor of 512 channels and (4 x 4) pixels, the same dimensions as the image. Finally, you add this noise tensor to the image. This introduces uncorrelated noise and is meant to increase the diversity in the image.</p><p>New starting weights are generated for every new layer, or generator, where this class is used. Within a layer, every following time the noise injection is called, you take another step with the optimizer and the weights that you use for each channel are optimized (i.e. learned).</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">InjectNoise</font></code></b></font></summary>1.   The weight should have the shape (1, channels, 1, 1).</details><!-- <details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">InjectNoise</font></code></b></font></summary>1.   Remember that you only make the noise for one channel (it is then multiplied by random values to create ones for the other channels).</details> --><!-- (not sure how??) You'll find the get_noise function from before helpful here --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: InjectNoise</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InjectNoise</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Inject Noise Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        channels: the number of channels the image has, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channels)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = nn.Parameter( <span class="comment"># You use nn.Parameter so that these weights can be optimized</span></span><br><span class="line">            <span class="comment"># Initiate the weights for the channels from a random normal distribution</span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            torch.randn(size = (<span class="number">1</span>, channels, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of InjectNoise: Given an image, </span></span><br><span class="line"><span class="string">        returns the image with random noise added.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: the feature map of shape (n_samples, channels, width, height)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># Set the appropriate shape for the noise!</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        n_samples, channels, width, height = image.shape</span><br><span class="line">        noise_shape = (n_samples, <span class="number">1</span>, width, height)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        noise = torch.randn(noise_shape, device=image.device) <span class="comment"># Creates the random noise</span></span><br><span class="line">        <span class="keyword">return</span> image + self.weight * noise <span class="comment"># Applies to image after multiplying by the weight for each channel</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.weight</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_noise_channels = <span class="number">3000</span></span><br><span class="line">test_noise_samples = <span class="number">20</span></span><br><span class="line">fake_images = torch.randn(test_noise_samples, test_noise_channels, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">inject_noise = InjectNoise(test_noise_channels)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(inject_noise.weight.std() - <span class="number">1</span>) &lt; <span class="number">0.1</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(inject_noise.weight.mean()) &lt; <span class="number">0.1</span></span><br><span class="line"><span class="keyword">assert</span> type(inject_noise.get_weight()) == torch.nn.parameter.Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(inject_noise.weight.shape) == (<span class="number">1</span>, test_noise_channels, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">inject_noise.weight = nn.Parameter(torch.ones_like(inject_noise.weight))</span><br><span class="line"><span class="comment"># Check that something changed</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images)).mean() &gt; <span class="number">0.1</span></span><br><span class="line"><span class="comment"># Check that the change is per-channel</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images).std(<span class="number">0</span>)).mean() &gt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images).std(<span class="number">1</span>)).mean() &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images).std(<span class="number">2</span>)).mean() &gt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images).std(<span class="number">3</span>)).mean() &gt; <span class="number">1e-4</span></span><br><span class="line"><span class="comment"># Check that the per-channel change is roughly normal</span></span><br><span class="line">per_channel_change = (inject_noise(fake_images) - fake_images).mean(<span class="number">1</span>).std()</span><br><span class="line"><span class="keyword">assert</span> per_channel_change &gt; <span class="number">0.9</span> <span class="keyword">and</span> per_channel_change &lt; <span class="number">1.1</span></span><br><span class="line"><span class="comment"># Make sure that the weights are being used at all</span></span><br><span class="line">inject_noise.weight = nn.Parameter(torch.zeros_like(inject_noise.weight))</span><br><span class="line"><span class="keyword">assert</span> torch.abs((inject_noise(fake_images) - fake_images)).mean() &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> len(inject_noise.weight.shape) == <span class="number">4</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Adaptive-Instance-Normalization-AdaIN"><a href="#Adaptive-Instance-Normalization-AdaIN" class="headerlink" title="Adaptive Instance Normalization (AdaIN)"></a>Adaptive Instance Normalization (AdaIN)</h2><p>The next component you will implement is AdaIN. To increase control over the image, you inject $w$ — the intermediate noise vector — multiple times throughout StyleGAN. This is done by transforming it into a set of style parameters and introducing the style to the image through AdaIN. Given an image ($x_i$) and the intermediate vector ($w$), AdaIN takes the instance normalization of the image and multiplies it by the style scale ($y_s$) and adds the style bias ($y_b$). You need to calculate the learnable style scale and bias by using linear mappings from $w$.</p><h1 id="text-AdaIN-boldsymbol-mathrm-x-i-boldsymbol-mathrm-y-boldsymbol-mathrm-y-s-i-frac-boldsymbol-mathrm-x-i-mu-boldsymbol-mathrm-x-i-sigma-boldsymbol-mathrm-x-i-boldsymbol-mathrm-y-b-i"><a href="#text-AdaIN-boldsymbol-mathrm-x-i-boldsymbol-mathrm-y-boldsymbol-mathrm-y-s-i-frac-boldsymbol-mathrm-x-i-mu-boldsymbol-mathrm-x-i-sigma-boldsymbol-mathrm-x-i-boldsymbol-mathrm-y-b-i" class="headerlink" title="$ \text{AdaIN}(\boldsymbol{\mathrm{x}}_i, \boldsymbol{\mathrm{y}}) = \boldsymbol{\mathrm{y}}_{s,i} \frac{\boldsymbol{\mathrm{x}}_i - \mu(\boldsymbol{\mathrm{x}}_i)}{\sigma(\boldsymbol{\mathrm{x}}_i)} + \boldsymbol{\mathrm{y}}_{b,i} $"></a>$ \text{AdaIN}(\boldsymbol{\mathrm{x}}_i, \boldsymbol{\mathrm{y}}) = \boldsymbol{\mathrm{y}}_{s,i} \frac{\boldsymbol{\mathrm{x}}_i - \mu(\boldsymbol{\mathrm{x}}_i)}{\sigma(\boldsymbol{\mathrm{x}}_i)} + \boldsymbol{\mathrm{y}}_{b,i} $</h1><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">forward</font></code></b></font></summary>1.   Remember the equation for AdaIN.2.   The instance normalized image, style scale, and style shift have already been calculated for you.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: AdaIN</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdaIN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    AdaIN Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        channels: the number of channels the image has, a scalar</span></span><br><span class="line"><span class="string">        w_dim: the dimension of the intermediate noise vector, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channels, w_dim)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the input per-dimension</span></span><br><span class="line">        self.instance_norm = nn.InstanceNorm2d(channels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># You want to map w to a set of style weights per channel.</span></span><br><span class="line">        <span class="comment"># Replace the Nones with the correct dimensions - keep in mind that </span></span><br><span class="line">        <span class="comment"># both linear maps transform a w vector into style weights </span></span><br><span class="line">        <span class="comment"># corresponding to the number of image channels.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        self.style_scale_transform = nn.Linear(w_dim, channels)</span><br><span class="line">        self.style_shift_transform = nn.Linear(w_dim, channels)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image, w)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of AdaIN: Given an image and intermediate noise vector w, </span></span><br><span class="line"><span class="string">        returns the normalized image that has been scaled and shifted by the style.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: the feature map of shape (n_samples, channels, width, height)</span></span><br><span class="line"><span class="string">            w: the intermediate noise vector</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        normalized_image = self.instance_norm(image)</span><br><span class="line">        style_scale = self.style_scale_transform(w)[:, :, <span class="keyword">None</span>, <span class="keyword">None</span>]</span><br><span class="line">        style_shift = self.style_shift_transform(w)[:, :, <span class="keyword">None</span>, <span class="keyword">None</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate the transformed image</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        transformed_image = style_scale * normalized_image + style_shift</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        <span class="keyword">return</span> transformed_image</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_style_scale_transform</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.style_scale_transform</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_style_shift_transform</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.style_shift_transform</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">w_channels = <span class="number">50</span></span><br><span class="line">image_channels = <span class="number">20</span></span><br><span class="line">image_size = <span class="number">30</span></span><br><span class="line">n_test = <span class="number">10</span></span><br><span class="line">adain = AdaIN(image_channels, w_channels)</span><br><span class="line">test_w = torch.randn(n_test, w_channels)</span><br><span class="line"><span class="keyword">assert</span> adain.style_scale_transform(test_w).shape == adain.style_shift_transform(test_w).shape</span><br><span class="line"><span class="keyword">assert</span> adain.style_scale_transform(test_w).shape[<span class="number">-1</span>] == image_channels</span><br><span class="line"><span class="keyword">assert</span> tuple(adain(torch.randn(n_test, image_channels, image_size, image_size), test_w).shape) == (n_test, image_channels, image_size, image_size)</span><br><span class="line"></span><br><span class="line">w_channels = <span class="number">3</span></span><br><span class="line">image_channels = <span class="number">2</span></span><br><span class="line">image_size = <span class="number">3</span></span><br><span class="line">n_test = <span class="number">1</span></span><br><span class="line">adain = AdaIN(image_channels, w_channels)</span><br><span class="line"></span><br><span class="line">adain.style_scale_transform.weight.data = torch.ones_like(adain.style_scale_transform.weight.data) / <span class="number">4</span></span><br><span class="line">adain.style_scale_transform.bias.data = torch.zeros_like(adain.style_scale_transform.bias.data)</span><br><span class="line">adain.style_shift_transform.weight.data = torch.ones_like(adain.style_shift_transform.weight.data) / <span class="number">5</span></span><br><span class="line">adain.style_shift_transform.bias.data = torch.zeros_like(adain.style_shift_transform.bias.data)</span><br><span class="line">test_input = torch.ones(n_test, image_channels, image_size, image_size)</span><br><span class="line">test_input[:, :, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">test_w = torch.ones(n_test, w_channels)</span><br><span class="line">test_output = adain(test_input, test_w)</span><br><span class="line"><span class="keyword">assert</span>(torch.abs(test_output[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>] - <span class="number">3</span> / <span class="number">5</span> + torch.sqrt(torch.tensor(<span class="number">9</span> / <span class="number">8</span>))) &lt; <span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">assert</span>(torch.abs(test_output[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>] - <span class="number">3</span> / <span class="number">5</span> - torch.sqrt(torch.tensor(<span class="number">9</span> / <span class="number">32</span>))) &lt; <span class="number">1e-4</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Progressive-Growing-in-StyleGAN"><a href="#Progressive-Growing-in-StyleGAN" class="headerlink" title="Progressive Growing in StyleGAN"></a>Progressive Growing in StyleGAN</h2><p>The final StyleGAN component that you will create is progressive growing. This helps StyleGAN to create high resolution images by gradually doubling the image’s size until the desired size.</p><p>You will start by creating a block for the StyleGAN generator. This is comprised of an upsampling layer, a convolutional layer, random noise injection, an AdaIN layer, and an activation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: MicroStyleGANGeneratorBlock</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MicroStyleGANGeneratorBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Micro StyleGAN Generator Block Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        in_chan: the number of channels in the input, a scalar</span></span><br><span class="line"><span class="string">        out_chan: the number of channels wanted in the output, a scalar</span></span><br><span class="line"><span class="string">        w_dim: the dimension of the intermediate noise vector, a scalar</span></span><br><span class="line"><span class="string">        kernel_size: the size of the convolving kernel</span></span><br><span class="line"><span class="string">        starting_size: the size of the starting image</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_chan, out_chan, w_dim, kernel_size, starting_size, use_upsample=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.use_upsample = use_upsample</span><br><span class="line">        <span class="comment"># Replace the Nones in order to:</span></span><br><span class="line">        <span class="comment"># 1. Upsample to the starting_size, bilinearly (https://pytorch.org/docs/master/generated/torch.nn.Upsample.html)</span></span><br><span class="line">        <span class="comment"># 2. Create a kernel_size convolution which takes in </span></span><br><span class="line">        <span class="comment">#    an image with in_chan and outputs one with out_chan (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)</span></span><br><span class="line">        <span class="comment"># 3. Create an object to inject noise</span></span><br><span class="line">        <span class="comment"># 4. Create an AdaIN object</span></span><br><span class="line">        <span class="comment"># 5. Create a LeakyReLU activation with slope 0.2</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        <span class="keyword">if</span> self.use_upsample:</span><br><span class="line">            self.upsample = nn.Upsample((starting_size, starting_size), mode=<span class="string">"bilinear"</span>)</span><br><span class="line">        self.conv = nn.Conv2d(in_chan, out_chan, <span class="number">3</span>, padding=<span class="number">1</span>) <span class="comment"># Padding is used to maintain the image size</span></span><br><span class="line">        self.inject_noise = InjectNoise(out_chan)</span><br><span class="line">        self.adain = AdaIN(out_chan, w_dim)</span><br><span class="line">        self.activation = nn.LeakyReLU(negative_slope = <span class="number">0.2</span>, inplace = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, w)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of MicroStyleGANGeneratorBlock: Given an x and w, </span></span><br><span class="line"><span class="string">        computes a StyleGAN generator block.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: the input into the generator, feature map of shape (n_samples, channels, width, height)</span></span><br><span class="line"><span class="string">            w: the intermediate noise vector</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> self.use_upsample:</span><br><span class="line">            x = self.upsample(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.inject_noise(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.adain(x, w)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">test_stylegan_block = MicroStyleGANGeneratorBlock(in_chan=<span class="number">128</span>, out_chan=<span class="number">64</span>, w_dim=<span class="number">256</span>, kernel_size=<span class="number">3</span>, starting_size=<span class="number">8</span>)</span><br><span class="line">test_x = torch.ones(<span class="number">1</span>, <span class="number">128</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">test_x[:, :, <span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">test_w = torch.ones(<span class="number">1</span>, <span class="number">256</span>)</span><br><span class="line">test_x = test_stylegan_block.upsample(test_x)</span><br><span class="line"><span class="keyword">assert</span> tuple(test_x.shape) == (<span class="number">1</span>, <span class="number">128</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_x.mean() - <span class="number">0.75</span>) &lt; <span class="number">1e-4</span></span><br><span class="line">test_x = test_stylegan_block.conv(test_x)</span><br><span class="line"><span class="keyword">assert</span> tuple(test_x.shape) == (<span class="number">1</span>, <span class="number">64</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">test_x = test_stylegan_block.inject_noise(test_x)</span><br><span class="line">test_x = test_stylegan_block.activation(test_x)</span><br><span class="line"><span class="keyword">assert</span> test_x.min() &lt; <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> -test_x.min() / test_x.max() &lt; <span class="number">0.4</span></span><br><span class="line">test_x = test_stylegan_block.adain(test_x, test_w) </span><br><span class="line">foo = test_stylegan_block(torch.ones(<span class="number">10</span>, <span class="number">128</span>, <span class="number">4</span>, <span class="number">4</span>), torch.ones(<span class="number">10</span>, <span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Now, you can implement progressive growing. </p><p>StyleGAN starts with a constant 4 x 4 (x 512 channel) tensor which is put through an iteration of the generator without upsampling. The output is some noise that can then be transformed into a blurry 4 x 4 image. This is where the progressive growing process begins. The 4 x 4 noise can be further passed through a generator block with upsampling to produce an 8 x 8 output. However, this will be done gradually.</p><p>You will simulate progressive growing from an 8 x 8 image to a 16 x 16 image. Instead of simply passing it to the generator block with upsampling, StyleGAN gradually trains the generator to the new size by mixing in an image that was only upsampled. By mixing an upsampled 8 x 8 image (which is 16 x 16) with increasingly more of the 16 x 16 generator output, the generator is more stable as it progressively trains. As such, you will do two separate operations with the 8 x 8 noise:</p><ol><li>Pass it into the next generator block to create an output noise, that you will then transform to an image.</li><li>Transform it into an image and then upsample it to be 16 x 16.</li></ol><p>You will now have two images that are both double the resolution of the 8 x 8 noise. Then, using an alpha ($\alpha$) term, you combine the higher resolution images obtained from (1) and (2). You would then pass this into the discriminator and use the feedback to update the weights of your generator. The key here is that the $\alpha$ term is gradually increased until eventually, only the image from (1), the generator, is used. That is your final image or you could continue this process to make a 32 x 32 image or 64 x 64, 128 x 128, etc. </p><p>This micro model you will implement will visualize what the model outputs at a particular stage of training, for a specific value of $\alpha$. However to reiterate, in practice, StyleGAN will slowly phase out the upsampled image by increasing the $\alpha$ parameter over many training steps, doing this process repeatedly with larger and larger alpha values until it is 1—at this point, the combined image is solely comprised of the image from the generator block. This method of gradually training the generator increases the stability and fidelity of the model.</p><!-- by passing a random noise vector in $z$ through the mapping function you wrote to get $w$. $w$ is then passed through the first block of the generator to create your first output noise. --><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">forward</font></code></b></font></summary>1.    You may find [torch.lerp](https://pytorch.org/docs/stable/generated/torch.lerp.html) helpful.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: MicroStyleGANGenerator</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MicroStyleGANGenerator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Micro StyleGAN Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        map_hidden_dim: the mapping inner dimension, a scalar</span></span><br><span class="line"><span class="string">        w_dim: the dimension of the intermediate noise vector, a scalar</span></span><br><span class="line"><span class="string">        in_chan: the dimension of the constant input, usually w_dim, a scalar</span></span><br><span class="line"><span class="string">        out_chan: the number of channels wanted in the output, a scalar</span></span><br><span class="line"><span class="string">        kernel_size: the size of the convolving kernel</span></span><br><span class="line"><span class="string">        hidden_chan: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, </span></span></span><br><span class="line"><span class="function"><span class="params">                 z_dim, </span></span></span><br><span class="line"><span class="function"><span class="params">                 map_hidden_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 w_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 in_chan,</span></span></span><br><span class="line"><span class="function"><span class="params">                 out_chan, </span></span></span><br><span class="line"><span class="function"><span class="params">                 kernel_size, </span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_chan)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.map = MappingLayers(z_dim, map_hidden_dim, w_dim)</span><br><span class="line">        <span class="comment"># Typically this constant is initiated to all ones, but you will initiate to a</span></span><br><span class="line">        <span class="comment"># Gaussian to better visualize the network's effect</span></span><br><span class="line">        self.starting_constant = nn.Parameter(torch.randn(<span class="number">1</span>, in_chan, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">        self.block0 = MicroStyleGANGeneratorBlock(in_chan, hidden_chan, w_dim, kernel_size, <span class="number">4</span>, use_upsample=<span class="keyword">False</span>)</span><br><span class="line">        self.block1 = MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, <span class="number">8</span>)</span><br><span class="line">        self.block2 = MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, <span class="number">16</span>)</span><br><span class="line">        <span class="comment"># You need to have a way of mapping from the output noise to an image, </span></span><br><span class="line">        <span class="comment"># so you learn a 1x1 convolution to transform the e.g. 512 channels into 3 channels</span></span><br><span class="line">        <span class="comment"># (Note that this is simplified, with clipping used in the real StyleGAN)</span></span><br><span class="line">        self.block1_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.block2_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.alpha = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">upsample_to_match_size</span><span class="params">(self, smaller_image, bigger_image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for upsampling an image to the size of another: Given a two images (smaller and bigger), </span></span><br><span class="line"><span class="string">        upsamples the first to have the same dimensions as the second.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            smaller_image: the smaller image to upsample</span></span><br><span class="line"><span class="string">            bigger_image: the bigger image whose dimensions will be upsampled to</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> F.interpolate(smaller_image, size=bigger_image.shape[<span class="number">-2</span>:], mode=<span class="string">'bilinear'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise, return_intermediate=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of MicroStyleGANGenerator: Given noise, </span></span><br><span class="line"><span class="string">        computes a StyleGAN iteration.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">            return_intermediate: a boolean, true to return the images as well (for testing) and false otherwise</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.starting_constant</span><br><span class="line">        w = self.map(noise)</span><br><span class="line">        x = self.block0(x, w)</span><br><span class="line">        x_small = self.block1(x, w) <span class="comment"># First generator run output</span></span><br><span class="line">        x_small_image = self.block1_to_image(x_small)</span><br><span class="line">        x_big = self.block2(x_small, w) <span class="comment"># Second generator run output </span></span><br><span class="line">        x_big_image = self.block2_to_image(x_big)</span><br><span class="line">        x_small_upsample = self.upsample_to_match_size(x_small_image, x_big_image) <span class="comment"># Upsample first generator run output to be same size as second generator run output </span></span><br><span class="line">        <span class="comment"># Interpolate between the upsampled image and the image from the generator using alpha</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        interpolation = torch.lerp(x_small_upsample,x_big_image,self.alpha)</span><br><span class="line">        <span class="comment">#### END CODE HERE #### </span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> return_intermediate:</span><br><span class="line">            <span class="keyword">return</span> interpolation, x_small_upsample, x_big_image</span><br><span class="line">        <span class="keyword">return</span> interpolation</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#UNIT TEST COMMENT: Required for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">z_dim = <span class="number">128</span></span><br><span class="line">out_chan = <span class="number">3</span></span><br><span class="line">truncation = <span class="number">0.7</span></span><br><span class="line"></span><br><span class="line">mu_stylegan = MicroStyleGANGenerator(</span><br><span class="line">    z_dim=z_dim, </span><br><span class="line">    map_hidden_dim=<span class="number">1024</span>,</span><br><span class="line">    w_dim=<span class="number">496</span>,</span><br><span class="line">    in_chan=<span class="number">512</span>,</span><br><span class="line">    out_chan=out_chan, </span><br><span class="line">    kernel_size=<span class="number">3</span>, </span><br><span class="line">    hidden_chan=<span class="number">256</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_samples = <span class="number">10</span></span><br><span class="line">test_result = mu_stylegan(get_truncated_noise(test_samples, z_dim, truncation))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check if the block works</span></span><br><span class="line"><span class="keyword">assert</span> tuple(test_result.shape) == (test_samples, out_chan, <span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that the interpolation is correct</span></span><br><span class="line">mu_stylegan.alpha = <span class="number">1.</span></span><br><span class="line">test_result, _, test_big =  mu_stylegan(</span><br><span class="line">    get_truncated_noise(test_samples, z_dim, truncation), </span><br><span class="line">    return_intermediate=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_result - test_big).mean() &lt; <span class="number">0.001</span></span><br><span class="line">mu_stylegan.alpha = <span class="number">0.</span></span><br><span class="line">test_result, test_small, _ =  mu_stylegan(</span><br><span class="line">    get_truncated_noise(test_samples, z_dim, truncation), </span><br><span class="line">    return_intermediate=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_result - test_small).mean() &lt; <span class="number">0.001</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Running-StyleGAN"><a href="#Running-StyleGAN" class="headerlink" title="Running StyleGAN"></a>Running StyleGAN</h2><p>Finally, you can put all the components together to run an iteration of your micro StyleGAN!</p><p>You can also visualize what this randomly initiated generator can produce. The code will automatically interpolate between different values of alpha so that you can intuitively see what it means to mix the low-resolution and high-resolution images using different values of alpha. In the generated image, the samples start from low alpha values and go to high alpha values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = [<span class="number">15</span>, <span class="number">15</span>]</span><br><span class="line"></span><br><span class="line">viz_samples = <span class="number">10</span></span><br><span class="line"><span class="comment"># The noise is exaggerated for visual effect</span></span><br><span class="line">viz_noise = get_truncated_noise(viz_samples, z_dim, truncation) * <span class="number">10</span></span><br><span class="line"></span><br><span class="line">mu_stylegan.eval()</span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, num=<span class="number">5</span>):</span><br><span class="line">    mu_stylegan.alpha = alpha</span><br><span class="line">    viz_result, _, _ =  mu_stylegan(</span><br><span class="line">        viz_noise, </span><br><span class="line">        return_intermediate=<span class="keyword">True</span>)</span><br><span class="line">    images += [tensor <span class="keyword">for</span> tensor <span class="keyword">in</span> viz_result]</span><br><span class="line">show_tensor_images(torch.stack(images), nrow=viz_samples, num_images=len(images))</span><br><span class="line">mu_stylegan = mu_stylegan.train()</span><br></pre></td></tr></table></figure><p><img src="output_22_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Components-of-StyleGAN&quot;&gt;&lt;a href=&quot;#Components-of-StyleGAN&quot; class=&quot;headerlink&quot; title=&quot;Components of StyleGAN&quot;&gt;&lt;/a&gt;Components of StyleG
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Evaluating GANs</title>
    <link href="https://zhangruochi.com/Evaluating-GANs/2020/10/10/"/>
    <id>https://zhangruochi.com/Evaluating-GANs/2020/10/10/</id>
    <published>2020-10-10T07:34:02.000Z</published>
    <updated>2020-10-10T07:34:24.089Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluating-GANs"><a href="#Evaluating-GANs" class="headerlink" title="Evaluating GANs"></a>Evaluating GANs</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to gain a better understanding of some of the challenges that come with evaluating GANs and a response you can take to alleviate some of them called Fréchet Inception Distance (FID).</p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Understand the challenges associated with evaluating GANs.</li><li>Write code to evaluate the Fréchet Inception Distance.</li></ol><h2 id="Challenges-With-Evaluating-GANs"><a href="#Challenges-With-Evaluating-GANs" class="headerlink" title="Challenges With Evaluating GANs"></a>Challenges With Evaluating GANs</h2><h4 id="Loss-is-Uninformative-of-Performance"><a href="#Loss-is-Uninformative-of-Performance" class="headerlink" title="Loss is Uninformative of Performance"></a>Loss is Uninformative of Performance</h4><p>One aspect that makes evaluating GANs challenging is that the loss tells us little about their performance. Unlike with classifiers, where a low loss on a test set indicates superior performance, a low loss for the generator or discriminator suggests that learning has stopped. </p><h4 id="No-Clear-Non-human-Metric"><a href="#No-Clear-Non-human-Metric" class="headerlink" title="No Clear Non-human Metric"></a>No Clear Non-human Metric</h4><p>If you define the goal of a GAN as “generating images which look real to people” then it’s technically possible to measure this directly: <a href="https://arxiv.org/abs/1904.01121" target="_blank" rel="noopener">you can ask people to act as a discriminator</a>. However, this takes significant time and money so ideally you can use a proxy for this. There is also no “perfect” discriminator that can differentiate reals from fakes - if there were, a lot of machine learning tasks would be solved ;)</p><p>In this notebook, you will implement Fréchet Inception Distance, one method which aims to solve these issues. </p><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>For this notebook, you will again be using <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" rel="noopener">CelebA</a>. You will start by loading a pre-trained generator which has been trained on CelebA.</p><p>Here, you will import some useful libraries and packages. You will also be provided with the generator and noise code from earlier assignments.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> CelebA</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for our testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (CelebA is rgb, so 3 is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_chan=<span class="number">3</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">8</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">8</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = noise.view(len(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure><h2 id="Loading-the-Pre-trained-Model"><a href="#Loading-the-Pre-trained-Model" class="headerlink" title="Loading the Pre-trained Model"></a>Loading the Pre-trained Model</h2><p>Now, you can set the arguments for the model and load the dataset:</p><ul><li>z_dim: the dimension of the noise vector</li><li>image_size: the image size of the input to Inception (more details in the following section)</li><li>device: the device type</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">image_size = <span class="number">299</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(image_size),</span><br><span class="line">    transforms.CenterCrop(image_size),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">in_coursera = <span class="keyword">True</span> <span class="comment"># Set this to false if you're running this outside Coursera</span></span><br><span class="line"><span class="keyword">if</span> in_coursera:</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    data = torch.Tensor(np.load(<span class="string">'fid_images_tensor.npz'</span>, allow_pickle=<span class="keyword">True</span>)[<span class="string">'arr_0'</span>])</span><br><span class="line">    dataset = torch.utils.data.TensorDataset(data, data)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dataset = CelebA(<span class="string">"."</span>, download=<span class="keyword">True</span>, transform=transform)</span><br></pre></td></tr></table></figure><p>Then, you can load and initialize the model with weights from a pre-trained model. This allows you to use the pre-trained model as if you trained it yourself.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen.load_state_dict(torch.load(<span class="string">f"pretrained_celeba.pth"</span>, map_location=torch.device(device))[<span class="string">"gen"</span>])</span><br><span class="line">gen = gen.eval()</span><br></pre></td></tr></table></figure><h2 id="Inception-v3-Network"><a href="#Inception-v3-Network" class="headerlink" title="Inception-v3 Network"></a>Inception-v3 Network</h2><p>Inception-V3 is a neural network trained on <a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a> to classify objects. You may recall from the lectures that ImageNet has over 1 million images to train on. As a result, Inception-V3 does a good job detecting features and classifying images. Here, you will load Inception-V3 as <code>inception_model</code>.</p><!--  In the past, people would use a pretrained Inception network to identify the classes of the objects generated by a GAN and measure how similar the distribution of classes generated was to the true image (using KL divergence). This is known as inception score. However, there are many problems with this metric. Barratt and Sharma's 2018 "[A Note on the Inception Score](https://arxiv.org/pdf/1801.01973.pdf)" highlights many issues with this approach. Among them, they highlight its instability, its exploitability, and the widespread use of Inception Score on models not trained on ImageNet.  --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> inception_v3</span><br><span class="line">inception_model = inception_v3(pretrained=<span class="keyword">False</span>)</span><br><span class="line">inception_model.load_state_dict(torch.load(<span class="string">"inception_v3_google-1a9a5a14.pth"</span>))</span><br><span class="line">inception_model.to(device)</span><br><span class="line">inception_model = inception_model.eval() <span class="comment"># Evaluation mode</span></span><br></pre></td></tr></table></figure><h2 id="Frechet-Inception-Distance"><a href="#Frechet-Inception-Distance" class="headerlink" title="Fréchet Inception Distance"></a>Fréchet Inception Distance</h2><p>Fréchet Inception Distance (FID) was proposed as an improvement over Inception Score and still uses the Inception-v3 network as part of its calculation. However, instead of using the classification labels of the Inception-v3 network, it uses the output from an earlier layer—the layer right before the labels. This is often called the feature layer. Research has shown that deep convolutional neural networks trained on difficult tasks, like classifying many classes, build increasingly sophisticated representations of features going deeper into the network. For example, the first few layers may learn to detect different kinds of edges and curves, while the later layers may have neurons that fire in response to human faces.</p><p>To get the feature layer of a convolutional neural network, you can replace the final fully connected layer with an identity layer that simply returns whatever input it received, unchanged. This essentially removes the final classification layer and leaves you with the intermediate outputs from the layer before.</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">inception_model.fc</font></code></b></font></summary>1.    You may find [torch.nn.Identity()](https://pytorch.org/docs/master/generated/torch.nn.Identity.html) helpful.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL: inception_model.fc</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You want to replace the final fully-connected (fc) layer </span></span><br><span class="line"><span class="comment"># with an identity function layer to cut off the classification</span></span><br><span class="line"><span class="comment"># layer and get a feature extractor</span></span><br><span class="line"><span class="comment">#### START CODE HERE ####</span></span><br><span class="line">inception_model.fc = nn.Identity()</span><br><span class="line"><span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_identity_noise = torch.randn(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"><span class="keyword">assert</span> torch.equal(test_identity_noise, inception_model.fc(test_identity_noise))</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h3 id="Frechet-Distance"><a href="#Frechet-Distance" class="headerlink" title="Fréchet Distance"></a>Fréchet Distance</h3><p>Fréchet distance uses the values from the feature layer for two sets of images, say reals and fakes, and compares different statistical properties between them to see how different they are. Specifically, Fréchet distance finds the shortest distance needed to walk along two lines, or two curves, simultaneously. The most intuitive explanation of Fréchet distance is as the “minimum leash distance” between two points. Imagine yourself and your dog, both moving along two curves. If you walked on one curve and your dog, attached to a leash, walked on the other at the same pace, what is the least amount of leash that you can give your dog so that you never need to give them more slack during your walk? Using this, the Fréchet distance measures the similarity between these two curves.</p><p>The basic idea is similar for calculating the Fréchet distance between two probability distributions. You’ll start by seeing what this looks like in one-dimensional, also called univariate, space.</p><h4 id="Univariate-Frechet-Distance"><a href="#Univariate-Frechet-Distance" class="headerlink" title="Univariate Fréchet Distance"></a>Univariate Fréchet Distance</h4><p>You can calculate the distance between two normal distributions $X$ and $Y$ with means $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$, as:</p><script type="math/tex; mode=display">d(X,Y) = (\mu_X-\mu_Y)^2 + (\sigma_X-\sigma_Y)^2</script><p>Pretty simple, right? Now you can see how it can be converted to be used in multi-dimensional, which is also called multivariate, space.</p><h4 id="Multivariate-Frechet-Distance"><a href="#Multivariate-Frechet-Distance" class="headerlink" title="Multivariate Fréchet Distance"></a>Multivariate Fréchet Distance</h4><p><strong>Covariance</strong></p><p>To find the Fréchet distance between two multivariate normal distributions, you first need to find the covariance instead of the standard deviation. The covariance, which is the multivariate version of variance (the square of standard deviation), is represented using a square matrix where the side length is equal to the number of dimensions. Since the feature vectors you will be using have 2048 values/weights, the covariance matrix will be 2048 x 2048. But for the sake of an example, this is a covariance matrix in a two-dimensional space:</p><p>$\Sigma = \left(\begin{array}{cc}<br>1 &amp; 0\\<br>0 &amp; 1<br>\end{array}\right)<br>$</p><p>The value at location $(i, j)$ corresponds to the covariance of vector $i$ with vector $j$. Since the covariance of $i$ with $j$ and $j$ with $i$ are equivalent, the matrix will always be symmetric with respect to the diagonal. The diagonal is the covariance of that element with itself. In this example, there are zeros everywhere except the diagonal. That means that the two dimensions are independent of one another, they are completely unrelated.</p><p>The following code cell will visualize this matrix.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import os</span></span><br><span class="line"><span class="comment">#os.environ['KMP_DUPLICATE_LIB_OK']='True'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> MultivariateNormal</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment"># This is for visualization</span></span><br><span class="line">mean = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance = torch.Tensor( <span class="comment"># This matrix shows independence - there are only non-zero values on the diagonal</span></span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">independent_dist = MultivariateNormal(mean, covariance)</span><br><span class="line">samples = independent_dist.sample((<span class="number">10000</span>,))</span><br><span class="line">res = sns.jointplot(samples[:, <span class="number">0</span>], samples[:, <span class="number">1</span>], kind=<span class="string">"kde"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><p>Now, here’s an example of a multivariate normal distribution that has covariance:</p><p>$\Sigma = \left(\begin{array}{cc}<br>2 &amp; -1\\<br>-1 &amp; 2<br>\end{array}\right)<br>$</p><p>And see how it looks:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mean = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">covariance = torch.Tensor(</span><br><span class="line">    [[<span class="number">2</span>, <span class="number">-1</span>],</span><br><span class="line">     [<span class="number">-1</span>, <span class="number">2</span>]]</span><br><span class="line">)</span><br><span class="line">covariant_dist = MultivariateNormal(mean, covariance)</span><br><span class="line">samples = covariant_dist.sample((<span class="number">10000</span>,))</span><br><span class="line">res = sns.jointplot(samples[:, <span class="number">0</span>], samples[:, <span class="number">1</span>], kind=<span class="string">"kde"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_20_0.png" alt="png"></p><p><strong>Formula</strong></p><p>Based on the paper, “<a href="https://core.ac.uk/reader/82269844" target="_blank" rel="noopener">The Fréchet distance between multivariate normal distributions</a>“ by Dowson and Landau (1982), the Fréchet distance between two multivariate normal distributions $X$ and $Y$ is:</p><p>$d(X, Y) = \Vert\mu_X-\mu_Y\Vert^2 + \mathrm{Tr}\left(\Sigma_X+\Sigma_Y - 2 \sqrt{\Sigma_X \Sigma_Y}\right)$</p><p>Similar to the formula for univariate Fréchet distance, you can calculate the distance between the means and the distance between the standard deviations. However, calculating the distance between the standard deviations changes slightly here, as it includes the matrix product and matrix square root. $\mathrm{Tr}$ refers to the trace, the sum of the diagonal elements of a matrix.</p><p>Now you can implement this!</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">frechet_distance</font></code></b></font></summary>1.   You want to implement the above equation in code.2.   You might find the functions `torch.norm` and `torch.trace` helpful here.3.   A matrix_sqrt function is defined for you above -- you need to use it instead of `torch.sqrt()` which only gets the elementwise square root instead of the matrix square root.4.   You can also use the `@` symbol for matrix multiplication.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="comment"># This is the matrix square root function you will be using</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrix_sqrt</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function that takes in a matrix and returns the square root of that matrix.</span></span><br><span class="line"><span class="string">    For an input matrix A, the output matrix B would be such that B @ B is the matrix A.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        x: a matrix</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = x.cpu().detach().numpy()</span><br><span class="line">    y = scipy.linalg.sqrtm(y)</span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(y.real, device=x.device)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: frechet_distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frechet_distance</span><span class="params">(mu_x, mu_y, sigma_x, sigma_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for returning the Fréchet distance between multivariate Gaussians,</span></span><br><span class="line"><span class="string">    parameterized by their means and covariance matrices.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        mu_x: the mean of the first Gaussian, (n_features)</span></span><br><span class="line"><span class="string">        mu_y: the mean of the second Gaussian, (n_features) </span></span><br><span class="line"><span class="string">        sigma_x: the covariance matrix of the first Gaussian, (n_features, n_features)</span></span><br><span class="line"><span class="string">        sigma_y: the covariance matrix of the second Gaussian, (n_features, n_features)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(mu_x - mu_y) + torch.trace(sigma_x + sigma_y - <span class="number">2</span> * matrix_sqrt(sigma_x @ sigma_y))</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"></span><br><span class="line">mean1 = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance1 = torch.Tensor( <span class="comment"># This matrix shows independence - there are only non-zero values on the diagonal</span></span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">dist1 = MultivariateNormal(mean1, covariance1)</span><br><span class="line"></span><br><span class="line">mean2 = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>]) <span class="comment"># Center the mean at the origin</span></span><br><span class="line">covariance2 = torch.Tensor( <span class="comment"># This matrix shows dependence </span></span><br><span class="line">    [[<span class="number">2</span>, <span class="number">-1</span>],</span><br><span class="line">     [<span class="number">-1</span>, <span class="number">2</span>]]</span><br><span class="line">)</span><br><span class="line">dist2 = MultivariateNormal(mean2, covariance2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    frechet_distance(</span><br><span class="line">        dist1.mean, dist2.mean,</span><br><span class="line">        dist1.covariance_matrix, dist2.covariance_matrix</span><br><span class="line">    ),</span><br><span class="line">    <span class="number">4</span> - <span class="number">2</span> * torch.sqrt(torch.tensor(<span class="number">3.</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> (frechet_distance(</span><br><span class="line">        dist1.mean, dist1.mean,</span><br><span class="line">        dist1.covariance_matrix, dist1.covariance_matrix</span><br><span class="line">    ).item() == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together!"></a>Putting it all together!</h2><p>Now, you can apply FID to your generator from earlier.</p><p>You will start by defining a bit of helper code to preprocess the image for the Inception-v3 network:<br><!-- This isn't exactly what FID is meant for, since inception scores expect a natural image, but it should give a rough idea of the diversity and quality of your images.  [TODO: move to bottom since image net is trained on nature (cat, dog) images, fidelity (quality)] --></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = torch.nn.functional.interpolate(img, size=(<span class="number">299</span>, <span class="number">299</span>), mode=<span class="string">'bilinear'</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><p>Then, you’ll define a function to calculate the covariance of the features that returns a covariance matrix given a list of values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_covariance</span><span class="params">(features)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(np.cov(features.detach().numpy(), rowvar=<span class="keyword">False</span>))</span><br></pre></td></tr></table></figure><p>Finally, you can use the pre-trained Inception-v3 model to compute features of the real and fake images. With these features, you can then get the covariance and means of these features across many samples. </p><p>First, you get the features of the real and fake images using the Inception-v3 model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">fake_features_list = []</span><br><span class="line">real_features_list = []</span><br><span class="line"></span><br><span class="line">gen.eval()</span><br><span class="line">n_samples = <span class="number">512</span> <span class="comment"># The total number of samples</span></span><br><span class="line">batch_size = <span class="number">4</span> <span class="comment"># Samples per iteration</span></span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">cur_samples = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># You don't need to calculate gradients here, so you do this to save memory</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> real_example, _ <span class="keyword">in</span> tqdm(dataloader, total=n_samples // batch_size): <span class="comment"># Go by batch</span></span><br><span class="line">            real_samples = real_example</span><br><span class="line">            real_features = inception_model(real_samples.to(device)).detach().to(<span class="string">'cpu'</span>) <span class="comment"># Move features to CPU</span></span><br><span class="line">            real_features_list.append(real_features)</span><br><span class="line"></span><br><span class="line">            fake_samples = get_noise(len(real_example), z_dim).to(device)</span><br><span class="line">            fake_samples = preprocess(gen(fake_samples))</span><br><span class="line">            fake_features = inception_model(fake_samples.to(device)).detach().to(<span class="string">'cpu'</span>)</span><br><span class="line">            fake_features_list.append(fake_features)</span><br><span class="line">            cur_samples += len(real_samples)</span><br><span class="line">            <span class="keyword">if</span> cur_samples &gt;= n_samples:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Error in loop"</span>)</span><br></pre></td></tr></table></figure><pre><code>HBox(children=(FloatProgress(value=0.0, max=128.0), HTML(value=&#39;&#39;)))</code></pre><p>Then, you can combine all of the values that you collected for the reals and fakes into large tensors:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Needed as is for autograding</span></span><br><span class="line">fake_features_all = torch.cat(fake_features_list)</span><br><span class="line">real_features_all = torch.cat(real_features_list)</span><br></pre></td></tr></table></figure><p>And calculate the covariance and means of these real and fake features:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the covariance matrix for the fake and real features</span></span><br><span class="line"><span class="comment"># and also calculate the means of the feature over the batch (for each feature dimension mean)</span></span><br><span class="line"><span class="comment">#### START CODE HERE ####</span></span><br><span class="line">mu_fake = torch.mean(fake_features_all, axis = <span class="number">0</span>)</span><br><span class="line">mu_real = torch.mean(real_features_all, axis = <span class="number">0</span>)</span><br><span class="line">sigma_fake = get_covariance(fake_features_all)</span><br><span class="line">sigma_real = get_covariance(real_features_all)</span><br><span class="line"><span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> tuple(sigma_fake.shape) == (fake_features_all.shape[<span class="number">1</span>], fake_features_all.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> torch.abs(sigma_fake[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">2.5e-2</span>) &lt; <span class="number">1e-2</span> <span class="keyword">and</span> torch.abs(sigma_fake[<span class="number">-1</span>, <span class="number">-1</span>] - <span class="number">5e-2</span>) &lt; <span class="number">1e-2</span></span><br><span class="line"><span class="keyword">assert</span> tuple(sigma_real.shape) == (real_features_all.shape[<span class="number">1</span>], real_features_all.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> torch.abs(sigma_real[<span class="number">0</span>, <span class="number">0</span>] - <span class="number">3.5768e-2</span>) &lt; <span class="number">1e-4</span> <span class="keyword">and</span> torch.abs(sigma_real[<span class="number">0</span>, <span class="number">1</span>] + <span class="number">5.3236e-4</span>) &lt; <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">assert</span> tuple(mu_fake.shape) == (fake_features_all.shape[<span class="number">1</span>],)</span><br><span class="line"><span class="keyword">assert</span> tuple(mu_real.shape) == (real_features_all.shape[<span class="number">1</span>],)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(mu_real[<span class="number">0</span>] - <span class="number">0.3099</span>) &lt; <span class="number">0.01</span> <span class="keyword">and</span> torch.abs(mu_real[<span class="number">1</span>] - <span class="number">0.2721</span>) &lt; <span class="number">0.01</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(mu_fake[<span class="number">0</span>] - <span class="number">0.37</span>) &lt; <span class="number">0.05</span> <span class="keyword">and</span> torch.abs(mu_real[<span class="number">1</span>] - <span class="number">0.27</span>) &lt; <span class="number">0.05</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>At this point, you can also visualize what the pairwise multivariate distributions of the inception features look like!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">indices = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">fake_dist = MultivariateNormal(mu_fake[indices], sigma_fake[indices][:, indices])</span><br><span class="line">fake_samples = fake_dist.sample((<span class="number">5000</span>,))</span><br><span class="line">real_dist = MultivariateNormal(mu_real[indices], sigma_real[indices][:, indices])</span><br><span class="line">real_samples = real_dist.sample((<span class="number">5000</span>,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df_fake = pd.DataFrame(fake_samples.numpy(), columns=indices)</span><br><span class="line">df_real = pd.DataFrame(real_samples.numpy(), columns=indices)</span><br><span class="line">df_fake[<span class="string">"is_real"</span>] = <span class="string">"no"</span></span><br><span class="line">df_real[<span class="string">"is_real"</span>] = <span class="string">"yes"</span></span><br><span class="line">df = pd.concat([df_fake, df_real])</span><br><span class="line">sns.pairplot(df, plot_kws=&#123;<span class="string">'alpha'</span>: <span class="number">0.1</span>&#125;, hue=<span class="string">'is_real'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7fa847b2ab38&gt;</code></pre><p><img src="output_37_1.png" alt="png"></p><p>Lastly, you can use your earlier <code>frechet_distance</code> function to calculate the FID and evaluate your GAN. You can see how similar/different the features of the generated images are to the features of the real images. The next cell might take five minutes or so to run in Coursera.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item())</span><br></pre></td></tr></table></figure><pre><code>86.48429107666016</code></pre><p>You’ll notice this model gets a pretty high FID, likely over 30. Since lower is better, and the best models on CelebA get scores in the single-digits, there’s clearly a ways to go with this model. You can use FID to compare different models, as well as different stages of training of the same model. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Evaluating-GANs&quot;&gt;&lt;a href=&quot;#Evaluating-GANs&quot; class=&quot;headerlink&quot; title=&quot;Evaluating GANs&quot;&gt;&lt;/a&gt;Evaluating GANs&lt;/h1&gt;&lt;h3 id=&quot;Goals&quot;&gt;&lt;a hre
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Build a Conditional GAN</title>
    <link href="https://zhangruochi.com/Build-a-Conditional-GAN/2020/10/09/"/>
    <id>https://zhangruochi.com/Build-a-Conditional-GAN/2020/10/09/</id>
    <published>2020-10-09T12:27:13.000Z</published>
    <updated>2020-10-09T12:28:13.015Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Build-a-Conditional-GAN"><a href="#Build-a-Conditional-GAN" class="headerlink" title="Build a Conditional GAN"></a>Build a Conditional GAN</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate.</p><p>You’ll then do some exploration of the generated images to visualize what the noise and class vectors mean.  </p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Learn the technical difference between a conditional and unconditional GAN.</li><li>Understand the distinction between the class and noise vector in a conditional GAN.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>For this assignment, you will be using the MNIST dataset again, but there’s nothing stopping you from applying this generator code to produce images of animals conditioned on the species or pictures of faces conditioned on facial characteristics.</p><p>Note that this assignment requires no changes to the architectures of the generator or discriminator, only changes to the data passed to both. The generator will no longer take <code>z_dim</code> as an argument, but  <code>input_dim</code> instead, since you need to pass in both the noise and class vectors. In addition to good variable naming, this also means that you can use the generator and discriminator code you have previously written with different parameters.</p><p>You will begin by importing the necessary libraries and building the generator and discriminator.</p><h4 id="Packages-and-Visualization"><a href="#Packages-and-Visualization" class="headerlink" title="Packages and Visualization"></a>Packages and Visualization</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for our testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>, nrow=<span class="number">5</span>, show=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu()</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    <span class="keyword">if</span> show:</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure><h4 id="Generator-and-Noise"><a href="#Generator-and-Noise" class="headerlink" title="Generator and Noise"></a>Generator and Noise</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="number">10</span>, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(input_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, input_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = noise.view(len(noise), self.input_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, input_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, input_dim, device=device)</span><br></pre></td></tr></table></figure><h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">      im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">            (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">      hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            self.make_disc_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_disc_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim * <span class="number">2</span>, <span class="number">1</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_disc_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; </span></span><br><span class="line"><span class="string">        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_chan)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        disc_pred = self.disc(image)</span><br><span class="line">        <span class="keyword">return</span> disc_pred.view(len(disc_pred), <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Class-Input"><a href="#Class-Input" class="headerlink" title="Class Input"></a>Class Input</h2><p>In conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0’s and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes, please create one-hot vectors for each label. There is a class within the PyTorch functional library that can help you.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">get_one_hot_labels</font></code></b></font></summary>1.   This code can be done in one line.2.   The documentation for [F.one_hot](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.one_hot) may be helpful.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_one_hot_labels</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_hot_labels</span><span class="params">(labels, n_classes)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        labels: tensor of labels from the dataloader, size (?)</span></span><br><span class="line"><span class="string">        n_classes: the total number of classes in the dataset, an integer scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> F.one_hot(labels, n_classes)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> (</span><br><span class="line">    get_one_hot_labels(</span><br><span class="line">        labels=torch.Tensor([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]]).long(),</span><br><span class="line">        n_classes=<span class="number">3</span></span><br><span class="line">    ).tolist() == </span><br><span class="line">    [[</span><br><span class="line">      [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line">      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">    ]]</span><br><span class="line">)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator.</p><p>To do this, you will need to write a function that combines two vectors. Remember that you need to ensure that the vectors are the same type: floats. Again, you can look to the PyTorch library for help.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">combine_vectors</font></code></b></font></summary>1.   This code can also be written in one line.2.   The documentation for [torch.cat](https://pytorch.org/docs/master/generated/torch.cat.html) may be helpful.3.   Specifically, you might want to look at what the `dim` argument of `torch.cat` does.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: combine_vectors</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_vectors</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">      x: (n_samples, ?) the first vector. </span></span><br><span class="line"><span class="string">        In this assignment, this will be the noise vector of shape (n_samples, z_dim), </span></span><br><span class="line"><span class="string">        but you shouldn't need to know the second dimension's size.</span></span><br><span class="line"><span class="string">      y: (n_samples, ?) the second vector.</span></span><br><span class="line"><span class="string">        Once again, in this assignment this will be the one-hot class vector </span></span><br><span class="line"><span class="string">        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Note: Make sure this function outputs a float no matter what inputs it receives</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    combined = torch.cat((x.float(),y.float()), dim = <span class="number">1</span>)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">combined = combine_vectors(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]));</span><br><span class="line"><span class="comment"># Check exact order of elements</span></span><br><span class="line"><span class="keyword">assert</span> torch.all(combined == torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>]]))</span><br><span class="line"><span class="comment"># Tests that items are of float type</span></span><br><span class="line"><span class="keyword">assert</span> (type(combined[<span class="number">0</span>][<span class="number">0</span>].item()) == float)</span><br><span class="line"><span class="comment"># Check shapes</span></span><br><span class="line">combined = combine_vectors(torch.randn(<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>), torch.randn(<span class="number">1</span>, <span class="number">8</span>, <span class="number">5</span>));</span><br><span class="line"><span class="keyword">assert</span> tuple(combined.shape) == (<span class="number">1</span>, <span class="number">12</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">assert</span> tuple(combine_vectors(torch.randn(<span class="number">1</span>, <span class="number">10</span>, <span class="number">12</span>).long(), torch.randn(<span class="number">1</span>, <span class="number">20</span>, <span class="number">12</span>).long()).shape) == (<span class="number">1</span>, <span class="number">30</span>, <span class="number">12</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can start to put it all together!<br>First, you will define some new parameters:</p><ul><li>mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it’s black-and-white) so 1 x 28 x 28</li><li>n_classes: the number of classes in MNIST (10, since there are the digits from 0 to 9)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnist_shape = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">n_classes = <span class="number">10</span></span><br></pre></td></tr></table></figure><p>And you also include the same parameters from previous assignments:</p><ul><li>criterion: the loss function</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>device: the device type</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">n_epochs = <span class="number">200</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">'.'</span>, download=<span class="keyword">False</span>, transform=transform),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models. For the generator, you will need to calculate the size of the input vector; recall that for conditional GANs, the generator’s input is the noise vector concatenated with the class vector. For the discriminator, you need to add a channel for every class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_input_dimensions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_input_dimensions</span><span class="params">(z_dim, mnist_shape, n_classes)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for getting the size of the conditional input dimensions </span></span><br><span class="line"><span class="string">    from z_dim, the image shape, and number of classes.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)</span></span><br><span class="line"><span class="string">        n_classes: the total number of classes in the dataset, an integer scalar</span></span><br><span class="line"><span class="string">                (10 for MNIST)</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        generator_input_dim: the input dimensionality of the conditional generator, </span></span><br><span class="line"><span class="string">                          which takes the noise and class vectors</span></span><br><span class="line"><span class="string">        discriminator_im_chan: the number of input channels to the discriminator</span></span><br><span class="line"><span class="string">                            (e.g. C x 28 x 28 for MNIST)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    generator_input_dim = z_dim + n_classes</span><br><span class="line">    discriminator_im_chan = mnist_shape[<span class="number">0</span>] + n_classes</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> generator_input_dim, discriminator_im_chan</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_input_dims</span><span class="params">()</span>:</span></span><br><span class="line">    gen_dim, disc_dim = get_input_dimensions(<span class="number">23</span>, (<span class="number">12</span>, <span class="number">23</span>, <span class="number">52</span>), <span class="number">9</span>)</span><br><span class="line">    <span class="keyword">assert</span> gen_dim == <span class="number">32</span></span><br><span class="line">    <span class="keyword">assert</span> disc_dim == <span class="number">21</span></span><br><span class="line">test_input_dims()</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)</span><br><span class="line"></span><br><span class="line">gen = Generator(input_dim=generator_input_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">disc = Discriminator(im_chan=discriminator_im_chan).to(device)</span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">gen = gen.apply(weights_init)</span><br><span class="line">disc = disc.apply(weights_init)</span><br></pre></td></tr></table></figure><p>Now to train, you would like both your generator and your discriminator to know what class of image should be generated. There are a few locations where you will need to implement code.</p><p>For example, if you’re generating a picture of the number “1”, you would need to:</p><ol><li>Tell that to the generator, so that it knows it should be generating a “1”</li><li>Tell that to the discriminator, so that it knows it should be looking at a “1”. If the discriminator is told it should be looking at a 1 but sees something that’s clearly an 8, it can guess that it’s probably fake</li></ol><p>There are no explicit unit tests here — if this block of code runs and you don’t change any of the other variables, then you’ve done it correctly!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED CELL</span></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">generator_losses = []</span><br><span class="line">discriminator_losses = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#UNIT TEST <span class="doctag">NOTE:</span> Initializations needed for grading</span></span><br><span class="line">noise_and_labels = <span class="keyword">False</span></span><br><span class="line">fake = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">fake_image_and_labels = <span class="keyword">False</span></span><br><span class="line">real_image_and_labels = <span class="keyword">False</span></span><br><span class="line">disc_fake_pred = <span class="keyword">False</span></span><br><span class="line">disc_real_pred = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># Dataloader returns the batches and the labels</span></span><br><span class="line">    <span class="keyword">for</span> real, labels <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line">        <span class="comment"># Flatten the batch of real images from the dataset</span></span><br><span class="line">        real = real.to(device)</span><br><span class="line"></span><br><span class="line">        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)</span><br><span class="line">        image_one_hot_labels = one_hot_labels[:, :, <span class="keyword">None</span>, <span class="keyword">None</span>]</span><br><span class="line">        image_one_hot_labels = image_one_hot_labels.repeat(<span class="number">1</span>, <span class="number">1</span>, mnist_shape[<span class="number">1</span>], mnist_shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the discriminator gradients</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line">        <span class="comment"># Get noise corresponding to the current batch_size </span></span><br><span class="line">        fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Now you can get the images from the generator</span></span><br><span class="line">        <span class="comment"># Steps: 1) Combine the noise vectors and the one-hot labels for the generator</span></span><br><span class="line">        <span class="comment">#        2) Generate the conditioned fake images</span></span><br><span class="line">       </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)</span><br><span class="line">        fake = gen(noise_and_labels)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Make sure that enough images were generated</span></span><br><span class="line">        <span class="keyword">assert</span> len(fake) == len(real)</span><br><span class="line">        <span class="comment"># Check that correct tensors were combined</span></span><br><span class="line">        <span class="keyword">assert</span> tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[<span class="number">1</span>] + one_hot_labels.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># It comes from the correct generator</span></span><br><span class="line">        <span class="keyword">assert</span> tuple(fake.shape) == (len(real), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Now you can get the predictions from the discriminator</span></span><br><span class="line">        <span class="comment"># Steps: 1) Create the input for the discriminator</span></span><br><span class="line">        <span class="comment">#           a) Combine the fake images with image_one_hot_labels, </span></span><br><span class="line">        <span class="comment">#              remember to detach the generator (.detach()) so you do not backpropagate through it</span></span><br><span class="line">        <span class="comment">#           b) Combine the real images with image_one_hot_labels</span></span><br><span class="line">        <span class="comment">#        2) Get the discriminator's prediction on the fakes as disc_fake_pred</span></span><br><span class="line">        <span class="comment">#        3) Get the discriminator's prediction on the reals as disc_real_pred</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        fake_image_and_labels = combine_vectors(fake,image_one_hot_labels)</span><br><span class="line">        real_image_and_labels = combine_vectors(real,image_one_hot_labels)</span><br><span class="line">        disc_fake_pred = disc(fake_image_and_labels)</span><br><span class="line">        disc_real_pred = disc(real_image_and_labels)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Make sure shapes are correct </span></span><br><span class="line">        <span class="keyword">assert</span> tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[<span class="number">1</span>] + image_one_hot_labels.shape[<span class="number">1</span>], <span class="number">28</span> ,<span class="number">28</span>)</span><br><span class="line">        <span class="keyword">assert</span> tuple(real_image_and_labels.shape) == (len(real), real.shape[<span class="number">1</span>] + image_one_hot_labels.shape[<span class="number">1</span>], <span class="number">28</span> ,<span class="number">28</span>)</span><br><span class="line">        <span class="comment"># Make sure that enough predictions were made</span></span><br><span class="line">        <span class="keyword">assert</span> len(disc_real_pred) == len(real)</span><br><span class="line">        <span class="comment"># Make sure that the inputs are different</span></span><br><span class="line">        <span class="keyword">assert</span> torch.any(fake_image_and_labels != real_image_and_labels)</span><br><span class="line">        <span class="comment"># Shapes must match</span></span><br><span class="line">        <span class="keyword">assert</span> tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape)</span><br><span class="line">        <span class="keyword">assert</span> tuple(disc_fake_pred.shape) == tuple(disc_real_pred.shape)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span><br><span class="line">        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span><br><span class="line">        disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">        disc_opt.step() </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        discriminator_losses += [disc_loss.item()]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update generator ###</span></span><br><span class="line">        <span class="comment"># Zero out the generator gradients</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)</span><br><span class="line">        <span class="comment"># This will error if you didn't concatenate your labels to your image correctly</span></span><br><span class="line">        disc_fake_pred = disc(fake_image_and_labels)</span><br><span class="line">        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span><br><span class="line">        gen_loss.backward()</span><br><span class="line">        gen_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the generator losses</span></span><br><span class="line">        generator_losses += [gen_loss.item()]</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            gen_mean = sum(generator_losses[-display_step:]) / display_step</span><br><span class="line">            disc_mean = sum(discriminator_losses[-display_step:]) / display_step</span><br><span class="line">            print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;gen_mean&#125;</span>, discriminator loss: <span class="subst">&#123;disc_mean&#125;</span>"</span>)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            step_bins = <span class="number">20</span></span><br><span class="line">            x_axis = sorted([i * step_bins <span class="keyword">for</span> i <span class="keyword">in</span> range(len(generator_losses) // step_bins)] * step_bins)</span><br><span class="line">            num_examples = (len(generator_losses) // step_bins) * step_bins</span><br><span class="line">            plt.plot(</span><br><span class="line">                range(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(generator_losses[:num_examples]).view(<span class="number">-1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">"Generator Loss"</span></span><br><span class="line">            )</span><br><span class="line">            plt.plot(</span><br><span class="line">                range(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(discriminator_losses[:num_examples]).view(<span class="number">-1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">"Discriminator Loss"</span></span><br><span class="line">            )</span><br><span class="line">            plt.legend()</span><br><span class="line">            plt.show()</span><br><span class="line">        <span class="keyword">elif</span> cur_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!"</span>)</span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h2><p>You can do a bit of exploration now!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Before you explore, you should put the generator</span></span><br><span class="line"><span class="comment"># in eval mode, both in general and so that batch norm</span></span><br><span class="line"><span class="comment"># doesn't cause you issues and is using its eval statistics</span></span><br><span class="line">gen = gen.eval()</span><br></pre></td></tr></table></figure><h4 id="Changing-the-Class-Vector"><a href="#Changing-the-Class-Vector" class="headerlink" title="Changing the Class Vector"></a>Changing the Class Vector</h4><p>You can generate some numbers with your new model! You can add interpolation as well to make it more interesting.</p><p>So starting from a image, you will produce intermediate images that look more and more like the ending image until you get to the final image. Your’re basically morphing one image into another. You can choose what these two images will be using your conditional GAN.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">n_interpolation = <span class="number">9</span> <span class="comment"># Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)</span></span><br><span class="line">interpolation_noise = get_noise(<span class="number">1</span>, z_dim, device=device).repeat(n_interpolation, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interpolate_class</span><span class="params">(first_number, second_number)</span>:</span></span><br><span class="line">    first_label = get_one_hot_labels(torch.Tensor([first_number]).long(), n_classes)</span><br><span class="line">    second_label = get_one_hot_labels(torch.Tensor([second_number]).long(), n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the interpolation vector between the two labels</span></span><br><span class="line">    percent_second_label = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, n_interpolation)[:, <span class="keyword">None</span>]</span><br><span class="line">    interpolation_labels = first_label * (<span class="number">1</span> - percent_second_label) + second_label * percent_second_label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Combine the noise and the labels</span></span><br><span class="line">    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))</span><br><span class="line">    fake = gen(noise_and_labels)</span><br><span class="line">    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">start_plot_number = <span class="number">1</span> <span class="comment"># Choose the start digit</span></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">end_plot_number = <span class="number">5</span> <span class="comment"># Choose the end digit</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">interpolate_class(start_plot_number, end_plot_number)</span><br><span class="line">_ = plt.axis(<span class="string">'off'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Uncomment the following lines of code if you would like to visualize a set of pairwise class </span></span><br><span class="line"><span class="comment">### interpolations for a collection of different numbers, all in a single grid of interpolations.</span></span><br><span class="line"><span class="comment">### You'll also see another visualization like this in the next code block!</span></span><br><span class="line"><span class="comment"># plot_numbers = [2, 3, 4, 5, 7]</span></span><br><span class="line"><span class="comment"># n_numbers = len(plot_numbers)</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(8, 8))</span></span><br><span class="line"><span class="comment"># for i, first_plot_number in enumerate(plot_numbers):</span></span><br><span class="line"><span class="comment">#     for j, second_plot_number in enumerate(plot_numbers):</span></span><br><span class="line"><span class="comment">#         plt.subplot(n_numbers, n_numbers, i * n_numbers + j + 1)</span></span><br><span class="line"><span class="comment">#         interpolate_class(first_plot_number, second_plot_number)</span></span><br><span class="line"><span class="comment">#         plt.axis('off')</span></span><br><span class="line"><span class="comment"># plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br></pre></td></tr></table></figure><p><img src="output_28_0.png" alt="png"></p><h4 id="Changing-the-Noise-Vector"><a href="#Changing-the-Noise-Vector" class="headerlink" title="Changing the Noise Vector"></a>Changing the Noise Vector</h4><p>Now, what happens if you hold the class constant, but instead you change the noise vector? You can also interpolate the noise vector and generate an image at each step.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">n_interpolation = <span class="number">9</span> <span class="comment"># How many intermediate images you want + 2 (for the start and end image)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This time you're interpolating between the noise instead of the labels</span></span><br><span class="line">interpolation_label = get_one_hot_labels(torch.Tensor([<span class="number">5</span>]).long(), n_classes).repeat(n_interpolation, <span class="number">1</span>).float()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interpolate_noise</span><span class="params">(first_noise, second_noise)</span>:</span></span><br><span class="line">    <span class="comment"># This time you're interpolating between the noise instead of the labels</span></span><br><span class="line">    percent_first_noise = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, n_interpolation)[:, <span class="keyword">None</span>].to(device)</span><br><span class="line">    interpolation_noise = first_noise * percent_first_noise + second_noise * (<span class="number">1</span> - percent_first_noise)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Combine the noise and the labels again</span></span><br><span class="line">    noise_and_labels = combine_vectors(interpolation_noise, interpolation_label.to(device))</span><br><span class="line">    fake = gen(noise_and_labels)</span><br><span class="line">    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate noise vectors to interpolate between</span></span><br><span class="line"><span class="comment">### Change me! ###</span></span><br><span class="line">n_noise = <span class="number">5</span> <span class="comment"># Choose the number of noise examples in the grid</span></span><br><span class="line">plot_noises = [get_noise(<span class="number">1</span>, z_dim, device=device) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_noise)]</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i, first_plot_noise <span class="keyword">in</span> enumerate(plot_noises):</span><br><span class="line">    <span class="keyword">for</span> j, second_plot_noise <span class="keyword">in</span> enumerate(plot_noises):</span><br><span class="line">        plt.subplot(n_noise, n_noise, i * n_noise + j + <span class="number">1</span>)</span><br><span class="line">        interpolate_noise(first_plot_noise, second_plot_noise)</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplots_adjust(top=<span class="number">1</span>, bottom=<span class="number">0</span>, left=<span class="number">0</span>, right=<span class="number">1</span>, hspace=<span class="number">0.1</span>, wspace=<span class="number">0</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure><p><img src="output_30_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Build-a-Conditional-GAN&quot;&gt;&lt;a href=&quot;#Build-a-Conditional-GAN&quot; class=&quot;headerlink&quot; title=&quot;Build a Conditional GAN&quot;&gt;&lt;/a&gt;Build a Condition
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Wasserstein GAN with Gradient Penalty (WGAN-GP)</title>
    <link href="https://zhangruochi.com/Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP/2020/10/09/"/>
    <id>https://zhangruochi.com/Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP/2020/10/09/</id>
    <published>2020-10-09T11:11:56.000Z</published>
    <updated>2020-10-09T11:12:30.465Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP"><a href="#Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP" class="headerlink" title="Wasserstein GAN with Gradient Penalty (WGAN-GP)"></a>Wasserstein GAN with Gradient Penalty (WGAN-GP)</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you’re going to build a Wasserstein GAN with Gradient Penalty (WGAN-GP) that solves some of the stability issues with the GANs that you have been using up until this point. Specifically, you’ll use a special kind of loss function known as the W-loss, where W stands for Wasserstein, and gradient penalties to prevent mode collapse.</p><p><em>Fun Fact: Wasserstein is named after a mathematician at Penn State, Leonid Vaseršteĭn. You’ll see it abbreviated to W (e.g. WGAN, W-loss, W-distance).</em></p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Get hands-on experience building a more stable GAN: Wasserstein GAN with Gradient Penalty (WGAN-GP).</li><li>Train the more advanced WGAN-GP model.</li></ol><h2 id="Generator-and-Critic"><a href="#Generator-and-Critic" class="headerlink" title="Generator and Critic"></a>Generator and Critic</h2><p>You will begin by importing some useful packages, defining visualization functions, building the generator, and building the critic. Since the changes for WGAN-GP are done to the loss function during training, you can simply reuse your previous GAN code for the generator and critic class. Remember that in WGAN-GP, you no longer use a discriminator that classifies fake and real as 0 and 1 but rather a critic that scores images with real numbers.</p><h4 id="Packages-and-Visualizations"><a href="#Packages-and-Visualizations" class="headerlink" title="Packages and Visualizations"></a>Packages and Visualizations</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu()</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_grad_hook</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function to keep track of gradients for visualization purposes, </span></span><br><span class="line"><span class="string">    which fills the grads list when using model.apply(grad_hook).</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    grads = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">grad_hook</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">            grads.append(m.weight.grad)</span><br><span class="line">    <span class="keyword">return</span> grads, grad_hook</span><br></pre></td></tr></table></figure><h4 id="Generator-and-Noise"><a href="#Generator-and-Noise" class="headerlink" title="Generator and Noise"></a>Generator and Noise</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN;</span></span><br><span class="line"><span class="string">        a transposed convolution, a batchnorm (except in the final layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor,</span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = noise.view(len(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">      n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">      z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">      device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure><h4 id="Critic"><a href="#Critic" class="headerlink" title="Critic"></a>Critic</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Critic Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Critic, self).__init__()</span><br><span class="line">        self.crit = nn.Sequential(</span><br><span class="line">            self.make_crit_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_crit_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_crit_block(hidden_dim * <span class="number">2</span>, <span class="number">1</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_crit_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a critic block of DCGAN;</span></span><br><span class="line"><span class="string">        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the critic: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_chan)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        crit_pred = self.crit(image)</span><br><span class="line">        <span class="keyword">return</span> crit_pred.view(len(crit_pred), <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Training-Initializations"><a href="#Training-Initializations" class="headerlink" title="Training Initializations"></a>Training Initializations</h2><p>Now you can start putting it all together.<br>As usual, you will start by setting the parameters:</p><ul><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>beta_1, beta_2: the momentum terms</li><li>c_lambda: weight of the gradient penalty</li><li>crit_repeats: number of times to update the critic per generator update - there are more details about this in the <em>Putting It All Together</em> section</li><li>device: the device type</li></ul><p>You will also load and transform the MNIST dataset to tensors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">100</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">50</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">beta_1 = <span class="number">0.5</span></span><br><span class="line">beta_2 = <span class="number">0.999</span></span><br><span class="line">c_lambda = <span class="number">10</span></span><br><span class="line">crit_repeats = <span class="number">5</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">'.'</span>, download=<span class="keyword">False</span>, transform=transform),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Then, you can initialize your generator, critic, and optimizers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line">crit = Critic().to(device) </span><br><span class="line">crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">gen = gen.apply(weights_init)</span><br><span class="line">crit = crit.apply(weights_init)</span><br></pre></td></tr></table></figure><h2 id="Gradient-Penalty"><a href="#Gradient-Penalty" class="headerlink" title="Gradient Penalty"></a>Gradient Penalty</h2><p>Calculating the gradient penalty can be broken into two functions: (1) compute the gradient with respect to the images and (2) compute the gradient penalty given the gradient.</p><p>You can start by getting the gradient. The gradient is computed by first creating a mixed image. This is done by weighing the fake and real image using epsilon and then adding them together. Once you have the intermediate image, you can get the critic’s output on the image. Finally, you compute the gradient of the critic score’s on the mixed images (output) with respect to the pixels of the mixed images (input). You will need to fill in the code to get the gradient wherever you see <em>None</em>. There is a test function in the next block for you to test your solution.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gradient</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gradient</span><span class="params">(crit, real, fake, epsilon)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the gradient of the critic's scores with respect to mixes of real and fake images.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        crit: the critic model</span></span><br><span class="line"><span class="string">        real: a batch of real images</span></span><br><span class="line"><span class="string">        fake: a batch of fake images</span></span><br><span class="line"><span class="string">        epsilon: a vector of the uniformly random proportions of real/fake per mixed image</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        gradient: the gradient of the critic's scores, with respect to the mixed image</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Mix the images together</span></span><br><span class="line">    mixed_images = real * epsilon + fake * (<span class="number">1</span> - epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the critic's scores on the mixed images</span></span><br><span class="line">    mixed_scores = crit(mixed_images)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Take the gradient of the scores with respect to the images</span></span><br><span class="line">    gradient = torch.autograd.grad(</span><br><span class="line">        <span class="comment"># Note: You need to take the gradient of outputs with respect to inputs.</span></span><br><span class="line">        <span class="comment"># This documentation may be useful, but it should not be necessary:</span></span><br><span class="line">        <span class="comment"># https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        inputs=mixed_images,</span><br><span class="line">        outputs=mixed_scores,</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        <span class="comment"># These other parameters have to do with the pytorch autograd engine works</span></span><br><span class="line">        grad_outputs=torch.ones_like(mixed_scores), </span><br><span class="line">        create_graph=<span class="keyword">True</span>,</span><br><span class="line">        retain_graph=<span class="keyword">True</span>,</span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> gradient</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="comment"># DO NOT MODIFY THIS</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_get_gradient</span><span class="params">(image_shape)</span>:</span></span><br><span class="line">    real = torch.randn(*image_shape, device=device) + <span class="number">1</span></span><br><span class="line">    fake = torch.randn(*image_shape, device=device) - <span class="number">1</span></span><br><span class="line">    epsilon_shape = [<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> image_shape]</span><br><span class="line">    epsilon_shape[<span class="number">0</span>] = image_shape[<span class="number">0</span>]</span><br><span class="line">    epsilon = torch.rand(epsilon_shape, device=device).requires_grad_()</span><br><span class="line">    gradient = get_gradient(crit, real, fake, epsilon)</span><br><span class="line">    <span class="keyword">assert</span> tuple(gradient.shape) == image_shape</span><br><span class="line">    <span class="keyword">assert</span> gradient.max() &gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> gradient.min() &lt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> gradient</span><br><span class="line"></span><br><span class="line">gradient = test_get_gradient((<span class="number">256</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>The second function you need to complete is to compute the gradient penalty given the gradient. First, you calculate the magnitude of each image’s gradient. The magnitude of a gradient is also called the norm. Then, you calculate the penalty by squaring the distance between each magnitude and the ideal norm of 1 and taking the mean of all the squared distances.</p><p>Again, you will need to fill in the code wherever you see <em>None</em>. There are hints below that you can view if you need help and there is a test function in the next block for you to test your solution.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">gradient_penalty</font></code></b></font></summary>1.   Make sure you take the mean at the end.2.   Note that the magnitude of each gradient has already been calculated for you.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: gradient_penalty</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_penalty</span><span class="params">(gradient)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the gradient penalty, given a gradient.</span></span><br><span class="line"><span class="string">    Given a batch of image gradients, you calculate the magnitude of each image's gradient</span></span><br><span class="line"><span class="string">    and penalize the mean quadratic distance of each magnitude to 1.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gradient: the gradient of the critic's scores, with respect to the mixed image</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        penalty: the gradient penalty</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Flatten the gradients so that each row captures one image</span></span><br><span class="line">    gradient = gradient.view(len(gradient), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the magnitude of every row</span></span><br><span class="line">    gradient_norm = gradient.norm(<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Penalize the mean squared distance of the gradient norms from 1</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    penalty = torch.mean((gradient_norm - <span class="number">1</span>)**<span class="number">2</span>)</span><br><span class="line"><span class="comment">#     print(penalty)</span></span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> penalty</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gradient_penalty</span><span class="params">(image_shape)</span>:</span></span><br><span class="line">    bad_gradient = torch.zeros(*image_shape)</span><br><span class="line">    bad_gradient_penalty = gradient_penalty(bad_gradient)</span><br><span class="line">    <span class="keyword">assert</span> torch.isclose(bad_gradient_penalty, torch.tensor(<span class="number">1.</span>))</span><br><span class="line"></span><br><span class="line">    image_size = torch.prod(torch.Tensor(image_shape[<span class="number">1</span>:]))</span><br><span class="line">    good_gradient = torch.ones(*image_shape) / torch.sqrt(image_size)</span><br><span class="line">    good_gradient_penalty = gradient_penalty(good_gradient)</span><br><span class="line">    <span class="keyword">assert</span> torch.isclose(good_gradient_penalty, torch.tensor(<span class="number">0.</span>))</span><br><span class="line"></span><br><span class="line">    random_gradient = test_get_gradient(image_shape)</span><br><span class="line">    random_gradient_penalty = gradient_penalty(random_gradient)</span><br><span class="line">    <span class="keyword">assert</span> torch.abs(random_gradient_penalty - <span class="number">1</span>) &lt; <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">test_gradient_penalty((<span class="number">256</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Losses"><a href="#Losses" class="headerlink" title="Losses"></a>Losses</h2><p>Next, you need to calculate the loss for the generator and the critic.</p><p>For the generator, the loss is calculated by maximizing the critic’s prediction on the generator’s fake images. The argument has the scores for all fake images in the batch, but you will use the mean of them.</p><p>There are optional hints below and a test function in the next block for you to test your solution.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">get_gen_loss</font></code></b></font></summary>1. This can be written in one line.2. This is the negative of the mean of the critic's scores.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span><span class="params">(crit_fake_pred)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of a generator given the critic's scores of the generator's fake images.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        crit_fake_pred: the critic's scores of the fake images</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        gen_loss: a scalar loss value for the current batch of the generator</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    gen_loss = -torch.mean(crit_fake_pred)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    get_gen_loss(torch.tensor(<span class="number">1.</span>)), torch.tensor(<span class="number">-1.0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    get_gen_loss(torch.rand(<span class="number">10000</span>)), torch.tensor(<span class="number">-0.5</span>), <span class="number">0.05</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>For the critic, the loss is calculated by maximizing the distance between the critic’s predictions on the real images and the predictions on the fake images while also adding a gradient penalty. The gradient penalty is weighed according to lambda. The arguments are the scores for all the images in the batch, and you will use the mean of them.</p><p>There are hints below if you get stuck and a test function in the next block for you to test your solution.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">get_crit_loss</font></code></b></font></summary>1. The higher the mean fake score, the higher the critic's loss is.2. What does this suggest about the mean real score?3. The higher the gradient penalty, the higher the critic's loss is, proportional to lambda.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_crit_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_crit_loss</span><span class="params">(crit_fake_pred, crit_real_pred, gp, c_lambda)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of a critic given the critic's scores for fake and real images,</span></span><br><span class="line"><span class="string">    the gradient penalty, and gradient penalty weight.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        crit_fake_pred: the critic's scores of the fake images</span></span><br><span class="line"><span class="string">        crit_real_pred: the critic's scores of the real images</span></span><br><span class="line"><span class="string">        gp: the unweighted gradient penalty</span></span><br><span class="line"><span class="string">        c_lambda: the current weight of the gradient penalty </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        crit_loss: a scalar for the critic's loss, accounting for the relevant factors</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    crit_loss =  torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> crit_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    get_crit_loss(torch.tensor(<span class="number">1.</span>), torch.tensor(<span class="number">2.</span>), torch.tensor(<span class="number">3.</span>), <span class="number">0.1</span>),</span><br><span class="line">    torch.tensor(<span class="number">-0.7</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">assert</span> torch.isclose(</span><br><span class="line">    get_crit_loss(torch.tensor(<span class="number">20.</span>), torch.tensor(<span class="number">-20.</span>), torch.tensor(<span class="number">2.</span>), <span class="number">10</span>),</span><br><span class="line">    torch.tensor(<span class="number">60.</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Putting-It-All-Together"><a href="#Putting-It-All-Together" class="headerlink" title="Putting It All Together"></a>Putting It All Together</h2><p>Before you put everything together, there are a few things to note.</p><ol><li>Even on GPU, the <strong>training will run more slowly</strong> than previous labs because the gradient penalty requires you to compute the gradient of a gradient — this means potentially a few minutes per epoch! For best results, run this for as long as you can while on GPU.</li><li>One important difference from earlier versions is that you will <strong>update the critic multiple times</strong> every time you update the generator This helps prevent the generator from overpowering the critic. Sometimes, you might see the reverse, with the generator updated more times than the critic. This depends on architectural (e.g. the depth and width of the network) and algorithmic choices (e.g. which loss you’re using). </li><li>WGAN-GP isn’t necessarily meant to improve overall performance of a GAN, but just <strong>increases stability</strong> and avoids mode collapse. In general, a WGAN will be able to train in a much more stable way than the vanilla DCGAN from last assignment, though it will generally run a bit slower. You should also be able to train your model for more epochs without it collapsing.</li></ol><!-- Once again, be warned that this runs very slowly on a CPU. One way to run this more quickly is to download the .ipynb and upload it to Google Drive, then open it with Google Colab and make the runtime type GPU and replace`device = "cpu"`with`device = "cuda"`and make sure that your `get_noise` function uses the right device.  --><p>Here is a snapshot of what your WGAN-GP outputs should resemble:<br><img src="MNIST_WGAN_Progression.png" alt="MNIST Digits Progression"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">generator_losses = []</span><br><span class="line">critic_losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line">        real = real.to(device)</span><br><span class="line"></span><br><span class="line">        mean_iteration_critic_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(crit_repeats):</span><br><span class="line">            <span class="comment">### Update critic ###</span></span><br><span class="line">            crit_opt.zero_grad()</span><br><span class="line">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">            fake = gen(fake_noise)</span><br><span class="line">            crit_fake_pred = crit(fake.detach())</span><br><span class="line">            crit_real_pred = crit(real)</span><br><span class="line"></span><br><span class="line">            epsilon = torch.rand(len(real), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, device=device, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">            gradient = get_gradient(crit, real, fake.detach(), epsilon)</span><br><span class="line">            gp = gradient_penalty(gradient)</span><br><span class="line">            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Keep track of the average critic loss in this batch</span></span><br><span class="line">            mean_iteration_critic_loss += crit_loss.item() / crit_repeats</span><br><span class="line">            <span class="comment"># Update gradients</span></span><br><span class="line">            crit_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">            <span class="comment"># Update optimizer</span></span><br><span class="line">            crit_opt.step()</span><br><span class="line">        critic_losses += [mean_iteration_critic_loss]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update generator ###</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line">        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        fake_2 = gen(fake_noise_2)</span><br><span class="line">        crit_fake_pred = crit(fake_2)</span><br><span class="line">        </span><br><span class="line">        gen_loss = get_gen_loss(crit_fake_pred)</span><br><span class="line">        gen_loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update the weights</span></span><br><span class="line">        gen_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">        generator_losses += [gen_loss.item()]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Visualization code ###</span></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            gen_mean = sum(generator_losses[-display_step:]) / display_step</span><br><span class="line">            crit_mean = sum(critic_losses[-display_step:]) / display_step</span><br><span class="line">            print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;gen_mean&#125;</span>, critic loss: <span class="subst">&#123;crit_mean&#125;</span>"</span>)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            step_bins = <span class="number">20</span></span><br><span class="line">            num_examples = (len(generator_losses) // step_bins) * step_bins</span><br><span class="line">            plt.plot(</span><br><span class="line">                range(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(generator_losses[:num_examples]).view(<span class="number">-1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">"Generator Loss"</span></span><br><span class="line">            )</span><br><span class="line">            plt.plot(</span><br><span class="line">                range(num_examples // step_bins), </span><br><span class="line">                torch.Tensor(critic_losses[:num_examples]).view(<span class="number">-1</span>, step_bins).mean(<span class="number">1</span>),</span><br><span class="line">                label=<span class="string">"Critic Loss"</span></span><br><span class="line">            )</span><br><span class="line">            plt.legend()</span><br><span class="line">            plt.show()</span><br><span class="line"></span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP&quot;&gt;&lt;a href=&quot;#Wasserstein-GAN-with-Gradient-Penalty-WGAN-GP&quot; class=&quot;headerlink&quot; title=&quot;Wa
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Deep Convolutional GAN (DCGAN)</title>
    <link href="https://zhangruochi.com/Deep-Convolutional-GAN-DCGAN/2020/10/09/"/>
    <id>https://zhangruochi.com/Deep-Convolutional-GAN-DCGAN/2020/10/09/</id>
    <published>2020-10-09T08:14:47.000Z</published>
    <updated>2020-10-09T11:38:32.317Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Deep-Convolutional-GAN-DCGAN"><a href="#Deep-Convolutional-GAN-DCGAN" class="headerlink" title="Deep Convolutional GAN (DCGAN)"></a>Deep Convolutional GAN (DCGAN)</h1><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>In this notebook, you’re going to create another GAN using the MNIST dataset. You will implement a Deep Convolutional GAN (DCGAN), a very successful and influential GAN model developed in 2015.</p><p><em>Note: <a href="https://arxiv.org/pdf/1511.06434v1.pdf" target="_blank" rel="noopener">here</a> is the paper if you are interested! It might look dense now, but soon you’ll be able to understand many parts of it :)</em></p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Get hands-on experience making a widely used GAN: Deep Convolutional GAN (DCGAN).</li><li>Train a powerful generative model.</li></ol><p><img src="dcgan-gen.png" alt="Generator architecture"></p><p>Figure: Architectural drawing of a generator from DCGAN from <a href="https://arxiv.org/pdf/1511.06434v1.pdf" target="_blank" rel="noopener">Radford et al (2016)</a>.</p><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h4 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h4><p>Here are the main features of DCGAN (don’t worry about memorizing these, you will be guided through the implementation!): </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Architecture guidelines for stable Deep Convolutional GANs</span><br><span class="line">• Replace any pooling layers with strided convolutions (discriminator) and fractional-strided</span><br><span class="line">convolutions (generator).</span><br><span class="line">• Use BatchNorm in both the generator and the discriminator.</span><br><span class="line">• Remove fully connected hidden layers for deeper architectures.</span><br><span class="line">• Use ReLU activation in generator for all layers except for the output, which uses Tanh.</span><br><span class="line">• Use LeakyReLU activation in the discriminator for all layers.</span><br></pre></td></tr></table></figure><ul><li>Use convolutions without any pooling layers</li><li>Use batchnorm in both the generator and the discriminator</li><li>Don’t use fully connected hidden layers</li><li>Use ReLU activation in the generator for all layers except for the output, which uses a Tanh activation.</li><li>Use LeakyReLU activation in the discriminator for all layers except for the output, which does not use an activation</li></ul><p>You will begin by importing some useful packages and data that will help you create your GAN. You are also provided a visualizer function to help see the images your GAN will create.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu()</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>The first component you will make is the generator. You may notice that instead of passing in the image dimension, you will pass the number of image channels to the generator. This is because with DCGAN, you use convolutions which don’t depend on the number of pixels on an image. However, the number of channels is important to determine the size of the filters.</p><p>You will build a generator using 4 layers (3 hidden layers + 1 output layer). As before, you will need to write a function to create a single block for the generator’s neural network.<br><!-- From the paper, we know to "[u]se batchnorm in both the generator and the discriminator" and "[u]se ReLU activation in generator for all layers except for the output, which uses Tanh." --><br>Since in DCGAN the activation function will be different for the output layer, you will need to check what layer is being created. You are supplied with some tests following the code cell so you can see if you’re on the right track!</p><p>At the end of the generator class, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. You are also given a function to create a noise vector. These functions are the same as the ones from the last assignment.</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">make_gen_block</font></code></b></font></summary>1. You'll find [nn.ConvTranspose2d](https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html) and [nn.BatchNorm2d](https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html) useful!</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Generator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            self.make_gen_block(z_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>),</span><br><span class="line">            self.make_gen_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            self.make_gen_block(hidden_dim, im_chan, kernel_size=<span class="number">4</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_gen_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a generator block of DCGAN, </span></span><br><span class="line"><span class="string">        corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#     Steps:</span></span><br><span class="line">        <span class="comment">#       1) Do a transposed convolution using the given parameters.</span></span><br><span class="line">        <span class="comment">#       2) Do a batchnorm, except for the last layer.</span></span><br><span class="line">        <span class="comment">#       3) Follow each batchnorm with a ReLU activation.</span></span><br><span class="line">        <span class="comment">#       4) If its the final layer, use a Tanh activation after the deconvolution.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build the neural block</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.ReLU(inplace = <span class="keyword">True</span>)</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># Final Layer</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span><br><span class="line">                nn.Tanh()</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unsqueeze_noise</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns a copy of that noise with width and height = 1 and channels = z_dim.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> noise.view(len(noise), self.z_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.unsqueeze_noise(noise)</span><br><span class="line">        <span class="keyword">return</span> self.gen(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device=device)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Test your make_gen_block() function</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">gen = Generator()</span><br><span class="line">num_test = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line">test_hidden_noise = get_noise(num_test, gen.z_dim)</span><br><span class="line">test_hidden_block = gen.make_gen_block(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line">test_uns_noise = gen.unsqueeze_noise(test_hidden_noise)</span><br><span class="line">hidden_output = test_hidden_block(test_uns_noise)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that it works with other strides</span></span><br><span class="line">test_hidden_block_stride = gen.make_gen_block(<span class="number">20</span>, <span class="number">20</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">test_final_noise = get_noise(num_test, gen.z_dim) * <span class="number">20</span></span><br><span class="line">test_final_block = gen.make_gen_block(<span class="number">10</span>, <span class="number">20</span>, final_layer=<span class="keyword">True</span>)</span><br><span class="line">test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)</span><br><span class="line">final_output = test_final_block(test_final_uns_noise)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line">test_gen_noise = get_noise(num_test, gen.z_dim)</span><br><span class="line">test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)</span><br><span class="line">gen_output = gen(test_uns_gen_noise)</span><br></pre></td></tr></table></figure><p>Here’s the test for your generator block:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TESTS</span></span><br><span class="line"><span class="keyword">assert</span> tuple(hidden_output.shape) == (num_test, <span class="number">20</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">assert</span> hidden_output.max() &gt; <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.min() == <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.2</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &lt; <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(test_hidden_block_stride(hidden_output).shape) == (num_test, <span class="number">20</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> final_output.max().item() == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> final_output.min().item() == <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(gen_output.shape) == (num_test, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="keyword">assert</span> gen_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"><span class="keyword">assert</span> gen_output.std() &lt; <span class="number">0.8</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>The second component you need to create is the discriminator.</p><p>You will use 3 layers in your discriminator’s neural network. Like with the generator, you will need create the function to create a single neural network block for the discriminator.<br>From the paper, we know that we need to “[u]se LeakyReLU activation in the discriminator for all layers.” And for the LeakyReLUs, “the slope of the leak was set to 0.2” in DCGAN.<br>There are also tests at the end for you to use.</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">make_disc_block</font></code></b></font></summary>1. You'll find [nn.Conv2d](https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html), [nn.BatchNorm2d](https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html), and [nn.LeakyReLU](https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html) useful!</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">              (MNIST is black-and-white, so 1 channel is your default)</span></span><br><span class="line"><span class="string">    hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_chan=<span class="number">1</span>, hidden_dim=<span class="number">16</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            self.make_disc_block(im_chan, hidden_dim),</span><br><span class="line">            self.make_disc_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            self.make_disc_block(hidden_dim * <span class="number">2</span>, <span class="number">1</span>, final_layer=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_disc_block</span><span class="params">(self, input_channels, output_channels, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, final_layer=False)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function to return a sequence of operations corresponding to a discriminator block of DCGAN, </span></span><br><span class="line"><span class="string">        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_channels: how many channels the input feature representation has</span></span><br><span class="line"><span class="string">            output_channels: how many channels the output feature representation should have</span></span><br><span class="line"><span class="string">            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span></span><br><span class="line"><span class="string">            stride: the stride of the convolution</span></span><br><span class="line"><span class="string">            final_layer: a boolean, true if it is the final layer and false otherwise </span></span><br><span class="line"><span class="string">                      (affects activation and batchnorm)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#     Steps:</span></span><br><span class="line">        <span class="comment">#       1) Add a convolutional layer using the given parameters.</span></span><br><span class="line">        <span class="comment">#       2) Do a batchnorm, except for the last layer.</span></span><br><span class="line">        <span class="comment">#       3) Follow each batchnorm with a LeakyReLU activation with slope 0.2.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Build the neural block</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final_layer:</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE #### #</span></span><br><span class="line">                nn.Conv2d(input_channels,output_channels,kernel_size,stride),</span><br><span class="line">                nn.BatchNorm2d(output_channels),</span><br><span class="line">                nn.LeakyReLU( negative_slope=<span class="number">0.2</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">          </span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># Final Layer</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                <span class="comment">#### START CODE HERE #### #</span></span><br><span class="line">                nn.Conv2d(input_channels,output_channels,kernel_size,stride)</span><br><span class="line">                <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        disc_pred = self.disc(image)</span><br><span class="line">        <span class="keyword">return</span> disc_pred.view(len(disc_pred), <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Test your make_disc_block() function</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">num_test = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">gen = Generator()</span><br><span class="line">disc = Discriminator()</span><br><span class="line">test_images = gen(get_noise(num_test, gen.z_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line">test_hidden_block = disc.make_disc_block(<span class="number">1</span>, <span class="number">5</span>, kernel_size=<span class="number">6</span>, stride=<span class="number">3</span>)</span><br><span class="line">hidden_output = test_hidden_block(test_images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the final block</span></span><br><span class="line">test_final_block = disc.make_disc_block(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">5</span>, final_layer=<span class="keyword">True</span>)</span><br><span class="line">final_output = test_final_block(test_images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line">disc_output = disc(test_images)</span><br></pre></td></tr></table></figure><p>Here’s a test for your discriminator block:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the hidden block</span></span><br><span class="line"><span class="keyword">assert</span> tuple(hidden_output.shape) == (num_test, <span class="number">5</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"><span class="comment"># Because of the LeakyReLU slope</span></span><br><span class="line"><span class="keyword">assert</span> -hidden_output.min() / hidden_output.max() &gt; <span class="number">0.15</span></span><br><span class="line"><span class="keyword">assert</span> -hidden_output.min() / hidden_output.max() &lt; <span class="number">0.25</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &gt; <span class="number">0.5</span></span><br><span class="line"><span class="keyword">assert</span> hidden_output.std() &lt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the final block</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(final_output.shape) == (num_test, <span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">assert</span> final_output.max() &gt; <span class="number">1.0</span></span><br><span class="line"><span class="keyword">assert</span> final_output.min() &lt; <span class="number">-1.0</span></span><br><span class="line"><span class="keyword">assert</span> final_output.std() &gt; <span class="number">0.3</span></span><br><span class="line"><span class="keyword">assert</span> final_output.std() &lt; <span class="number">0.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the whole thing:</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> tuple(disc_output.shape) == (num_test, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> disc_output.std() &gt; <span class="number">0.25</span></span><br><span class="line"><span class="keyword">assert</span> disc_output.std() &lt; <span class="number">0.5</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can put it all together!<br>Remember that these are your parameters:</p><ul><li>criterion: the loss function</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>beta_1, beta_2: the momentum term</li><li>device: the device type</li></ul><p>In addition, be warned that <strong>this runs very slowly on the default CPU</strong>. One way to run this more quickly is to download the .ipynb and upload it to Google Drive, then open it with Google Colab, click on <code>Runtime -&gt; Change runtime type</code> and set hardware accelerator to GPU and replace<br><code>device = &quot;cpu&quot;</code><br>with<br><code>device = &quot;cuda&quot;</code>. The code should then run without any more changes, over 1,000 times faster. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># A learning rate of 0.0002 works well on DCGAN</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># These parameters control the optimizer's momentum, which you can read more about here:</span></span><br><span class="line"><span class="comment"># https://distill.pub/2017/momentum/ but you don’t need to worry about it for this course!</span></span><br><span class="line">beta_1 = <span class="number">0.5</span> </span><br><span class="line">beta_2 = <span class="number">0.999</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can tranform the image values to be between -1 and 1 (the range of the tanh activation)</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">'.'</span>, download=<span class="keyword">False</span>, transform=transform),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Then, you can initialize your generator, discriminator, and optimizers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line">disc = Discriminator().to(device) </span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># You initialize the weights to the normal distribution</span></span><br><span class="line"><span class="comment"># with mean 0 and standard deviation 0.02</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">gen = gen.apply(weights_init)</span><br><span class="line">disc = disc.apply(weights_init)</span><br></pre></td></tr></table></figure><p>Finally, you can train your GAN!<br>For each epoch, you will process the entire dataset in batches. For every batch, you will update the discriminator and generator. Then, you can see DCGAN’s results!</p><p>Here’s roughly the progression you should be expecting. On GPU this takes about 30 seconds per thousand steps. On CPU, this can take about 8 hours per thousand steps. You might notice that in the image of Step 5000, the generator is disproprotionately producing things that look like ones. If the discriminator didn’t learn to detect this imbalance quickly enough, then the generator could just produce more ones. As a result, it may have ended up tricking the discriminator so well that there would be no more improvement, known as mode collapse:<br><img src="MNIST_DCGAN_Progression.png" alt="MNIST Digits Progression"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">50</span></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">mean_generator_loss = <span class="number">0</span></span><br><span class="line">mean_discriminator_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line">        real = real.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Update discriminator ##</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line">        fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        fake = gen(fake_noise)</span><br><span class="line">        disc_fake_pred = disc(fake.detach())</span><br><span class="line">        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span><br><span class="line">        disc_real_pred = disc(real)</span><br><span class="line">        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span><br><span class="line">        disc_loss = (disc_fake_loss + disc_real_loss) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        disc_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Update generator ##</span></span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line">        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">        fake_2 = gen(fake_noise_2)</span><br><span class="line">        disc_fake_pred = disc(fake_2)</span><br><span class="line">        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span><br><span class="line">        gen_loss.backward()</span><br><span class="line">        gen_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">        mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Visualization code ##</span></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            mean_generator_loss = <span class="number">0</span></span><br><span class="line">            mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Deep-Convolutional-GAN-DCGAN&quot;&gt;&lt;a href=&quot;#Deep-Convolutional-GAN-DCGAN&quot; class=&quot;headerlink&quot; title=&quot;Deep Convolutional GAN (DCGAN)&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Your First GAN</title>
    <link href="https://zhangruochi.com/Your-First-GAN/2020/10/09/"/>
    <id>https://zhangruochi.com/Your-First-GAN/2020/10/09/</id>
    <published>2020-10-09T08:09:48.000Z</published>
    <updated>2020-10-09T08:10:33.928Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Your-First-GAN"><a href="#Your-First-GAN" class="headerlink" title="Your First GAN"></a>Your First GAN</h1><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>In this notebook, you’re going to create your first generative adversarial network (GAN) for this course! Specifically, you will build and train a GAN that can generate hand-written images of digits (0-9). You will be using PyTorch in this specialization, so if you’re not familiar with this framework, you may find the <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch documentation</a> useful. The hints will also often include links to relevant documentation.</p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol><li>Build the generator and discriminator components of a GAN from scratch.</li><li>Create generator and discriminator loss functions.</li><li>Train your GAN and visualize the generated images.</li></ol><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will begin by importing some useful packages and the dataset you will use to build and train your GAN. You are also provided with a visualizer function to help you investigate the images your GAN will create.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST <span class="comment"># Training dataset</span></span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>) <span class="comment"># Set for testing purposes, please do not change!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in a uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_unflat = image_tensor.detach().cpu().view(<span class="number">-1</span>, *size)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="MNIST-Dataset"><a href="#MNIST-Dataset" class="headerlink" title="MNIST Dataset"></a>MNIST Dataset</h4><p>The training images your discriminator will be using is from a dataset called <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a>. It contains 60,000 images of handwritten digits, from 0 to 9, like these:</p><p><img src="MnistExamples.png" alt="MNIST Digits"></p><p>You may notice that the images are quite pixelated — this is because they are all only 28 x 28! The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or “color channel”, is needed to represent them (more on this later in the course).</p><h4 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h4><p>You will represent the data using <a href="https://pytorch.org/docs/stable/tensors.html" target="_blank" rel="noopener">tensors</a>. Tensors are a generalization of matrices: for example, a stack of three matrices with the amounts of red, green, and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64.</p><p>Tensors are easy to manipulate and supported by <a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch</a>, the machine learning library you will be using. Feel free to explore them more, but you can imagine these as multi-dimensional matrices or vectors!</p><h4 id="Batches"><a href="#Batches" class="headerlink" title="Batches"></a>Batches</h4><p>While you could train your model after generating one image, it is extremely inefficient and leads to less stable training. In GANs, and in machine learning in general, you will process multiple images per training step. These are called batches.</p><p>This means that your generator will generate an entire batch of images and receive the discriminator’s feedback on each before updating the model. The same goes for the discriminator, it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated.</p><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>The first step is to build the generator component.</p><p>You will start by creating a function to make a single layer/block for the generator’s neural network. Each block should include a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" target="_blank" rel="noopener">linear transformation</a> to map to another shape, a <a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html" target="_blank" rel="noopener">batch normalization</a> for stabilization, and finally a non-linear activation function (you use a <a href="https://pytorch.org/docs/master/generated/torch.nn.ReLU.html" target="_blank" rel="noopener">ReLU here</a>) so the output can be transformed in complex ways. You will learn more about activations and batch normalization later in the course.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_generator_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_generator_block</span><span class="params">(input_dim, output_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for returning a block of the generator's neural network</span></span><br><span class="line"><span class="string">    given input and output dimensions.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        a generator neural network layer, with a linear transformation </span></span><br><span class="line"><span class="string">          followed by a batch normalization and then a relu activation</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment"># Hint: Replace all of the "None" with the appropriate dimensions.</span></span><br><span class="line">        <span class="comment"># The documentation may be useful if you're less familiar with PyTorch:</span></span><br><span class="line">        <span class="comment"># https://pytorch.org/docs/stable/nn.html.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        nn.Linear(input_dim, output_dim),</span><br><span class="line">        nn.BatchNorm1d(output_dim),</span><br><span class="line">        nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the generator block function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_block</span><span class="params">(in_features, out_features, num_test=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    block = get_generator_block(in_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check the three parts</span></span><br><span class="line">    <span class="keyword">assert</span> len(block) == <span class="number">3</span></span><br><span class="line">    <span class="keyword">assert</span> type(block[<span class="number">0</span>]) == nn.Linear</span><br><span class="line">    <span class="keyword">assert</span> type(block[<span class="number">1</span>]) == nn.BatchNorm1d</span><br><span class="line">    <span class="keyword">assert</span> type(block[<span class="number">2</span>]) == nn.ReLU</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check the output shape</span></span><br><span class="line">    test_input = torch.randn(num_test, in_features)</span><br><span class="line">    test_output = block(test_input)</span><br><span class="line">    <span class="keyword">assert</span> tuple(test_output.shape) == (num_test, out_features)</span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.55</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.65</span></span><br><span class="line"></span><br><span class="line">test_gen_block(<span class="number">25</span>, <span class="number">12</span>)</span><br><span class="line">test_gen_block(<span class="number">15</span>, <span class="number">28</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Now you can build the generator class. It will take 3 values:</p><ul><li>The noise vector dimension</li><li>The image dimension</li><li>The initial hidden dimension</li></ul><p>Using these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a <a href="https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html" target="_blank" rel="noopener">sigmoid function</a>. </p><p>Finally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network.</p><details><summary><font size="3" color="green"><b>Optional hints for <code><font size="4">Generator</font></code></b></font></summary>1. The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function.2. [nn.Linear](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) and [nn.Sigmoid](https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html) will be useful here. </details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Generator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">          (MNIST images are 28 x 28 = 784 so that is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, z_dim=<span class="number">10</span>, im_dim=<span class="number">784</span>, hidden_dim=<span class="number">128</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        <span class="comment"># Build the neural network</span></span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            get_generator_block(z_dim, hidden_dim),</span><br><span class="line">            get_generator_block(hidden_dim, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            get_generator_block(hidden_dim * <span class="number">2</span>, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            get_generator_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">8</span>),</span><br><span class="line">            <span class="comment"># There is a dropdown with hints if you need them! </span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            nn.Linear(hidden_dim * <span class="number">8</span>, im_dim),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, noise)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class="line"><span class="string">        returns generated images.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.gen(noise)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Needed for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_gen</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the sequential model</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.gen</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the generator class</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_generator</span><span class="params">(z_dim, im_dim, hidden_dim, num_test=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check there are six modules in the sequential part</span></span><br><span class="line">    <span class="keyword">assert</span> len(gen) == <span class="number">6</span></span><br><span class="line">    test_input = torch.randn(num_test, z_dim)</span><br><span class="line">    test_output = gen(test_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check that the output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(test_output.shape) == (num_test, im_dim)</span><br><span class="line">    <span class="keyword">assert</span> test_output.max() &lt; <span class="number">1</span>, <span class="string">"Make sure to use a sigmoid"</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.min() &gt; <span class="number">0</span>, <span class="string">"Make sure to use a sigmoid"</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.05</span>, <span class="string">"Don't use batchnorm here"</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.15</span>, <span class="string">"Don't use batchnorm here"</span></span><br><span class="line"></span><br><span class="line">test_generator(<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">test_generator(<span class="number">20</span>, <span class="number">8</span>, <span class="number">24</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Noise"><a href="#Noise" class="headerlink" title="Noise"></a>Noise</h2><p>To be able to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don’t all look the same — think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.</p><p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. <code>torch.ones(3, 3, device=device)</code>, or move it onto the target device using <code>torch.ones(3, 3).to(device)</code>. You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as <code>torch.ones_like</code>. In general, use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code> where possible.</p><details><summary><font size="3" color="green"><b>Optional hint for <code><font size="4">get_noise</font></code></b></font></summary>1. You will probably find [torch.randn](https://pytorch.org/docs/master/generated/torch.randn.html) useful here.</details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_noise</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),</span></span><br><span class="line"><span class="string">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        n_samples: the number of samples to generate, a scalar</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> To use this on GPU with device='cuda', make sure to pass the device </span></span><br><span class="line">    <span class="comment"># argument to the function you use to generate the noise.</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn(n_samples, z_dim, device = device)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the noise vector function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_get_noise</span><span class="params">(n_samples, z_dim, device=<span class="string">'cpu'</span>)</span>:</span></span><br><span class="line">    noise = get_noise(n_samples, z_dim, device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make sure a normal distribution was used</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(noise.shape) == (n_samples, z_dim)</span><br><span class="line">    <span class="keyword">assert</span> torch.abs(noise.std() - torch.tensor(<span class="number">1.0</span>)) &lt; <span class="number">0.01</span></span><br><span class="line">    <span class="keyword">assert</span> str(noise.device).startswith(device)</span><br><span class="line"></span><br><span class="line">test_get_noise(<span class="number">1000</span>, <span class="number">100</span>, <span class="string">'cpu'</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    test_get_noise(<span class="number">1000</span>, <span class="number">32</span>, <span class="string">'cuda'</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>The second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.</p><p><em>Note: You use leaky ReLUs to prevent the “dying ReLU” problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures!</em> </p><div class="table-container"><table><thead><tr><th style="text-align:center">REctified Linear Unit (ReLU)</th><th style="text-align:center">Leaky ReLU</th></tr></thead><tbody><tr><td style="text-align:center"><img src="relu-graph.png" alt></td><td style="text-align:center"><img src="lrelu-graph.png" alt></td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_discriminator_block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_discriminator_block</span><span class="params">(input_dim, output_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Block</span></span><br><span class="line"><span class="string">    Function for returning a neural network of the discriminator given input and output dimensions.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class="line"><span class="string">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        a discriminator neural network layer, with a linear transformation </span></span><br><span class="line"><span class="string">          followed by an nn.LeakyReLU activation with negative slope of 0.2 </span></span><br><span class="line"><span class="string">          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        nn.Linear(input_dim,output_dim),</span><br><span class="line">        nn.LeakyReLU(negative_slope = <span class="number">0.2</span>, inplace = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the discriminator block function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_block</span><span class="params">(in_features, out_features, num_test=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    block = get_discriminator_block(in_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check there are two parts</span></span><br><span class="line">    <span class="keyword">assert</span> len(block) == <span class="number">2</span></span><br><span class="line">    test_input = torch.randn(num_test, in_features)</span><br><span class="line">    test_output = block(test_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check that the shape is right</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(test_output.shape) == (num_test, out_features)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check that the LeakyReLU slope is about 0.2</span></span><br><span class="line">    <span class="keyword">assert</span> -test_output.min() / test_output.max() &gt; <span class="number">0.1</span></span><br><span class="line">    <span class="keyword">assert</span> -test_output.min() / test_output.max() &lt; <span class="number">0.3</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &gt; <span class="number">0.3</span></span><br><span class="line">    <span class="keyword">assert</span> test_output.std() &lt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">test_disc_block(<span class="number">25</span>, <span class="number">12</span>)</span><br><span class="line">test_disc_block(<span class="number">15</span>, <span class="number">28</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Now you can use these blocks to make a discriminator! The discriminator class holds 2 values:</p><ul><li>The image dimension</li><li>The hidden dimension</li></ul><p>The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator’s neural network you are given a forward pass function that takes in an image tensor to be classified.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class="line"><span class="string">            (MNIST images are 28x28 = 784 so that is your default)</span></span><br><span class="line"><span class="string">        hidden_dim: the inner dimension, a scalar</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, im_dim=<span class="number">784</span>, hidden_dim=<span class="number">128</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            get_discriminator_block(im_dim, hidden_dim * <span class="number">4</span>),</span><br><span class="line">            get_discriminator_block(hidden_dim * <span class="number">4</span>, hidden_dim * <span class="number">2</span>),</span><br><span class="line">            get_discriminator_block(hidden_dim * <span class="number">2</span>, hidden_dim),</span><br><span class="line">            <span class="comment"># Hint: You want to transform the final output into a single value,</span></span><br><span class="line">            <span class="comment">#       so add one more linear map.</span></span><br><span class="line">            <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">            nn.Linear(hidden_dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class="line"><span class="string">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            image: a flattened image tensor with dimension (im_dim)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.disc(image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Needed for grading</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_disc</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the sequential model</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.disc</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verify the discriminator class</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_discriminator</span><span class="params">(z_dim, hidden_dim, num_test=<span class="number">100</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    disc = Discriminator(z_dim, hidden_dim).get_disc()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check there are three parts</span></span><br><span class="line">    <span class="keyword">assert</span> len(disc) == <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check the linear layer is correct</span></span><br><span class="line">    test_input = torch.randn(num_test, z_dim)</span><br><span class="line">    test_output = disc(test_input)</span><br><span class="line">    <span class="keyword">assert</span> tuple(test_output.shape) == (num_test, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make sure there's no sigmoid</span></span><br><span class="line">    <span class="keyword">assert</span> test_input.max() &gt; <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> test_input.min() &lt; <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">test_discriminator(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">test_discriminator(<span class="number">20</span>, <span class="number">8</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Now you can put it all together!<br>First, you will set your parameters:</p><ul><li>criterion: the loss function</li><li>n_epochs: the number of times you iterate through the entire dataset when training</li><li>z_dim: the dimension of the noise vector</li><li>display_step: how often to display/visualize the images</li><li>batch_size: the number of images per forward/backward pass</li><li>lr: the learning rate</li><li>device: the device type, here using a GPU (which runs CUDA), not CPU</li></ul><p>Next, you will load the MNIST dataset as tensors using a dataloader.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set your parameters</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">n_epochs = <span class="number">200</span></span><br><span class="line">z_dim = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.00001</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line"><span class="comment"># Load MNIST dataset as tensors</span></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    MNIST(<span class="string">'.'</span>, download=<span class="keyword">False</span>, transform=transforms.ToTensor()),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Now, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(z_dim).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">disc = Discriminator().to(device) </span><br><span class="line">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br></pre></td></tr></table></figure><p>Before you train your GAN, you will need to create functions to calculate the discriminator’s loss and the generator’s loss. This is how the discriminator and generator will know how they are doing and improve themselves. Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</p><p>Remember that you have already defined a loss function earlier (<code>criterion</code>) and you are encouraged to use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code>. If you use <code>torch.ones</code> or <code>torch.zeros</code>, you’ll need to pass <code>device=device</code> to them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_disc_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_disc_loss</span><span class="params">(gen, disc, criterion, real, num_images, z_dim, device)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the discriminator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class="line"><span class="string">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class="line"><span class="string">        criterion: the loss function, which should be used to compare </span></span><br><span class="line"><span class="string">               the discriminator's predictions to the ground truth reality of the images </span></span><br><span class="line"><span class="string">               (e.g. fake = 0, real = 1)</span></span><br><span class="line"><span class="string">        real: a batch of real images</span></span><br><span class="line"><span class="string">        num_images: the number of images the generator should produce, </span></span><br><span class="line"><span class="string">                which is also the length of the real images</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        disc_loss: a torch scalar loss value for the current batch</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">    <span class="comment">#       1) Create noise vectors and generate a batch (num_images) of fake images. </span></span><br><span class="line">    <span class="comment">#            Make sure to pass the device argument to the noise.</span></span><br><span class="line">    <span class="comment">#       2) Get the discriminator's prediction of the fake image </span></span><br><span class="line">    <span class="comment">#            and calculate the loss. Don't forget to detach the generator!</span></span><br><span class="line">    <span class="comment">#            (Remember the loss function you set earlier -- criterion. You need a </span></span><br><span class="line">    <span class="comment">#            'ground truth' tensor in order to calculate the loss. </span></span><br><span class="line">    <span class="comment">#            For example, a ground truth tensor for a fake image is all zeros.)</span></span><br><span class="line">    <span class="comment">#       3) Get the discriminator's prediction of the real image and calculate the loss.</span></span><br><span class="line">    <span class="comment">#       4) Calculate the discriminator's loss by averaging the real and fake loss</span></span><br><span class="line">    <span class="comment">#            and set it to disc_loss.</span></span><br><span class="line">    <span class="comment">#     Note: Please do not use concatenation in your solution. The tests are being updated to </span></span><br><span class="line">    <span class="comment">#           support this, but for now, average the two losses as described in step (4).</span></span><br><span class="line">    <span class="comment">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    fake_images = gen(get_noise(num_images, z_dim, device=device))</span><br><span class="line">    fake_images.detach_()  </span><br><span class="line">    fake_loss = criterion(disc(fake_images),torch.zeros((num_images,<span class="number">1</span>),device = device))</span><br><span class="line">    real_loss = criterion(disc(real),torch.ones((num_images,<span class="number">1</span>),device = device))</span><br><span class="line">    disc_loss = ( fake_loss + real_loss ) / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> disc_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_reasonable</span><span class="params">(num_images=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Don't use explicit casts to cuda - use the device argument</span></span><br><span class="line">    <span class="keyword">import</span> inspect, re</span><br><span class="line">    lines = inspect.getsource(get_disc_loss)</span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r"to\(.cuda.\)"</span>, lines)) <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r"\.cuda\(\)"</span>, lines)) <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = torch.zeros_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.ones(num_images, <span class="number">1</span>)</span><br><span class="line">    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">assert</span> tuple(disc_loss.shape) == (num_images, z_dim)</span><br><span class="line">    <span class="keyword">assert</span> torch.all(torch.abs(disc_loss - <span class="number">0.5</span>) &lt; <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">    gen = torch.ones_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.zeros(num_images, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class="string">'cpu'</span>)) &lt; <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_disc_loss</span><span class="params">(max_tests = <span class="number">10</span>)</span>:</span></span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = Generator(z_dim).to(device)</span><br><span class="line">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">    disc = Discriminator().to(device) </span><br><span class="line">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line">    num_steps = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> dataloader:</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line">        real = real.view(cur_batch_size, <span class="number">-1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate discriminator loss</span></span><br><span class="line">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class="line">        <span class="keyword">assert</span> (disc_loss - <span class="number">0.68</span>).abs() &lt; <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check that they detached correctly</span></span><br><span class="line">        <span class="keyword">assert</span> gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.grad <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        old_weight = disc.disc[<span class="number">0</span>][<span class="number">0</span>].weight.data.clone()</span><br><span class="line">        disc_opt.step()</span><br><span class="line">        new_weight = disc.disc[<span class="number">0</span>][<span class="number">0</span>].weight.data</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check that some discriminator weights changed</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.all(torch.eq(old_weight, new_weight))</span><br><span class="line">        num_steps += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> num_steps &gt;= max_tests:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">test_disc_reasonable()</span><br><span class="line">test_disc_loss()</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span><span class="params">(gen, disc, criterion, num_images, z_dim, device)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the generator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class="line"><span class="string">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class="line"><span class="string">        criterion: the loss function, which should be used to compare </span></span><br><span class="line"><span class="string">               the discriminator's predictions to the ground truth reality of the images </span></span><br><span class="line"><span class="string">               (e.g. fake = 0, real = 1)</span></span><br><span class="line"><span class="string">        num_images: the number of images the generator should produce, </span></span><br><span class="line"><span class="string">                which is also the length of the real images</span></span><br><span class="line"><span class="string">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class="line"><span class="string">        device: the device type</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        gen_loss: a torch scalar loss value for the current batch</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">    <span class="comment">#       1) Create noise vectors and generate a batch of fake images. </span></span><br><span class="line">    <span class="comment">#           Remember to pass the device argument to the get_noise function.</span></span><br><span class="line">    <span class="comment">#       2) Get the discriminator's prediction of the fake image.</span></span><br><span class="line">    <span class="comment">#       3) Calculate the generator's loss. Remember the generator wants</span></span><br><span class="line">    <span class="comment">#          the discriminator to think that its fake images are real</span></span><br><span class="line">    <span class="comment">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    noises = get_noise(num_images,z_dim, device = device)</span><br><span class="line">    fake_images = gen(noises)</span><br><span class="line">    out = disc(fake_images)</span><br><span class="line">    gen_loss = criterion(out, torch.ones(num_images, <span class="number">1</span>).to(device))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_reasonable</span><span class="params">(num_images=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Don't use explicit casts to cuda - use the device argument</span></span><br><span class="line">    <span class="keyword">import</span> inspect, re</span><br><span class="line">    lines = inspect.getsource(get_gen_loss)</span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r"to\(.cuda.\)"</span>, lines)) <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">assert</span> (re.search(<span class="string">r"\.cuda\(\)"</span>, lines)) <span class="keyword">is</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = torch.zeros_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.all(torch.abs(gen_loss_tensor) &lt; <span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">#Verify shape. Related to gen_noise parametrization</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class="line"></span><br><span class="line">    gen = torch.ones_like</span><br><span class="line">    disc = nn.Identity()</span><br><span class="line">    criterion = torch.mul <span class="comment"># Multiply</span></span><br><span class="line">    real = torch.zeros(num_images, <span class="number">1</span>)</span><br><span class="line">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class="string">'cpu'</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.all(torch.abs(gen_loss_tensor - <span class="number">1</span>) &lt; <span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">#Verify shape. Related to gen_noise parametrization</span></span><br><span class="line">    <span class="keyword">assert</span> tuple(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gen_loss</span><span class="params">(num_images)</span>:</span></span><br><span class="line">    z_dim = <span class="number">64</span></span><br><span class="line">    gen = Generator(z_dim).to(device)</span><br><span class="line">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line">    disc = Discriminator().to(device) </span><br><span class="line">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class="line">    </span><br><span class="line">    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check that the loss is reasonable</span></span><br><span class="line">    <span class="keyword">assert</span> (gen_loss - <span class="number">0.7</span>).abs() &lt; <span class="number">0.1</span></span><br><span class="line">    gen_loss.backward()</span><br><span class="line">    old_weight = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.clone()</span><br><span class="line">    gen_opt.step()</span><br><span class="line">    new_weight = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight</span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> torch.all(torch.eq(old_weight, new_weight))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test_gen_reasonable(<span class="number">10</span>)</span><br><span class="line">test_gen_loss(<span class="number">18</span>)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Success!</code></pre><p>Finally, you can put everything together! For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. </p><p>It’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It’s important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.</p><p>After you’ve submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.</p><!-- In addition, be warned that this runs very slowly on a CPU. One way to run this more quickly is to use Google Colab: 1.   Download the .ipynb2.   Upload it to Google Drive and open it with Google Colab3.   Make the runtime type GPU (under “Runtime” -> “Change runtime type” -> Select “GPU” from the dropdown)4.   Replace `device = "cpu"` with `device = "cuda"`5.   Make sure your `get_noise` function uses the right device --><p>But remember, don’t expect anything spectacular: this is only the first lesson. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels.</p><p>You should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes:<br><img src="MNIST_Progression.png" alt="MNIST Digits"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: </span></span><br><span class="line"></span><br><span class="line">cur_step = <span class="number">0</span></span><br><span class="line">mean_generator_loss = <span class="number">0</span></span><br><span class="line">mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">test_generator = <span class="keyword">True</span> <span class="comment"># Whether the generator should be tested</span></span><br><span class="line">gen_loss = <span class="keyword">False</span></span><br><span class="line">error = <span class="keyword">False</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">    <span class="keyword">for</span> real, _ <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        cur_batch_size = len(real)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Flatten the batch of real images from the dataset</span></span><br><span class="line">        real = real.view(cur_batch_size, <span class="number">-1</span>).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update discriminator ###</span></span><br><span class="line">        <span class="comment"># Zero out the gradients before backpropagation</span></span><br><span class="line">        disc_opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate discriminator loss</span></span><br><span class="line">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update gradients</span></span><br><span class="line">        disc_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update optimizer</span></span><br><span class="line">        disc_opt.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For testing purposes, to keep track of the generator weights</span></span><br><span class="line">        <span class="keyword">if</span> test_generator:</span><br><span class="line">            old_generator_weights = gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.detach().clone()</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Update generator ###</span></span><br><span class="line">        <span class="comment">#     Hint: This code will look a lot like the discriminator updates!</span></span><br><span class="line">        <span class="comment">#     These are the steps you will need to complete:</span></span><br><span class="line">        <span class="comment">#       1) Zero out the gradients.</span></span><br><span class="line">        <span class="comment">#       2) Calculate the generator loss, assigning it to gen_loss.</span></span><br><span class="line">        <span class="comment">#       3) Backprop through the generator: update the gradients and optimizer.</span></span><br><span class="line">        <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">        </span><br><span class="line">        gen_opt.zero_grad()</span><br><span class="line">        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)</span><br><span class="line">        gen_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">        gen_opt.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#### END CODE HERE ####</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># For testing purposes, to check that your code changes the generator weights</span></span><br><span class="line">        <span class="keyword">if</span> test_generator:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">assert</span> lr &gt; <span class="number">0.0000002</span> <span class="keyword">or</span> (gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.grad.abs().max() &lt; <span class="number">0.0005</span> <span class="keyword">and</span> epoch == <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">assert</span> torch.any(gen.gen[<span class="number">0</span>][<span class="number">0</span>].weight.detach().clone() != old_generator_weights)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                error = <span class="keyword">True</span></span><br><span class="line">                print(<span class="string">"Runtime tests have failed"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">        mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">        mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Visualization code ###</span></span><br><span class="line">        <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span> <span class="keyword">and</span> cur_step &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"Step <span class="subst">&#123;cur_step&#125;</span>: Generator loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class="line">            fake = gen(fake_noise)</span><br><span class="line">            show_tensor_images(fake)</span><br><span class="line">            show_tensor_images(real)</span><br><span class="line">            mean_generator_loss = <span class="number">0</span></span><br><span class="line">            mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">        cur_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Your-First-GAN&quot;&gt;&lt;a href=&quot;#Your-First-GAN&quot; class=&quot;headerlink&quot; title=&quot;Your First GAN&quot;&gt;&lt;/a&gt;Your First GAN&lt;/h1&gt;&lt;h3 id=&quot;Goal&quot;&gt;&lt;a href=&quot;#G
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Generative Adversarial Network" scheme="https://zhangruochi.com/tags/Generative-Adversarial-Network/"/>
    
  </entry>
  
  <entry>
    <title>Dyna-Q and Dyna-Q+</title>
    <link href="https://zhangruochi.com/Dyna-Q-and-Dyna-Q/2020/09/30/"/>
    <id>https://zhangruochi.com/Dyna-Q-and-Dyna-Q/2020/09/30/</id>
    <published>2020-09-30T08:50:06.000Z</published>
    <updated>2020-09-30T08:50:40.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-Dyna-Q-and-Dyna-Q"><a href="#Assignment-Dyna-Q-and-Dyna-Q" class="headerlink" title="Assignment: Dyna-Q and Dyna-Q+"></a>Assignment: Dyna-Q and Dyna-Q+</h1><p>Welcome to this programming assignment! In this notebook, you will:</p><ol><li>implement the Dyna-Q and Dyna-Q+ algorithms. </li><li>compare their performance on an environment which changes to become ‘better’ than it was before, that is, the task becomes easier. </li></ol><p>We will give you the environment and infrastructure to run the experiment and visualize the performance. The assignment will be graded automatically by comparing the behavior of your agent to our implementations of the algorithms. The random seed will be set explicitly to avoid different behaviors due to randomness. </p><p>Please go through the cells in order. </p><h2 id="The-Shortcut-Maze-Environment"><a href="#The-Shortcut-Maze-Environment" class="headerlink" title="The Shortcut Maze Environment"></a>The Shortcut Maze Environment</h2><p>In this maze environment, the goal is to reach the goal state (G) as fast as possible from the starting state (S). There are four actions â€“ up, down, right, left â€“ which take the agent deterministically from a state to the corresponding neighboring states, except when movement is blocked by a wall (denoted by grey) or the edge of the maze, in which case the agent remains where it is. The reward is +1 on reaching the goal state, 0 otherwise. On reaching the goal state G, the agent returns to the start state S to being a new episode. This is a discounted, episodic task with $\gamma = 0.95$.</p><p><img src="shortcut_env.png" alt="environment" width="400"></p><p>Later in the assignment, we will use a variant of this maze in which a ‘shortcut’ opens up after a certain number of timesteps. We will test if the the Dyna-Q and Dyna-Q+ agents are able to find the newly-opened shorter route to the goal state.</p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>We import the following libraries that are required for this assignment. Primarily, we shall be using the following libraries:</p><ol><li>numpy: the fundamental package for scientific computing with Python.</li><li>matplotlib: the library for plotting graphs in Python.</li><li>RL-Glue: the library for reinforcement learning experiments.</li></ol><p><strong>Please do not import other libraries</strong> as this will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> ShortcutMazeEnvironment</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">'font.size'</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">'figure.figsize'</span>: [<span class="number">8</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure><h2 id="Section-1-Dyna-Q"><a href="#Section-1-Dyna-Q" class="headerlink" title="Section 1: Dyna-Q"></a>Section 1: Dyna-Q</h2><p>Let’s start with a quick recap of the tabular Dyna-Q algorithm.</p><div style="width:80%"><img src="DynaQ.png" alt="DynaQ_pseudocode"></div><p>Dyna-Q involves four basic steps:</p><ol><li>Action selection: given an observation, select an action to be performed (here, using the $\epsilon$-greedy method).</li><li>Direct RL: using the observed next state and reward, update the action values (here, using one-step tabular Q-learning).</li><li>Model learning: using the observed next state and reward, update the model (here, updating a table as the environment is assumed to be deterministic).</li><li>Planning: update the action values by generating $n$ simulated experiences using certain starting states and actions (here, using the random-sample one-step tabular Q-planning method). This is also known as the ‘Indirect RL’ step. The process of choosing the state and action to simulate an experience with is known as ‘search control’.</li></ol><p>Steps 1 and 2 are parts of the <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=153" target="_blank" rel="noopener">tabular Q-learning algorithm</a> and are denoted by line numbers (a)â€“(d) in the pseudocode above. Step 3 is performed in line (e), and Step 4 in the block of lines (f).</p><p>We highly recommend revising the Dyna videos in the course and the material in the RL textbook (in particular, <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=183" target="_blank" rel="noopener">Section 8.2</a>).</p><p>Alright, let’s begin coding.</p><p>As you already know by now, you will develop an agent which interacts with the given environment via RL-Glue. More specifically, you will implement the usual methods <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code> in your <code>DynaQAgent</code> class, along with a couple of helper methods specific to Dyna-Q, namely <code>update_model</code> and <code>planning_step</code>. We will provide detailed comments in each method describing what your code should do. </p><p>Let’s break this down in pieces and do it one-by-one.</p><p>First of all, check out the <code>agent_init</code> method below. As in earlier assignments, some of the attributes are initialized with the data passed inside <code>agent_info</code>. In particular, pay attention to the attributes which are new to <code>DynaQAgent</code>, since you shall be using them later. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynaQAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                num_states (int): The number of states,</span></span><br><span class="line"><span class="string">                num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">                epsilon (float): The parameter for epsilon-greedy exploration,</span></span><br><span class="line"><span class="string">                step_size (float): The step-size,</span></span><br><span class="line"><span class="string">                discount (float): The discount factor,</span></span><br><span class="line"><span class="string">                planning_steps (int): The number of planning steps per environmental interaction</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                random_seed (int): the seed for the RNG used in epsilon-greedy</span></span><br><span class="line"><span class="string">                planning_random_seed (int): the seed for the RNG used in the planner</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, we get the relevant information from agent_info </span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> we use np.random.RandomState(seed) to set the two different RNGs</span></span><br><span class="line">        <span class="comment"># for the planner and the rest of the code</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.num_states = agent_info[<span class="string">"num_states"</span>]</span><br><span class="line">            self.num_actions = agent_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">"You need to pass both 'num_states' and 'num_actions' \</span></span><br><span class="line"><span class="string">                   in agent_info to initialize the action-value table"</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">"discount"</span>, <span class="number">0.95</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">"step_size"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">"epsilon"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.planning_steps = agent_info.get(<span class="string">"planning_steps"</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">'random_seed'</span>, <span class="number">42</span>))</span><br><span class="line">        self.planning_rand_generator = np.random.RandomState(agent_info.get(<span class="string">'planning_random_seed'</span>, <span class="number">42</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, we initialize the attributes required by the agent, e.g., q_values, model, etc.</span></span><br><span class="line">        <span class="comment"># A simple way to implement the model is to have a dictionary of dictionaries, </span></span><br><span class="line">        <span class="comment">#        mapping each state to a dictionary which maps actions to (reward, next state) tuples.</span></span><br><span class="line">        self.q_values = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.actions = list(range(self.num_actions))</span><br><span class="line">        self.past_action = <span class="number">-1</span></span><br><span class="line">        self.past_state = <span class="number">-1</span></span><br><span class="line">        self.model = &#123;&#125; <span class="comment"># model is a dictionary of dictionaries, which maps states to actions to </span></span><br><span class="line">                        <span class="comment"># (reward, next_state) tuples</span></span><br></pre></td></tr></table></figure><p>Now let’s create the <code>update_model</code> method, which performs the ‘Model Update’ step in the pseudocode. It takes a <code>(s, a, s&#39;, r)</code> tuple and stores the next state and reward corresponding to a state-action pair.</p><p>Remember, because the environment is deterministic, an easy way to implement the model is to have a dictionary of encountered states, each mapping to a dictionary of actions taken in those states, which in turn maps to a tuple of next state and reward. In this way, the model can be easily accessed by <code>model[s][a]</code>, which would return the <code>(s&#39;, r)</code> tuple.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_model</span><span class="params">(self, past_state, past_action, state, reward)</span>:</span></span><br><span class="line">    <span class="string">"""updates the model </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        past_state       (int): s</span></span><br><span class="line"><span class="string">        past_action      (int): a</span></span><br><span class="line"><span class="string">        state            (int): s'</span></span><br><span class="line"><span class="string">        reward           (int): r</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Update the model with the (s,a,s',r) tuple (1~4 lines)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> past_state <span class="keyword">in</span> self.model:</span><br><span class="line">        self.model[past_state] = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> past_action <span class="keyword">in</span> self.model[past_state]:</span><br><span class="line">        self.model[past_state][past_action] = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    self.model[past_state][past_action] = (state,reward)</span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-update-model"><a href="#Test-update-model" class="headerlink" title="Test update_model()"></a>Test <code>update_model()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (past_state, past_action, state, reward)</span></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="comment"># action 2 in state 0 leads back to state 0 with a reward of 1</span></span><br><span class="line">    <span class="comment"># or taking action 3 leads to state 1 with reward of 2</span></span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="comment"># taking action 0 in state 2 leads to state 1 with a reward of 1</span></span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><p>Next, you will implement the planning step, the crux of the Dyna-Q algorithm. You shall be calling this <code>planning_step</code> method at every timestep of every trajectory.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">planning_step</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""performs planning, i.e. indirect RL.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The indirect RL step:</span></span><br><span class="line">    <span class="comment"># - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Query the model with this state-action pair for the predicted next state and reward.(~1 line)</span></span><br><span class="line">    <span class="comment"># - Update the action values with this simulated experience.                            (2~4 lines)</span></span><br><span class="line">    <span class="comment"># - Repeat for the required number of planning steps.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the update equation is different for terminal and non-terminal transitions. </span></span><br><span class="line">    <span class="comment"># To differentiate between a terminal and a non-terminal next state, assume that the model stores</span></span><br><span class="line">    <span class="comment"># the terminal state as a dummy state like -1</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Important: remember you have a random number generator 'planning_rand_generator' as </span></span><br><span class="line">    <span class="comment">#     a part of the class which you need to use as self.planning_rand_generator.choice()</span></span><br><span class="line">    <span class="comment">#     For the sake of reproducibility and grading, *do not* use anything else like </span></span><br><span class="line">    <span class="comment">#     np.random.choice() for performing search control.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.planning_steps):</span><br><span class="line">    </span><br><span class="line">        s = self.planning_rand_generator.choice(list(self.model.keys()))</span><br><span class="line">        a = self.planning_rand_generator.choice(list(self.model[s].keys()))</span><br><span class="line"></span><br><span class="line">        (next_s,r) = self.model[s][a]</span><br><span class="line">        <span class="keyword">if</span> next_s == <span class="number">-1</span>:</span><br><span class="line">            expect = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            expect = self.gamma * np.max(self.q_values[next_s])</span><br><span class="line">        self.q_values[s,a] += self.step_size * (r + expect - self.q_values[s,a])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-planning-step"><a href="#Test-planning-step" class="headerlink" title="Test planning_step()"></a>Test <code>planning_step()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">2</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">agent.planning_step()</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q_values, expected_values))</span><br></pre></td></tr></table></figure><p>Now before you move on to implement the rest of the agent methods, here are the helper functions that you’ve used in the previous assessments for choosing an action using an $\epsilon$-greedy policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">    <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q_values (Numpy array): the array of action values</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        action (int): an action with the highest value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    top = float(<span class="string">"-inf"</span>)</span><br><span class="line">    ties = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">        <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">            top = q_values[i]</span><br><span class="line">            ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">            ties.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action_egreedy</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""returns an action using an epsilon-greedy policy w.r.t. the current action-value function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Important: assume you have a random number generator 'rand_generator' as a part of the class</span></span><br><span class="line"><span class="string">                which you can use as self.rand_generator.choice() or self.rand_generator.rand()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (List): coordinates of the agent (two elements)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action taken w.r.t. the aforementioned epsilon-greedy policy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">        action = self.rand_generator.choice(self.actions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        values = self.q_values[state]</span><br><span class="line">        action = self.argmax(values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><p>Next, you will implement the rest of the agent-related methods, namely <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the experiment starts, </span></span><br><span class="line"><span class="string">    called after the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) the first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># given the state, select the action using self.choose_action_egreedy()), </span></span><br><span class="line">    <span class="comment"># and save current state and action (~2 lines)</span></span><br><span class="line">    <span class="comment">### self.past_state = ?</span></span><br><span class="line">    <span class="comment">### self.past_action = ?</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = self.choose_action_egreedy(state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">            last step</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The action the agent takes given this state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># - Direct-RL step (~1-3 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step (~1 line)</span></span><br><span class="line">    <span class="comment"># - `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment"># - Action Selection step (~1 line)</span></span><br><span class="line">    <span class="comment"># Save the current state and action before returning the action to be performed. (~2 lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward + self.gamma * np.max(self.q_values[state]) - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, state, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    action = self.choose_action_egreedy(state)</span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = action</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Called when the agent terminates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">            terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># - Direct RL update with this final transition (1~2 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step with this final transition (~1 line)</span></span><br><span class="line">    <span class="comment"># - One final `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: the final transition needs to be handled carefully. Since there is no next state, </span></span><br><span class="line">    <span class="comment">#       you will have to pass a dummy state (like -1), which you will be using in the planning_step() to </span></span><br><span class="line">    <span class="comment">#       differentiate between updates with usual terminal and non-terminal transitions.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward  - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, <span class="number">-1</span>, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-agent-start-agent-step-and-agent-end"><a href="#Test-agent-start-agent-step-and-agent-end" class="headerlink" title="Test agent_start(), agent_step(), and agent_end()"></a>Test <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">2</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ----------------</span></span><br><span class="line"><span class="comment"># test agent start</span></span><br><span class="line"><span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.model == &#123;&#125;</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q_values == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.3439</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">1</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.41051</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.01</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br></pre></td></tr></table></figure><h3 id="Experiment-Dyna-Q-agent-in-the-maze-environment"><a href="#Experiment-Dyna-Q-agent-in-the-maze-environment" class="headerlink" title="Experiment: Dyna-Q agent in the maze environment"></a>Experiment: Dyna-Q agent in the maze environment</h3><p>Alright. Now we have all the components of the <code>DynaQAgent</code> ready. Let’s try it out on the maze environment! </p><p>The next cell runs an experiment on this maze environment to test your implementation. The initial action values are $0$, the step-size parameter is $0.125$. and the exploration parameter is $\epsilon=0.1$. After the experiment, the sum of rewards in each episode should match the correct result.</p><p>We will try planning steps of $0,5,50$ and compare their performance in terms of the average number of steps taken to reach the goal state in the aforementioned maze environment. For scientific rigor, we will run each experiment $30$ times. In each experiment, we set the initial random-number-generator (RNG) seeds for a fair comparison across algorithms.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_episodes = exp_parameters[<span class="string">'num_episodes'</span>]</span><br><span class="line">    planning_steps_all = agent_parameters[<span class="string">'planning_steps'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = env_parameters                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  <span class="comment"># We pass the agent the information it needs. </span></span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"epsilon"</span>: agent_parameters[<span class="string">"epsilon"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes)) <span class="comment"># for collecting metrics </span></span><br><span class="line">    log_data = &#123;<span class="string">'planning_steps_all'</span> : planning_steps_all&#125;                     <span class="comment"># that shall be plotted later</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Planning steps : '</span>, planning_steps)</span><br><span class="line">        os.system(<span class="string">'sleep 0.5'</span>)                    <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"planning_steps"</span>] = planning_steps  </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = i</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = i</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)          <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_episodes):</span><br><span class="line"></span><br><span class="line">                rl_glue.rl_start()                <span class="comment"># We start an episode. Here we aren't using rl_glue.rl_episode()</span></span><br><span class="line">                                                  <span class="comment"># like the other assessments because we'll be requiring some </span></span><br><span class="line">                is_terminal = <span class="keyword">False</span>               <span class="comment"># data from within the episodes in some of the experiments here </span></span><br><span class="line">                num_steps = <span class="number">0</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, _, action, is_terminal = rl_glue.rl_step()  <span class="comment"># The environment and agent take a step </span></span><br><span class="line">                    num_steps += <span class="number">1</span>                                      <span class="comment"># and return the reward and action taken.</span></span><br><span class="line"></span><br><span class="line">                all_averages[idx][i][j] = num_steps</span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'all_averages'</span>] = all_averages</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_data</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_steps_per_episode</span><span class="params">(data)</span>:</span></span><br><span class="line">    all_averages = data[<span class="string">'all_averages'</span>]</span><br><span class="line">    planning_steps_all = data[<span class="string">'planning_steps_all'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line">        plt.plot(np.mean(all_averages[i], axis=<span class="number">0</span>), label=<span class="string">'Planning steps = '</span>+str(planning_steps))</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Episodes'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Steps\nper\nepisode'</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">    plt.axhline(y=<span class="number">16</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">40</span>,                 <span class="comment"># The number of episodes per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">0</span>, <span class="number">5</span>, <span class="number">50</span>]       <span class="comment"># The list of planning_steps we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">dataq = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_steps_per_episode(dataq)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  0100%|██████████| 30/30 [00:07&lt;00:00,  3.97it/s]Planning steps :  5100%|██████████| 30/30 [00:09&lt;00:00,  3.24it/s]Planning steps :  50100%|██████████| 30/30 [00:53&lt;00:00,  1.79s/it]</code></pre><p><img src="output_27_6.png" alt="png"></p><p>What do you notice?</p><p>As the number of planning steps increases, the number of episodes taken to reach the goal decreases rapidly. Remember that the RNG seed was set the same for all the three values of planning steps, resulting in the same number of steps taken to reach the goal in the first episode. Thereafter, the performance improves. The slowest improvement is when there are $n=0$ planning steps, i.e., for the non-planning Q-learning agent, even though the step size parameter was optimized for it. Note that the grey dotted line shows the minimum number of steps required to reach the goal state under the optimal greedy policy.</p><hr><h3 id="Experiment-s-Dyna-Q-agent-in-the-changing-maze-environment"><a href="#Experiment-s-Dyna-Q-agent-in-the-changing-maze-environment" class="headerlink" title="Experiment(s): Dyna-Q agent in the _changing_ maze environment"></a>Experiment(s): Dyna-Q agent in the _changing_ maze environment</h3><p>Great! Now let us see how Dyna-Q performs on the version of the maze in which a shorter path opens up after 3000 steps. The rest of the transition and reward dynamics remain the same. </p><p><img src="shortcut_env_after.png" alt="environment" width="800"></p><p>Before you proceed, take a moment to think about what you expect to see. Will Dyna-Q find the new, shorter path to the goal? If so, why? If not, why not?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment_with_state_visitations</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters, result_file_name)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_max_steps = exp_parameters[<span class="string">'num_max_steps'</span>]</span><br><span class="line">    planning_steps_all = agent_parameters[<span class="string">'planning_steps'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">"change_at_n"</span> : env_parameters[<span class="string">"change_at_n"</span>]&#125;                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  </span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"epsilon"</span>: agent_parameters[<span class="string">"epsilon"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    state_visits_before_change = np.zeros((len(planning_steps_all), num_runs, <span class="number">54</span>))  <span class="comment"># For saving the number of</span></span><br><span class="line">    state_visits_after_change = np.zeros((len(planning_steps_all), num_runs, <span class="number">54</span>))   <span class="comment">#     state-visitations </span></span><br><span class="line">    cum_reward_all = np.zeros((len(planning_steps_all), num_runs, num_max_steps))   <span class="comment"># For saving the cumulative reward</span></span><br><span class="line">    log_data = &#123;<span class="string">'planning_steps_all'</span> : planning_steps_all&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, planning_steps <span class="keyword">in</span> enumerate(planning_steps_all):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Planning steps : '</span>, planning_steps)</span><br><span class="line">        os.system(<span class="string">'sleep 1'</span>)          <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"planning_steps"</span>] = planning_steps  <span class="comment"># We pass the agent the information it needs. </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = run</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = run</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)  <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            num_steps = <span class="number">0</span></span><br><span class="line">            cum_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line"></span><br><span class="line">                state, _ = rl_glue.rl_start()  <span class="comment"># We start the experiment. We'll be collecting the </span></span><br><span class="line">                is_terminal = <span class="keyword">False</span>            <span class="comment"># state-visitation counts to visiualize the learned policy</span></span><br><span class="line">                <span class="keyword">if</span> num_steps &lt; env_parameters[<span class="string">"change_at_n"</span>]: </span><br><span class="line">                    state_visits_before_change[idx][run][state] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state_visits_after_change[idx][run][state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal <span class="keyword">and</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()  </span><br><span class="line">                    num_steps += <span class="number">1</span></span><br><span class="line">                    cum_reward += reward</span><br><span class="line">                    cum_reward_all[idx][run][num_steps] = cum_reward</span><br><span class="line">                    <span class="keyword">if</span> num_steps &lt; env_parameters[<span class="string">"change_at_n"</span>]:</span><br><span class="line">                        state_visits_before_change[idx][run][state] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        state_visits_after_change[idx][run][state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'state_visits_before'</span>] = state_visits_before_change</span><br><span class="line">    log_data[<span class="string">'state_visits_after'</span>] = state_visits_after_change</span><br><span class="line">    log_data[<span class="string">'cum_reward_all'</span>] = cum_reward_all</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cumulative_reward</span><span class="params">(data_all, item_key, y_key, y_axis_label, legend_prefix, title)</span>:</span></span><br><span class="line">    data_y_all = data_all[y_key]</span><br><span class="line">    items = data_all[item_key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(items):</span><br><span class="line">        plt.plot(np.mean(data_y_all[i], axis=<span class="number">0</span>), label=legend_prefix+str(item))</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=<span class="number">3000</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Timesteps'</span>)</span><br><span class="line">    plt.ylabel(y_axis_label, rotation=<span class="number">0</span>, labelpad=<span class="number">60</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>Did you notice that the environment changes after a fixed number of _steps_ and not episodes? </p><p>This is because the environment is separate from the agent, and the environment changes irrespective of the length of each episode (i.e., the number of environmental interactions per episode) that the agent perceives. And hence we are now plotting the data per step or interaction of the agent and the environment, in order to comfortably see the differences in the behaviours of the agents before and after the environment changes.  </p><p>Okay, now we will first plot the cumulative reward obtained by the agent per interaction with the environment, averaged over 10 runs of the experiment on this changing world. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">10</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">5</span>, <span class="number">10</span>, <span class="number">50</span>]      <span class="comment"># The list of planning_steps we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">dataq = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, <span class="string">"Dyna-Q_shortcut_steps"</span>)    </span><br><span class="line">plot_cumulative_reward(dataq, <span class="string">'planning_steps_all'</span>, <span class="string">'cum_reward_all'</span>, <span class="string">'Cumulative\nreward'</span>, <span class="string">'Planning steps = '</span>, <span class="string">'Dyna-Q : Varying planning_steps'</span>)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  5100%|██████████| 10/10 [00:10&lt;00:00,  1.08s/it]Planning steps :  10100%|██████████| 10/10 [00:16&lt;00:00,  1.70s/it]Planning steps :  50100%|██████████| 10/10 [01:19&lt;00:00,  7.99s/it]</code></pre><p><img src="output_34_6.png" alt="png"></p><p>We observe that the slope of the curves is almost constant. If the agent had discovered the shortcut and begun using it, we would expect to see an increase in the slope of the curves towards the later stages of training. This is because the agent can get to the goal state faster and get the positive reward. Note that the timestep at which the shortcut opens up is marked by the grey dotted line.</p><p>Note that this trend is constant across the increasing number of planning steps.</p><p>Now let’s check the heatmap of the state visitations of the agent with <code>planning_steps=10</code> during training, before and after the shortcut opens up after 3000 timesteps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_state_visitations</span><span class="params">(data, plot_titles, idx)</span>:</span></span><br><span class="line">    data_keys = [<span class="string">"state_visits_before"</span>, <span class="string">"state_visits_after"</span>]</span><br><span class="line">    positions = [<span class="number">211</span>,<span class="number">212</span>]</span><br><span class="line">    titles = plot_titles</span><br><span class="line">    wall_ends = [<span class="keyword">None</span>,<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line"></span><br><span class="line">        state_visits = data[data_keys[i]][idx]</span><br><span class="line">        average_state_visits = np.mean(state_visits, axis=<span class="number">0</span>)</span><br><span class="line">        grid_state_visits = np.rot90(average_state_visits.reshape((<span class="number">6</span>,<span class="number">9</span>)).T)</span><br><span class="line">        grid_state_visits[<span class="number">2</span>,<span class="number">1</span>:wall_ends[i]] = np.nan <span class="comment"># walls</span></span><br><span class="line">        <span class="comment">#print(average_state_visits.reshape((6,9)))</span></span><br><span class="line">        plt.subplot(positions[i])</span><br><span class="line">        plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">'gray'</span>, linewidth=<span class="number">1</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">        plt.text(<span class="number">3</span>+<span class="number">0.5</span>, <span class="number">0</span>+<span class="number">0.5</span>, <span class="string">'S'</span>, horizontalalignment=<span class="string">'center'</span>, verticalalignment=<span class="string">'center'</span>)</span><br><span class="line">        plt.text(<span class="number">8</span>+<span class="number">0.5</span>, <span class="number">5</span>+<span class="number">0.5</span>, <span class="string">'G'</span>, horizontalalignment=<span class="string">'center'</span>, verticalalignment=<span class="string">'center'</span>)</span><br><span class="line">        plt.title(titles[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        cm = plt.get_cmap()</span><br><span class="line">        cm.set_bad(<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">1.</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    cbar = plt.colorbar(cax=cax)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">plot_state_visitations(dataq, [<span class="string">'Dyna-Q : State visitations before the env changes'</span>, <span class="string">'Dyna-Q : State visitations after the env changes'</span>], <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="output_37_0.png" alt="png"></p><p>What do you observe?</p><p>The state visitation map looks almost the same before and after the shortcut opens. This means that the Dyna-Q agent hasn’t quite discovered and started exploiting the new shortcut.</p><p>Now let’s try increasing the exploration parameter $\epsilon$ to see if it helps the Dyna-Q agent discover the shortcut. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment_only_cumulative_reward</span><span class="params">(env, agent, env_parameters, agent_parameters, exp_parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">'num_runs'</span>]</span><br><span class="line">    num_max_steps = exp_parameters[<span class="string">'num_max_steps'</span>]</span><br><span class="line">    epsilons = agent_parameters[<span class="string">'epsilons'</span>]</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">"change_at_n"</span> : env_parameters[<span class="string">"change_at_n"</span>]&#125;                     </span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span> : agent_parameters[<span class="string">"num_states"</span>],  </span><br><span class="line">                  <span class="string">"num_actions"</span> : agent_parameters[<span class="string">"num_actions"</span>],</span><br><span class="line">                  <span class="string">"planning_steps"</span>: agent_parameters[<span class="string">"planning_steps"</span>], </span><br><span class="line">                  <span class="string">"discount"</span>: env_parameters[<span class="string">"discount"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span> : agent_parameters[<span class="string">"step_size"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    log_data = &#123;<span class="string">'epsilons'</span> : epsilons&#125; </span><br><span class="line">    cum_reward_all = np.zeros((len(epsilons), num_runs, num_max_steps))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> eps_idx, epsilon <span class="keyword">in</span> enumerate(epsilons):</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Agent : Dyna-Q, epsilon : %f'</span> % epsilon)</span><br><span class="line">        os.system(<span class="string">'sleep 1'</span>)          <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">"epsilon"</span>] = epsilon</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">'random_seed'</span>] = run</span><br><span class="line">            agent_info[<span class="string">'planning_random_seed'</span>] = run</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)  <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            num_steps = <span class="number">0</span></span><br><span class="line">            cum_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line"></span><br><span class="line">                rl_glue.rl_start()  <span class="comment"># We start the experiment</span></span><br><span class="line">                is_terminal = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal <span class="keyword">and</span> num_steps &lt; num_max_steps<span class="number">-1</span> :</span><br><span class="line">                    reward, _, action, is_terminal = rl_glue.rl_step()  <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                    <span class="comment"># the reward, and action taken.</span></span><br><span class="line">                    num_steps += <span class="number">1</span></span><br><span class="line">                    cum_reward += reward</span><br><span class="line">                    cum_reward_all[eps_idx][run][num_steps] = cum_reward</span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">'cum_reward_all'</span>] = cum_reward_all</span><br><span class="line">    <span class="keyword">return</span> log_data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : <span class="number">10</span>,</span><br><span class="line">    <span class="string">"epsilons"</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]    <span class="comment"># The list of epsilons we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">data = run_experiment_only_cumulative_reward(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_cumulative_reward(data, <span class="string">'epsilons'</span>, <span class="string">'cum_reward_all'</span>, <span class="string">'Cumulative\nreward'</span>, <span class="string">r'$\epsilon$ = '</span>, <span class="string">r'Dyna-Q : Varying $\epsilon$'</span>)</span><br></pre></td></tr></table></figure><pre><code>Agent : Dyna-Q, epsilon : 0.100000100%|██████████| 30/30 [00:52&lt;00:00,  1.75s/it]Agent : Dyna-Q, epsilon : 0.200000100%|██████████| 30/30 [00:49&lt;00:00,  1.65s/it]Agent : Dyna-Q, epsilon : 0.400000100%|██████████| 30/30 [00:50&lt;00:00,  1.69s/it]Agent : Dyna-Q, epsilon : 0.800000100%|██████████| 30/30 [00:52&lt;00:00,  1.74s/it]</code></pre><p><img src="output_40_8.png" alt="png"></p><p>What do you observe?</p><p>Increasing the exploration via the $\epsilon$-greedy strategy does not seem to be helping. In fact, the agent’s cumulative reward decreases because it is spending more and more time trying out the exploratory actions.</p><p>Can we do better…? </p><h2 id="Section-2-Dyna-Q"><a href="#Section-2-Dyna-Q" class="headerlink" title="Section 2: Dyna-Q+"></a>Section 2: Dyna-Q+</h2><p>The motivation behind Dyna-Q+ is to give a bonus reward for actions that haven’t been tried for a long time, since there is a greater chance that the dynamics for that actions might have changed.</p><p>In particular, if the modeled reward for a transition is $r$, and the transition has not been tried in $\tau(s,a)$ time steps, then planning updates are done as if that transition produced a reward of $r + \kappa \sqrt{ \tau(s,a)}$, for some small $\kappa$. </p><p>Let’s implement that!</p><p>Based on your <code>DynaQAgent</code>, create a new class <code>DynaQPlusAgent</code> to implement the aforementioned exploration heuristic. Additionally :</p><ol><li>actions that had never been tried before from a state should now be allowed to be considered in the planning step,</li><li>and the initial model for such actions is that they lead back to the same state with a reward of zero.</li></ol><p>At this point, you might want to refer to the video lectures and <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=188" target="_blank" rel="noopener">Section 8.3</a> of the RL textbook for a refresher on Dyna-Q+.</p><p>As usual, let’s break this down in pieces and do it one-by-one.</p><p>First of all, check out the <code>agent_init</code> method below. In particular, pay attention to the attributes which are new to <code>DynaQPlusAgent</code>â€“ state-visitation counts $\tau$ and the scaling parameter $\kappa$ â€“ because you shall be using them later. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynaQPlusAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                num_states (int): The number of states,</span></span><br><span class="line"><span class="string">                num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">                epsilon (float): The parameter for epsilon-greedy exploration,</span></span><br><span class="line"><span class="string">                step_size (float): The step-size,</span></span><br><span class="line"><span class="string">                discount (float): The discount factor,</span></span><br><span class="line"><span class="string">                planning_steps (int): The number of planning steps per environmental interaction</span></span><br><span class="line"><span class="string">                kappa (float): The scaling factor for the reward bonus</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                random_seed (int): the seed for the RNG used in epsilon-greedy</span></span><br><span class="line"><span class="string">                planning_random_seed (int): the seed for the RNG used in the planner</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, we get the relevant information from agent_info </span></span><br><span class="line">        <span class="comment"># Note: we use np.random.RandomState(seed) to set the two different RNGs</span></span><br><span class="line">        <span class="comment"># for the planner and the rest of the code</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.num_states = agent_info[<span class="string">"num_states"</span>]</span><br><span class="line">            self.num_actions = agent_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">"You need to pass both 'num_states' and 'num_actions' \</span></span><br><span class="line"><span class="string">                   in agent_info to initialize the action-value table"</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">"discount"</span>, <span class="number">0.95</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">"step_size"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">"epsilon"</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.planning_steps = agent_info.get(<span class="string">"planning_steps"</span>, <span class="number">10</span>)</span><br><span class="line">        self.kappa = agent_info.get(<span class="string">"kappa"</span>, <span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">'random_seed'</span>, <span class="number">42</span>))</span><br><span class="line">        self.planning_rand_generator = np.random.RandomState(agent_info.get(<span class="string">'planning_random_seed'</span>, <span class="number">42</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, we initialize the attributes required by the agent, e.g., q_values, model, tau, etc.</span></span><br><span class="line">        <span class="comment"># The visitation-counts can be stored as a table as well, like the action values </span></span><br><span class="line">        self.q_values = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.tau = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.actions = list(range(self.num_actions))</span><br><span class="line">        self.past_action = <span class="number">-1</span></span><br><span class="line">        self.past_state = <span class="number">-1</span></span><br><span class="line">        self.model = &#123;&#125;</span><br></pre></td></tr></table></figure><p>Now first up, implement the <code>update_model</code> method. Note that this is different from Dyna-Q in the aforementioned way.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_model</span><span class="params">(self, past_state, past_action, state, reward)</span>:</span></span><br><span class="line">    <span class="string">"""updates the model </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        past_state  (int): s</span></span><br><span class="line"><span class="string">        past_action (int): a</span></span><br><span class="line"><span class="string">        state       (int): s'</span></span><br><span class="line"><span class="string">        reward      (int): r</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Recall that when adding a state-action to the model, if the agent is visiting the state</span></span><br><span class="line">    <span class="comment">#    for the first time, then the remaining actions need to be added to the model as well</span></span><br><span class="line">    <span class="comment">#    with zero reward and a transition into itself.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: do *not* update the visitation-counts here. We will do that in `agent_step`.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># (3 lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> past_state <span class="keyword">not</span> <span class="keyword">in</span> self.model:</span><br><span class="line">        self.model[past_state] = &#123;past_action : (state, reward)&#125;</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> self.actions:</span><br><span class="line">            <span class="keyword">if</span> action != past_action:</span><br><span class="line">                self.model[past_state][action] = (past_state, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.model[past_state][past_action] = (state, reward)</span><br></pre></td></tr></table></figure><h3 id="Test-update-model-1"><a href="#Test-update-model-1" class="headerlink" title="Test update_model()"></a>Test <code>update_model()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><p>Next, you will implement the <code>planning_step()</code> method. This will be very similar to the one you implemented in <code>DynaQAgent</code>, but here you will be adding the exploration bonus to the reward in the simulated transition.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">planning_step</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""performs planning, i.e. indirect RL.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The indirect RL step:</span></span><br><span class="line">    <span class="comment"># - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Query the model with this state-action pair for the predicted next state and reward.(~1 line)</span></span><br><span class="line">    <span class="comment"># - **Add the bonus to the reward** (~1 line)</span></span><br><span class="line">    <span class="comment"># - Update the action values with this simulated experience.                            (2~4 lines)</span></span><br><span class="line">    <span class="comment"># - Repeat for the required number of planning steps.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the update equation is different for terminal and non-terminal transitions. </span></span><br><span class="line">    <span class="comment"># To differentiate between a terminal and a non-terminal next state, assume that the model stores</span></span><br><span class="line">    <span class="comment"># the terminal state as a dummy state like -1</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Important: remember you have a random number generator 'planning_rand_generator' as </span></span><br><span class="line">    <span class="comment">#     a part of the class which you need to use as self.planning_rand_generator.choice()</span></span><br><span class="line">    <span class="comment">#     For the sake of reproducibility and grading, *do not* use anything else like </span></span><br><span class="line">    <span class="comment">#     np.random.choice() for performing search control.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.planning_steps):</span><br><span class="line">    </span><br><span class="line">        s = self.planning_rand_generator.choice(list(self.model.keys()))</span><br><span class="line">        a = self.planning_rand_generator.choice(list(self.model[s].keys()))</span><br><span class="line"></span><br><span class="line">        (next_s,r) = self.model[s][a]</span><br><span class="line">        r += self.kappa * np.sqrt(self.tau[s][a])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> next_s == <span class="number">-1</span>:</span><br><span class="line">            expect = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            expect = self.gamma * np.max(self.q_values[next_s])</span><br><span class="line">        self.q_values[s,a] += self.step_size * (r + expect - self.q_values[s,a])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-planning-step-1"><a href="#Test-planning-step-1" class="headerlink" title="Test planning_step()"></a>Test <code>planning_step()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test code for planning_step() ##</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">"kappa"</span>: <span class="number">0.001</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">2</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">2</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.planning_step()</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;, </span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.10014142</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.00036373</span>, <span class="number">0</span>, <span class="number">0.00017321</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br></pre></td></tr></table></figure><p>Again, before you move on to implement the rest of the agent methods, here are the couple of helper functions that you’ve used in the previous assessments for choosing an action using an $\epsilon$-greedy policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">    <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q_values (Numpy array): the array of action values</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        action (int): an action with the highest value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    top = float(<span class="string">"-inf"</span>)</span><br><span class="line">    ties = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">        <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">            top = q_values[i]</span><br><span class="line">            ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">            ties.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action_egreedy</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""returns an action using an epsilon-greedy policy w.r.t. the current action-value function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Important: assume you have a random number generator 'rand_generator' as a part of the class</span></span><br><span class="line"><span class="string">                which you can use as self.rand_generator.choice() or self.rand_generator.rand()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (List): coordinates of the agent (two elements)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action taken w.r.t. the aforementioned epsilon-greedy policy</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">        action = self.rand_generator.choice(self.actions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        values = self.q_values[state]</span><br><span class="line">        action = self.argmax(values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><p>Now implement the rest of the agent-related methods, namely <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>. Again, these will be very similar to the ones in the <code>DynaQAgent</code>, but you will have to think of a way to update the counts since the last visit.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># given the state, select the action using self.choose_action_egreedy(), </span></span><br><span class="line">    <span class="comment"># and save current state and action (~2 lines)</span></span><br><span class="line">    <span class="comment">### self.past_state = ?</span></span><br><span class="line">    <span class="comment">### self.past_action = ?</span></span><br><span class="line">    <span class="comment"># Note that the last-visit counts are not updated here.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = self.choose_action_egreedy(state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">            last step</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The action the agent is taking.</span></span><br><span class="line"><span class="string">    """</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update the last-visited counts (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Direct-RL step (1~3 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step (~1 line)</span></span><br><span class="line">    <span class="comment"># - `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment"># - Action Selection step (~1 line)</span></span><br><span class="line">    <span class="comment"># Save the current state and action before returning the action to be performed. (~2 lines)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.tau += <span class="number">1</span></span><br><span class="line">    self.tau[self.past_state,self.past_action] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward + self.gamma * np.max(self.q_values[state]) - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, state, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    action = self.choose_action_egreedy(state)</span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = action</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Called when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">            terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Again, add the same components you added in agent_step to augment Dyna-Q into Dyna-Q+</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.tau += <span class="number">1</span></span><br><span class="line">    self.tau[self.past_state,self.past_action] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward  - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, <span class="number">-1</span>, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure><h3 id="Test-agent-start-agent-step-and-agent-end-1"><a href="#Test-agent-start-agent-step-and-agent-end-1" class="headerlink" title="Test agent_start(), agent_step(), and agent_end()"></a>Test <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">"num_states"</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"step_size"</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">"discount"</span>: <span class="number">1.0</span>,</span><br><span class="line">              <span class="string">"kappa"</span>: <span class="number">0.001</span>,</span><br><span class="line">              <span class="string">"random_seed"</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">"planning_steps"</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">"planning_random_seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>) <span class="comment"># state</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.tau, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> agent.model == &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_tau = np.array([</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.tau == expected_tau)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.0191</span>, <span class="number">0.271</span>, <span class="number">0.0</span>, <span class="number">0.0191</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.000183847763</span>, <span class="number">0.000424264069</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;, </span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_tau = np.array([</span><br><span class="line">    [<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.tau == expected_tau)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.0191</span>, <span class="number">0.344083848</span>, <span class="number">0</span>, <span class="number">0.0444632051</span>],</span><br><span class="line">    [<span class="number">0.0191732051</span>, <span class="number">0.19</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.000183847763</span>, <span class="number">0.000424264069</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;<span class="number">0</span>: &#123;<span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>), <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">0</span>, <span class="number">0</span>), <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>)&#125;, <span class="number">2</span>: &#123;<span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">0</span>: (<span class="number">2</span>, <span class="number">0</span>), <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>)&#125;, <span class="number">1</span>: &#123;<span class="number">1</span>: (<span class="number">-1</span>, <span class="number">1</span>), <span class="number">0</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>)&#125;&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure><h3 id="Experiment-Dyna-Q-agent-in-the-changing-environment"><a href="#Experiment-Dyna-Q-agent-in-the-changing-environment" class="headerlink" title="Experiment: Dyna-Q+ agent in the _changing_ environment"></a>Experiment: Dyna-Q+ agent in the _changing_ environment</h3><p>Okay, now we’re ready to test our Dyna-Q+ agent on the Shortcut Maze. As usual, we will average the results over 30 independent runs of the experiment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">"num_max_steps"</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">"discount"</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">"change_at_n"</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">"num_actions"</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">"step_size"</span> : <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">"planning_steps"</span> : [<span class="number">50</span>]      </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQPlusAgent          <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">data_qplus = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, <span class="string">"Dyna-Q+"</span>)</span><br></pre></td></tr></table></figure><pre><code>Planning steps :  50100%|██████████| 30/30 [04:21&lt;00:00,  8.72s/it]</code></pre><p>Let’s compare the Dyna-Q and Dyna-Q+ agents with <code>planning_steps=50</code> each.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cumulative_reward_comparison</span><span class="params">(data1, data2)</span>:</span></span><br><span class="line"></span><br><span class="line">    cum_reward_q = data1[<span class="string">'cum_reward_all'</span>][<span class="number">2</span>]</span><br><span class="line">    cum_reward_qPlus = data2[<span class="string">'cum_reward_all'</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.plot(np.mean(cum_reward_qPlus, axis=<span class="number">0</span>), label=<span class="string">'Dyna-Q+'</span>)</span><br><span class="line">    plt.plot(np.mean(cum_reward_q, axis=<span class="number">0</span>), label=<span class="string">'Dyna-Q'</span>)</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=<span class="number">3000</span>, linestyle=<span class="string">'--'</span>, color=<span class="string">'grey'</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Timesteps'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Cumulative\nreward'</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">60</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.title(<span class="string">'Average performance of Dyna-Q and Dyna-Q+ agents in the Shortcut Maze\n'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">plot_cumulative_reward_comparison(dataq, data_qplus)</span><br></pre></td></tr></table></figure><p><img src="output_64_0.png" alt="png"></p><p>What do you observe? (For reference, your graph should look like <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=189" target="_blank" rel="noopener">Figure 8.5 in Chapter 8</a> of the RL textbook)</p><p>The slope of the curve increases for the Dyna-Q+ curve shortly after the shortcut opens up after 3000 steps, which indicates that the rate of receiving the positive reward increases. This implies that the Dyna-Q+ agent finds the shorter path to the goal.</p><p>To verify this, let us plot the state-visitations of the Dyna-Q+ agent before and after the shortcut opens up.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">plot_state_visitations(data_qplus, [<span class="string">'Dyna-Q+ : State visitations before the env changes'</span>, <span class="string">'Dyna-Q+ : State visitations after the env changes'</span>], <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="output_66_0.png" alt="png"></p><p>What do you observe?</p><p>Before the shortcut opens up, like Dyna-Q, the Dyna-Q+ agent finds the sole, long path to the goal. But because the Dyna-Q+ agent keeps exploring, it succeeds in discovering the shortcut once it opens up, which leads to the goal faster. So the bonus reward heuristic is effective in helping the agent explore and find changes in the environment without degrading the performance. </p><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations! You have:</p><ol><li>implemented Dyna-Q, a model-based approach to RL;</li><li>implemented Dyna-Q+, a variant of Dyna-Q with an exploration bonus that encourages exploration; </li><li>conducted scientific experiments to empirically validate the exploration/exploitation dilemma in the planning context on an environment that changes with time.</li></ol><p>Some points to ponder about:</p><ol><li>At what cost does Dyna-Q+ improve over Dyna-Q?</li><li>In general, what is the trade-off of using model-based methods like Dyna-Q over model-free methods like Q-learning?</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-Dyna-Q-and-Dyna-Q&quot;&gt;&lt;a href=&quot;#Assignment-Dyna-Q-and-Dyna-Q&quot; class=&quot;headerlink&quot; title=&quot;Assignment: Dyna-Q and Dyna-Q+&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Planning and learning with Tabular Methods</title>
    <link href="https://zhangruochi.com/Planning-and-learning-with-Tabular-Methods/2020/09/29/"/>
    <id>https://zhangruochi.com/Planning-and-learning-with-Tabular-Methods/2020/09/29/</id>
    <published>2020-09-29T08:09:45.000Z</published>
    <updated>2020-09-29T08:19:02.711Z</updated>
    
    <content type="html"><![CDATA[<p>We develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heuristic search, and methods that can be used without a model, such as Monte Carlo and temporal-difference methods. These are respectively called <strong>model-based</strong> and <strong>model-free</strong> reinforcement learning methods. Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning. Although there are real di↵erences between these two kinds of methods, there are also great similarities.</p><ul><li>All state-space planning methods involve computing value functions as a key intermediate step toward improving the policy</li><li>They compute value functions by updates or backup operations applied to simulated experience.</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul><li>By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions.</li><li>Distribution model produce a description of all possibilities and their probabilities. Sample model produce just one of the possibilities and their probabilities.</li><li>当给定一个 state 和一个 action 时，distribution model 可以生成所有可能的状态转移，而sample model只能给出一个可能的状态转移</li><li>当给定一个 state 和 Policy 时，distribution model 可以获得所有可能的 episode 并得到他们出现的概率，但 sample model 只能给出一个 episode</li></ul><p>总之，distribution model 比 sample model包含更多信息，但现实中往往更容易获得sample model。简单来说，distribution model 包含了所有状态的转移概率，但sample model更像是管中窥豹，可见一斑。在DP中，我们用到的是distribution model，而在MC中我们用到的是sample model。model 是对环境的一种表达方式，（不一定是真实或完全正确的），可以用来产生仿真经验（simulation experience）。</p><h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p>从Model中生成或提升Policy 的计算过程称为 Planning:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>注意本文讨论的Planning都是state space Planning，这种Planning有两个特点：</p><ul><li>通过计算values function 来进行Policy 提升</li><li>根据simulated experience来计算value function</li></ul><p>Planning（如DP） 和learning（如MC、TD）方法的核心都是用backing-up 更新公式计算value function 的估计值。区别在于Planning 所用经验是有模型生成的simulated exprience，而learning method使用的经验是由真实环境生成的real exprience。  但两者都满足上述state space Planning结构，这表示很多思想和算法可以相互借鉴，在应用中常常用 learning 中 value function 估计值的更新公式取代 Planning 中的 value function 估计值的更新公式。例如，我们可以将Q learning 和 planning 结合，得到random-sample one-step tabular Q-planning 方法：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%"></center><p>one-step tabular Q-learning最终会收敛到一个对应于真实环境的optimal Policy，而 random-sample one-step tabular Q-planning 则收敛到一个对应于model 的optimal Policy。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heur
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Q-Learning and Expected Sarsa </title>
    <link href="https://zhangruochi.com/Q-Learning-and-Expected-Sarsa/2020/09/29/"/>
    <id>https://zhangruochi.com/Q-Learning-and-Expected-Sarsa/2020/09/29/</id>
    <published>2020-09-29T02:52:00.000Z</published>
    <updated>2020-09-29T08:10:50.768Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Q-Learning-and-Expected-Sarsa"><a href="#Assignment-2-Q-Learning-and-Expected-Sarsa" class="headerlink" title="Assignment 2 - Q-Learning and Expected Sarsa"></a>Assignment 2 - Q-Learning and Expected Sarsa</h1><p>Welcome to Course 2 Programming Assignment 2. In this notebook, you will:</p><ol><li>Implement Q-Learning with $\epsilon$-greedy action selection</li><li>Implement Expected Sarsa with $\epsilon$-greedy action selection</li><li>Investigate how these two algorithms behave on Cliff World (described on page 132 of the textbook)</li></ol><p>We will provide you with the environment and infrastructure to run an experiment (called the experiment program in RL-Glue). This notebook will provide all the code you need to run your experiment and visualise learning performance.</p><p>This assignment will be graded automatically by comparing the behavior of your agent to our implementations of Expected Sarsa and Q-learning. The random seed will be set to avoid different behavior due to randomness. We will highlight the functions you have to use for generating random samples and the number of times these functions should be called. </p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>You will need the following libraries for this assignment. We are using:</p><ol><li>numpy: the fundamental package for scientific computing with Python.</li><li>scipy: a Python library for scientific and technical computing.</li><li>matplotlib: library for plotting graphs in Python.</li><li>RL-Glue: library for reinforcement learning experiments.</li></ol><p>DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> sem</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> cliffworld_env</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">'font.size'</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">'figure.figsize'</span>: [<span class="number">10</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure><h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>In this section you will implement and test a Q-Learning agent with $\epsilon$-greedy action selection (Section 6.5 in the textbook). </p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_init_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">"num_states"</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">"epsilon"</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">"step_size"</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">"discount"</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">"seed"</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action]  += self.step_size * (reward + self.discount * np.max(current_q) - \</span><br><span class="line">                                                                        self.q[self.prev_state][self.prev_action] )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">        <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        top = float(<span class="string">"-inf"</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p><p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">3</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = QLearningAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q == expected_values)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>,  <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.02</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>, <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.1</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure><h1 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h1><p>In this section you will implement an Expected Sarsa agent with $\epsilon$-greedy action selection (Section 6.6 in the textbook). </p><h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpectedSarsaAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_init_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">"num_actions"</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">"num_states"</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">"epsilon"</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">"step_size"</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">"discount"</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">"seed"</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment's step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        expect = (<span class="number">1</span> - self.epsilon) * np.max(current_q) + self.epsilon * np.average(current_q)</span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward + self.discount * expect - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, q_values)</span>:</span></span><br><span class="line">        <span class="string">"""argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        top = float(<span class="string">"-inf"</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure><h3 id="Test-1"><a href="#Test-1" class="headerlink" title="Test"></a>Test</h3><p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p><p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">3</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = ExpectedSarsaAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(agent.q == expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.28</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure><h1 id="Solving-the-Cliff-World"><a href="#Solving-the-Cliff-World" class="headerlink" title="Solving the Cliff World"></a>Solving the Cliff World</h1><p>We described the Cliff World environment in the video “Expected Sarsa in the Cliff World” in Lesson 3. This is an undiscounted episodic task and thus we set $\gamma$=1. The agent starts in the bottom left corner of the gridworld below and takes actions that move it in the four directions. Actions that would move the agent off of the cliff incur a reward of -100 and send the agent back to the start state. The reward for all other transitions is -1. An episode terminates when the agent reaches the bottom right corner. </p><p><img src="cliffworld.png" alt="Drawing" style="width: 600px;"></p><p>Using the experiment program in the cell below we now compare the agents on the Cliff World environment and plot the sum of rewards during each episode for the two agents.</p><p>The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">"Q-learning"</span>: QLearningAgent,</span><br><span class="line">    <span class="string">"Expected Sarsa"</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125; <span class="comment"># Contains sum of rewards during episode</span></span><br><span class="line">all_state_visits = &#123;&#125; <span class="comment"># Contains state visit counts during the last 10 episodes</span></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">48</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"step_size"</span>: <span class="number">0.5</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">100</span> <span class="comment"># The number of runs</span></span><br><span class="line">num_episodes = <span class="number">100</span> <span class="comment"># The number of episodes in each run</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    all_reward_sums[algorithm] = []</span><br><span class="line">    all_state_visits[algorithm] = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">        rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">        reward_sums = []</span><br><span class="line">        state_visits = np.zeros(<span class="number">48</span>)</span><br><span class="line">        last_episode_total_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">            <span class="keyword">if</span> episode &lt; num_episodes - <span class="number">10</span>:</span><br><span class="line">                <span class="comment"># Runs an episode</span></span><br><span class="line">                rl_glue.rl_episode(<span class="number">10000</span>) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="comment"># Runs an episode while keeping track of visited states</span></span><br><span class="line">                state, action = rl_glue.rl_start()</span><br><span class="line">                state_visits[state] += <span class="number">1</span></span><br><span class="line">                is_terminal = <span class="keyword">False</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">                    state_visits[state] += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            reward_sums.append(rl_glue.rl_return() - last_episode_total_reward)</span><br><span class="line">            last_episode_total_reward = rl_glue.rl_return()</span><br><span class="line">            </span><br><span class="line">        all_reward_sums[algorithm].append(reward_sums)</span><br><span class="line">        all_state_visits[algorithm].append(state_visits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot results</span></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    plt.plot(np.mean(all_reward_sums[algorithm], axis=<span class="number">0</span>), label=algorithm)</span><br><span class="line">plt.xlabel(<span class="string">"Episodes"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Sum of\n rewards\n during\n episode"</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line">plt.ylim(<span class="number">-30</span>,<span class="number">0</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 100/100 [00:12&lt;00:00,  8.12it/s]100%|██████████| 100/100 [00:16&lt;00:00,  6.15it/s]</code></pre><p><img src="output_26_1.png" alt="png"></p><p>To see why these two agents behave differently, let’s inspect the states they visit most. Run the cell below to generate plots showing the number of timesteps that the agents spent in each state over the last 10 episodes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm, position <span class="keyword">in</span> [(<span class="string">"Q-learning"</span>, <span class="number">211</span>), (<span class="string">"Expected Sarsa"</span>, <span class="number">212</span>)]:</span><br><span class="line">    plt.subplot(position)</span><br><span class="line">    average_state_visits = np.array(all_state_visits[algorithm]).mean(axis=<span class="number">0</span>)</span><br><span class="line">    grid_state_visits = average_state_visits.reshape((<span class="number">4</span>,<span class="number">12</span>))</span><br><span class="line">    grid_state_visits[<span class="number">0</span>,<span class="number">1</span>:<span class="number">-1</span>] = np.nan</span><br><span class="line">    plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">'gray'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.title(algorithm)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    cm = plt.get_cmap()</span><br><span class="line">    cm.set_bad(<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">0.85</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    </span><br><span class="line">cbar = plt.colorbar(cax=cax)</span><br><span class="line">cbar.ax.set_ylabel(<span class="string">"Visits during\n the last 10\n episodes"</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">70</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_28_0.png" alt="png"></p><p>The Q-learning agent learns the optimal policy, one that moves along the cliff and reaches the goal in as few steps as possible. However, since the agent does not follow the optimal policy and uses $\epsilon$-greedy exploration, it occasionally falls off the cliff. The Expected Sarsa agent takes exploration into account and follows a safer path.</p><p>Previously we used a fixed step-size of 0.5 for the agents. What happens with other step-sizes? Does this difference in performance persist?</p><p>In the next experiment we will try 10 different step-sizes from 0.1 to 1.0 and compare the sum of rewards per episode averaged over the first 100 episodes (similar to the interim performance curves in Figure 6.3 of the textbook). Shaded regions show standard errors.</p><p>This cell takes around 10 minutes to run. The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">"Q-learning"</span>: QLearningAgent,</span><br><span class="line">    <span class="string">"Expected Sarsa"</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line">step_sizes = np.linspace(<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">10</span>)</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">4</span>, <span class="string">"num_states"</span>: <span class="number">48</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>, <span class="string">"discount"</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">30</span></span><br><span class="line">num_episodes = <span class="number">100</span></span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line"></span><br><span class="line">algorithms = [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]</span><br><span class="line">cross_product = list(product(algorithms, step_sizes, range(num_runs)))</span><br><span class="line"><span class="keyword">for</span> algorithm, step_size, run <span class="keyword">in</span> tqdm(cross_product):</span><br><span class="line">    <span class="keyword">if</span> (algorithm, step_size) <span class="keyword">not</span> <span class="keyword">in</span> all_reward_sums:</span><br><span class="line">        all_reward_sums[(algorithm, step_size)] = []</span><br><span class="line"></span><br><span class="line">    agent_info[<span class="string">"step_size"</span>] = step_size</span><br><span class="line">    agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">    rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    last_episode_total_reward = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>)</span><br><span class="line">    all_reward_sums[(algorithm, step_size)].append(rl_glue.rl_return()/num_episodes)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">"Q-learning"</span>, <span class="string">"Expected Sarsa"</span>]:</span><br><span class="line">    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    algorithm_stds = np.array([sem(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    plt.plot(step_sizes, algorithm_means, marker=<span class="string">'o'</span>, linestyle=<span class="string">'solid'</span>, label=algorithm)</span><br><span class="line">    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">"Step-size"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Sum of\n rewards\n per episode"</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">50</span>)</span><br><span class="line">plt.xticks(step_sizes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 600/600 [01:38&lt;00:00,  6.08it/s]</code></pre><p><img src="output_30_1.png" alt="png"></p><p>Expected Sarsa shows an advantage over Q-learning in this problem across a wide range of step-sizes.</p><p>Congratulations! Now you have:</p><ul><li>implemented Q-Learning with $\epsilon$-greedy action selection</li><li>implemented Expected Sarsa with $\epsilon$-greedy action selection</li><li>investigated the behavior of these two algorithms on Cliff World</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Q-Learning-and-Expected-Sarsa&quot;&gt;&lt;a href=&quot;#Assignment-2-Q-Learning-and-Expected-Sarsa&quot; class=&quot;headerlink&quot; title=&quot;Assignme
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Policy Evaluation in Cliff Walking Environment</title>
    <link href="https://zhangruochi.com/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/"/>
    <id>https://zhangruochi.com/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/</id>
    <published>2020-09-28T08:04:11.000Z</published>
    <updated>2020-09-28T08:06:19.087Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-Policy-Evaluation-in-Cliff-Walking-Environment"><a href="#Assignment-Policy-Evaluation-in-Cliff-Walking-Environment" class="headerlink" title="Assignment: Policy Evaluation in Cliff Walking Environment"></a>Assignment: Policy Evaluation in Cliff Walking Environment</h1><p>Welcome to the Course 2 Module 2 Programming Assignment! In this assignment, you will implement one of the fundamental sample and bootstrapping based model free reinforcement learning agents for prediction. This is namely one that uses one-step temporal difference learning, also known as TD(0). The task is to design an agent for policy evaluation in the Cliff Walking environment. Recall that policy evaluation is the prediction problem where the goal is to accurately estimate the values of states given some policy.</p><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ul><li>Implement parts of the Cliff Walking environment, to get experience specifying MDPs [Section 1].</li><li>Implement an agent that uses bootstrapping and, particularly, TD(0) [Section 2].</li><li>Apply TD(0) to estimate value functions for different policies, i.e., run policy evaluation experiments [Section 3].</li></ul><h2 id="The-Cliff-Walking-Environment"><a href="#The-Cliff-Walking-Environment" class="headerlink" title="The Cliff Walking Environment"></a>The Cliff Walking Environment</h2><p>The Cliff Walking environment is a gridworld with a discrete state space and discrete action space. The agent starts at grid cell S. The agent can move (deterministically) to the four neighboring cells by taking actions Up, Down, Left or Right. Trying to move out of the boundary results in staying in the same location. So, for example, trying to move left when at a cell on the leftmost column results in no movement at all and the agent remains in the same location. The agent receives -1 reward per step in most states, and -100 reward when falling off of the cliff. This is an episodic task; termination occurs when the agent reaches the goal grid cell G. Falling off of the cliff results in resetting to the start state, without termination.</p><p>The diagram below showcases the description above and also illustrates two of the policies we will be evaluating.</p><p><img src="cliffwalk.png" style="height:400px"></p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages."></a>Packages.</h2><p>We import the following libraries that are required for this assignment. We shall be using the following libraries:</p><ol><li>jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li><li>numpy: the fundamental package for scientific computing with Python.</li><li>matplotlib: the library for plotting graphs in Python.</li><li>RL-Glue: the library for reinforcement learning experiments.</li><li>BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.</li><li>Manager: the file allowing for visualization and testing.</li><li>itertools.product: the function that can be used easily to compute permutations.</li><li>tqdm.tqdm: Provides progress bars for visualizing the status of loops.</li></ol><p><strong>Please do not import other libraries</strong> — this will break the autograder.</p><p><strong>NOTE: For this notebook, there is no need to make any calls to methods of random number generators. Spurious or missing calls to random number generators may affect your results.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> Agent <span class="keyword">import</span> BaseAgent </span><br><span class="line"><span class="keyword">from</span> Environment <span class="keyword">import</span> BaseEnvironment  </span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> manager <span class="keyword">import</span> Manager</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure><h2 id="Section-1-Environment"><a href="#Section-1-Environment" class="headerlink" title="Section 1. Environment"></a>Section 1. Environment</h2><p>In the first part of this assignment, you will get to see how the Cliff Walking environment is implemented. You will also get to implement parts of it to aid your understanding of the environment and more generally how MDPs are specified. In particular, you will implement the logic for:</p><ol><li>Converting 2-dimensional coordinates to a single index for the state,</li><li>One of the actions (action up), and,</li><li>Reward and termination.</li></ol><p>Given below is an annotated diagram of the environment with more details that may help in completing the tasks of this part of the assignment. Note that we will be creating a more general environment where the height and width positions can be variable but the start, goal and cliff grid cells have the same relative positions (bottom left, bottom right and the cells between the start and goal grid cells respectively).</p><p><img src="cliffwalk-annotated.png" style="height:400px"></p><p>Once you have gone through the code and begun implementing solutions, it may be a good idea to come back here and see if you can convince yourself that the diagram above is an accurate representation of the code given and the code you have written.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty CliffWalkEnvironment class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CliffWalkEnvironment</span><span class="params">(BaseEnvironment)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># helper method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span><span class="params">(self, loc)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h2 id="env-init"><a href="#env-init" class="headerlink" title="env_init()"></a>env_init()</h2><p>The first function we add to the environment is the initialization function which is called once when an environment object is created. In this function, the grid dimensions and special locations (start and goal locations and the cliff locations) are stored for easy use later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_init</span><span class="params">(self, env_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the environment called when the experiment first starts.</span></span><br><span class="line"><span class="string">        Note:</span></span><br><span class="line"><span class="string">            Initialize a tuple with the reward, first state, boolean</span></span><br><span class="line"><span class="string">            indicating if it's terminal.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Note, we can setup the following variables later, in env_start() as it is equivalent. </span></span><br><span class="line">        <span class="comment"># Code is left here to adhere to the note above, but these variables are initialized once more</span></span><br><span class="line">        <span class="comment"># in env_start() [See the env_start() function below.]</span></span><br><span class="line">        </span><br><span class="line">        reward = <span class="keyword">None</span></span><br><span class="line">        state = <span class="keyword">None</span> <span class="comment"># See Aside</span></span><br><span class="line">        termination = <span class="keyword">None</span></span><br><span class="line">        self.reward_state_term = (reward, state, termination)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably </span></span><br><span class="line">        <span class="comment"># used with the term "state" for our purposes and for this assignment in particular. </span></span><br><span class="line">        <span class="comment"># A difference arises in the use of the terms when we have what is called Partial Observability where </span></span><br><span class="line">        <span class="comment"># the environment may return states that may not fully represent all the information needed to </span></span><br><span class="line">        <span class="comment"># predict values or make decisions (i.e., the environment is non-Markovian.)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the default height to 4 and width to 12 (as in the diagram given above)</span></span><br><span class="line">        self.grid_h = env_info.get(<span class="string">"grid_height"</span>, <span class="number">4</span>) </span><br><span class="line">        self.grid_w = env_info.get(<span class="string">"grid_width"</span>, <span class="number">12</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Now, we can define a frame of reference. Let positive x be towards the direction down and </span></span><br><span class="line">        <span class="comment"># positive y be towards the direction right (following the row-major NumPy convention.)</span></span><br><span class="line">        <span class="comment"># Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1 </span></span><br><span class="line">        <span class="comment"># and max y is then grid_w - 1. So, we have:</span></span><br><span class="line">        <span class="comment"># Starting location of agent is the bottom-left corner, (max x, min y). </span></span><br><span class="line">        self.start_loc = (self.grid_h - <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Goal location is the bottom-right corner. (max x, max y).</span></span><br><span class="line">        self.goal_loc = (self.grid_h - <span class="number">1</span>, self.grid_w - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The cliff will contain all the cells between the start_loc and goal_loc.</span></span><br><span class="line">        self.cliff = [(self.grid_h - <span class="number">1</span>, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, (self.grid_w - <span class="number">1</span>))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to </span></span><br><span class="line">        <span class="comment"># verify that your understanding of the above code is correct for the default case, i.e., where </span></span><br><span class="line">        <span class="comment"># height = 4 and width = 12.</span></span><br></pre></td></tr></table></figure><h2 id="Implement-state"><a href="#Implement-state" class="headerlink" title="Implement state()"></a><em>Implement</em> state()</h2><p>The agent location can be described as a two-tuple or coordinate (x, y) describing the agent’s position.<br>However, we can convert the (x, y) tuple into a single index and provide agents with just this integer.<br>One reason for this choice is that the spatial aspect of the problem is secondary and there is no need<br>for the agent to know about the exact dimensions of the environment.<br>From the agent’s viewpoint, it is just perceiving some states, accessing their corresponding values<br>in a table, and updating them. Both the coordinate (x, y) state representation and the converted coordinate representation are thus equivalent in this sense.</p><p>Given a grid cell location, the state() function should return the state; a single index corresponding to the location in the grid.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Example: Suppose grid_h is 2 and grid_w is 2. Then, we can write the grid cell two-tuple or coordinate</span><br><span class="line">states as follows (following the usual 0-index convention):</span><br><span class="line">|(0, 0) (0, 1)| |0 1|</span><br><span class="line">|(1, 0) (1, 1)| |2 3|</span><br><span class="line">Assuming row-major order as NumPy does,  we can flatten the latter to get a vector [0 1 2 3].</span><br><span class="line">So, if loc = (0, 0) we return 0. While, for loc = (1, 1) we return 3.</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [state]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Modify the return statement of this function to return a correct single index as </span></span><br><span class="line"><span class="comment"># the state (see the logic for this in the previous cell.)</span></span><br><span class="line"><span class="comment"># Lines: 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state</span><span class="params">(self, loc)</span>:</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> loc[<span class="number">0</span>] * <span class="number">12</span> + loc[<span class="number">1</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR STATE (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below corresponds to the annotated diagram for the environment</span></span><br><span class="line"><span class="comment">#       given previously and is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_state</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    coords_to_test = [(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">11</span>), (<span class="number">1</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">0</span>), (<span class="number">3</span>, <span class="number">9</span>), (<span class="number">3</span>, <span class="number">11</span>)]</span><br><span class="line">    true_states = [<span class="number">0</span>, <span class="number">11</span>, <span class="number">17</span>, <span class="number">36</span>, <span class="number">45</span>, <span class="number">47</span>]</span><br><span class="line">    output_states = [env.state(coords) <span class="keyword">for</span> coords <span class="keyword">in</span> coords_to_test]</span><br><span class="line">    <span class="keyword">assert</span>(output_states == true_states)</span><br><span class="line">test_state()</span><br></pre></td></tr></table></figure><h2 id="env-start"><a href="#env-start" class="headerlink" title="env_start()"></a>env_start()</h2><p>In env_start(), we initialize the agent location to be the start location and return the state corresponding to it as the first state for the agent to act upon. Additionally, we also set the reward and termination terms to be 0 and False respectively as they are consistent with the notion that there is no reward nor termination before the first action is even taken.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_start</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the episode starts, called before the</span></span><br><span class="line"><span class="string">    agent starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first state from the environment.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    reward = <span class="number">0</span></span><br><span class="line">    <span class="comment"># agent_loc will hold the current location of the agent</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br><span class="line">    <span class="comment"># state is the one dimensional state representation of the agent location.</span></span><br><span class="line">    state = self.state(self.agent_loc)</span><br><span class="line">    termination = <span class="keyword">False</span></span><br><span class="line">    self.reward_state_term = (reward, state, termination)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h2 id="Implement-env-step"><a href="#Implement-env-step" class="headerlink" title="Implement env_step()"></a><em>Implement</em> env_step()</h2><p>Once an action is taken by the agent, the environment must provide a new state, reward and termination signal. </p><p>In the Cliff Walking environment, agents move around using a 4-cell neighborhood called the Von Neumann neighborhood (<a href="https://en.wikipedia.org/wiki/Von_Neumann_neighborhood" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Von_Neumann_neighborhood</a>). Thus, the agent has 4 available actions at each state. Three of the actions have been implemented for you and your first task is to implement the logic for the fourth action (Action UP).</p><p>Your second task for this function is to implement the reward logic. Look over the environment description given earlier in this notebook if you need a refresher for how the reward signal is defined.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [env_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the code for action UP and implement the logic for reward and termination.</span></span><br><span class="line"><span class="comment"># Lines: ~7.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        action: The action taken by the agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (float, state, Boolean): a tuple of the reward, state,</span></span><br><span class="line"><span class="string">            and boolean indicating if it's terminal.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> action == <span class="number">0</span>: <span class="comment"># UP (Task 1)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Hint: Look at the code given for the other actions and think about the logic in them.</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] - <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &gt;= <span class="number">0</span>:</span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">1</span>: <span class="comment"># LEFT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &gt;= <span class="number">0</span>: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">2</span>: <span class="comment"># DOWN</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] + <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &lt; self.grid_h: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">3</span>: <span class="comment"># RIGHT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &lt; self.grid_w: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">raise</span> Exception(str(action) + <span class="string">" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!"</span>)</span><br><span class="line"></span><br><span class="line">    reward = <span class="number">-1</span></span><br><span class="line">    terminal = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Consider the initialization of reward and terminal variables above. Then, note the </span></span><br><span class="line">    <span class="comment"># conditional statements and comments given below and carefully ensure to set the variables reward </span></span><br><span class="line">    <span class="comment"># and terminal correctly for each case.</span></span><br><span class="line">    <span class="keyword">if</span> self.agent_loc == self.goal_loc: <span class="comment"># Reached Goal!</span></span><br><span class="line">        terminal = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">elif</span> self.agent_loc <span class="keyword">in</span> self.cliff: <span class="comment"># Fell into the cliff!</span></span><br><span class="line">        reward = <span class="number">-100</span></span><br><span class="line">        self.agent_loc = self.start_loc</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)</span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR ACTION UP (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is again limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_action_up</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">test_action_up()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR REWARD &amp; TERMINATION (10 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_reward</span><span class="params">()</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">0</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-100</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">2</span>, <span class="number">11</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == <span class="number">-1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">11</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="keyword">True</span>)</span><br><span class="line">test_reward()</span><br></pre></td></tr></table></figure><h2 id="env-cleanup"><a href="#env-cleanup" class="headerlink" title="env_cleanup()"></a>env_cleanup()</h2><p>There is not much cleanup to do for the Cliff Walking environment. Here, we simply reset the agent location to be the start location in this function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Cleanup done after the environment ends"""</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br></pre></td></tr></table></figure><h2 id="Section-2-Agent"><a href="#Section-2-Agent" class="headerlink" title="Section 2. Agent"></a>Section 2. Agent</h2><p>In this second part of the assignment, you will be implementing the key updates for Temporal Difference Learning. There are two cases to consider depending on whether an action leads to a terminal state or not.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty TDAgent class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span><span class="params">(self)</span>:</span>        </span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h2 id="agent-init"><a href="#agent-init" class="headerlink" title="agent_init()"></a>agent_init()</h2><p>As we did with the environment, we first initialize the agent once when a TDAgent object is created. In this function, we create a random number generator, seeded with the seed provided in the agent_info dictionary to get reproducible results. We also set the policy, discount and step size based on the agent_info dictionary. Finally, with a convention that the policy is always specified as a mapping from states to actions and so is an array of size (# States, # Actions), we initialize a values array of shape (# States,) to zeros.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">    <span class="string">"""Setup for the agent called when the experiment first starts."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a random number generator with the provided seed to seed the agent for reproducibility.</span></span><br><span class="line">    self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Policy will be given, recall that the goal is to accurately estimate its corresponding value function. </span></span><br><span class="line">    self.policy = agent_info.get(<span class="string">"policy"</span>)</span><br><span class="line">    <span class="comment"># Discount factor (gamma) to use in the updates.</span></span><br><span class="line">    self.discount = agent_info.get(<span class="string">"discount"</span>)</span><br><span class="line">    <span class="comment"># The learning rate or step size parameter (alpha) to use in updates.</span></span><br><span class="line">    self.step_size = agent_info.get(<span class="string">"step_size"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize an array of zeros that will hold the values.</span></span><br><span class="line">    <span class="comment"># Recall that the policy can be represented as a (# States, # Actions) array. With the </span></span><br><span class="line">    <span class="comment"># assumption that this is the case, we can use the first dimension of the policy to</span></span><br><span class="line">    <span class="comment"># initialize the array for values.</span></span><br><span class="line">    self.values = np.zeros((self.policy.shape[<span class="number">0</span>],))</span><br></pre></td></tr></table></figure><h1 id="agent-start"><a href="#agent-start" class="headerlink" title="agent_start()"></a>agent_start()</h1><p>In agent_start(), we choose an action based on the initial state and policy we are evaluating. We also cache the state so that we can later update its value when we perform a Temporal Difference update. Finally, we return the action chosen so that the RL loop can continue and the environment can execute this action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">    <span class="string">"""The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the environment's env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first action the agent takes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># The policy can be represented as a (# States, # Actions) array. So, we can use </span></span><br><span class="line">    <span class="comment"># the second dimension here when choosing an action.</span></span><br><span class="line">    action = self.rand_generator.choice(range(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="Implement-agent-step"><a href="#Implement-agent-step" class="headerlink" title="Implement agent_step()"></a><em>Implement</em> agent_step()</h2><p>In agent_step(), the agent must:</p><ul><li>Perform an update to improve the value estimate of the previously visited state, and</li><li>Act based on the state provided by the environment.</li></ul><p>The latter of the two steps above has been implemented for you. Implement the former. Note that, unlike later in agent_end(), the episode has not yet ended in agent_step(). in other words, the previously observed state was not a terminal state.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">    <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment's step after the last action, i.e., where the agent ended up after the</span></span><br><span class="line"><span class="string">            last action</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action the agent is taking.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: We should perform an update with the last state given that we now have the reward and</span></span><br><span class="line">    <span class="comment"># next state. We break this into two steps. Recall for example that the Monte-Carlo update </span></span><br><span class="line">    <span class="comment"># had the form: V[S_t] = V[S_t] + alpha * (target - V[S_t]), where the target was the return, G_t.</span></span><br><span class="line">    target = reward + self.discount * self.values[state]</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Having updated the value for the last state, we now act based on the current </span></span><br><span class="line">    <span class="comment"># state, and set the last state to be current one as we will next be making an </span></span><br><span class="line">    <span class="comment"># update with it when agent_step is called next once the action we return from this function </span></span><br><span class="line">    <span class="comment"># is executed in the environment.</span></span><br><span class="line"></span><br><span class="line">    action = self.rand_generator.choice(range(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="Implement-agent-end"><a href="#Implement-agent-end" class="headerlink" title="Implement agent_end()"></a><em>Implement</em> agent_end()</h2><p>Implement the TD update for the case where an action leads to a terminal state.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_end]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">    <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the terminal state.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Here too, we should perform an update with the last state given that we now have the </span></span><br><span class="line">    <span class="comment"># reward. Note that in this case, the action led to termination. Once more, we break this into </span></span><br><span class="line">    <span class="comment"># two steps, computing the target and the update itself that uses the target and the </span></span><br><span class="line">    <span class="comment"># current value estimate for the state whose value we are updating.</span></span><br><span class="line">    target = reward</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><h2 id="agent-cleanup"><a href="#agent-cleanup" class="headerlink" title="agent_cleanup()"></a>agent_cleanup()</h2><p>In cleanup, we simply reset the last state to be None to ensure that we are not storing any states past an episode.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Cleanup done after the agent ends."""</span></span><br><span class="line">    self.last_state = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><h2 id="agent-message"><a href="#agent-message" class="headerlink" title="agent_message()"></a>agent_message()</h2><p>agent_message() can generally be used to get different kinds of information about an RLGlue agent in the interaction loop of RLGlue. Here, we conditonally check for a message matching “get_values” and use it to retrieve the values table the agent has been updating over time.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">    <span class="string">"""A function used to pass information from the agent to the experiment.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        message: The message passed to the agent.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The response (or answer) to the message.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">"get_values"</span>:</span><br><span class="line">        <span class="keyword">return</span> self.values</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"TDAgent.agent_message(): Message not understood!"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR TD-UPDATES (20 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test belows serve as a good check in debugging your code for the TD updates. However, </span></span><br><span class="line"><span class="comment">#       as with the other tests, it is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_td_updates</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -1 and does not lead to a terminal state. This is in a simple two state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The first state's current value estimate is 0 while the second is 1.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>], [<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">"policy"</span>: np.array(policy_list), <span class="string">"discount"</span>: <span class="number">0.99</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>, <span class="number">1.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = <span class="number">-1</span></span><br><span class="line">    next_state = <span class="number">1</span></span><br><span class="line">    agent.agent_step(reward, next_state)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], <span class="number">-0.001</span>) <span class="keyword">and</span> np.isclose(agent.values[<span class="number">1</span>], <span class="number">1.</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -100 and lead to a terminal state. This is in a simple one state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The state's current value estimate is 0.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">"policy"</span>: np.array(policy_list), <span class="string">"discount"</span>: <span class="number">0.99</span>, <span class="string">"step_size"</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = <span class="number">-100</span></span><br><span class="line">    next_state = <span class="number">0</span></span><br><span class="line">    agent.agent_end(reward)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], <span class="number">-10</span>))</span><br><span class="line">    </span><br><span class="line">test_td_updates()</span><br></pre></td></tr></table></figure><h2 id="Section-3-Policy-Evaluation-Experiments"><a href="#Section-3-Policy-Evaluation-Experiments" class="headerlink" title="Section 3. Policy Evaluation Experiments"></a>Section 3. Policy Evaluation Experiments</h2><p>Finally, in this last part of the assignment, you will get to see the TD policy evaluation algorithm in action by looking at the estimated values, the per state value error and after the experiment is complete, the Mean Squared Value Error curve vs. episode number, summarizing how the value error changed over time.</p><p>The code below runs one run of an experiment given env_info and agent_info dictionaries. A “manager” object is created for visualizations and is used in part for the autograder. By default, the run will be for 5000 episodes. The true_values_file is specified to compare the learned value function with the values stored in the true_values_file. Plotting of the learned value  function occurs by default after every 100 episodes. In addition, when true_values_file is specified, the value error per state and the root mean square value error will also be plotted.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No. </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(env_info, agent_info, </span></span></span><br><span class="line"><span class="function"><span class="params">                   num_episodes=<span class="number">5000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   experiment_name=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                   plot_freq=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   true_values_file=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                   value_error_threshold=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    env = CliffWalkEnvironment</span><br><span class="line">    agent = TDAgent</span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line"></span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line">        <span class="keyword">if</span> episode % plot_freq == <span class="number">0</span>:</span><br><span class="line">            values = rl_glue.agent.agent_message(<span class="string">"get_values"</span>)</span><br><span class="line">            manager.visualize(values, episode)</span><br><span class="line"></span><br><span class="line">    values = rl_glue.agent.agent_message(<span class="string">"get_values"</span>)</span><br><span class="line">    <span class="keyword">if</span> true_values_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># Grading: The Manager will check that the values computed using your TD agent match </span></span><br><span class="line">        <span class="comment"># the true values (within some small allowance) across the states. In addition, it also</span></span><br><span class="line">        <span class="comment"># checks whether the root mean squared value error is close to 0.</span></span><br><span class="line">        manager.run_tests(values, value_error_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> values</span><br></pre></td></tr></table></figure><p>The cell below just runs a policy evaluation experiment with the determinstic optimal policy that strides just above the cliff. You should observe that the per state value error and RMSVE curve asymptotically go towards 0. The arrows in the four directions denote the probabilities of taking each action. This experiment is ungraded but should serve as a good test for the later experiments. The true values file provided for this experiment may help with debugging as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">"discount"</span>: <span class="number">1</span>, <span class="string">"step_size"</span>: <span class="number">0.01</span>, <span class="string">"seed"</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Optimal Policy that strides just along the cliff</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line"></span><br><span class="line">true_values_file = <span class="string">"optimal_policy_value_fn.npy"</span></span><br><span class="line">_ = run_experiment(env_info, agent_info, num_episodes=<span class="number">5000</span>, experiment_name=<span class="string">"Policy Evaluation on Optimal Policy"</span>,</span><br><span class="line">                   plot_freq=<span class="number">500</span>, true_values_file=true_values_file)</span><br></pre></td></tr></table></figure><pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The Safe Policy</span></span><br><span class="line"><span class="comment"># Hint: Fill in the array below (as done in the previous cell) based on the safe policy illustration </span></span><br><span class="line"><span class="comment"># in the environment diagram. This is the policy that strides as far as possible away from the cliff. </span></span><br><span class="line"><span class="comment"># We call it a "safe" policy because if the environment has any stochasticity, this policy would do a good job in </span></span><br><span class="line"><span class="comment"># keeping the agent from falling into the cliff (in contrast to the optimal policy shown before). </span></span><br><span class="line"><span class="comment"># BOILERPLATE:</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">24</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">12</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">11</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">policy[<span class="number">11</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">23</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH SAFE POLICY</span></span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line">v = run_experiment(env_info, agent_info,</span><br><span class="line">               experiment_name=<span class="string">"Policy Evaluation On Safe Policy"</span>,</span><br><span class="line">               num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">500</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A Near Optimal Stochastic Policy</span></span><br><span class="line"><span class="comment"># Now, we try a stochastic policy that deviates a little from the optimal policy seen above. </span></span><br><span class="line"><span class="comment"># This means we can get different results due to randomness.</span></span><br><span class="line"><span class="comment"># We will thus average the value function estimates we get over multiple runs. </span></span><br><span class="line"><span class="comment"># This can take some time, upto about 5 minutes from previous testing. </span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The autograder will compare . Re-run this cell upon making any changes.</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">"grid_height"</span>: <span class="number">4</span>, <span class="string">"grid_width"</span>: <span class="number">12</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">"discount"</span>: <span class="number">1</span>, <span class="string">"step_size"</span>: <span class="number">0.01</span>&#125;</span><br><span class="line"></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">'grid_width'</span>] * env_info[<span class="string">'grid_height'</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line">agent_info.update(&#123;<span class="string">"policy"</span>: policy&#125;)</span><br><span class="line">agent_info.update(&#123;<span class="string">"step_size"</span>: <span class="number">0.01</span>&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH NEAR OPTIMAL STOCHASTIC POLICY (40 POINTS)</span></span><br><span class="line">arr = []</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(<span class="number">30</span>)):</span><br><span class="line">    env_info[<span class="string">'seed'</span>] = i</span><br><span class="line">    agent_info[<span class="string">'seed'</span>] = i</span><br><span class="line">    v = run_experiment(env_info, agent_info,</span><br><span class="line">                   experiment_name=<span class="string">"Policy Evaluation On Optimal Policy"</span>,</span><br><span class="line">                   num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">10000</span>)</span><br><span class="line">    arr.append(v)</span><br><span class="line">average_v = np.array(arr).mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations, you have completed assignment 2! In this assignment, we investigated a very useful concept for sample-based online learning: temporal difference. We particularly looked at the prediction problem where the goal is to find the value function corresponding to a given policy. In the next assignment, by learning the action-value function instead of the state-value function, you will get to see how temporal difference learning can be used in control as well.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-Policy-Evaluation-in-Cliff-Walking-Environment&quot;&gt;&lt;a href=&quot;#Assignment-Policy-Evaluation-in-Cliff-Walking-Environment&quot; clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Assignment 4: Chatbot</title>
    <link href="https://zhangruochi.com/Assignment-4-Chatbot/2020/09/28/"/>
    <id>https://zhangruochi.com/Assignment-4-Chatbot/2020/09/28/</id>
    <published>2020-09-28T05:50:01.000Z</published>
    <updated>2020-09-28T09:00:27.567Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-4-Chatbot"><a href="#Assignment-4-Chatbot" class="headerlink" title="Assignment 4: Chatbot"></a>Assignment 4: Chatbot</h1><p><img src="cbot.jpg" height="400" width="400"> </p><p>Welcome to the last assignment of Course 4. Before you get started, we want to congratulate you on getting here. It is your 16th programming assignment in this Specialization and we are very proud of you! In this assignment, you are going to use the <a href="https://arxiv.org/abs/2001.04451" target="_blank" rel="noopener">Reformer</a>, also known as the efficient Transformer, to generate a dialogue between two bots. You will feed conversations to your model and it will learn how to understand the context of each one. Not only will it learn how to answer questions but it will also know how to ask questions if it needs more info. For example, after a customer asks for a train ticket, the chatbot can ask what time the said customer wants to leave. You can use this concept to automate call centers, hotel receptions, personal trainers, or any type of customer service. By completing this assignment, you will:</p><ul><li>Understand how the Reformer works</li><li>Explore the <a href="https://arxiv.org/abs/1810.00278" target="_blank" rel="noopener">MultiWoz</a> dataset</li><li>Process the data to feed it into the model</li><li>Train your model</li><li>Generate a dialogue by feeding a question to the model</li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">Part 1:   Exploring the MultiWoz dataset</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li><li><a href="#2">Part 2:   Processing the data for Reformer inputs</a><ul><li><a href="#2.1">2.1   Tokenizing, batching with bucketing</a></li></ul></li><li><a href="#3">Part 3:   Reversible layers</a><ul><li><a href="#ex02">Exercise 02</a></li><li><a href="#ex03">Exercise 03</a></li><li><a href="#3.1">3.1   Reversible layers and randomness</a></li></ul></li><li><a href="#4">Part 4:   ReformerLM Training</a><ul><li><a href="#ex04">Exercise 04</a></li><li><a href="#ex05">Exercise 05</a></li></ul></li><li><a href="#5">Part 5:   Decode from a pretrained model</a><ul><li><a href="#ex06">Exercise 06</a></li></ul></li></ul><p><a name="1"></a></p><h1 id="Part-1-Exploring-the-MultiWoz-dataset"><a href="#Part-1-Exploring-the-MultiWoz-dataset" class="headerlink" title="Part 1:   Exploring the MultiWoz dataset"></a>Part 1:   Exploring the MultiWoz dataset</h1><p>You will start by exploring the MultiWoz dataset. The dataset you are about to use has more than 10,000 human annotated dialogues and spans multiple domains and topics. Some dialogues include multiple domains and others include single domains. In this section, you will load and explore this dataset, as well as develop a function to extract the dialogues.</p><p>Let’s first import the modules we will be using:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax   </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line">!pip list | grep trax</span><br></pre></td></tr></table></figure><p>Let’s also declare some constants we will be using in the exercises.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># filename of the MultiWOZ dialogue dataset</span></span><br><span class="line">DATA_FILE = <span class="string">'data.json'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data directory</span></span><br><span class="line">DATA_DIR = <span class="string">'./data'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dictionary where we will load the dialogue dataset</span></span><br><span class="line">DIALOGUE_DB = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary filename</span></span><br><span class="line">VOCAB_FILE = <span class="string">'en_32k.subword'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary file directory</span></span><br><span class="line">VOCAB_DIR = <span class="string">'data/vocabs'</span></span><br></pre></td></tr></table></figure><p>Let’s now load the MultiWOZ 2.1 dataset. We have already provided it for you in your workspace. It is in JSON format so we should load it as such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># help function to load a JSON file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_json</span><span class="params">(directory, file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">f'<span class="subst">&#123;directory&#125;</span>/<span class="subst">&#123;file&#125;</span>'</span>) <span class="keyword">as</span> file: </span><br><span class="line">        db = json.load(file)</span><br><span class="line">    <span class="keyword">return</span> db</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the dialogue data set into our dictionary</span></span><br><span class="line">DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)</span><br></pre></td></tr></table></figure><p>Let’s see how many dialogues we have in the dictionary. 1 key-value pair is one dialogue so we can just get the dictionary’s length.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The number of dialogues is: <span class="subst">&#123;len(DIALOGUE_DB)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>The number of dialogues is: 10438</code></pre><p>The dialogues are composed of multiple files and the filenames are used as keys in our dictionary. Those with multi-domain dialogues have “MUL” in their filenames while single domain dialogues have either “SNG” or “WOZ”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print 7 keys from the dataset to see the filenames</span></span><br><span class="line">print(list(DIALOGUE_DB.keys())[<span class="number">0</span>:<span class="number">7</span>])</span><br></pre></td></tr></table></figure><pre><code>[&#39;SNG01856.json&#39;, &#39;SNG0129.json&#39;, &#39;PMUL1635.json&#39;, &#39;MUL2168.json&#39;, &#39;SNG0073.json&#39;, &#39;SNG01445.json&#39;, &#39;MUL2105.json&#39;]</code></pre><p>As you can see from the cells above, there are 10,438 conversations, each in its own file.  You will train your model on all those conversations. Each file is also loaded into a dictionary and each has two keys which are the following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get keys of the fifth file in the list above</span></span><br><span class="line">print(DIALOGUE_DB[<span class="string">'SNG0073.json'</span>].keys())</span><br></pre></td></tr></table></figure><pre><code>dict_keys([&#39;goal&#39;, &#39;log&#39;])</code></pre><p>The <code>goal</code> also points to a dictionary and it contains several keys pertaining to the objectives of the conversation. For example below, we can see that the conversation will be about booking a taxi.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'goal'</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;taxi&#39;: {&#39;info&#39;: {&#39;leaveAt&#39;: &#39;17:15&#39;,   &#39;destination&#39;: &#39;pizza hut fen ditton&#39;,   &#39;departure&#39;: &quot;saint john&#39;s college&quot;},  &#39;reqt&#39;: [&#39;car type&#39;, &#39;phone&#39;],  &#39;fail_info&#39;: {}}, &#39;police&#39;: {}, &#39;hospital&#39;: {}, &#39;hotel&#39;: {}, &#39;attraction&#39;: {}, &#39;train&#39;: {}, &#39;message&#39;: [&quot;You want to book a &lt;span class=&#39;emphasis&#39;&gt;taxi&lt;/span&gt;. The taxi should go to &lt;span class=&#39;emphasis&#39;&gt;pizza hut fen ditton&lt;/span&gt; and should depart from &lt;span class=&#39;emphasis&#39;&gt;saint john&#39;s college&lt;/span&gt;&quot;,  &quot;The taxi should &lt;span class=&#39;emphasis&#39;&gt;leave after 17:15&lt;/span&gt;&quot;,  &quot;Make sure you get &lt;span class=&#39;emphasis&#39;&gt;car type&lt;/span&gt; and &lt;span class=&#39;emphasis&#39;&gt;contact number&lt;/span&gt;&quot;], &#39;restaurant&#39;: {}}</code></pre><p>The <code>log</code> on the other hand contains the dialog. It is a list of dictionaries and each element of this list contains several descriptions as well. Let’s look at an example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get first element of the log list</span></span><br><span class="line">DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;text&#39;: &quot;I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton.&quot;, &#39;metadata&#39;: {}, &#39;dialog_act&#39;: {&#39;Taxi-Inform&#39;: [[&#39;Dest&#39;, &#39;pizza hut fen ditton&#39;],   [&#39;Depart&#39;, &quot;saint john &#39;s college&quot;]]}, &#39;span_info&#39;: [[&#39;Taxi-Inform&#39;, &#39;Dest&#39;, &#39;pizza hut fen ditton&#39;, 11, 14],  [&#39;Taxi-Inform&#39;, &#39;Depart&#39;, &quot;saint john &#39;s college&quot;, 6, 9]]}</code></pre><p>For this assignment, we are only interested in the conversation which is in the <code>text</code> field.<br>The conversation goes back and forth between two persons. Let’s call them ‘Person 1’ and ‘Person 2’. This implies that<br>data[‘SNG0073.json’][‘log’][0][‘text’] is ‘Person 1’ and<br>data[‘SNG0073.json’][‘log’][1][‘text’] is ‘Person 2’ and so on. The even offsets are ‘Person 1’ and the odd offsets are ‘Person 2’.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">' Person 1: '</span>, DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>][<span class="string">'text'</span>])</span><br><span class="line">print(<span class="string">' Person 2: '</span>,DIALOGUE_DB[<span class="string">'SNG0073.json'</span>][<span class="string">'log'</span>][<span class="number">1</span>][<span class="string">'text'</span>])</span><br></pre></td></tr></table></figure><pre><code> Person 1:  I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton. Person 2:  What time do you want to leave and what time do you want to arrive by?</code></pre><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p>You will now implement the <code>get_conversation()</code> function that will extract the conversations from the dataset’s file.</p><p><strong>Instructions:</strong> Implement a function to extract conversations from the input file.<br>As described above, the conversation is in the <code>text</code> field in each of the elements in the <code>log</code> list of the file. If the log list has <code>x</code> number of elements, then the function will get the <code>text</code> entries of each of those elements. Your function should return the conversation, prepending each field with either ‘ Person 1: ‘ if ‘x’ is even or ‘ Person 2: ‘ if ‘x’ is odd. You can use the Python modulus operator ‘%’ to help select the even/odd entries. Important note: Do not print a newline character (i.e. <code>\n</code>) when generating the string. For example, in the code cell above, your function should output something like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person 1: I would like a taxi from Saint John&apos;s college to Pizza Hut Fen Ditton. Person 2: What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure><p>and <strong>not</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1:  I would like a taxi from Saint John&apos;s college to Pizza Hut Fen Ditton.</span><br><span class="line">Person 2:  What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_conversation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conversation</span><span class="params">(file, data_db)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file (string): filename of the dialogue file saved as json</span></span><br><span class="line"><span class="string">        data_db (dict): dialogue database</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: A string containing the 'text' fields of  data[file]['log'][x]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize empty string</span></span><br><span class="line">    result = <span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get length of file's log list</span></span><br><span class="line">    len_msg_log = len(data_db[file][<span class="string">'log'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set the delimiter strings</span></span><br><span class="line">    delimiter_1 = <span class="string">' Person 1: '</span></span><br><span class="line">    delimiter_2 = <span class="string">' Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over the file's log list</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len_msg_log):</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># get i'th element of file log list</span></span><br><span class="line">        cur_log = data_db[file][<span class="string">'log'</span>][i][<span class="string">'text'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check if i is even</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:                   </span><br><span class="line">            <span class="comment"># append the 1st delimiter string</span></span><br><span class="line">            result += delimiter_1</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="comment"># append the 2nd delimiter string</span></span><br><span class="line">            result += delimiter_2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the message text from the log</span></span><br><span class="line">        result += cur_log</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> w4_unittest</span><br><span class="line">w4_unittest.test_get_conversation(get_conversation)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file = <span class="string">'SNG01856.json'</span></span><br><span class="line">conversation = get_conversation(file, DIALOGUE_DB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print raw output</span></span><br><span class="line">print(conversation)</span><br></pre></td></tr></table></figure><pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</code></pre><p><strong>Expected Result:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&apos;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&apos;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.</span><br><span class="line">Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</span><br></pre></td></tr></table></figure></p><p>We can have a utility pretty print function just so we can visually follow the conversation more easily.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_conversation</span><span class="params">(conversation)</span>:</span></span><br><span class="line">    </span><br><span class="line">    delimiter_1 = <span class="string">'Person 1: '</span></span><br><span class="line">    delimiter_2 = <span class="string">'Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    split_list_d1 = conversation.split(delimiter_1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> sublist <span class="keyword">in</span> split_list_d1[<span class="number">1</span>:]:</span><br><span class="line">        split_list_d2 = sublist.split(delimiter_2)</span><br><span class="line">        print(colored(<span class="string">f'Person 1: <span class="subst">&#123;split_list_d2[<span class="number">0</span>]&#125;</span>'</span>, <span class="string">'red'</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> len(split_list_d2) &gt; <span class="number">1</span>:</span><br><span class="line">            print(colored(<span class="string">f'Person 2: <span class="subst">&#123;split_list_d2[<span class="number">1</span>]&#125;</span>'</span>, <span class="string">'green'</span>))</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">print_conversation(conversation)</span><br></pre></td></tr></table></figure><p>For this assignment, we will just use the outputs of the calls to <code>get_conversation</code> to train the model. But just to expound, there are also other information in the MultiWoz dataset that can be useful in other contexts. Each element of the log list has more information about it. For example, above, if you were to look at the other fields for the following, “am looking for a place to stay that has cheap price range it should be in a type of hotel”, you will get the following. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">'SNG01856.json'</span>][<span class="string">'log'</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>{&#39;text&#39;: &#39;am looking for a place to to stay that has cheap price range it should be in a type of hotel&#39;, &#39;metadata&#39;: {}, &#39;dialog_act&#39;: {&#39;Hotel-Inform&#39;: [[&#39;Type&#39;, &#39;hotel&#39;], [&#39;Price&#39;, &#39;cheap&#39;]]}, &#39;span_info&#39;: [[&#39;Hotel-Inform&#39;, &#39;Type&#39;, &#39;hotel&#39;, 20, 20],  [&#39;Hotel-Inform&#39;, &#39;Price&#39;, &#39;cheap&#39;, 10, 10]]}</code></pre><p>The dataset also comes with hotel, hospital, taxi, train, police, and restaurant databases. For example, in case you need to call a doctor, or a hotel, or a taxi, this will allow you to automate the entire conversation. Take a look at the files accompanying the data set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the attractions file</span></span><br><span class="line">attraction_file = open(<span class="string">'data/attraction_db.json'</span>)</span><br><span class="line">attractions = json.load(attraction_file)</span><br><span class="line">print(attractions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;pool way, whitehill road, off newmarket road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;entrance fee&#39;: &#39;?&#39;, &#39;id&#39;: &#39;1&#39;, &#39;location&#39;: [52.208789, 0.154883], &#39;name&#39;: &#39;abbey pool and astroturf pitch&#39;, &#39;openhours&#39;: &#39;?&#39;, &#39;phone&#39;: &#39;01223902088&#39;, &#39;postcode&#39;: &#39;cb58nt&#39;, &#39;pricerange&#39;: &#39;?&#39;, &#39;type&#39;: &#39;swimmingpool&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hospital file</span></span><br><span class="line">hospital_file = open(<span class="string">'data/hospital_db.json'</span>)</span><br><span class="line">hospitals = json.load(hospital_file)</span><br><span class="line">print(hospitals[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;department&#39;: &#39;neurosciences critical care unit&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223216297&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hotel file</span></span><br><span class="line">hotel_file = open(<span class="string">'data/hotel_db.json'</span>)</span><br><span class="line">hotels = json.load(hotel_file)</span><br><span class="line">print(hotels[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;124 tenison road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;internet&#39;: &#39;yes&#39;, &#39;parking&#39;: &#39;no&#39;, &#39;id&#39;: &#39;0&#39;, &#39;location&#39;: [52.1963733, 0.1987426], &#39;name&#39;: &#39;a and b guest house&#39;, &#39;phone&#39;: &#39;01223315702&#39;, &#39;postcode&#39;: &#39;cb12dp&#39;, &#39;price&#39;: {&#39;double&#39;: &#39;70&#39;, &#39;family&#39;: &#39;90&#39;, &#39;single&#39;: &#39;50&#39;}, &#39;pricerange&#39;: &#39;moderate&#39;, &#39;stars&#39;: &#39;4&#39;, &#39;takesbookings&#39;: &#39;yes&#39;, &#39;type&#39;: &#39;guesthouse&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the police file</span></span><br><span class="line">police_file = open(<span class="string">'data/police_db.json'</span>)</span><br><span class="line">police = json.load(police_file)</span><br><span class="line">print(police[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;name&#39;: &#39;Parkside Police Station&#39;, &#39;address&#39;: &#39;Parkside, Cambridge&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223358966&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of a restuarant file</span></span><br><span class="line">restaurant_file = open(<span class="string">'data/restaurant_db.json'</span>)</span><br><span class="line">restaurants = json.load(restaurant_file)</span><br><span class="line">print(restaurants[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure><pre><code>{&#39;address&#39;: &#39;Regent Street City Centre&#39;, &#39;area&#39;: &#39;centre&#39;, &#39;food&#39;: &#39;italian&#39;, &#39;id&#39;: &#39;19210&#39;, &#39;introduction&#39;: &#39;Pizza hut is a large chain with restaurants nationwide offering convenience pizzas pasta and salads to eat in or take away&#39;, &#39;location&#39;: [52.20103, 0.126023], &#39;name&#39;: &#39;pizza hut city centre&#39;, &#39;phone&#39;: &#39;01223323737&#39;, &#39;postcode&#39;: &#39;cb21ab&#39;, &#39;pricerange&#39;: &#39;cheap&#39;, &#39;type&#39;: &#39;restaurant&#39;}</code></pre><p>For more information about the multiwoz 2.1 data set, please run the cell below to read the <code>ReadMe.txt</code> file. Feel free to open any other file to explore it. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'data/README'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    print(file.read())</span><br></pre></td></tr></table></figure><pre><code>###########################################################################################################  Copyright Cambridge Dialogue Systems Group, 2018 ###########################################################################################################Dataset contains the following files:1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have &quot;MUL&quot; in their names. Single domain dialogues have either &quot;SNG&quot; or &quot;WOZ&quot; in their names.2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.7. police_db.json: the Cambridge police station information.8. taxi_db.json: slot-value list for taxi domain.9. valListFile.txt: list of dialogues for validation.10. testListFile.txt: list of dialogues for testing.11. system_acts.json:  There are 6 domains (&#39;Booking&#39;, &#39;Restaurant&#39;, &#39;Hotel&#39;, &#39;Attraction&#39;, &#39;Taxi&#39;, &#39;Train&#39;) and 1 dummy domain (&#39;general&#39;).  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. &#39;Hotel-inform&#39; means it is an &#39;inform&#39; act in the Hotel domain.  Dialogue acts which cannot take slots, e.g., &#39;good bye&#39;, are defined under the &#39;general&#39; domain.  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.  If a dialogue act takes no slots, e.g., dialogue act &#39;offer booking&#39; for an utterance &#39;would you like to take a reservation?&#39;, its slot-value pair is [&#39;none&#39;, &#39;none&#39;]  There are four types of values:  1) If a slot takes a binary value, e.g., &#39;has Internet&#39; or &#39;has park&#39;, the value is either &#39;yes&#39; or &#39;no&#39;.  2) If a slot is under the act &#39;request&#39;, e.g., &#39;request&#39; about &#39;area&#39;, the value is expressed as &#39;?&#39;.  3) The value that appears in the utterance e.g., the name of a restaurant.  4) If for some reason the turn does not have an annotation then it is labeled as &quot;No Annotation.&quot;12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. </code></pre><p>As you can see, there are many other aspects of the MultiWoz dataset. Nonetheless, you’ll see that even with just the conversations, your model will still be able to generate useful responses. This concludes our exploration of the dataset. In the next section, we will do some preprocessing before we feed it into our model for training.</p><p><a name="2"></a></p><h1 id="Part-2-Processing-the-data-for-Reformer-inputs"><a href="#Part-2-Processing-the-data-for-Reformer-inputs" class="headerlink" title="Part 2:   Processing the data for Reformer inputs"></a>Part 2:   Processing the data for Reformer inputs</h1><p>You will now use the <code>get_conversation()</code> function to process the data. The Reformer expects inputs of this form: </p><p><strong>Person 1: Why am I so happy? Person 2: Because you are learning NLP Person 1: … Person 2: …*</strong></p><p>And the conversation keeps going with some text. As you can see ‘Person 1’ and ‘Person 2’ act as delimiters so the model automatically recognizes the person and who is talking. It can then come up with the corresponding text responses for each person. Let’s proceed to process the text in this fashion for the Reformer. First, let’s grab all the conversation strings from all dialogue files and put them in a list.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the keys are the file names</span></span><br><span class="line">all_files = DIALOGUE_DB.keys()</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize empty list</span></span><br><span class="line">untokenized_data = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># loop over all files</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> all_files:</span><br><span class="line">    <span class="comment"># this is the graded function you coded</span></span><br><span class="line">    <span class="comment"># returns a string delimited by Person 1 and Person 2</span></span><br><span class="line">    result = get_conversation(file, DIALOGUE_DB)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># append to the list</span></span><br><span class="line">    untokenized_data.append(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the first element to check if it's the same as the one we got before</span></span><br><span class="line">print(untokenized_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</code></pre><p>Now let us split the list to a train and eval dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle the list we generated above</span></span><br><span class="line">random.shuffle(untokenized_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a cutoff (5% of the total length for this assignment)</span></span><br><span class="line"><span class="comment"># convert to int because we will use it as a list index</span></span><br><span class="line">cut_off = int(len(untokenized_data) * <span class="number">.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># slice the list. the last elements after the cut_off value will be the eval set. the rest is for training. </span></span><br><span class="line">train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'number of conversations in the data set: <span class="subst">&#123;len(untokenized_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'number of conversations in train set: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'number of conversations in eval set: <span class="subst">&#123;len(eval_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>number of conversations in the data set: 10438number of conversations in train set: 9917number of conversations in eval set: 521</code></pre><p><a name="2.1"></a></p><h2 id="2-1-Tokenizing-batching-with-bucketing"><a href="#2-1-Tokenizing-batching-with-bucketing" class="headerlink" title="2.1   Tokenizing, batching with bucketing"></a>2.1   Tokenizing, batching with bucketing</h2><p>We can now proceed in generating tokenized batches of our data. Let’s first define a utility generator function to yield elements from our data sets:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stream</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># loop over the entire data</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="comment"># get a random element</span></span><br><span class="line">        d = random.choice(data)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># yield a tuple pair of identical values </span></span><br><span class="line">        <span class="comment"># (i.e. our inputs to the model will also be our targets during training)</span></span><br><span class="line">        <span class="keyword">yield</span> (d, d)</span><br></pre></td></tr></table></figure><p>Now let’s define our data pipeline for tokenizing and batching our data. As in the previous assignments, we will bucket by length and also have an upper bound on the token length.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trax allows us to use combinators to generate our data pipeline</span></span><br><span class="line">data_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># randomize the stream</span></span><br><span class="line">    trax.data.Shuffle(),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># tokenize the data</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=VOCAB_DIR,</span><br><span class="line">                       vocab_file=VOCAB_FILE),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># filter too long sequences</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># bucket by length</span></span><br><span class="line">    trax.data.BucketByLength(boundaries=[<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>],</span><br><span class="line">                             batch_sizes=[<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,   <span class="number">2</span>, <span class="number">1</span>]),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add loss weights but do not add it to the padding tokens (i.e. 0)</span></span><br><span class="line">    trax.data.AddLossWeights(id_to_mask=<span class="number">0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply the data pipeline to our train and eval sets</span></span><br><span class="line">train_stream = data_pipeline(stream(train_data))</span><br><span class="line">eval_stream = data_pipeline(stream(eval_data))</span><br></pre></td></tr></table></figure><p>Peek into the train stream.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the stream generators will yield (input, target, weights). let's just grab the input for inspection</span></span><br><span class="line">inp, _, _ = next(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the shape. format is (batch size, token length)</span></span><br><span class="line">print(<span class="string">"input shape: "</span>, inp.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># detokenize the first element</span></span><br><span class="line">print(trax.data.detokenize(inp[<span class="number">0</span>], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))</span><br></pre></td></tr></table></figure><pre><code>input shape:  (4, 512) Person 1: I need a place to stay that has free wifi.  Person 2: There are 32 options in Cambridge, what price range are you looking for? Person 1: I&#39;m looking for something in the cheap price range, but I need it to have a 4 star rating. I don&#39;t need any parking though. Person 2: Again I have many to choose from that meet those criteria. Would you like a suggestion? Person 1: Ok, yes, if you could suggest one that comes with free parking that would be great! Person 2: I will book it for you,is there anything else I can do for you ? Person 1: I also need a Vietnamese restaurant. Person 2: My apologies it appears that I forgot to book your lodging. I recommend Alexander Bed and Breakfast, would you like me to book it for you? Person 1: Oh yes, please do. I need it for 8 people and 5 nights, beginning friday Person 2: You are booked with the reference number E9100B48. I can help you with the Vietnamese restaurant now. Do you have an area in mind? Person 1: I just want the restaurant to be in the same price range as my hotel Person 2: There is one cheap vietnamese restaurant in town. It is thanh binh. Do you want to book? Person 1: No, just provide me with the address and area for that restaurant if you could Person 2: The restaurant is located at 17 Magdalene Street City Centre in the West.  Can I help you with anything else? Person 1: Yes, will you book me a taxi to the restaurant from the hotel, please Person 2: And what time would you like that taxi? Person 1: I would like to leave the hotel by 22:15. Person 2: Your taxi service was book with a red volkswagen. The contact number is 07797935179 in case you need to contact them. Person 1: Thank you, that will be all. Person 2: You are welcome enjoy your meal. Have a good evenening</code></pre><p><a name="3"></a></p><h1 id="Part-3-Reversible-layers"><a href="#Part-3-Reversible-layers" class="headerlink" title="Part 3:   Reversible layers"></a>Part 3:   Reversible layers</h1><p>When running large deep models, you will often run out of memory as each layer allocates memory to store activations for use in backpropagation. To save this resource, you need to be able to recompute these activations during the backward pass without storing them during the forward pass. Take a look first at the leftmost diagram below. </p><p><img src="reversible2.PNG" height="400" width="600"></p><p>This is how the residual networks are implemented in the standard Transformer. It follows that, given <code>F()</code> is Attention and <code>G()</code> is Feed-forward(FF).<br>: </p><p>\begin{align}<br>\mathrm{y}_\mathrm{a} &amp;= \mathrm{x} + \mathrm{F}\left(\mathrm{x}\right)\tag{1} \\<br>\mathrm{y}_{b}&amp;=\mathrm{y}_{a}+\mathrm{G}\left(\mathrm{y}_{a}\right)\tag{2}\\<br>\end{align}</p><p>As you can see, it requires that $\mathrm{x}$ and $\mathrm{y}_{a}$ be saved so it can be used during backpropagation. We want to avoid this to conserve memory and this is where reversible residual connections come in. They are shown in the middle and rightmost diagrams above. The key idea is that we will start with two copies of the input to the model and at each layer we will only update one of them. The activations that we <em>don’t</em> update are the ones that will be used to compute the residuals. </p><p>Now in this reversible set up you get the following instead: </p><p>\begin{align}<br>\mathrm{y}_{1}&amp;=\mathrm{x}_{1}+\mathrm{F}\left(\mathrm{x}_{2}\right)\tag{3}\\<br>\mathrm{y}_{2}&amp;=\mathrm{x}_{2}+\mathrm{G}\left(\mathrm{y}_{1}\right)\tag{4}\\<br>\end{align}<br>To recover $\mathrm{(x_1,x_2)}$ from $\mathrm{(y_1, y_2)}$ </p><p>\begin{align}<br>\mathrm{x}_{2}&amp;=\mathrm{y}_{2}-\mathrm{G}\left(\mathrm{y}_{1}\right)\tag{5}\\<br>\mathrm{x}_{1}&amp;=\mathrm{y}_{1}-\mathrm{F}\left(\mathrm{x}_{2}\right)\tag{6}\\<br>\end{align}</p><p>With this configuration, we’re now able to run the network fully in reverse. You’ll notice that during the backward pass, $\mathrm{x2}$ and $\mathrm{x1}$ can be recomputed based solely on the values of $\mathrm{y2}$ and $\mathrm{y1}$. No need to save it during the forward pass.</p><p><a name="ex02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> You will implement the <code>reversible_layer_forward</code> function using equations 3 and 4 above. This function takes in the input vector <code>x</code> and the functions <code>f</code> and <code>g</code> and returns the concatenation of $y_1 and y_2$. For this exercise, we will be splitting <code>x</code> before going through the reversible residual steps$\mathrm{^1}$.  We can then use those two vectors for the <code>reversible_layer_reverse</code> function. Utilize <code>np.concatenate()</code> to form the output being careful to match the axis of the <code>np.split()</code>.</p><p>$\mathrm{^1}$<em>Take note that this is just for demonstrating the concept in this exercise and there are other ways of processing the input. As you’ll see in the Reformer architecture later, the initial input (i.e. <code>x</code>) can instead be duplicated instead of split.</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_forward</span><span class="params">(x, f, g)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        x (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by 'x', f and g</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    x1, x2 = np.split(x, <span class="number">2</span>, axis=<span class="number">-1</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y1 using equation 3</span></span><br><span class="line">    y1 = x1 + f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y2 using equation 4</span></span><br><span class="line">    y2 = x2 + g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray</span></span><br><span class="line">    y = np.concatenate((y1,y2), axis = <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_forward(reversible_layer_forward)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p>You will now implement the <code>reversible_layer_reverse</code> function  which is possible because at every time step you have $x_1$ and $x_2$ and $y_2$ and $y_1$, along with the function <code>f</code>, and <code>g</code>. Where <code>f</code> is the attention and <code>g</code> is the feedforward. This allows you to compute equations 5 and 6.</p><p><strong>Instructions:</strong> Implement the <code>reversible_layer_reverse</code>. Your function takes in the output vector from  <code>reversible_layer_forward</code> and functions f and g. Using equations 5 and 6 above, it computes the inputs to the layer,  $x_1$ and $x_2$.  The output, x, is the concatenation of  $x_1, x_2$. Utilize <code>np.concatenate()</code>  to form the output being careful to match the axis of the <code>np.split()</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_reverse</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_reverse</span><span class="params">(y, f, g)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        y (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix of the form of 'y'</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix of the form of 'y'</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by 'y', f and g</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    y1, y2 = np.split(y, <span class="number">2</span>, axis=<span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x2 using equation 5</span></span><br><span class="line">    x2 = y2 - g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x1 using equation 6</span></span><br><span class="line">    x1 = y1 - f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate x1 and x2 along the depth dimension</span></span><br><span class="line">    x = np.concatenate((x1,x2),axis = <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_reverse(reversible_layer_reverse)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: assert at the end can be used in grading as well</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + <span class="number">2</span></span><br><span class="line">g = <span class="keyword">lambda</span> x: x * <span class="number">3</span></span><br><span class="line">input_vector = np.random.uniform(size=(<span class="number">32</span>,))</span><br><span class="line"></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector)</span><br></pre></td></tr></table></figure><p><a name="3.1"></a></p><h2 id="3-1-Reversible-layers-and-randomness"><a href="#3-1-Reversible-layers-and-randomness" class="headerlink" title="3.1   Reversible layers and randomness"></a>3.1   Reversible layers and randomness</h2><p>This is why we were learning about fastmath’s random functions and keys in Course 3 Week 1. Utilizing the same key, <code>trax.fastmath.random.uniform()</code> will return the same values. This is required for the backward pass to return the correct layer inputs when random noise is introduced in the layer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Layers like dropout have noise, so let's simulate it here:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + np.random.uniform(size=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that the above doesn't work any more:</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="keyword">not</span> np.allclose(reversed_vector, input_vector)  <span class="comment"># Fails!!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It failed because the noise when reversing used a different random seed.</span></span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">27686</span></span><br><span class="line">rng = trax.fastmath.random.get_prng(random_seed)</span><br><span class="line">f = <span class="keyword">lambda</span> x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that it works now as the same rng is used on forward and reverse.</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector,  atol=<span class="number">1e-07</span>)</span><br></pre></td></tr></table></figure><p><a name="4"></a></p><h1 id="Part-4-ReformerLM-Training"><a href="#Part-4-ReformerLM-Training" class="headerlink" title="Part 4:   ReformerLM Training"></a>Part 4:   ReformerLM Training</h1><p>You will now proceed to training your model. Since you have already know the two main components that differentiates it from the standard Transformer, LSH in Course 1 and reversible layers above, you can just use the pre-built model already implemented in Trax. It will have this architecture:</p><p><img src="Reformer.jpg"></p><p>Similar to the Transformer you learned earlier, you want to apply an attention and feed forward layer to your inputs. For the Reformer, we improve the memory efficiency by using <strong>reversible decoder blocks</strong> and you can picture its implementation in Trax like below:</p><p><img src="ReversibleDecoder.png"></p><p>You can see that it takes the initial inputs <code>x1</code> and <code>x2</code> and does the first equation of the reversible networks you learned in Part 3. As you’ve also learned, the reversible residual has two equations for the forward-pass so doing just one of them will just constitute half of the reversible decoder block. Before doing the second equation (i.e. second half of the reversible residual), it first needs to swap the elements to take into account the stack semantics in Trax. It simply puts <code>x2</code> on top of the stack so it can be fed to the add block of the half-residual layer. It then swaps the two outputs again so it can be fed to the next layer of the network. All of these arrives at the two equations in Part 3 and it can be used to recompute the activations during the backward pass.</p><p>These are already implemented for you in Trax and in the following exercise, you’ll get to practice how to call them to build your network.</p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement a wrapper function that returns a Reformer Language Model. You can use Trax’s <a href="https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.reformer.reformer.ReformerLM" target="_blank" rel="noopener">ReformerLM</a> to do this quickly. It will have the same architecture as shown above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM</span><span class="params">(vocab_size=<span class="number">33000</span>, n_layers=<span class="number">2</span>, mode=<span class="string">'train'</span>, attention_type=tl.SelfAttention)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        vocab_size (int): size of the vocabulary</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers</span></span><br><span class="line"><span class="string">        mode (string): setting of the model which can be 'train', 'eval', or 'predict' </span></span><br><span class="line"><span class="string">        attention_type(class): attention class to use </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        model (ReformerLM): a reformer language model implemented in Trax</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    <span class="comment"># initialize an instance of Trax's ReformerLM class</span></span><br><span class="line">    model = trax.models.reformer.ReformerLM( </span><br><span class="line">        <span class="comment"># set vocab size</span></span><br><span class="line">        vocab_size = vocab_size,</span><br><span class="line">        <span class="comment"># set number of layers</span></span><br><span class="line">        n_layers = n_layers,</span><br><span class="line">        <span class="comment"># set mode</span></span><br><span class="line">        mode = mode,</span><br><span class="line">        <span class="comment"># set attention type</span></span><br><span class="line">        attention_type = attention_type</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># display the model</span></span><br><span class="line">temp_model = ReformerLM(<span class="string">'train'</span>)</span><br><span class="line">print(str(temp_model))</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> temp_model</span><br></pre></td></tr></table></figure><pre><code>Serial[  ShiftRight(1)  Embedding_train_512  Dropout  PositionalEncoding  Dup_out2  ReversibleSerial_in2_out2[    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm      ]      SelfAttention    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm        Dense_2048        Dropout        FastGelu        Dense_512        Dropout      ]    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm      ]      SelfAttention    ]    ReversibleSwap_in2_out2    ReversibleHalfResidualV2_in2_out2[      Serial[        LayerNorm        Dense_2048        Dropout        FastGelu        Dense_512        Dropout      ]    ]    ReversibleSwap_in2_out2  ]  Concatenate_in2  LayerNorm  Dropout  Dense_train  LogSoftmax]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_ReformerLM(ReformerLM)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><p><a name="ex05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>You will now write a function that takes in your model and trains it. </p><p><strong>Instructions:</strong> Implement the <code>training_loop</code> below to train the neural network above. Here is a list of things you should do:</p><ul><li>Create <code>TrainTask</code> and <code>EvalTask</code></li><li>Create the training loop <code>trax.supervised.training.Loop</code></li><li>Pass in the following depending to train_task :<ul><li><code>labeled_data=train_gen</code></li><li><code>loss_layer=tl.CrossEntropyLoss()</code></li><li><code>optimizer=trax.optimizers.Adam(0.01)</code></li><li><code>lr_schedule=lr_schedule</code></li><li><code>n_steps_per_checkpoint=10</code>  </li></ul></li></ul><p>You will be using your CrossEntropyLoss loss function with Adam optimizer. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam" target="_blank" rel="noopener">trax</a> documentation to get a full understanding. </p><ul><li>Pass in the following to eval_task:<ul><li><code>labeled_data=eval_gen</code></li><li><code>metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]</code></li></ul></li></ul><p>This function should return a <code>training.Loop</code> object. To read more about this check the <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop" target="_blank" rel="noopener">docs</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span><span class="params">(ReformerLM, train_gen, eval_gen, output_dir = <span class="string">"./model/"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you are building</span></span><br><span class="line"><span class="string">        train_gen (generator): train data generator.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Validation generator. </span></span><br><span class="line"><span class="string">        output_dir (string): Path to save the model output. Defaults to './model/'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop for the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># use the warmup_and_rsqrt_decay learning rate schedule</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(</span><br><span class="line">        n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the train task</span></span><br><span class="line">    train_task = training.TrainTask(            </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = train_gen,</span><br><span class="line">        <span class="comment"># loss layer</span></span><br><span class="line">        loss_layer = tl.CrossEntropyLoss(),</span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line">        optimizer=trax.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">        <span class="comment"># lr_schedule</span></span><br><span class="line">        lr_schedule=lr_schedule,</span><br><span class="line">        <span class="comment"># n_steps</span></span><br><span class="line">        n_steps_per_checkpoint=<span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define the eval task</span></span><br><span class="line">    eval_task = training.EvalTask(                      </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = eval_gen,</span><br><span class="line">        metrics = [tl.CrossEntropyLoss(), tl.Accuracy()]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    loop = training.Loop(ReformerLM(mode=<span class="string">'train'</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: Use the train task and eval task for grading train_model</span></span><br><span class="line">test_loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">train_task = test_loop._task</span><br><span class="line">eval_task = test_loop._eval_task</span><br><span class="line"></span><br><span class="line">print(train_task)</span><br><span class="line">print(eval_task)</span><br></pre></td></tr></table></figure><pre><code>&lt;trax.supervised.training.TrainTask object at 0x7fd4ddf95dd0&gt;&lt;trax.supervised.training.EvalTask object at 0x7fd4dc2a2f50&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_tasks(train_task, eval_task)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we will now test your function</span></span><br><span class="line">!rm -f model/model.pkl.gz</span><br><span class="line">loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>Step      1: Ran 1 train steps in 58.71 secsStep      1: train CrossEntropyLoss |  10.41530514Step      1: eval  CrossEntropyLoss |  10.41272354Step      1: eval          Accuracy |  0.00000000Step     10: Ran 9 train steps in 163.46 secsStep     10: train CrossEntropyLoss |  10.25675583Step     10: eval  CrossEntropyLoss |  9.94296360Step     10: eval          Accuracy |  0.11201393</code></pre><p><strong>Approximate Expected output:</strong>  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Step      1: Ran 1 train steps in 55.73 secs</span><br><span class="line">Step      1: train CrossEntropyLoss |  10.41907787</span><br><span class="line">Step      1: eval  CrossEntropyLoss |  10.41005802</span><br><span class="line">Step      1: eval          Accuracy |  0.00000000</span><br><span class="line"></span><br><span class="line">Step     10: Ran 9 train steps in 108.21 secs</span><br><span class="line">Step     10: train CrossEntropyLoss |  10.15449715</span><br><span class="line">Step     10: eval  CrossEntropyLoss |  9.63478279</span><br><span class="line">Step     10: eval          Accuracy |  0.16350447</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">&lt;a name=&quot;5&quot;&gt;&lt;/a&gt;</span><br><span class="line"># Part 5:   Decode from a pretrained model</span><br><span class="line"></span><br><span class="line">We will now proceed on decoding using the model architecture you just implemented. As in the previous weeks, we will be giving you a pretrained model so you can observe meaningful output during inference. You will be using the [autoregressive_sample_stream()](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample_stream) decoding method from Trax to do fast inference. Let&apos;s define a few parameters to initialize our model.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention</span><br><span class="line">def attention(*args, **kwargs):</span><br><span class="line">    # number of input positions to remember in a cache when doing fast inference. </span><br><span class="line">    kwargs[&apos;predict_mem_len&apos;] = 120</span><br><span class="line">    # number of input elements to drop once the fast inference input cache fills up.</span><br><span class="line">    kwargs[&apos;predict_drop_len&apos;] = 120</span><br><span class="line">    # return the attention layer with the parameters defined above</span><br><span class="line">    return tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"># define the model using the ReformerLM function you implemented earlier.</span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=33000,</span><br><span class="line">    n_layers=6,</span><br><span class="line">    mode=&apos;predict&apos;,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.</span><br><span class="line">shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)</span><br></pre></td></tr></table></figure><p>We can now initialize our model from a file containing the pretrained weights. We will save this starting state so we can reset the model state when we generate a new conversation. This will become clearer in the <code>generate_dialogue()</code> function later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize from file</span></span><br><span class="line">model.init_from_file(<span class="string">'chatbot_model1.pkl.gz'</span>,</span><br><span class="line">                     weights_only=<span class="keyword">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save the starting state</span></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure><p>Let’s define a few utility functions as well to help us tokenize and detokenize. We can use the <a href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize" target="_blank" rel="noopener">tokenize()</a> and <a href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize" target="_blank" rel="noopener">detokenize()</a> from <code>trax.data.tf_inputs</code> to do this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentence, vocab_file, vocab_dir)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span><span class="params">(tokens, vocab_file, vocab_dir)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)</span><br></pre></td></tr></table></figure><p>We are now ready to define our decoding function. This will return a generator that yields that next symbol output by the model. It will be able to predict the next words by just feeding it a starting sentence.</p><p><a name="ex06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement the function below to return a generator that predicts the next word of the conversation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM_output_gen</span><span class="params">(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create input tokens using the the tokenize function</span></span><br><span class="line">    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add batch dimension to array. Convert from (n,) to (x, n) where </span></span><br><span class="line">    <span class="comment"># x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)</span></span><br><span class="line">    input_tokens_with_batch = np.expand_dims(input_tokens, axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># call the autoregressive_sample_stream function from trax</span></span><br><span class="line">    output_gen = trax.supervised.decoding.autoregressive_sample_stream( </span><br><span class="line">        <span class="comment"># model</span></span><br><span class="line">        ReformerLM,</span><br><span class="line">        <span class="comment"># inputs will be the tokens with batch dimension</span></span><br><span class="line">        inputs = input_tokens_with_batch,</span><br><span class="line">        <span class="comment"># temperature</span></span><br><span class="line">        temperature = temperature</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output_gen</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">WEIGHTS_FROM_FILE = ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'weights'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    WEIGHTS_FROM_FILE = pickle.load(file)</span><br><span class="line"></span><br><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    kwargs[<span class="string">'predict_mem_len'</span>] = <span class="number">120</span></span><br><span class="line">    kwargs[<span class="string">'predict_drop_len'</span>] = <span class="number">120</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">test_model = ReformerLM(vocab_size=<span class="number">5</span>, n_layers=<span class="number">1</span>, mode=<span class="string">'predict'</span>, attention_type=attention)</span><br><span class="line"></span><br><span class="line">test_output_gen = ReformerLM_output_gen(test_model, <span class="string">"test"</span>, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_model.init_weights_and_state(shape11)</span><br><span class="line"></span><br><span class="line">test_model.weights = WEIGHTS_FROM_FILE</span><br><span class="line"></span><br><span class="line">output = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    output.append(next(test_output_gen)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> test_model </span><br><span class="line"><span class="keyword">del</span> WEIGHTS_FROM_FILE</span><br><span class="line"><span class="keyword">del</span> test_output_gen</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure><pre><code>[1, 0, 4, 3, 0, 4]</code></pre><p><strong><em>Expected value:</em></strong></p><p>[1, 0, 4, 3, 0, 4]</p><p>Great! Now you will be able to see the model in action. The utility function below will call the generator you just implemented and will just format the output to be easier to read. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    kwargs[<span class="string">'predict_mem_len'</span>] = <span class="number">120</span>  <span class="comment"># max length for predictions</span></span><br><span class="line">    kwargs[<span class="string">'predict_drop_len'</span>] = <span class="number">120</span>  <span class="comment"># never drop old stuff</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=<span class="number">33000</span>,</span><br><span class="line">    n_layers=<span class="number">6</span>,</span><br><span class="line">    mode=<span class="string">'predict'</span>,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.init_from_file(<span class="string">'chatbot_model1.pkl.gz'</span>,</span><br><span class="line">                     weights_only=<span class="keyword">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_dialogue</span><span class="params">(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        model_state (np.array): initial state of the model before decoding</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        max_len (int): maximum number of tokens to generate </span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    """</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the delimiters we used during training</span></span><br><span class="line">    delimiter_1 = <span class="string">'Person 1: '</span> </span><br><span class="line">    delimiter_2 = <span class="string">'Person 2: '</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize detokenized output</span></span><br><span class="line">    sentence = <span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># token counter</span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output tokens. we insert a ': ' for formatting</span></span><br><span class="line">    result = [tokenize(<span class="string">': '</span>, vocab_file=vocab_file, vocab_dir=vocab_dir)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># reset the model state when starting a new dialogue</span></span><br><span class="line">    ReformerLM.state = model_state</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calls the output generator implemented earlier</span></span><br><span class="line">    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print the starting sentence</span></span><br><span class="line">    print(start_sentence.split(delimiter_2)[<span class="number">0</span>].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.</span></span><br><span class="line">    <span class="keyword">for</span> o <span class="keyword">in</span> output:</span><br><span class="line">        </span><br><span class="line">        result.append(o)</span><br><span class="line">        </span><br><span class="line">        sentence = detokenize(np.concatenate(result, axis=<span class="number">0</span>), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> sentence.endswith(delimiter_1):</span><br><span class="line">            sentence = sentence.split(delimiter_1)[<span class="number">0</span>]</span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;delimiter_2&#125;</span><span class="subst">&#123;sentence&#125;</span>'</span>)</span><br><span class="line">            sentence = <span class="string">''</span></span><br><span class="line">            result.clear()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> sentence.endswith(delimiter_2):</span><br><span class="line">            sentence = sentence.split(delimiter_2)[<span class="number">0</span>]</span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;delimiter_1&#125;</span><span class="subst">&#123;sentence&#125;</span>'</span>)</span><br><span class="line">            sentence = <span class="string">''</span></span><br><span class="line">            result.clear()</span><br><span class="line"></span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> counter &gt; max_len:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>We can now feed in different starting sentences and see how the model generates the dialogue. You can even input your own starting sentence. Just remember to ask a question that covers the topics in the Multiwoz dataset so you can generate a meaningful conversation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Are there theatres in town? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Are there theatres in town?Person 2: : There are 4 theatres in town. Do you have a preference on area? Person 1: No, I don&#39;t care. Which one do you recommend? Person 2: I would recommend the Mumford Theatre. Would you like more information on it? Person 1: Yes, could I get the postcode and phone number please? Person 2: The phone number is 08451962320 and the postcode is cb11pt. The phone number is 084519/ 15/15 - would you like to book a table? </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Is there a hospital nearby? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Is there a hospital nearby?Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need anything else? Person 1: No, that&#39;s all I need. Thanks. Person 2: You&#39;re welcome. Have a good day.Good bye.Person 1: Thanks again. Goodbye. Person 2: You&#39;re welcome. Have a good day.Good bye.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">' Person 1: Can you book a taxi? Person 2: '</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><pre><code>Person 1: Can you book a taxi?Person 2: : I sure can. Where are you going? Person 1: I&#39;m going to be picked up from the city centre north b and b. Person 2: I have booked you a grey volkswagen. The contact number is 0783212843. Person 1: Thank you. That&#39;s all I need. Person 2: Thank you for using our services. Have a great day!k you.Good bye.Person 1: Actually, I&#39;ry about there. </code></pre><p><strong>Congratulations! You just wrapped up the final assignment of this course and the entire specialization!</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-4-Chatbot&quot;&gt;&lt;a href=&quot;#Assignment-4-Chatbot&quot; class=&quot;headerlink&quot; title=&quot;Assignment 4: Chatbot&quot;&gt;&lt;/a&gt;Assignment 4: Chatbot&lt;/h1
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Assignment 3: Question Answering</title>
    <link href="https://zhangruochi.com/Assignment-3-Question-Answering/2020/09/27/"/>
    <id>https://zhangruochi.com/Assignment-3-Question-Answering/2020/09/27/</id>
    <published>2020-09-27T09:13:45.000Z</published>
    <updated>2020-09-27T09:15:12.027Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-3-Question-Answering"><a href="#Assignment-3-Question-Answering" class="headerlink" title="Assignment 3: Question Answering"></a>Assignment 3: Question Answering</h1><p>Welcome to this week’s assignment of course 4. In this you will explore question answering. You will implement the “Text to Text Transfer from Transformers” (better known as T5). Since you implemented transformers from scratch last week you will now be able to use them. </p><p><img src="qa.png"> </p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">Overview</a></li><li><a href="#0">Part 0: Importing the Packages</a></li><li><a href="#1">Part 1: C4 Dataset</a><ul><li><a href="#1.1">1.1 Pre-Training Objective</a></li><li><a href="#1.2">1.2 Process C4</a><ul><li><a href="#1.2.1">1.2.1 Decode to natural language</a></li></ul></li><li><a href="#1.3">1.3 Tokenizing and Masking</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li><li><a href="#1.4">1.4 Creating the Pairs</a></li></ul></li><li><a href="#2">Part 2: Transfomer</a><ul><li><a href="#2.1">2.1 Transformer Encoder</a><ul><li><a href="#2.1.1">2.1.1 The Feedforward Block</a><ul><li><a href="#ex02">Exercise 02</a></li></ul></li><li><a href="#2.1.2">2.1.2 The Encoder Block</a><ul><li><a href="#ex03">Exercise 03</a></li></ul></li><li><a href="#2.1.3">2.1.3 The Transformer Encoder</a>            <ul><li><a href="#ex04">Exercise 04</a></li></ul></li></ul></li></ul></li></ul><p><a name="0"></a></p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>This assignment will be different from the two previous ones. Due to memory and time constraints of this environment you will not be able to train a model and use it for inference. Instead you will create the necessary building blocks for the transformer encoder model and will use a pretrained version of the same model in two ungraded labs after this assignment.</p><p>After completing these 3 (1 graded and 2 ungraded) labs you will:</p><ul><li>Implement the code neccesary for Bidirectional Encoder Representation from Transformer (BERT).</li><li>Understand how the C4 dataset is structured.</li><li>Use a pretrained model for inference.</li><li>Understand how the “Text to Text Transfer from Transformers” or T5 model works. </li></ul><p><a name="0"></a></p><h1 id="Part-0-Importing-the-Packages"><a href="#Part-0-Importing-the-Packages" class="headerlink" title="Part 0: Importing the Packages"></a>Part 0: Importing the Packages</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> decoding</span><br><span class="line"></span><br><span class="line"><span class="comment"># Will come handy later.</span></span><br><span class="line">wrapper = textwrap.TextWrapper(width=<span class="number">70</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p><a name="1"></a></p><h2 id="Part-1-C4-Dataset"><a href="#Part-1-C4-Dataset" class="headerlink" title="Part 1: C4 Dataset"></a>Part 1: C4 Dataset</h2><p>The <a href="https://www.tensorflow.org/datasets/catalog/c4" target="_blank" rel="noopener">C4</a> is a huge data set. For the purpose of this assignment you will use a few examples out of it which are present in <code>data.txt</code>. C4 is based on the <a href="https://commoncrawl.org/" target="_blank" rel="noopener">common crawl</a> project. Feel free to read more on their website. </p><p>Run the cell below to see how the examples look like. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load example jsons</span></span><br><span class="line">example_jsons = list(map(ast.literal_eval, open(<span class="string">'data.txt'</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Printing the examples to see how the data looks like</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(<span class="string">f'example number <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>: \n\n<span class="subst">&#123;example_jsons[i]&#125;</span> \n'</span>)</span><br></pre></td></tr></table></figure><pre><code>example number 1: {&#39;content-length&#39;: b&#39;1970&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.&#39;, &#39;timestamp&#39;: b&#39;2019-04-25T12:57:54Z&#39;, &#39;url&#39;: b&#39;https://klyq.com/beginners-bbq-class-taking-place-in-missoula/&#39;} example number 2: {&#39;content-length&#39;: b&#39;12064&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Discussion in \&#39;Mac OS X Lion (10.7)\&#39; started by axboi87, Jan 20, 2012.\nI\&#39;ve got a 500gb internal drive and a 240gb SSD.\nWhen trying to restore using disk utility i\&#39;m given the error &quot;Not enough space on disk ____ to restore&quot;\nBut I shouldn\&#39;t have to do that!!!\nAny ideas or workarounds before resorting to the above?\nUse Carbon Copy Cloner to copy one drive to the other. I\&#39;ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\&#39;t be bootable. CCC usually works in &quot;file mode&quot; and it can easily copy a larger drive (that\&#39;s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\nI\&#39;ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\&#39;t fit is there was slightly more than 4 GB of data.&#39;, &#39;timestamp&#39;: b&#39;2019-04-21T10:07:13Z&#39;, &#39;url&#39;: b&#39;https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/&#39;} example number 3: {&#39;content-length&#39;: b&#39;5235&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.&#39;, &#39;timestamp&#39;: b&#39;2019-04-25T10:40:23Z&#39;, &#39;url&#39;: b&#39;https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way&#39;} example number 4: {&#39;content-length&#39;: b&#39;4967&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&quot;How many backlinks per day for new site?\nDiscussion in &#39;Black Hat SEO&#39; started by Omoplata, Dec 3, 2010.\n1) for a newly created site, what&#39;s the max # backlinks per day I should do to be safe?\n2) how long do I have to let my site age before I can start making more blinks?\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?&quot;, &#39;timestamp&#39;: b&#39;2019-04-21T12:46:19Z&#39;, &#39;url&#39;: b&#39;https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/&#39;} example number 5: {&#39;content-length&#39;: b&#39;4499&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\xe2\x80\x99s included in the mill levy measure.&#39;, &#39;timestamp&#39;: b&#39;2019-04-20T14:33:21Z&#39;, &#39;url&#39;: b&#39;http://bond.dpsk12.org/category/news/&#39;} </code></pre><p>Notice the <code>b</code> before each string? This means that this data comes as bytes rather than strings. Strings are actually lists of bytes so for the rest of the assignments the name <code>strings</code> will be used to describe the data. </p><p>To check this run the following cell:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type(example_jsons[<span class="number">0</span>].get(<span class="string">'text'</span>))</span><br></pre></td></tr></table></figure><pre><code>bytes</code></pre><p><a name="1.1"></a></p><h3 id="1-1-Pre-Training-Objective"><a href="#1-1-Pre-Training-Objective" class="headerlink" title="1.1 Pre-Training Objective"></a>1.1 Pre-Training Objective</h3><p><strong>Note:</strong> The word “mask” will be used throughout this assignment in context of hiding/removing word(s)</p><p>You will be implementing the BERT loss as shown in the following image. </p><p><img src="loss.png" width="600" height="400"></p><p>Assume you have the following text: <span style="color:blue"> **Thank you <span style="color:red">for inviting </span> me to your party <span style="color:red">last</span>  week** &lt;/span&gt; </span></p><p>Now as input you will mask the words in red in the text: </p><p><span style="color:blue"> <strong>Input:</strong></span> Thank you  <strong>X</strong> me to your party <strong>Y</strong> week.</p><p><span style="color:blue"><strong>Output:</strong></span> The model should predict the words(s) for <strong>X</strong> and <strong>Y</strong>. </p><p><strong>Z</strong> is used to represent the end.</p><p><a name="1.2"></a></p><h3 id="1-2-Process-C4"><a href="#1-2-Process-C4" class="headerlink" title="1.2 Process C4"></a>1.2 Process C4</h3><p>C4 only has the plain string <code>text</code> field, so you will tokenize and have <code>inputs</code> and <code>targets</code> out of it for supervised learning. Given your inputs, the goal is to predict the targets during training. </p><p>You will now take the <code>text</code> and convert it to <code>inputs</code> and <code>targets</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Grab text field from dictionary</span></span><br><span class="line">natural_language_texts = [example_json[<span class="string">'text'</span>] <span class="keyword">for</span> example_json <span class="keyword">in</span> example_jsons]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First text example</span></span><br><span class="line">natural_language_texts[<span class="number">4</span>]</span><br></pre></td></tr></table></figure><pre><code>b&#39;The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\xe2\x80\x99s included in the mill levy measure.&#39;</code></pre><p><a name="1.2.1"></a></p><h4 id="1-2-1-Decode-to-natural-language"><a href="#1-2-1-Decode-to-natural-language" class="headerlink" title="1.2.1 Decode to natural language"></a>1.2.1 Decode to natural language</h4><p>The following functions will help you <code>detokenize</code> and<code>tokenize</code> the text data.  </p><p>The <code>sentencepiece</code> vocabulary was used to convert from text to ids. This vocabulary file is loaded and used in this helper functions.</p><p><code>natural_language_texts</code> has the text from the examples we gave you. </p><p>Run the cells below to see what is going on. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Special tokens</span></span><br><span class="line">PAD, EOS, UNK = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span><span class="params">(np_array)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(</span><br><span class="line">        np_array,</span><br><span class="line">        vocab_type=<span class="string">'sentencepiece'</span>,</span><br><span class="line">        vocab_file=<span class="string">'sentencepiece.model'</span>,</span><br><span class="line">        vocab_dir=<span class="string">'.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(s)</span>:</span></span><br><span class="line">  <span class="comment"># The trax.data.tokenize function operates on streams,</span></span><br><span class="line">  <span class="comment"># that's why we have to create 1-element stream with iter</span></span><br><span class="line">  <span class="comment"># and later retrieve the result with next.</span></span><br><span class="line">    <span class="keyword">return</span> next(trax.data.tokenize(</span><br><span class="line">        iter([s]),</span><br><span class="line">        vocab_type=<span class="string">'sentencepiece'</span>,</span><br><span class="line">        vocab_file=<span class="string">'sentencepiece.model'</span>,</span><br><span class="line">        vocab_dir=<span class="string">'.'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># printing the encoding of each word to see how subwords are tokenized</span></span><br><span class="line">tokenized_text = [(tokenize(word).tolist(), word) <span class="keyword">for</span> word <span class="keyword">in</span> natural_language_texts[<span class="number">0</span>].split()]</span><br><span class="line">print(tokenized_text, <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><pre><code>[([12847, 277], b&#39;Beginners&#39;), ([15068], b&#39;BBQ&#39;), ([4501], b&#39;Class&#39;), ([3, 12297], b&#39;Taking&#39;), ([3399], b&#39;Place&#39;), ([16], b&#39;in&#39;), ([5964, 7115, 9, 55], b&#39;Missoula!&#39;), ([531], b&#39;Do&#39;), ([25], b&#39;you&#39;), ([241], b&#39;want&#39;), ([12], b&#39;to&#39;), ([129], b&#39;get&#39;), ([394], b&#39;better&#39;), ([44], b&#39;at&#39;), ([492], b&#39;making&#39;), ([3326], b&#39;delicious&#39;), ([15068, 58], b&#39;BBQ?&#39;), ([148], b&#39;You&#39;), ([56], b&#39;will&#39;), ([43], b&#39;have&#39;), ([8], b&#39;the&#39;), ([1004, 6], b&#39;opportunity,&#39;), ([474], b&#39;put&#39;), ([48], b&#39;this&#39;), ([30], b&#39;on&#39;), ([39], b&#39;your&#39;), ([4793], b&#39;calendar&#39;), ([230, 5], b&#39;now.&#39;), ([2721, 6], b&#39;Thursday,&#39;), ([1600], b&#39;September&#39;), ([1630, 727], b&#39;22nd&#39;), ([1715], b&#39;join&#39;), ([1150], b&#39;World&#39;), ([4501], b&#39;Class&#39;), ([15068], b&#39;BBQ&#39;), ([16127, 6], b&#39;Champion,&#39;), ([9137], b&#39;Tony&#39;), ([2659, 5595], b&#39;Balay&#39;), ([45], b&#39;from&#39;), ([301, 782, 3624], b&#39;Lonestar&#39;), ([14627, 15], b&#39;Smoke&#39;), ([12612, 277, 5], b&#39;Rangers.&#39;), ([216], b&#39;He&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([2119], b&#39;teaching&#39;), ([3, 9], b&#39;a&#39;), ([19529], b&#39;beginner&#39;), ([593], b&#39;level&#39;), ([853], b&#39;class&#39;), ([21], b&#39;for&#39;), ([921], b&#39;everyone&#39;), ([113], b&#39;who&#39;), ([2746], b&#39;wants&#39;), ([12], b&#39;to&#39;), ([129], b&#39;get&#39;), ([394], b&#39;better&#39;), ([28], b&#39;with&#39;), ([70], b&#39;their&#39;), ([17712], b&#39;culinary&#39;), ([1098, 5], b&#39;skills.&#39;), ([216], b&#39;He&#39;), ([56], b&#39;will&#39;), ([3884], b&#39;teach&#39;), ([25], b&#39;you&#39;), ([762], b&#39;everything&#39;), ([25], b&#39;you&#39;), ([174], b&#39;need&#39;), ([12], b&#39;to&#39;), ([214], b&#39;know&#39;), ([12], b&#39;to&#39;), ([5978], b&#39;compete&#39;), ([16], b&#39;in&#39;), ([3, 9], b&#39;a&#39;), ([3, 23405, 4547], b&#39;KCBS&#39;), ([15068], b&#39;BBQ&#39;), ([2259, 6], b&#39;competition,&#39;), ([379], b&#39;including&#39;), ([2097, 6], b&#39;techniques,&#39;), ([5459, 6], b&#39;recipes,&#39;), ([13618, 7, 6], b&#39;timelines,&#39;), ([3604], b&#39;meat&#39;), ([1801], b&#39;selection&#39;), ([11], b&#39;and&#39;), ([27856, 6], b&#39;trimming,&#39;), ([303], b&#39;plus&#39;), ([24190], b&#39;smoker&#39;), ([11], b&#39;and&#39;), ([1472], b&#39;fire&#39;), ([251, 5], b&#39;information.&#39;), ([37], b&#39;The&#39;), ([583], b&#39;cost&#39;), ([12], b&#39;to&#39;), ([36], b&#39;be&#39;), ([16], b&#39;in&#39;), ([8], b&#39;the&#39;), ([853], b&#39;class&#39;), ([19], b&#39;is&#39;), ([25264], b&#39;$35&#39;), ([399], b&#39;per&#39;), ([568, 6], b&#39;person,&#39;), ([11], b&#39;and&#39;), ([21], b&#39;for&#39;), ([21380, 7], b&#39;spectators&#39;), ([34], b&#39;it&#39;), ([19], b&#39;is&#39;), ([339, 5], b&#39;free.&#39;), ([15746, 26], b&#39;Included&#39;), ([16], b&#39;in&#39;), ([8], b&#39;the&#39;), ([583], b&#39;cost&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([893], b&#39;either&#39;), ([3, 9], b&#39;a&#39;), ([3, 17, 18, 9486], b&#39;t-shirt&#39;), ([42], b&#39;or&#39;), ([3, 9, 1409, 29], b&#39;apron&#39;), ([11], b&#39;and&#39;), ([25], b&#39;you&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([12246], b&#39;tasting&#39;), ([5977], b&#39;samples&#39;), ([13], b&#39;of&#39;), ([284], b&#39;each&#39;), ([3604], b&#39;meat&#39;), ([24], b&#39;that&#39;), ([19], b&#39;is&#39;), ([2657, 5], b&#39;prepared.&#39;)] </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We can see that detokenize successfully undoes the tokenization</span></span><br><span class="line">print(<span class="string">f"tokenized: <span class="subst">&#123;tokenize(<span class="string">'Beginners'</span>)&#125;</span>\ndetokenized: <span class="subst">&#123;detokenize(tokenize(<span class="string">'Beginners'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>tokenized: [12847   277]detokenized: Beginners</code></pre><p>As you can see above, you were able to take a piece of string and tokenize it. </p><p>Now you will create <code>input</code> and <code>target</code> pairs that will allow you to train your model. T5 uses the ids at the end of the vocab file as sentinels. For example, it will replace: </p><ul><li><code>vocab_size - 1</code> by <code>&lt;Z&gt;</code></li><li><code>vocab_size - 2</code> by <code>&lt;Y&gt;</code></li><li>and so forth. </li></ul><p>It assigns every word a <code>chr</code>.</p><p>The <code>pretty_decode</code> function below, which you will use in a bit, helps in handling the type when decoding. Take a look and try to understand what the function is doing.</p><p>Notice that:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">string.ascii_letters = <span class="string">'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'</span></span><br></pre></td></tr></table></figure></p><p><strong>NOTE:</strong> Targets may have more than the 52 sentinels we replace, but this is just to give you an idea of things.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = trax.data.vocab_size(</span><br><span class="line">    vocab_type=<span class="string">'sentencepiece'</span>,</span><br><span class="line">    vocab_file=<span class="string">'sentencepiece.model'</span>,</span><br><span class="line">    vocab_dir=<span class="string">'.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sentinels</span><span class="params">(vocab_size=vocab_size, display=False)</span>:</span></span><br><span class="line">    sentinels = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(reversed(string.ascii_letters), <span class="number">1</span>):</span><br><span class="line">        decoded_text = detokenize([vocab_size - i]) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Sentinels, ex: &lt;Z&gt; - &lt;a&gt;</span></span><br><span class="line">        sentinels[decoded_text] = <span class="string">f'&lt;<span class="subst">&#123;char&#125;</span>&gt;'</span>    </span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> display:</span><br><span class="line">            print(<span class="string">f'The sentinel is &lt;<span class="subst">&#123;char&#125;</span>&gt; and the decoded token is:'</span>, decoded_text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sentinels</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sentinels = get_sentinels(vocab_size, display=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>The sentinel is &lt;Z&gt; and the decoded token is: InternaționalThe sentinel is &lt;Y&gt; and the decoded token is: erwachseneThe sentinel is &lt;X&gt; and the decoded token is: CushionThe sentinel is &lt;W&gt; and the decoded token is: imunitarThe sentinel is &lt;V&gt; and the decoded token is: IntellectualThe sentinel is &lt;U&gt; and the decoded token is: traditiThe sentinel is &lt;T&gt; and the decoded token is: disguiseThe sentinel is &lt;S&gt; and the decoded token is: exerceThe sentinel is &lt;R&gt; and the decoded token is: nourisheThe sentinel is &lt;Q&gt; and the decoded token is: predominantThe sentinel is &lt;P&gt; and the decoded token is: amitiéThe sentinel is &lt;O&gt; and the decoded token is: erkenntThe sentinel is &lt;N&gt; and the decoded token is: dimensionThe sentinel is &lt;M&gt; and the decoded token is: inférieurThe sentinel is &lt;L&gt; and the decoded token is: refugiThe sentinel is &lt;K&gt; and the decoded token is: cheddarThe sentinel is &lt;J&gt; and the decoded token is: unterliegThe sentinel is &lt;I&gt; and the decoded token is: garanteazThe sentinel is &lt;H&gt; and the decoded token is: făcuteThe sentinel is &lt;G&gt; and the decoded token is: réglageThe sentinel is &lt;F&gt; and the decoded token is: pedepseThe sentinel is &lt;E&gt; and the decoded token is: GermainThe sentinel is &lt;D&gt; and the decoded token is: distinctlyThe sentinel is &lt;C&gt; and the decoded token is: SchraubThe sentinel is &lt;B&gt; and the decoded token is: emanatThe sentinel is &lt;A&gt; and the decoded token is: trimestreThe sentinel is &lt;z&gt; and the decoded token is: disrespectThe sentinel is &lt;y&gt; and the decoded token is: ErasmusThe sentinel is &lt;x&gt; and the decoded token is: AustraliaThe sentinel is &lt;w&gt; and the decoded token is: permeabilThe sentinel is &lt;v&gt; and the decoded token is: deseoriThe sentinel is &lt;u&gt; and the decoded token is: manipulatedThe sentinel is &lt;t&gt; and the decoded token is: suggérThe sentinel is &lt;s&gt; and the decoded token is: corespundThe sentinel is &lt;r&gt; and the decoded token is: nitroThe sentinel is &lt;q&gt; and the decoded token is: oyonsThe sentinel is &lt;p&gt; and the decoded token is: AccountThe sentinel is &lt;o&gt; and the decoded token is: échéanThe sentinel is &lt;n&gt; and the decoded token is: launderingThe sentinel is &lt;m&gt; and the decoded token is: genealogyThe sentinel is &lt;l&gt; and the decoded token is: QuickBooksThe sentinel is &lt;k&gt; and the decoded token is: constitutedThe sentinel is &lt;j&gt; and the decoded token is: FertigungThe sentinel is &lt;i&gt; and the decoded token is: goutteThe sentinel is &lt;h&gt; and the decoded token is: regulăThe sentinel is &lt;g&gt; and the decoded token is: overwhelminglyThe sentinel is &lt;f&gt; and the decoded token is: émergThe sentinel is &lt;e&gt; and the decoded token is: broyeurThe sentinel is &lt;d&gt; and the decoded token is: poveștiThe sentinel is &lt;c&gt; and the decoded token is: emulatorThe sentinel is &lt;b&gt; and the decoded token is: halloweenThe sentinel is &lt;a&gt; and the decoded token is: combustibil</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretty_decode</span><span class="params">(encoded_str_list, sentinels=sentinels)</span>:</span></span><br><span class="line">    <span class="comment"># If already a string, just do the replacements.</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(encoded_str_list, (str, bytes)):</span><br><span class="line">        <span class="keyword">for</span> token, char <span class="keyword">in</span> sentinels.items():</span><br><span class="line">            encoded_str_list = encoded_str_list.replace(token, char)</span><br><span class="line">        <span class="keyword">return</span> encoded_str_list</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># We need to decode and then prettyfy it.</span></span><br><span class="line">    <span class="keyword">return</span> pretty_decode(detokenize(encoded_str_list))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretty_decode(<span class="string">"I want to dress up as an Intellectual this halloween."</span>)</span><br></pre></td></tr></table></figure><pre><code>&#39;I want to dress up as an &lt;V&gt; this &lt;b&gt;.&#39;</code></pre><p>The functions above make your <code>inputs</code> and <code>targets</code> more readable. For example, you might see something like this once you implement the masking function below. </p><ul><li><span style="color:red"> Input sentence: </span> Younes and Lukasz were working together in the lab yesterday after lunch. </li><li><span style="color:red">Input: </span> Younes and Lukasz  <strong>Z</strong> together in the <strong>Y</strong> yesterday after lunch.</li><li><span style="color:red">Target: </span> <strong>Z</strong> were working <strong>Y</strong> lab.</li></ul><p><a name="1.3"></a></p><h3 id="1-3-Tokenizing-and-Masking"><a href="#1-3-Tokenizing-and-Masking" class="headerlink" title="1.3 Tokenizing and Masking"></a>1.3 Tokenizing and Masking</h3><p>You will now implement the <code>tokenize_and_mask</code> function. This function  will allow you to tokenize and mask input words with a noise probability. We usually mask 15% of the words.</p><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: tokenize_and_mask</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_mask</span><span class="params">(text, vocab_size=vocab_size, noise=<span class="number">0.15</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                      randomizer=np.random.uniform, tokenize=tokenize)</span>:</span></span><br><span class="line">    <span class="string">"""Tokenizes and masks a given input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        text (str or bytes): Text input.</span></span><br><span class="line"><span class="string">        vocab_size (int, optional): Size of the vocabulary. Defaults to vocab_size.</span></span><br><span class="line"><span class="string">        noise (float, optional): Probability of masking a token. Defaults to 0.15.</span></span><br><span class="line"><span class="string">        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.</span></span><br><span class="line"><span class="string">        tokenize (function, optional): Tokenizer function. Defaults to tokenize.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tuple: Tuple of lists of integers associated to inputs and targets.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current sentinel number (starts at 0)</span></span><br><span class="line">    cur_sentinel_num = <span class="number">0</span></span><br><span class="line">    <span class="comment"># inputs</span></span><br><span class="line">    inps = []</span><br><span class="line">    <span class="comment"># targets</span></span><br><span class="line">    targs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># prev_no_mask is True if the previous token was NOT masked, False otherwise</span></span><br><span class="line">    <span class="comment"># set prev_no_mask to True</span></span><br><span class="line">    prev_no_mask = <span class="keyword">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop through tokenized `text`</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokenize(text):</span><br><span class="line">        <span class="comment"># check if the `noise` is greater than a random value (weighted coin flip)</span></span><br><span class="line">        <span class="keyword">if</span> randomizer() &lt; noise:</span><br><span class="line">            <span class="comment"># check to see if the previous token was not masked</span></span><br><span class="line">            <span class="keyword">if</span> prev_no_mask==<span class="keyword">True</span>: <span class="comment"># add new masked token at end_id</span></span><br><span class="line">                <span class="comment"># number of masked tokens increases by 1</span></span><br><span class="line">                cur_sentinel_num += <span class="number">1</span></span><br><span class="line">                <span class="comment"># compute `end_id` by subtracting current sentinel value out of the total vocabulary size</span></span><br><span class="line">                end_id = vocab_size - cur_sentinel_num</span><br><span class="line">                <span class="comment"># append `end_id` at the end of the targets</span></span><br><span class="line">                targs.append(end_id)</span><br><span class="line">                <span class="comment"># append `end_id` at the end of the inputs</span></span><br><span class="line">                inps.append(end_id)</span><br><span class="line">            <span class="comment"># append `token` at the end of the targets</span></span><br><span class="line">            targs.append(token)</span><br><span class="line">            <span class="comment"># set prev_no_mask accordingly</span></span><br><span class="line">            prev_no_mask = <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># don't have two masked tokens in a row</span></span><br><span class="line">            <span class="comment"># append `token ` at the end of the inputs</span></span><br><span class="line">            inps.append(token)</span><br><span class="line">            <span class="comment"># set prev_no_mask accordingly</span></span><br><span class="line">            prev_no_mask = <span class="keyword">True</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> inps, targs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Some logic to mock a np.random value generator</span></span><br><span class="line"><span class="comment"># Needs to be in the same cell for it to always generate same output</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testing_rnd</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dummy_generator</span><span class="params">()</span>:</span></span><br><span class="line">        vals = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">        cyclic_vals = itertools.cycle(vals)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            <span class="keyword">yield</span> next(cyclic_vals)</span><br><span class="line"></span><br><span class="line">    dumr = itertools.cycle(dummy_generator())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dummy_randomizer</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> next(dumr)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy_randomizer</span><br><span class="line"></span><br><span class="line">input_str = natural_language_texts[<span class="number">0</span>]</span><br><span class="line">print(<span class="string">f"input string:\n\n<span class="subst">&#123;input_str&#125;</span>\n"</span>)</span><br><span class="line">inps, targs = tokenize_and_mask(input_str, randomizer=testing_rnd())</span><br><span class="line">print(<span class="string">f"tokenized inputs:\n\n<span class="subst">&#123;inps&#125;</span>\n"</span>)</span><br><span class="line">print(<span class="string">f"targets:\n\n<span class="subst">&#123;targs&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>input string:b&#39;Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.&#39;tokenized inputs:[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5, 216, 31993, 2119, 3, 9, 19529, 593, 853, 21, 921, 31992, 12, 129, 394, 28, 70, 17712, 1098, 5, 31991, 3884, 25, 762, 25, 174, 12, 214, 12, 31990, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 31989, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 31988, 6, 303, 24190, 11, 1472, 251, 5, 37, 31987, 36, 16, 8, 853, 19, 25264, 399, 568, 31986, 21, 21380, 7, 34, 19, 339, 5, 15746, 31985, 8, 583, 56, 36, 893, 3, 9, 3, 31984, 9486, 42, 3, 9, 1409, 29, 11, 25, 31983, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 31982]targets:[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 31993, 56, 36, 31992, 113, 2746, 31991, 216, 56, 31990, 5978, 16, 31989, 379, 2097, 31988, 11, 27856, 31987, 583, 12, 31986, 6, 11, 31985, 26, 16, 31984, 17, 18, 31983, 56, 36, 31982, 5]</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">b'Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'</span><br><span class="line"></span><br><span class="line">tokenized inputs:</span><br><span class="line"></span><br><span class="line">[<span class="number">31999</span>, <span class="number">15068</span>, <span class="number">4501</span>, <span class="number">3</span>, <span class="number">12297</span>, <span class="number">3399</span>, <span class="number">16</span>, <span class="number">5964</span>, <span class="number">7115</span>, <span class="number">31998</span>, <span class="number">531</span>, <span class="number">25</span>, <span class="number">241</span>, <span class="number">12</span>, <span class="number">129</span>, <span class="number">394</span>, <span class="number">44</span>, <span class="number">492</span>, <span class="number">31997</span>, <span class="number">58</span>, <span class="number">148</span>, <span class="number">56</span>, <span class="number">43</span>, <span class="number">8</span>, <span class="number">1004</span>, <span class="number">6</span>, <span class="number">474</span>, <span class="number">31996</span>, <span class="number">39</span>, <span class="number">4793</span>, <span class="number">230</span>, <span class="number">5</span>, <span class="number">2721</span>, <span class="number">6</span>, <span class="number">1600</span>, <span class="number">1630</span>, <span class="number">31995</span>, <span class="number">1150</span>, <span class="number">4501</span>, <span class="number">15068</span>, <span class="number">16127</span>, <span class="number">6</span>, <span class="number">9137</span>, <span class="number">2659</span>, <span class="number">5595</span>, <span class="number">31994</span>, <span class="number">782</span>, <span class="number">3624</span>, <span class="number">14627</span>, <span class="number">15</span>, <span class="number">12612</span>, <span class="number">277</span>, <span class="number">5</span>, <span class="number">216</span>, <span class="number">31993</span>, <span class="number">2119</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">19529</span>, <span class="number">593</span>, <span class="number">853</span>, <span class="number">21</span>, <span class="number">921</span>, <span class="number">31992</span>, <span class="number">12</span>, <span class="number">129</span>, <span class="number">394</span>, <span class="number">28</span>, <span class="number">70</span>, <span class="number">17712</span>, <span class="number">1098</span>, <span class="number">5</span>, <span class="number">31991</span>, <span class="number">3884</span>, <span class="number">25</span>, <span class="number">762</span>, <span class="number">25</span>, <span class="number">174</span>, <span class="number">12</span>, <span class="number">214</span>, <span class="number">12</span>, <span class="number">31990</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">23405</span>, <span class="number">4547</span>, <span class="number">15068</span>, <span class="number">2259</span>, <span class="number">6</span>, <span class="number">31989</span>, <span class="number">6</span>, <span class="number">5459</span>, <span class="number">6</span>, <span class="number">13618</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">3604</span>, <span class="number">1801</span>, <span class="number">31988</span>, <span class="number">6</span>, <span class="number">303</span>, <span class="number">24190</span>, <span class="number">11</span>, <span class="number">1472</span>, <span class="number">251</span>, <span class="number">5</span>, <span class="number">37</span>, <span class="number">31987</span>, <span class="number">36</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">853</span>, <span class="number">19</span>, <span class="number">25264</span>, <span class="number">399</span>, <span class="number">568</span>, <span class="number">31986</span>, <span class="number">21</span>, <span class="number">21380</span>, <span class="number">7</span>, <span class="number">34</span>, <span class="number">19</span>, <span class="number">339</span>, <span class="number">5</span>, <span class="number">15746</span>, <span class="number">31985</span>, <span class="number">8</span>, <span class="number">583</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">893</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">31984</span>, <span class="number">9486</span>, <span class="number">42</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">1409</span>, <span class="number">29</span>, <span class="number">11</span>, <span class="number">25</span>, <span class="number">31983</span>, <span class="number">12246</span>, <span class="number">5977</span>, <span class="number">13</span>, <span class="number">284</span>, <span class="number">3604</span>, <span class="number">24</span>, <span class="number">19</span>, <span class="number">2657</span>, <span class="number">31982</span>]</span><br><span class="line"></span><br><span class="line">targets:</span><br><span class="line"></span><br><span class="line">[<span class="number">31999</span>, <span class="number">12847</span>, <span class="number">277</span>, <span class="number">31998</span>, <span class="number">9</span>, <span class="number">55</span>, <span class="number">31997</span>, <span class="number">3326</span>, <span class="number">15068</span>, <span class="number">31996</span>, <span class="number">48</span>, <span class="number">30</span>, <span class="number">31995</span>, <span class="number">727</span>, <span class="number">1715</span>, <span class="number">31994</span>, <span class="number">45</span>, <span class="number">301</span>, <span class="number">31993</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">31992</span>, <span class="number">113</span>, <span class="number">2746</span>, <span class="number">31991</span>, <span class="number">216</span>, <span class="number">56</span>, <span class="number">31990</span>, <span class="number">5978</span>, <span class="number">16</span>, <span class="number">31989</span>, <span class="number">379</span>, <span class="number">2097</span>, <span class="number">31988</span>, <span class="number">11</span>, <span class="number">27856</span>, <span class="number">31987</span>, <span class="number">583</span>, <span class="number">12</span>, <span class="number">31986</span>, <span class="number">6</span>, <span class="number">11</span>, <span class="number">31985</span>, <span class="number">26</span>, <span class="number">16</span>, <span class="number">31984</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">31983</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">31982</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure><p>You will now use the inputs and the targets from the <code>tokenize_and_mask</code> function you implemented above. Take a look at the masked sentence using your <code>inps</code> and <code>targs</code> from the sentence above. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Inputs: \n\n'</span>, pretty_decode(inps))</span><br><span class="line">print(<span class="string">'\nTargets: \n\n'</span>, pretty_decode(targs))</span><br></pre></td></tr></table></figure><pre><code>Inputs:  &lt;Z&gt; BBQ Class Taking Place in Missoul &lt;Y&gt; Do you want to get better at making &lt;X&gt;? You will have the opportunity, put &lt;W&gt; your calendar now. Thursday, September 22 &lt;V&gt; World Class BBQ Champion, Tony Balay &lt;U&gt;onestar Smoke Rangers. He &lt;T&gt; teaching a beginner level class for everyone&lt;S&gt; to get better with their culinary skills.&lt;R&gt; teach you everything you need to know to &lt;Q&gt; a KCBS BBQ competition,&lt;P&gt;, recipes, timelines, meat selection &lt;O&gt;, plus smoker and fire information. The&lt;N&gt; be in the class is $35 per person &lt;M&gt; for spectators it is free. Include &lt;L&gt; the cost will be either a  &lt;K&gt;shirt or apron and you &lt;J&gt; tasting samples of each meat that is prepared &lt;I&gt;Targets:  &lt;Z&gt; Beginners &lt;Y&gt;a! &lt;X&gt; delicious BBQ &lt;W&gt; this on &lt;V&gt;nd join &lt;U&gt; from L &lt;T&gt; will be&lt;S&gt; who wants&lt;R&gt; He will &lt;Q&gt; compete in&lt;P&gt; including techniques &lt;O&gt; and trimming&lt;N&gt; cost to &lt;M&gt;, and &lt;L&gt;d in &lt;K&gt;t- &lt;J&gt; will be &lt;I&gt;.</code></pre><p><a name="1.4"></a></p><h3 id="1-4-Creating-the-Pairs"><a href="#1-4-Creating-the-Pairs" class="headerlink" title="1.4 Creating the Pairs"></a>1.4 Creating the Pairs</h3><p>You will now create pairs using your dataset. You will iterate over your data and create (inp, targ) pairs using the functions that we have given you. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply tokenize_and_mask</span></span><br><span class="line">inputs_targets_pairs = [tokenize_and_mask(text) <span class="keyword">for</span> text <span class="keyword">in</span> natural_language_texts]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_input_target_pairs</span><span class="params">(inputs_targets_pairs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i, inp_tgt_pair <span class="keyword">in</span> enumerate(inputs_targets_pairs, <span class="number">1</span>):</span><br><span class="line">        inps, tgts = inp_tgt_pair</span><br><span class="line">        inps, tgts = pretty_decode(inps), pretty_decode(tgts)</span><br><span class="line">        print(<span class="string">f'[<span class="subst">&#123;i&#125;</span>]\n\n'</span></span><br><span class="line">              <span class="string">f'inputs:\n<span class="subst">&#123;wrapper.fill(text=inps)&#125;</span>\n\n'</span></span><br><span class="line">              <span class="string">f'targets:\n<span class="subst">&#123;wrapper.fill(text=tgts)&#125;</span>\n\n\n\n'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_input_target_pairs(inputs_targets_pairs)</span><br></pre></td></tr></table></figure><pre><code>[1]inputs:Beginners BBQ Class Taking &lt;Z&gt; in Missoul &lt;Y&gt;! Do you want to getbetter at making delicious &lt;X&gt;? You will have the opportunity, &lt;W&gt;this on &lt;V&gt; calendar now. Thursday &lt;U&gt; September 22 &lt;T&gt; join&lt;S&gt; ClassBBQ Champion, Tony Balay from Lonestar Smoke&lt;R&gt;ers &lt;Q&gt; He will beteaching a beginner&lt;P&gt; class &lt;O&gt; everyone who wants&lt;N&gt; get better withtheir &lt;M&gt; skills &lt;L&gt; He will teach &lt;K&gt; everything you need to know to&lt;J&gt; in a KCBS BBQ &lt;I&gt; techniques, recipes, timelines, meat&lt;H&gt; andtrimming, plus smoker and fire information. The cost to be&lt;G&gt; theclass is $35 &lt;F&gt; person, and&lt;E&gt; spectators it is free. Included in thecost will&lt;D&gt; either &lt;C&gt; t- &lt;B&gt; or apron and you will be tastingsamples &lt;A&gt; each meat that &lt;z&gt; prepared.targets:&lt;Z&gt; Place &lt;Y&gt;a &lt;X&gt; BBQ &lt;W&gt; put &lt;V&gt; your &lt;U&gt;, &lt;T&gt;nd&lt;S&gt; World&lt;R&gt; Rang&lt;Q&gt;.&lt;P&gt; level &lt;O&gt; for&lt;N&gt; to &lt;M&gt; culinary &lt;L&gt;. &lt;K&gt; you &lt;J&gt; compete &lt;I&gt;competition, including&lt;H&gt; selection&lt;G&gt; in &lt;F&gt; per&lt;E&gt; for&lt;D&gt; be&lt;C&gt;a&lt;B&gt;shirt &lt;A&gt; of &lt;z&gt; is[2]inputs:&lt;Z&gt; in &#39;Mac OS X &lt;Y&gt; (10 &lt;X&gt;7)&#39; started by axb &lt;W&gt;i87, Jan 20, 2012.I&#39;ve got &lt;V&gt;a 500g &lt;U&gt; drive &lt;T&gt; a 240gb SSD. When trying to restoreusing&lt;S&gt; utility i&#39;m given the error &quot;Not enough space on disk&lt;R&gt;____to restore &lt;Q&gt; But I shouldn&#39;t have to do that!!! Any ideas orwork&lt;P&gt;s before &lt;O&gt;ing to the above? Use Carbon Copy Cloner to copyone drive to the other. I&#39;&lt;N&gt; done &lt;M&gt; several times going from &lt;L&gt;Dto &lt;K&gt; SSD and I wound &lt;J&gt; a bootable SSD drive. One step you &lt;I&gt;remember not to skip is to use Disk Utility to partition the SSD asGUID partition scheme&lt;H&gt; doing the &lt;G&gt;ne. If it came Apple &lt;F&gt;itionScheme, even if&lt;E&gt; let&lt;D&gt;CC do the clone, the resulting drive&lt;C&gt; boot&lt;B&gt;. C &lt;A&gt; usually works &lt;z&gt; &quot;file mode&quot; and it can easily copy alarger drive (that&#39;s mostly empty &lt;y&gt; onto a smaller drive.&lt;x&gt; you&lt;w&gt;CCC to clone a drive you did&lt;v&gt; boot&lt;u&gt;, it can work &lt;t&gt; copy mode &lt;s&gt;destination&lt;r&gt; must be&lt;q&gt; size or larger than the drive youare&lt;p&gt;cloning from &lt;o&gt;if &lt;n&gt; recall &lt;m&gt;ve actually done this somehowon Disk Utility &lt;l&gt; times&lt;k&gt;booting from &lt;j&gt;a different drive (or eventhe dvd)&lt;i&gt; not running disk utility from the drive your clo&lt;h&gt;ing)and had it work just fine from larger to smaller bootable clo&lt;g&gt;.Definitely format the drive cloning to first &lt;f&gt; as bootable Appleetc.. Thanks for &lt;e&gt; this out. My only experience &lt;d&gt; DU to go largerto smaller was when &lt;c&gt; trying to make  &lt;b&gt; install stick and I wasunable to restore InstallESD &lt;a&gt;dmg to a 4 GB Théâtre ofKeep thereason that wouldn&#39;t fit isdürftig was slightly moreutti GB of data.targets:&lt;Z&gt; Discussion &lt;Y&gt; Lion &lt;X&gt;. &lt;W&gt;o &lt;V&gt;  &lt;U&gt;b internal &lt;T&gt; and&lt;S&gt;disk&lt;R&gt;  &lt;Q&gt;&quot;&lt;P&gt;around &lt;O&gt; resort&lt;N&gt;ve &lt;M&gt; this &lt;L&gt; larger HD &lt;K&gt;smaller &lt;J&gt; up with &lt;I&gt; have to&lt;H&gt; HFS+ before&lt;G&gt;clo &lt;F&gt; Part&lt;E&gt;you&lt;D&gt; C&lt;C&gt; won&#39;t be &lt;B&gt;able &lt;A&gt;CC &lt;z&gt; in &lt;y&gt;)&lt;x&gt; If&lt;w&gt; tell&lt;v&gt; NOT&lt;u&gt;from &lt;t&gt; in block &lt;s&gt; where the&lt;r&gt; drive&lt;q&gt; the same&lt;p&gt;  &lt;o&gt; ( &lt;n&gt; I&lt;m&gt;). I&#39; &lt;l&gt; several&lt;k&gt; ( &lt;j&gt; &lt;i&gt; so&lt;h&gt;n&lt;g&gt;ne &lt;f&gt;,&lt;e&gt;pointing &lt;d&gt;using &lt;c&gt; I was &lt;b&gt;a Lion &lt;a&gt;. Théâtre USB stick butKeep coursedürftigthereutti than 4[3]inputs:&lt;Z&gt;il plaid &lt;Y&gt;lycra &lt;X&gt; spandex shortall with metallic slinky&lt;W&gt;sets. Attache &lt;V&gt; metallic elastic belt with O &lt;U&gt;ring. Head &lt;T&gt;included. Great hip hop&lt;S&gt; jazz dance costume.&lt;R&gt; in the USA.targets:&lt;Z&gt; Fo &lt;Y&gt;  &lt;X&gt; and &lt;W&gt; in &lt;V&gt;d &lt;U&gt;- &lt;T&gt;band&lt;S&gt; or&lt;R&gt; Made[4]inputs:How many backlink &lt;Z&gt; per day for new site? Discussion &lt;Y&gt; &#39;Black &lt;X&gt;SEO&#39; started by Omoplata, Dec 3, 2010. 1) for a &lt;W&gt; created site,what&#39;s &lt;V&gt; max &lt;U&gt;links per day I should do to be safe? 2) how &lt;T&gt; doI have&lt;S&gt; let my site&lt;R&gt; before I can start making more blinks? I didabout 6000 forum profiles every 24 hours for 10 days for &lt;Q&gt; of mysites&lt;P&gt; had a brand new domain. There is &lt;O&gt; backlinks for every&lt;N&gt;these &lt;M&gt; profile so &lt;L&gt;s 18 000 backlinks every 24 hours and nothinghappened in terms of being penalized &lt;K&gt; sandboxed. This is now maybe3 months ago &lt;J&gt; the site &lt;I&gt; ranking on first page for&lt;H&gt;a lot&lt;G&gt; mytargeted keywords. build more you can in starting &lt;F&gt; do manualsubmission and not spammy&lt;E&gt; means manual +&lt;D&gt; to&lt;C&gt; post.. &lt;B&gt; after1 month you can &lt;A&gt; a &lt;z&gt; blast.. Wow, dude, you built 18k backlink&lt;y&gt; a day&lt;x&gt; a brand&lt;w&gt;? How quickly did&lt;v&gt; rank up? What kind ofcompetition/search&lt;u&gt; did &lt;t&gt; keywords have?targets:&lt;Z&gt;s &lt;Y&gt; in &lt;X&gt; Hat &lt;W&gt; newly &lt;V&gt; the &lt;U&gt; # back &lt;T&gt; long&lt;S&gt; to&lt;R&gt; age&lt;Q&gt; one&lt;P&gt; which &lt;O&gt; three&lt;N&gt; of &lt;M&gt; forum &lt;L&gt; that &lt;K&gt; or &lt;J&gt; and &lt;I&gt;is&lt;H&gt; &lt;G&gt; of &lt;F&gt; but&lt;E&gt; type&lt;D&gt; relevant&lt;C&gt; the &lt;B&gt; then &lt;A&gt; make &lt;z&gt;big &lt;y&gt;s&lt;x&gt; on&lt;w&gt; new site&lt;v&gt; you&lt;u&gt;es &lt;t&gt; those[5]inputs:The Denver Board of Education opened the 2017-18 school year with anupdate &lt;Z&gt; projects that include new construction &lt;Y&gt; upgrades, heatmitigation &lt;X&gt; quality learning environments. We &lt;W&gt; excited &lt;V&gt;Denver students will be the beneficiaries &lt;U&gt;a four year, $572 millionGeneral Oblig &lt;T&gt; Bond.&lt;S&gt; the passage of the bond, our constructionteam has worked to schedule&lt;R&gt; projects over &lt;Q&gt; four-year term&lt;P&gt;bond. Denver voters on Tuesday approved bond and mill funding &lt;O&gt;for&lt;N&gt; in Denver Public Schools, agreeing to invest $572 million inbond funding &lt;M&gt; build and improve schools and &lt;L&gt;6.6 million inoperating dollars to support proven initiatives, &lt;K&gt; as early &lt;J&gt;Denver voters say &lt;I&gt; to bond and mill levy funding&lt;H&gt; for&lt;G&gt;PSstudents and schools. Click to learn more about the details of thevoter-approved &lt;F&gt; measure. Denver voters&lt;E&gt;. 8 approved bond and millfunding&lt;D&gt; for DPS students and schools. Learn more about what’sincluded in the mill &lt;C&gt;y measure.targets:&lt;Z&gt; on &lt;Y&gt;, &lt;X&gt; and &lt;W&gt; are &lt;V&gt; that &lt;U&gt; of  &lt;T&gt;ation&lt;S&gt; Since&lt;R&gt; the&lt;Q&gt; the&lt;P&gt; of the &lt;O&gt; measures&lt;N&gt; students &lt;M&gt; to &lt;L&gt; $5 &lt;K&gt; such &lt;J&gt;literacy. &lt;I&gt; yes&lt;H&gt; support&lt;G&gt; D &lt;F&gt; bond&lt;E&gt; on Nov&lt;D&gt; measures&lt;C&gt;lev</code></pre><p><a name="2"></a></p><h1 id="Part-2-Transfomer"><a href="#Part-2-Transfomer" class="headerlink" title="Part 2: Transfomer"></a>Part 2: Transfomer</h1><p>We now load a Transformer model checkpoint that has been pre-trained using the above C4 dataset and decode from it. This will save you a lot of time rather than have to train your model yourself. Later in this notebook, we will show you how to fine-tune your model.</p><p><img src="fulltransformer.png" width="300" height="600"></p><p>Start by loading in the model. We copy the checkpoint to local dir for speed, otherwise initialization takes a very long time. Last week you implemented the decoder part for the transformer. Now you will implement the encoder part. Concretely you will implement the following. </p><p><img src="encoder.png" width="300" height="600"></p><p><a name="2.1"></a></p><h3 id="2-1-Transformer-Encoder"><a href="#2-1-Transformer-Encoder" class="headerlink" title="2.1 Transformer Encoder"></a>2.1 Transformer Encoder</h3><p>You will now implement the transformer encoder. Concretely you will implement two functions. The first function is <code>FeedForwardBlock</code>.</p><p><a name="2.1.1"></a></p><h4 id="2-1-1-The-Feedforward-Block"><a href="#2-1-1-The-Feedforward-Block" class="headerlink" title="2.1.1 The Feedforward Block"></a>2.1.1 The Feedforward Block</h4><p>The <code>FeedForwardBlock</code> function is an important one so you will start by implementing it. To do so, you need to return a list of the following: </p><ul><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm" target="_blank" rel="noopener"><code>tl.LayerNorm()</code></a> = layer normalization.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener"><code>tl.Dense(d_ff)</code></a> = fully connected layer.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu" target="_blank" rel="noopener"><code>activation</code></a> = activation relu, tanh, sigmoid etc. </li><li><code>dropout_middle</code> = we gave you this function (don’t worry about its implementation).</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener"><code>tl.Dense(d_model)</code></a> = fully connected layer with same dimension as the model.</li><li><code>dropout_final</code> = we gave you this function (don’t worry about its implementation).</li></ul><p>You can always take a look at <a href="https://trax-ml.readthedocs.io/en/latest/" target="_blank" rel="noopener">trax documentation</a> if needed.</p><p><strong>Instructions</strong>: Implement the feedforward part of the transformer. You will be returning a list. </p><p><a name="ex02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: FeedForwardBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FeedForwardBlock</span><span class="params">(d_model, d_ff, dropout, dropout_shared_axes, mode, activation)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a list of layers implementing a feed-forward block.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: int:  depth of embedding</span></span><br><span class="line"><span class="string">        d_ff: int: depth of feed-forward layer</span></span><br><span class="line"><span class="string">        dropout: float: dropout rate (how much to drop out)</span></span><br><span class="line"><span class="string">        dropout_shared_axes: list of integers, axes to share dropout mask</span></span><br><span class="line"><span class="string">        mode: str: 'train' or 'eval'</span></span><br><span class="line"><span class="string">        activation: the non-linearity in feed-forward layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A list of layers which maps vectors to vectors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    dropout_middle = tl.Dropout(rate=dropout,</span><br><span class="line">                                shared_axes=dropout_shared_axes, </span><br><span class="line">                                mode=mode)</span><br><span class="line">  </span><br><span class="line">    dropout_final = tl.Dropout(rate=dropout, </span><br><span class="line">                               shared_axes=dropout_shared_axes, </span><br><span class="line">                               mode=mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    ff_block = [ </span><br><span class="line">        <span class="comment"># trax Layer normalization </span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># trax Dense layer using `d_ff`</span></span><br><span class="line">        tl.Dense(d_ff),</span><br><span class="line">        <span class="comment"># activation() layer - you need to call (use parentheses) this func!</span></span><br><span class="line">        activation(),</span><br><span class="line">        <span class="comment"># dropout middle layer</span></span><br><span class="line">        dropout_middle,</span><br><span class="line">        <span class="comment"># trax Dense layer using `d_model`</span></span><br><span class="line">        tl.Dense(d_model),</span><br><span class="line">        <span class="comment"># dropout final layer</span></span><br><span class="line">        dropout_final,</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ff_block</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the block layout</span></span><br><span class="line">feed_forward_example = FeedForwardBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.8</span>, dropout_shared_axes=<span class="number">0</span>, mode = <span class="string">'train'</span>, activation = tl.Relu)</span><br><span class="line">print(feed_forward_example)</span><br></pre></td></tr></table></figure><pre><code>[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]</span><br></pre></td></tr></table></figure><p><a name="2.1.2"></a></p><h4 id="2-1-2-The-Encoder-Block"><a href="#2-1-2-The-Encoder-Block" class="headerlink" title="2.1.2 The Encoder Block"></a>2.1.2 The Encoder Block</h4><p>The encoder block will use the <code>FeedForwardBlock</code>. </p><p>You will have to build two residual connections. Inside the first residual connection you will have the <code>tl.layerNorm()</code>, <code>attention</code>, and <code>dropout_</code> layers. The second residual connection will have the <code>feed_forward</code>.  </p><p>You will also need to implement <code>feed_forward</code>, <code>attention</code> and <code>dropout_</code> blocks. </p><p>So far you haven’t seen the <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.Attention" target="_blank" rel="noopener"><code>tl.Attention()</code></a> and <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual" target="_blank" rel="noopener"><code>tl.Residual()</code></a> layers so you can check the docs by clicking on them.</p><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: EncoderBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EncoderBlock</span><span class="params">(d_model, d_ff, n_heads, dropout, dropout_shared_axes,</span></span></span><br><span class="line"><span class="function"><span class="params">                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns a list of layers that implements a Transformer encoder block.</span></span><br><span class="line"><span class="string">    The input to the layer is a pair, (activations, mask), where the mask was</span></span><br><span class="line"><span class="string">    created from the original source tokens to prevent attending to the padding</span></span><br><span class="line"><span class="string">    part of the input.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model (int): depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        dropout_shared_axes (int): axes on which to share dropout mask.</span></span><br><span class="line"><span class="string">        mode (str): 'train' or 'eval'.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string">        FeedForwardBlock (function): A function that returns the feed forward block.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: A list of layers that maps (activations, mask) to (activations, mask).</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Attention block</span></span><br><span class="line">    attention = tl.Attention( </span><br><span class="line">        <span class="comment"># Use dimension of the model</span></span><br><span class="line">        d_feature=d_model,</span><br><span class="line">        <span class="comment"># Set it equal to number of attention heads</span></span><br><span class="line">        n_heads=n_heads,</span><br><span class="line">        <span class="comment"># Set it equal `dropout`</span></span><br><span class="line">        dropout=dropout,</span><br><span class="line">        <span class="comment"># Set it equal `mode`</span></span><br><span class="line">        mode=mode</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Call the function `FeedForwardBlock` (implemented before) and pass in the parameters</span></span><br><span class="line">    feed_forward = FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes, mode, ff_activation)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Dropout block</span></span><br><span class="line">    dropout_ = tl.Dropout( </span><br><span class="line">        <span class="comment"># set it equal to `dropout`</span></span><br><span class="line">        rate=dropout,</span><br><span class="line">        <span class="comment"># set it equal to the axes on which to share dropout mask</span></span><br><span class="line">        shared_axes=dropout_shared_axes,</span><br><span class="line">        <span class="comment"># set it equal to `mode`</span></span><br><span class="line">        mode=mode</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    encoder_block = [ </span><br><span class="line">        <span class="comment"># add `Residual` layer</span></span><br><span class="line">        tl.Residual(</span><br><span class="line">            <span class="comment"># add norm layer</span></span><br><span class="line">            tl.LayerNorm(),</span><br><span class="line">            <span class="comment"># add attention</span></span><br><span class="line">            attention,</span><br><span class="line">            <span class="comment"># add dropout</span></span><br><span class="line">            dropout_,</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># add another `Residual` layer</span></span><br><span class="line">        tl.Residual(</span><br><span class="line">            <span class="comment"># add feed forward</span></span><br><span class="line">            feed_forward,</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> encoder_block</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the block layout</span></span><br><span class="line">encoder_example = EncoderBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, n_heads=<span class="number">6</span>, dropout=<span class="number">0.8</span>, dropout_shared_axes=<span class="number">0</span>, mode = <span class="string">'train'</span>, ff_activation=tl.Relu)</span><br><span class="line">print(encoder_example)</span><br></pre></td></tr></table></figure><pre><code>[Serial_in2_out2[  Branch_in2_out3[    None    Serial_in2_out2[      LayerNorm      Serial_in2_out2[        Dup_out2        Dup_out2        Serial_in4_out2[          Parallel_in3_out3[            Dense_512            Dense_512            Dense_512          ]          PureAttention_in4_out2          Dense_512        ]      ]      Dropout    ]  ]  Add_in2], Serial[  Branch_out2[    None    Serial[      LayerNorm      Dense_2048      Relu      Dropout      Dense_512      Dropout    ]  ]  Add_in2]]</code></pre><h4 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[Serial_in2_out2[</span><br><span class="line">  Branch_in2_out3[</span><br><span class="line">    None</span><br><span class="line">    Serial_in2_out2[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Serial_in2_out2[</span><br><span class="line">        Dup_out2</span><br><span class="line">        Dup_out2</span><br><span class="line">        Serial_in4_out2[</span><br><span class="line">          Parallel_in3_out3[</span><br><span class="line">            Dense_512</span><br><span class="line">            Dense_512</span><br><span class="line">            Dense_512</span><br><span class="line">          ]</span><br><span class="line">          PureAttention_in4_out2</span><br><span class="line">          Dense_512</span><br><span class="line">        ]</span><br><span class="line">      ]</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">], Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Dense_2048</span><br><span class="line">      Relu</span><br><span class="line">      Dropout</span><br><span class="line">      Dense_512</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">]]</span><br></pre></td></tr></table></figure><p><a name="2.1.3"></a></p><h3 id="2-1-3-The-Transformer-Encoder"><a href="#2-1-3-The-Transformer-Encoder" class="headerlink" title="2.1.3 The Transformer Encoder"></a>2.1.3 The Transformer Encoder</h3><p>Now that you have implemented the <code>EncoderBlock</code>, it is time to build the full encoder. BERT, or Bidirectional Encoder Representations from Transformers is one such encoder. </p><p>You will implement its core code in the function below by using the functions you have coded so far. </p><p>The model takes in many hyperparameters, such as the <code>vocab_size</code>, the number of classes, the dimension of your model, etc. You want to build a generic function that will take in many parameters, so you can use it later. At the end of the day, anyone can just load in an API and call transformer, but we think it is important to make sure you understand how it is built. Let’s get started. </p><p><strong>Instructions:</strong> For this encoder you will need a <code>positional_encoder</code> first (which is already provided) followed by <code>n_layers</code> encoder blocks, which are the same encoder blocks you previously built. Once you store the <code>n_layers</code> <code>EncoderBlock</code> in a list, you are going to encode a <code>Serial</code> layer with the following sublayers: </p><ul><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch" target="_blank" rel="noopener"><code>tl.Branch</code></a>: helps with the branching and has the following sublayers:<ul><li><code>positional_encoder</code>.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PaddingMask" target="_blank" rel="noopener"><code>tl.PaddingMask()</code></a>: layer that maps integer sequences to padding masks.</li></ul></li><li>Your list of <code>EncoderBlock</code>s</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select" target="_blank" rel="noopener"><code>tl.Select([0], n_in=2)</code></a>:  Copies, reorders, or deletes stack elements according to indices.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm" target="_blank" rel="noopener"><code>tl.LayerNorm()</code></a>.</li><li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean" target="_blank" rel="noopener"><code>tl.Mean()</code></a>: Mean along the first axis.</li><li><code>tl.Dense()</code> with n_units set to n_classes. </li><li><code>tl.LogSoftmax()</code>   </li></ul><p>Please refer to the <a href="https://trax-ml.readthedocs.io/en/latest/" target="_blank" rel="noopener">trax documentation</a> for further information. </p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TransformerEncoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TransformerEncoder</span><span class="params">(vocab_size=vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                       n_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       d_ff=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       n_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       dropout_shared_axes=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                       max_len=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       mode=<span class="string">'train'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       ff_activation=tl.Relu,</span></span></span><br><span class="line"><span class="function"><span class="params">                       EncoderBlock=EncoderBlock)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns a Transformer encoder model.</span></span><br><span class="line"><span class="string">    The input to the model is a tensor of tokens.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int): vocab size. Defaults to vocab_size.</span></span><br><span class="line"><span class="string">        n_classes (int): how many classes on output. Defaults to 10.</span></span><br><span class="line"><span class="string">        d_model (int): depth of embedding. Defaults to 512.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer. Defaults to 2048.</span></span><br><span class="line"><span class="string">        n_layers (int): number of encoder/decoder layers. Defaults to 6.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads. Defaults to 8.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out). Defaults to 0.1.</span></span><br><span class="line"><span class="string">        dropout_shared_axes (int): axes on which to share dropout mask. Defaults to None.</span></span><br><span class="line"><span class="string">        max_len (int): maximum symbol length for positional encoding. Defaults to 2048.</span></span><br><span class="line"><span class="string">        mode (str): 'train' or 'eval'. Defaults to 'train'.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer. Defaults to tl.Relu.</span></span><br><span class="line"><span class="string">        EncoderBlock (function): Returns the encoder block. Defaults to EncoderBlock.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: A Transformer model as a layer that maps</span></span><br><span class="line"><span class="string">        from a tensor of tokens to activations over a set of output classes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    positional_encoder = [</span><br><span class="line">        tl.Embedding(vocab_size, d_model),</span><br><span class="line">        tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode),</span><br><span class="line">        tl.PositionalEncoding(max_len=max_len)</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use the function `EncoderBlock` (implemented above) and pass in the parameters over `n_layers`</span></span><br><span class="line">    encoder_blocks = [EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,</span><br><span class="line">                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_layers)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Assemble and return the model.</span></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        <span class="comment"># Encode</span></span><br><span class="line">        tl.Branch(</span><br><span class="line">            <span class="comment"># Use `positional_encoder`</span></span><br><span class="line">            positional_encoder,</span><br><span class="line">            <span class="comment"># Use trax padding mask</span></span><br><span class="line">            tl.PaddingMask(),</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># Use `encoder_blocks`</span></span><br><span class="line">        encoder_blocks,</span><br><span class="line">        <span class="comment"># Use select layer</span></span><br><span class="line">        tl.Select([<span class="number">0</span>], n_in=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># Use trax layer normalization</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># Map to output categories.</span></span><br><span class="line">        <span class="comment"># Use trax mean. set axis to 1</span></span><br><span class="line">        tl.Mean(axis = <span class="number">1</span>),</span><br><span class="line">        <span class="comment"># Use trax Dense using `n_classes`</span></span><br><span class="line">        tl.Dense(n_classes),</span><br><span class="line">        <span class="comment"># Use trax log softmax</span></span><br><span class="line">        tl.LogSoftmax(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this cell to see the structure of your model</span></span><br><span class="line"><span class="comment"># Only 1 layer is used to keep the output readable</span></span><br><span class="line">TransformerEncoder(n_layers=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>Serial[  Branch_out2[    [Embedding_32000_512, Dropout, PositionalEncoding]    PaddingMask(0)  ]  Serial_in2_out2[    Branch_in2_out3[      None      Serial_in2_out2[        LayerNorm        Serial_in2_out2[          Dup_out2          Dup_out2          Serial_in4_out2[            Parallel_in3_out3[              Dense_512              Dense_512              Dense_512            ]            PureAttention_in4_out2            Dense_512          ]        ]        Dropout      ]    ]    Add_in2  ]  Serial[    Branch_out2[      None      Serial[        LayerNorm        Dense_2048        Relu        Dropout        Dense_512        Dropout      ]    ]    Add_in2  ]  Select[0]_in2  LayerNorm  Mean  Dense_10  LogSoftmax]</code></pre><h4 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    [Embedding_32000_512, Dropout, PositionalEncoding]</span><br><span class="line">    PaddingMask(<span class="number">0</span>)</span><br><span class="line">  ]</span><br><span class="line">  Serial_in2_out2[</span><br><span class="line">    Branch_in2_out3[</span><br><span class="line">      None</span><br><span class="line">      Serial_in2_out2[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Serial_in2_out2[</span><br><span class="line">          Dup_out2</span><br><span class="line">          Dup_out2</span><br><span class="line">          Serial_in4_out2[</span><br><span class="line">            Parallel_in3_out3[</span><br><span class="line">              Dense_512</span><br><span class="line">              Dense_512</span><br><span class="line">              Dense_512</span><br><span class="line">            ]</span><br><span class="line">            PureAttention_in4_out2</span><br><span class="line">            Dense_512</span><br><span class="line">          ]</span><br><span class="line">        ]</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Dense_2048</span><br><span class="line">        Relu</span><br><span class="line">        Dropout</span><br><span class="line">        Dense_512</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Select[<span class="number">0</span>]_in2</span><br><span class="line">  LayerNorm</span><br><span class="line">  Mean</span><br><span class="line">  Dense_10</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p><strong>NOTE Congratulations! You have completed all of the graded functions of this assignment.</strong> Since the rest of the assignment takes a lot of time and memory to run we are providing some extra ungraded labs for you to see this model in action.</p><p><strong>Keep it up!</strong></p><p>To see this model in action continue to the next 2 ungraded labs. <strong>We strongly recommend you to try the colab versions of them as they will yield a much smoother experience.</strong> The links to the colabs can be found within the ungraded labs or if you already know how to open files within colab here are some shortcuts (if not, head to the ungraded labs which contain some extra instructions):</p><p><a href="https://drive.google.com/file/d/1EHAbMnW6u-GqYWh5r3Z8uLbz4KNpKOAv/view?usp=sharing" target="_blank" rel="noopener">BERT Loss Model Colab</a></p><p><a href="https://drive.google.com/file/d/1c-8KJkTySRGqCx_JjwjvXuRBTNTqEE0N/view?usp=sharing" target="_blank" rel="noopener">T5 SQuAD Model Colab</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-3-Question-Answering&quot;&gt;&lt;a href=&quot;#Assignment-3-Question-Answering&quot; class=&quot;headerlink&quot; title=&quot;Assignment 3: Question Answeri
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Transformer Summarizer</title>
    <link href="https://zhangruochi.com/Transformer-Summarizer/2020/09/27/"/>
    <id>https://zhangruochi.com/Transformer-Summarizer/2020/09/27/</id>
    <published>2020-09-27T07:23:13.000Z</published>
    <updated>2020-09-27T07:24:19.391Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Transformer-Summarizer"><a href="#Assignment-2-Transformer-Summarizer" class="headerlink" title="Assignment 2: Transformer Summarizer"></a>Assignment 2: Transformer Summarizer</h1><p>Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. </p><p><img src="transformerNews.png"></p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">Introduction</a></li><li><a href="#1">Part 1: Importing the dataset</a><ul><li><a href="#1.1">1.1 Encode &amp; Decode helper functions</a></li><li><a href="#1.2">1.2 Defining parameters</a></li><li><a href="#1.3">1.3 Exploring the data</a></li></ul></li><li><a href="#2">Part 2: Summarization with transformer</a><ul><li><a href="#2.1">2.1 Dot product attention</a><ul><li><a href="#ex01">Exercise 01</a></li></ul></li><li><a href="#2.2">2.2 Causal Attention</a><ul><li><a href="#ex02">Exercise 02</a></li></ul></li><li><a href="#2.3">2.3 Transformer decoder block</a><ul><li><a href="#ex03">Exercise 03</a></li></ul></li><li><a href="#2.4">2.4 Transformer Language model</a><ul><li><a href="#ex04">Exercise 04</a></li></ul></li></ul></li><li><a href="#3">Part 3: Training</a><ul><li><a href="#3.1">3.1 Training the model</a><ul><li><a href="#ex05">Exercise 05</a></li></ul></li></ul></li><li><a href="#4">Part 4: Evaluation</a><ul><li><a href="#4.1">4.1 Loading in a trained model</a></li></ul></li><li><a href="#5">Part 5: Testing with your own input</a> <ul><li><a href="#ex06">Exercise 6</a></li><li><a href="#5.1">5.1 Greedy decoding</a><ul><li><a href="#ex07">Exercise 07</a></li></ul></li></ul></li></ul><p><a name="0"></a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let’s get started, by completing this assignment you will learn to:  </p><ul><li>Use built-in functions to preprocess your data</li><li>Implement DotProductAttention</li><li>Implement Causal Attention</li><li>Understand how attention works</li><li>Build the transformer model</li><li>Evaluate your model</li><li>Summarize an article</li></ul><p>As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line">wrapper = textwrap.TextWrapper(width=<span class="number">70</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.fastmath <span class="keyword">import</span> numpy <span class="keyword">as</span> jnp</span><br><span class="line"></span><br><span class="line"><span class="comment"># to print the entire np array</span></span><br><span class="line">np.set_printoptions(threshold=sys.maxsize)</span><br></pre></td></tr></table></figure><p><a name="1"></a></p><h2 id="Part-1-Importing-the-dataset"><a href="#Part-1-Importing-the-dataset" class="headerlink" title="Part 1: Importing the dataset"></a>Part 1: Importing the dataset</h2><p>Trax makes it easy to work with Tensorflow’s datasets:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This will download the dataset if no data_dir is specified.</span></span><br><span class="line"><span class="comment"># Downloading and processing can take bit of time,</span></span><br><span class="line"><span class="comment"># so we have the data already in 'data/' for you</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Importing CNN/DailyMail articles dataset</span></span><br><span class="line">train_stream_fn = trax.data.TFDS(<span class="string">'cnn_dailymail'</span>,</span><br><span class="line">                                 data_dir=<span class="string">'data/'</span>,</span><br><span class="line">                                 keys=(<span class="string">'article'</span>, <span class="string">'highlights'</span>),</span><br><span class="line">                                 train=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This should be much faster as the data is downloaded already.</span></span><br><span class="line">eval_stream_fn = trax.data.TFDS(<span class="string">'cnn_dailymail'</span>,</span><br><span class="line">                                data_dir=<span class="string">'data/'</span>,</span><br><span class="line">                                keys=(<span class="string">'article'</span>, <span class="string">'highlights'</span>),</span><br><span class="line">                                train=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p><a name="1.1"></a></p><h2 id="1-1-Tokenize-amp-Detokenize-helper-functions"><a href="#1-1-Tokenize-amp-Detokenize-helper-functions" class="headerlink" title="1.1 Tokenize &amp; Detokenize helper functions"></a>1.1 Tokenize &amp; Detokenize helper functions</h2><p>Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your <a href="https://github.com/google/trax" target="_blank" rel="noopener">Trax</a> models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: </p><ul><li><span style="color:blue"> word2Ind: </span> a dictionary mapping the word to its index.</li><li><span style="color:blue"> ind2Word:</span> a dictionary mapping the index to its word.</li><li><span style="color:blue"> word2Count:</span> a dictionary mapping the word to the number of times it appears. </li><li><span style="color:blue"> num_words:</span> total number of words that have appeared. </li></ul><p>Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:</p><ul><li><span style="color:blue"> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.</li><li><span style="color:blue"> detokenize: </span> converts a token list to its corresponding sentence (i.e. string).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(input_str, EOS=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Input str to features dict, ready for inference"""</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Use the trax.data.tokenize method. It takes streams and returns streams,</span></span><br><span class="line">    <span class="comment"># we get around it by making a 1-element stream with `iter`.</span></span><br><span class="line">    inputs =  next(trax.data.tokenize(iter([input_str]),</span><br><span class="line">                                      vocab_dir=<span class="string">'vocab_dir/'</span>,</span><br><span class="line">                                      vocab_file=<span class="string">'summarize32k.subword.subwords'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Mark the end of the sentence with EOS</span></span><br><span class="line">    <span class="keyword">return</span> list(inputs) + [EOS]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span><span class="params">(integers)</span>:</span></span><br><span class="line">    <span class="string">"""List of ints to str"""</span></span><br><span class="line">  </span><br><span class="line">    s = trax.data.detokenize(integers,</span><br><span class="line">                             vocab_dir=<span class="string">'vocab_dir/'</span>,</span><br><span class="line">                             vocab_file=<span class="string">'summarize32k.subword.subwords'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> wrapper.fill(s)</span><br></pre></td></tr></table></figure><p><a name="1.2"></a></p><h2 id="1-2-Preprocessing-for-Language-Models-Concatenate-It"><a href="#1-2-Preprocessing-for-Language-Models-Concatenate-It" class="headerlink" title="1.2 Preprocessing for Language Models: Concatenate It!"></a>1.2 Preprocessing for Language Models: Concatenate It!</h2><p>This week you will use a language model — Transformer Decoder — to solve<br>an input-output problem. As you know, language models only predict the next<br>word, they have no notion of inputs. To create a single input suitable for<br>a language model, we concatenate inputs with targets putting a separator<br>in between. We also need to create a mask — with 0s at inputs and 1s at targets — so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Special tokens</span></span><br><span class="line">SEP = <span class="number">0</span> <span class="comment"># Padding or separator token</span></span><br><span class="line">EOS = <span class="number">1</span> <span class="comment"># End of sentence token</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenate tokenized inputs and targets using 0 as separator.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(stream)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> (article, summary) <span class="keyword">in</span> stream:</span><br><span class="line">        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])</span><br><span class="line">        mask = [<span class="number">0</span>] * (len(list(article)) + <span class="number">2</span>) + [<span class="number">1</span>] * (len(list(summary)) + <span class="number">1</span>) <span class="comment"># Accounting for EOS and SEP</span></span><br><span class="line">        <span class="keyword">yield</span> joint, joint, np.array(mask)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can combine a few data preprocessing steps into a pipeline like this.</span></span><br><span class="line">input_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># Tokenizes</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=<span class="string">'vocab_dir/'</span>,</span><br><span class="line">                       vocab_file=<span class="string">'summarize32k.subword.subwords'</span>),</span><br><span class="line">    <span class="comment"># Uses function defined above</span></span><br><span class="line">    preprocess,</span><br><span class="line">    <span class="comment"># Filters out examples longer than 2048</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply preprocessing to data streams.</span></span><br><span class="line">train_stream = input_pipeline(train_stream_fn())</span><br><span class="line">eval_stream = input_pipeline(eval_stream_fn())</span><br><span class="line"></span><br><span class="line">train_input, train_target, train_mask = next(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> sum((train_input - train_target)**<span class="number">2</span>) == <span class="number">0</span>  <span class="comment"># They are the same in Language Model (LM).</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prints mask, 0s on article, 1s on summary</span></span><br><span class="line">print(<span class="string">f'Single example mask:\n\n <span class="subst">&#123;train_mask&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>Single example mask: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prints: [Example][&lt;EOS&gt;][&lt;pad&gt;][Example Summary][&lt;EOS&gt;]</span></span><br><span class="line">print(<span class="string">f'Single example:\n\n <span class="subst">&#123;detokenize(train_input)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>Single example: By . Margot Peppers . Nigerian and Cameroonian pop star Dencia has hitout at Lupita Nyong&#39;o for her new contract with Lancome, accusing herof bowing to &#39;white people companies&#39;. In an angry tweet directed atthe 12 Years A Slave star, she wrote: &#39;Oh @Lupita_Nyongo cln&#39;t talkabt the bleaching creams white people (Companies) make cuz the whiteman pays her, they own her!! [sic]&#39;. The comment comes just a monthafter Miss Nyong&#39;o mentioned Dencia - who has been accused ofmarketing her own brand of skin-bleaching cream called Whitenicious -in a speech about learning to value the color of her own skin. Scrolldown for video . Butting heads: Nigerian and Cameroonian pop starDencia has hit out at Lupita Nyong&#39;o for her new contract withLancome, accusing her of bowing to &#39;white people companies&#39; Fightingwords: In a tweet directed at the 12 Years A Slave star, she wrote:&#39;Oh @Lupita_Nyongo cln&#39;t talk abt the bleaching creams white people(Companies) make cuz the white man pays her, they own her!! [sic]&#39; Thepop star is no stranger to . controversy; in a February interview withEbony, she all but admitted . that Whitenicious is intended as a skin-lightener, not as a cure for . dark spots as it claims. &#39;When . youtake that picture and you put a picture of Dencia darker, this is .what you&#39;re telling people - the product really works,&#39; she said. &#39;Andguess what? People really want to buy it. It&#39;s what it is. I don&#39;treally care.&#39; Given her defiant and hypocritical attitude, it&#39;s nosurprise the fiery singer was angered when Miss Nyong&#39;o called her outin a speech at Essence&#39;s Black Women in Hollywood event on February27. Influential: In a recent speech, Miss Nyong&#39;o read out loud aletter from a fan who said she decided not to buy Dencia&#39;s skin-whitening cream Whitenicious because the actress had inspired her tolove her own skin . On-screen: Miss Nyong&#39;o won an Oscar for BestSupporting Actress for her role in 2013 film 12 Years A Slave . In hertalk, the 30-year-old opened up about how conventional standards ofbeauty once affected her self-esteem, reading aloud a letter writtento her by a young girl who viewed her as a role model. &#39;Dear Lupita,&#39;reads the letter. &#39;I think you&#39;re really lucky to be this black butyet this successful in Hollywood overnight. I was just about to buyDencia&#39;s Whitenicious cream to lighten my skin when you appeared onthe world map and saved me.&#39; &#39;My heart bled a little when I read thosewords,&#39; the actress said through tears, explaining how as a child,she, too, would pray that she&#39;d one day wake up with lighter skin.Hypocritical: Dencia is no stranger to controversy; in a Februaryinterview with Ebony, she essentially admitted that Whitenicious isintended as a skin-lightener, not as a cure for dark spots as itclaims . Perpetuating the problem: &#39;When you take that picture and youput a picture of Dencia darker, this is what you&#39;re telling people -the product really works,&#39; she said. &#39;And guess what? People reallywant to buy it&#39; But while the actress saw the letter as a source ofinspiration, Dencia took it as a personal attack. After her angrytweet at Miss Nyong&#39;o, criticism poured in, with one person tweeting:&#39;B**** lupita is the new face of Lancôme!! SHE WINS!! And you&#39;re justTRASH [sic]&#39;. In her response, Dencia said of the cosmetics company:&#39;But they sell bleaching cream tho [sic]&#39;. The pop star is likelyreferring to Lancome&#39;s Blanc Expert range of cosmetics, which areactually advertised as &#39;brighteners&#39; that &#39;regulate melanin productionand awaken the luminosity of the skin&#39;. And as far as Dencia&#39;s claimthat Lancome is a &#39;white people company&#39;, a quick perusal of thewebsite reveals that it has a number of concealers and foundations indarker skin tones.&lt;EOS&gt;&lt;pad&gt;Dencia&#39;s comment is hypocriticalconsidering she recently courted controversy for marketing &#39;dark spotremover&#39; Whitenicious, which is frequently used as a skin-whiteningcream .&lt;EOS&gt;</code></pre><p><a name="1.3"></a></p><h2 id="1-3-Batching-with-bucketing"><a href="#1-3-Batching-with-bucketing" class="headerlink" title="1.3 Batching with bucketing"></a>1.3 Batching with bucketing</h2><p>As in the previous week, we use bucketing to create batches of data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bucketing to create batched generators.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Buckets are defined in terms of boundaries and batch sizes.</span></span><br><span class="line"><span class="comment"># Batch_sizes[i] determines the batch size for items with length &lt; boundaries[i]</span></span><br><span class="line"><span class="comment"># So below, we'll take a batch of 16 sentences of length &lt; 128 , 8 of length &lt; 256,</span></span><br><span class="line"><span class="comment"># 4 of length &lt; 512. And so on. </span></span><br><span class="line">boundaries =  [<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>]</span><br><span class="line">batch_sizes = [<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,    <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the streams.</span></span><br><span class="line">train_batch_stream = trax.data.BucketByLength(</span><br><span class="line">    boundaries, batch_sizes)(train_stream)</span><br><span class="line"></span><br><span class="line">eval_batch_stream = trax.data.BucketByLength(</span><br><span class="line">    boundaries, batch_sizes)(eval_stream)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Every execution will result in generation of a different article</span></span><br><span class="line"><span class="comment"># Try running this cell multiple times to see how the length of the examples affects the batch size</span></span><br><span class="line">input_batch, _, mask_batch = next(train_batch_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Shape of the input_batch</span></span><br><span class="line">input_batch.shape</span><br></pre></td></tr></table></figure><pre><code>(2, 1024)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print corresponding integer values</span></span><br><span class="line">print(input_batch[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>[   27  1091    23    46  3873  1248 16013   256 11599 23297   102    68 24308     7     5  1037  1958   320  1477   105  2557   186  4133    28 18175  1348  1287     3  4927  7577    28  8478 10120 19134  7951   364  7317  4990    79     2   393     2   186  8962  2995  9813  4476  3632  2270     5     2   705     2   721 10731    16   186 17136    16   193    54   102    41  1459   320    31 16946    47     2   119  3770   278   355    28   622   263    78  2613     3   312  4543     4  8662  3788  3632  2270     5     6  3048 23524     2  1210     2  1958   320  2033   105    61     2 19134  7951   364  7317  4990    79 24810    17   213  1091     2   931   320   213 16946    47   415 20579 20964    58  1782   863   213  7726     2   213  7599  3938  4133    28 26719     4   752  1480  2868   132    68   583  3898 20579 20964    58   240   197     3  4531  9531  2959   127   132    28 27439  9275  1628  1602     3  8406  5364    11  4927  7577    28  8478 10120 19134  7951   364  7317  4990    79     2   393     2   497     2   186    68 24308  8962  2995  9813  4476  3632  2270     5     2   705     2   721 10731    16   186 17136    16   193    54   809    31   278    78  2613  7511    15  1037 20274    21   379 21549  7150    11  9813  4476  3632  2270     5    80 18649  1496   667   213 17136    45    78    15   882  1838   213  2439  7883   379    27  1147     6   104     6   292   966    43 11850   213  1621     2   931   320   213 13021     4     2    35    22   206    19  5632   213  1018   111   213  2948   186   213 25931     4     3  2713  7801   320    28  6105    32   922  1838   213  6350   141   102 24114    75    78  2613   186  7511    41  2362     2    41   233  3632  2270     5     6  3048 23524  1955    78    28 11261  1797  1782   198    25    92  3787  3103   527 13747   320   213  7599     2   487   159   213   669 27884     4  1622 27872   391  5977  3103   527  2918   186  1472   320    18    46   810   132    28  2439  7726  3898   213 13021     4   127     3    34    31 18649  3347     2   148 19134  7951   364  7317  4990    79   186  9813  4476  3632  2270     5    18 17136    45    78    31  5369   186 19175     5     3     9  2789    25 11203     2   412    25   213   966   186    54  1697     3 12849    14    11  7317  4990    79 12365   146 24810    17   213  1091  4617 27439  9275  1628  4543     4  8662  3788  3632  2270     5     6  3048 23524     2   186   131  4133    28 26719     4  6901   809   213    60     6  1797  6350   809   213   414     8 12370    21    12   186   710   171   864  2362   809   213  1610   379     9 16946    47   415  3357 15581    81     7     5  1431  1890   163  4336  7188    20    78  3632  2270     5     6  3048 23524     7     5   661     2    35   646    25 17926 25290 16741    20  4140     2   213 13021     4   127     3 19134  7951   364  7317  4990    79  1353  3873  1248 11599 23297     2 17260  8041   893   213  5627   527    28   966     2  2439  1740  1524  7726   186 23638    16 24668 21273   204     2   931   320  1882     3  4531  9531  2959   127 19134  7951   364  7317  4990    79    43  9363     4   760    70    35    62    19  2851  2754   103  1353    70  1480 22646   272  7304   132    28  1501   809   213  1881  1610     3   305  1353   475   809   213 16946    47   415  7411    84    78   281  3997    88   226 20934     4     3  9813  4476  3632  2270     5  1353    43  3873  1248   966 17260  8041 16704   464   186  2439  1740  1524  7726   186  1233   320   213  1156 10835    78   281  1696    88   226 20934     4     3    27   924  3729    23    46  4648  1019  3112  1859   809 18235  5333  9141 25733   812    10     1     0  4927  7577    28  8478 10120 19134  7951   364  7317  4990    79     2   393     2   186  8962  2995  9813  4476  3632  2270     5     2   705     2   721  2557   102  4925  1838    28   622    78  2613  1859 16346 27439  6774  1628   312    15  1037     2  4543     4  8662  3788  3632  2270     5     6  3048 23524     2  1210     2  1958   320  1477   105     2 19134  7951   364  7317  4990    79 12365 24810    17    68 16346 27439  6774  1628   305  4133 26719     4  6901   186   864  2362   320   423    68  1955 16346 27439  6774  1628    27  1147     6   104     6   292  2635 11850   213  1621  2104     1     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0]</code></pre><p>Things to notice:</p><ul><li>First we see the corresponding values of the words.</li><li>The first 1, which represents the <code>&lt;EOS&gt;</code> tag of the article.</li><li>Followed by a 0, which represents a <code>&lt;pad&gt;</code> tag.</li><li>After the first 0 (<code>&lt;pad&gt;</code> tag) the corresponding values are of the words that are used for the summary of the article.</li><li>The second 1 represents the <code>&lt;EOS&gt;</code> tag for the summary.</li><li>All the trailing 0s represent <code>&lt;pad&gt;</code> tags which are appended to maintain consistent length (If you don’t see them then it would mean it is already of max length)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print the article and its summary</span></span><br><span class="line">print(<span class="string">'Article:\n\n'</span>, detokenize(input_batch[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>Article: A woman has been charged with reckless manslaughter after herboyfriend&#39;s mother tried to stop them fighting and suffered a fatalheart attack. Claudia Yanira Hernandez Soriano, 25, and Juan FranciscoMartinez Rojas, 28, started punching and scratching each other afterthey returned to their Bergen, New Jersey home following a party earlyon Monday. When Ana Angelina Rojas-Jovel, 45, tried to break them up,Hernandez Soriano assaulted the woman, according to the Bergen CountyProsecutor. &#39;During the assault, the victim apparently suffered acardiac event which resulted in her death,&#39; Prosecutor John L.Molinelli said in a statement. Fight: Claudia Yanira HernandezSoriano, 25, above, and her boyfriend Juan Francisco Martinez Rojas,28, started punching and scratching each other at their home on Mondaywhen his mother intervened . Injured: Martinez Rojas&#39; booking shotshows the scratches on his face from the domestic dispute . A seven-year-old child also witnessed the fight, according to the prosecutor,but he did not reveal the relationship between the adults and theyoungster. Police responded to a 911 call from the apartment justafter 4am on Monday and when they arrived, they found Rojas-Jovel deadon a bedroom floor. &#39;There were no obvious signs of trauma to thevictim, however... the [couple] displayed signs of injury and appearedto have been involved in a domestic assault,&#39; the prosecutor said. Intheir booking photos, both Hernandez Soriano and Martinez Rojas havescratches on their faces and necks. The pair were interviewed, as werethe child and other residents. Scene: Soriano allegedly then assaultedthe woman, Ana Angelina Rojas-Jovel, and she suffered a cardiac arrestat the first-floor apartment at the house (pictured) and died beforepolice arrived at the scene . The Bergen County Medical Examiner&#39;sOffice conducted an autopsy on Rojas-Jovel&#39;s body, but results werepending toxicology tests, the prosecutor said. Hernandez Soriano wascharged with manslaughter, endangering the welfare of a child,domestic violence simple assault and hindering apprehension, accordingto authorities. Molinelli said Hernandez Soriano also hid evidence -but would not detail what it was - which investigators later recoveredin a search at the crime scene. She was held at the Bergen County Jailon $250,000 bail. Martinez Rojas was also charged with childendangerment and domestic violence simple assault and sent to thecounty jail on $75,000 bail. A court hearing has been scheduled forThursday morning at Hackensack Superior Court.&lt;EOS&gt;&lt;pad&gt;ClaudiaYaniraHernandez Soriano, 25, and Juan Francisco Martinez Rojas, 28, startedfighting after returning from a party on Monday morning . When hismother, Ana Angelina Rojas-Jovel, 45, tried to stop them, HernandezSoriano allegedly assaulted her . She suffered cardiac arrest andpolice arrived to find her dead . A seven-year-old girl witnessed thefight .&lt;EOS&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;</code></pre><p>You can see that the data has the following structure:</p><ul><li><span style="color:blue"> [Article] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; <code>&lt;pad&gt;</code> -&gt; <span style="color:blue"> [Article Summary] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; (possibly) multiple <code>&lt;pad&gt;</code></li></ul><p>The loss is taken only on the summary using cross_entropy as loss function. </p><p><a name="2"></a></p><h1 id="Part-2-Summarization-with-transformer"><a href="#Part-2-Summarization-with-transformer" class="headerlink" title="Part 2: Summarization with transformer"></a>Part 2: Summarization with transformer</h1><p>Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps. </p><p>You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.</p><p><img src="transformer_decoder_zoomin.png"></p><p><a name="2.1"></a></p><h2 id="2-1-Dot-product-attention"><a href="#2-1-Dot-product-attention" class="headerlink" title="2.1 Dot product attention"></a>2.1 Dot product attention</h2><p>Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output. </p><p><img src="dotproduct.png"></p><p>Here are some helper functions that will help you create tensors and display useful information:</p><ul><li><code>create_tensor</code>  creates a <code>jax numpy array</code> from a list of lists.</li><li><code>display_tensor</code> prints out the shape and the actual tensor.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tensor</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="string">"""Create tensor from list of lists"""</span></span><br><span class="line">    <span class="keyword">return</span> jnp.array(t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_tensor</span><span class="params">(t, name)</span>:</span></span><br><span class="line">    <span class="string">"""Display shape and tensor"""</span></span><br><span class="line">    print(<span class="string">f'<span class="subst">&#123;name&#125;</span> shape: <span class="subst">&#123;t.shape&#125;</span>\n'</span>)</span><br><span class="line">    print(<span class="string">f'<span class="subst">&#123;t&#125;</span>\n'</span>)</span><br></pre></td></tr></table></figure><p>Before implementing it yourself, you can play around with a toy example of <code>dot product attention</code> without the softmax  operation. Technically it would not be <code>dot product attention</code> without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.</p><p>The formula for attention is this one:</p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\</script><p>$d_{k}$ stands for the dimension of queries and keys.</p><p>The <code>query</code>, <code>key</code>, <code>value</code> and <code>mask</code> vectors are provided for this example.</p><p>Notice that the masking is done using very negative values that will yield a similar effect to using $-\infty $. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">q = create_tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">display_tensor(q, <span class="string">'query'</span>)</span><br><span class="line">k = create_tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">display_tensor(k, <span class="string">'key'</span>)</span><br><span class="line">v = create_tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">display_tensor(v, <span class="string">'value'</span>)</span><br><span class="line">m = create_tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">-1e9</span>, <span class="number">0</span>]])</span><br><span class="line">display_tensor(m, <span class="string">'mask'</span>)</span><br></pre></td></tr></table></figure><pre><code>query shape: (2, 3)[[1 0 0] [0 1 0]]key shape: (2, 3)[[1 2 3] [4 5 6]]value shape: (2, 3)[[0 1 0] [1 0 1]]mask shape: (2, 2)[[ 0.e+00  0.e+00] [-1.e+09  0.e+00]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">query shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">key shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">value shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">mask shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">0.e+00</span>  <span class="number">0.e+00</span>]</span><br><span class="line"> [<span class="number">-1.e+09</span>  <span class="number">0.e+00</span>]]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q_dot_k = q @ k.T / jnp.sqrt(<span class="number">3</span>)</span><br><span class="line">display_tensor(q_dot_k, <span class="string">'query dot key'</span>)</span><br></pre></td></tr></table></figure><pre><code>query dot key shape: (2, 2)[[0.57735026 2.309401  ] [1.1547005  2.8867514 ]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">query dot key shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0.57735026</span> <span class="number">2.309401</span>  ]</span><br><span class="line"> [<span class="number">1.1547005</span>  <span class="number">2.8867514</span> ]]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">masked = q_dot_k + m</span><br><span class="line">display_tensor(masked, <span class="string">'masked query dot key'</span>)</span><br></pre></td></tr></table></figure><pre><code>masked query dot key shape: (2, 2)[[ 5.7735026e-01  2.3094010e+00] [-1.0000000e+09  2.8867514e+00]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">masked query dot key shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">5.7735026e-01</span>  <span class="number">2.3094010e+00</span>]</span><br><span class="line"> [<span class="number">-1.0000000e+09</span>  <span class="number">2.8867514e+00</span>]]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(masked @ v, <span class="string">'masked query dot key dot value'</span>)</span><br></pre></td></tr></table></figure><pre><code>masked query dot key dot value shape: (2, 3)[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00] [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">masked query dot key dot value shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">2.3094010e+00</span>  <span class="number">5.7735026e-01</span>  <span class="number">2.3094010e+00</span>]</span><br><span class="line"> [ <span class="number">2.8867514e+00</span> <span class="number">-1.0000000e+09</span>  <span class="number">2.8867514e+00</span>]]</span><br></pre></td></tr></table></figure></p><p>In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">q_with_batch = q[<span class="keyword">None</span>,:]</span><br><span class="line">display_tensor(q_with_batch, <span class="string">'query with batch dim'</span>)</span><br><span class="line">k_with_batch = k[<span class="keyword">None</span>,:]</span><br><span class="line">display_tensor(k_with_batch, <span class="string">'key with batch dim'</span>)</span><br><span class="line">v_with_batch = v[<span class="keyword">None</span>,:]</span><br><span class="line">display_tensor(v_with_batch, <span class="string">'value with batch dim'</span>)</span><br><span class="line">m_bool = create_tensor([[<span class="keyword">True</span>, <span class="keyword">True</span>], [<span class="keyword">False</span>, <span class="keyword">True</span>]])</span><br><span class="line">display_tensor(m_bool, <span class="string">'boolean mask'</span>)</span><br></pre></td></tr></table></figure><pre><code>query with batch dim shape: (1, 2, 3)[[[1 0 0]  [0 1 0]]]key with batch dim shape: (1, 2, 3)[[[1 2 3]  [4 5 6]]]value with batch dim shape: (1, 2, 3)[[[0 1 0]  [1 0 1]]]boolean mask shape: (2, 2)[[ True  True] [False  True]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">query with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">key with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]]</span><br><span class="line"></span><br><span class="line">value with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]]</span><br><span class="line"></span><br><span class="line">boolean mask shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ True  True]</span><br><span class="line"> [False  True]]</span><br></pre></td></tr></table></figure></p><p><a name="ex01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Implement the dot product attention. Concretely, implement the following equation</p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\</script><p>$Q$ - query,<br>$K$ - key,<br>$V$ - values,<br>$M$ - mask,<br>${d_k}$ - depth/dimension of the queries and keys (used for scaling down)</p><p>You can implement this formula either by <code>trax</code> numpy (trax.math.numpy) or regular <code>numpy</code> but it is recommended to use <code>jnp</code>.</p><p>Something to take into consideration is that within trax, the masks are tensors of <code>True/False</code> values not 0’s and $-\infty$ as in the previous example. Within the graded function don’t think of applying the mask by summing up matrices, instead use <code>jnp.where()</code> and treat the <strong>mask as a tensor of boolean values with <code>False</code> for values that need to be masked and True for the ones that don’t.</strong></p><p>Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as <code>@</code> for dot product or <code>.T</code> for transposing. Use <code>jnp.matmul()</code> and <code>jnp.swapaxes()</code> instead.</p><p>This is the self-attention block for the transformer decoder. Good luck!  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: DotProductAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DotProductAttention</span><span class="params">(query, key, value, mask)</span>:</span></span><br><span class="line">    <span class="string">"""Dot product self-attention.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)</span></span><br><span class="line"><span class="string">        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)</span></span><br><span class="line"><span class="string">        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k</span></span><br><span class="line"><span class="string">        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> query.shape[<span class="number">-1</span>] == key.shape[<span class="number">-1</span>] == value.shape[<span class="number">-1</span>], <span class="string">"Embedding dimensions of q, k, v aren't all the same"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># Save depth/dimension of the query embedding for scaling down the dot product</span></span><br><span class="line">    depth = query.shape[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate scaled query key dot product according to formula above</span></span><br><span class="line">    dots = jnp.matmul(query, jnp.swapaxes(key, <span class="number">1</span>, <span class="number">2</span>)) / jnp.sqrt(depth)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Apply the mask</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>: <span class="comment"># The 'None' in this line does not need to be replaced</span></span><br><span class="line">        dots = jnp.where(mask, dots, jnp.full_like(dots, <span class="number">-1e9</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Softmax formula implementation</span></span><br><span class="line">    <span class="comment"># Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers</span></span><br><span class="line">    <span class="comment"># Hint: Last axis should be used and keepdims should be True</span></span><br><span class="line">    <span class="comment"># Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)</span></span><br><span class="line">    logsumexp = trax.fastmath.logsumexp(dots,axis=<span class="number">-1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take exponential of dots minus logsumexp to get softmax</span></span><br><span class="line">    <span class="comment"># Use jnp.exp()</span></span><br><span class="line">    dots = jnp.exp( dots - logsumexp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Multiply dots by value to get self-attention</span></span><br><span class="line">    <span class="comment"># Use jnp.matmul()</span></span><br><span class="line">    attention = jnp.matmul(dots, value)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> attention</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)</span><br></pre></td></tr></table></figure><pre><code>DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],              [1.        , 0.        , 1.        ]]], dtype=float32)</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">DeviceArray([[[<span class="number">0.8496746</span> , <span class="number">0.15032545</span>, <span class="number">0.8496746</span> ],</span><br><span class="line">              [<span class="number">1.</span>        , <span class="number">0.</span>        , <span class="number">1.</span>        ]]], dtype=float32)</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">&lt;a name='2.2'&gt;&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">## <span class="number">2.2</span> Causal Attention</span><br><span class="line"></span><br><span class="line">Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. </span><br><span class="line"></span><br><span class="line">&lt;img src = <span class="string">"causal.png"</span>&gt;</span><br><span class="line"></span><br><span class="line">In the image above, a word can see everything that is before it, but <span class="keyword">not</span> what is after it. To implement causal attention, you will have to transform vectors <span class="keyword">and</span> <span class="keyword">do</span> many reshapes. You will need to implement the functions below.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;a name='ex02'&gt;&lt;/a&gt;</span><br><span class="line">### Exercise <span class="number">02</span></span><br><span class="line"></span><br><span class="line">Implement the following functions that will be needed <span class="keyword">for</span> Causal Attention:</span><br><span class="line"></span><br><span class="line">- &lt;span style='color:blue'&gt; compute_attention_heads &lt;/span&gt;: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\times$ n_heads, seqlen, d_head).</span><br><span class="line">- &lt;span style='color:blue'&gt; dot_product_self_attention &lt;/span&gt;: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention.</span><br><span class="line">- &lt;span style='color:blue'&gt; compute_attention_output &lt;/span&gt;: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\times$ d_head). These operations concatenate (stack/merge) the heads. </span><br><span class="line"></span><br><span class="line">Next there are some toy tensors which may serve to give you an idea of the data shapes <span class="keyword">and</span> opperations involved in Causal Attention. They are also useful to test out your functions! </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">tensor2d = create_tensor(q)</span><br><span class="line">display_tensor(tensor2d, 'query matrix (2D tensor)')</span><br><span class="line"></span><br><span class="line">tensor4d2b = create_tensor([[q, q], [q, q]])</span><br><span class="line">display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')</span><br><span class="line"></span><br><span class="line">tensor3dc = create_tensor([jnp.concatenate([q, q], axis = <span class="number">-1</span>)])</span><br><span class="line">display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')</span><br><span class="line"></span><br><span class="line">tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = <span class="number">-1</span>), jnp.concatenate([q, q], axis = <span class="number">-1</span>), jnp.concatenate([q, q], axis = <span class="number">-1</span>)])</span><br><span class="line">display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')</span><br></pre></td></tr></table></figure></p><pre><code>query matrix (2D tensor) shape: (2, 3)[[1 0 0] [0 1 0]]batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)[[[[1 0 0]   [0 1 0]]  [[1 0 0]   [0 1 0]]] [[[1 0 0]   [0 1 0]]  [[1 0 0]   [0 1 0]]]]one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)[[[1 0 0 1 0 0]  [0 1 0 0 1 0]]]three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)[[[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]]]</code></pre><p>It is important to know that the following 3 functions would normally be defined within the <code>CausalAttention</code> function further below. </p><p>However this makes these functions harder to test. Because of this, these functions are shown individually using a <code>closure</code> (when necessary) that simulates them being inside of the <code>CausalAttention</code> function. This is done because they rely on some variables that can be accessed from within <code>CausalAttention</code>.</p><h3 id="Support-Functions"><a href="#Support-Functions" class="headerlink" title="Support Functions"></a>Support Functions</h3><p><span style="color:blue"> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\times$ n_heads, seqlen, d_head).</p><p><strong>For the closures you only have to fill the inner function.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_attention_heads_closure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_attention_heads_closure</span><span class="params">(n_heads, d_head)</span>:</span></span><br><span class="line">    <span class="string">""" Function that simulates environment inside CausalAttention function.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_head (int):  dimensionality of heads.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        function: compute_attention_heads function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_attention_heads</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="string">""" Compute the attention heads.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Size of the x's batch dimension</span></span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Length of the sequence</span></span><br><span class="line">        <span class="comment"># Should be size of x's first dimension without counting the batch dim</span></span><br><span class="line">        seqlen = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape()</span></span><br><span class="line">        <span class="comment"># batch_size, seqlen, n_heads*d_head -&gt; batch_size, seqlen, n_heads, d_head</span></span><br><span class="line">        x = jnp.reshape(x, (batch_size, seqlen,n_heads, d_head))</span><br><span class="line">        <span class="comment"># Transpose x using jnp.transpose()</span></span><br><span class="line">        <span class="comment"># batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head</span></span><br><span class="line">        <span class="comment"># Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them</span></span><br><span class="line">        x = jnp.transpose(x, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape()</span></span><br><span class="line">        <span class="comment"># batch_size, n_heads, seqlen, d_head -&gt; batch_size*n_heads, seqlen, d_head</span></span><br><span class="line">        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compute_attention_heads</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(tensor3dc3b, <span class="string">"input tensor"</span>)</span><br><span class="line">result_cah = compute_attention_heads_closure(<span class="number">2</span>,<span class="number">3</span>)(tensor3dc3b)</span><br><span class="line">display_tensor(result_cah, <span class="string">"output tensor"</span>)</span><br></pre></td></tr></table></figure><pre><code>input tensor shape: (3, 2, 6)[[[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]]]output tensor shape: (6, 2, 3)[[[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">input tensor shape: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">output tensor shape: (<span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure></p><p><span style="color:blue"> dot_product_self_attention </span>: Creates a mask matrix with <code>False</code> values above the diagonal and <code>True</code> values below and calls DotProductAttention which implements dot product self attention.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: dot_product_self_attention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dot_product_self_attention</span><span class="params">(q, k, v)</span>:</span></span><br><span class="line">    <span class="string">""" Masked dot product self attention.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q (jax.interpreters.xla.DeviceArray): queries.</span></span><br><span class="line"><span class="string">        k (jax.interpreters.xla.DeviceArray): keys.</span></span><br><span class="line"><span class="string">        v (jax.interpreters.xla.DeviceArray): values.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)</span></span><br><span class="line">    mask_size = q.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)</span></span><br><span class="line">    <span class="comment"># Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_</span></span><br><span class="line">    <span class="comment"># Use jnp.tril() - Lower triangle of an array and jnp.ones()</span></span><br><span class="line">    mask = jnp.tril(jnp.ones((<span class="number">1</span>, mask_size, mask_size), dtype=jnp.bool_), k=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> DotProductAttention(q, k, v, mask)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)</span><br></pre></td></tr></table></figure><pre><code>DeviceArray([[[0.        , 1.        , 0.        ],              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DeviceArray([[[<span class="number">0.</span>        , <span class="number">1.</span>        , <span class="number">0.</span>        ],</span><br><span class="line">              [<span class="number">0.8496746</span> , <span class="number">0.15032543</span>, <span class="number">0.8496746</span> ]]], dtype=float32)</span><br></pre></td></tr></table></figure></p><p><span style="color:blue"> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\times$ d_head). These operations concatenate (stack/merge) the heads. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_attention_output_closure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_attention_output_closure</span><span class="params">(n_heads, d_head)</span>:</span></span><br><span class="line">    <span class="string">""" Function that simulates environment inside CausalAttention function.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_head (int):  dimensionality of heads.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        function: compute_attention_output function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_attention_output</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="string">""" Compute the attention output.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Length of the sequence</span></span><br><span class="line">        <span class="comment"># Should be size of x's first dimension without counting the batch dim</span></span><br><span class="line">        seqlen = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)</span></span><br><span class="line">        x = jnp.reshape(x,(<span class="number">-1</span>, n_heads, seqlen, d_head))</span><br><span class="line">        <span class="comment"># Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)</span></span><br><span class="line">        x = jnp.transpose(x, (<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Reshape to allow to concatenate the heads</span></span><br><span class="line">        <span class="keyword">return</span> jnp.reshape(x, (<span class="number">-1</span>, seqlen, n_heads * d_head))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compute_attention_output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(result_cah, <span class="string">"input tensor"</span>)</span><br><span class="line">result_cao = compute_attention_output_closure(<span class="number">2</span>,<span class="number">3</span>)(result_cah)</span><br><span class="line">display_tensor(result_cao, <span class="string">"output tensor"</span>)</span><br></pre></td></tr></table></figure><pre><code>input tensor shape: (6, 2, 3)[[[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]] [[1 0 0]  [0 1 0]]]output tensor shape: (3, 2, 6)[[[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]] [[1 0 0 1 0 0]  [0 1 0 0 1 0]]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">input tensor shape: (<span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">output tensor shape: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure></p><h3 id="Causal-Attention-Function"><a href="#Causal-Attention-Function" class="headerlink" title="Causal Attention Function"></a>Causal Attention Function</h3><p>Now it is time for you to put everything together within the <code>CausalAttention</code> or Masked multi-head attention function:</p><p><img src="masked-attention.png"> </p><p><strong>Instructions:</strong> Implement the causal attention.<br>Your model returns the causal attention through a $tl.Serial$ with the following:</p><ul><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch" target="_blank" rel="noopener">tl.Branch</a> </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn" target="_blank" rel="noopener">tl.Fn</a></span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn" target="_blank" rel="noopener">tl.Fn</a></span>: Takes in compute_attention_output_closure to allow for parallel computing.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a></span>: Final Dense layer, with dimension <code>d_feature</code>.</li></ul><p>Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn" target="_blank" rel="noopener"><code>tl.Fn()</code></a> function. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: CausalAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CausalAttention</span><span class="params">(d_feature, </span></span></span><br><span class="line"><span class="function"><span class="params">                    n_heads, </span></span></span><br><span class="line"><span class="function"><span class="params">                    compute_attention_heads_closure=compute_attention_heads_closure,</span></span></span><br><span class="line"><span class="function"><span class="params">                    dot_product_self_attention=dot_product_self_attention,</span></span></span><br><span class="line"><span class="function"><span class="params">                    compute_attention_output_closure=compute_attention_output_closure,</span></span></span><br><span class="line"><span class="function"><span class="params">                    mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Transformer-style multi-headed causal attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_feature (int):  dimensionality of feature embedding.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        compute_attention_heads_closure (function): Closure around compute_attention heads.</span></span><br><span class="line"><span class="string">        dot_product_self_attention (function): dot_product_self_attention function. </span></span><br><span class="line"><span class="string">        compute_attention_output_closure (function): Closure around compute_attention_output. </span></span><br><span class="line"><span class="string">        mode (str): 'train' or 'eval'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: Multi-headed self-attention model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> d_feature % n_heads == <span class="number">0</span></span><br><span class="line">    d_head = d_feature // n_heads</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span></span><br><span class="line">    <span class="comment"># Since you are dealing with closures you might need to call the outer </span></span><br><span class="line">    <span class="comment"># function with the correct parameters to get the actual uncalled function.</span></span><br><span class="line">    ComputeAttentionHeads = tl.Fn(<span class="string">'AttnHeads'</span>, compute_attention_heads_closure(n_heads, d_head), n_out=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        tl.Branch( <span class="comment"># creates three towers for one input, takes activations and creates queries keys and values</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># queries</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># keys</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># values</span></span><br><span class="line">        ),</span><br><span class="line">        </span><br><span class="line">        tl.Fn(<span class="string">'DotProductAttn'</span>, dot_product_self_attention, n_out=<span class="number">1</span>), <span class="comment"># takes QKV</span></span><br><span class="line">        <span class="comment"># HINT: The second argument to tl.Fn() is an uncalled function</span></span><br><span class="line">        <span class="comment"># Since you are dealing with closures you might need to call the outer </span></span><br><span class="line">        <span class="comment"># function with the correct parameters to get the actual uncalled function.</span></span><br><span class="line">        tl.Fn(<span class="string">'AttnOutput'</span>, compute_attention_output_closure(n_heads, d_head), n_out=<span class="number">1</span>), <span class="comment"># to allow for parallel</span></span><br><span class="line">        tl.Dense(d_feature) <span class="comment"># Final dense layer</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the causal attention model</span></span><br><span class="line">print(CausalAttention(d_feature=<span class="number">512</span>, n_heads=<span class="number">8</span>))</span><br></pre></td></tr></table></figure><pre><code>Serial[  Branch_out3[    [Dense_512, AttnHeads]    [Dense_512, AttnHeads]    [Dense_512, AttnHeads]  ]  DotProductAttn_in3  AttnOutput  Dense_512]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Branch_out3[</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">  ]</span><br><span class="line">  DotProductAttn_in3</span><br><span class="line">  AttnOutput</span><br><span class="line">  Dense_512</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p><a name="2.3"></a></p><h2 id="2-3-Transformer-decoder-block"><a href="#2-3-Transformer-decoder-block" class="headerlink" title="2.3 Transformer decoder block"></a>2.3 Transformer decoder block</h2><p>Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.</p><p><img src="transformer_decoder_1.png" style="height:300px"> </p><p>To implement this function, you will have to call the <code>CausalAttention</code> or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: </p><ul><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm" target="_blank" rel="noopener">tl.LayerNorm</a> </span>: used to layer normalize</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a> </span>: the dense layer</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu" target="_blank" rel="noopener">ff_activation</a> </span>: feed forward activation (we use ReLu) here.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout" target="_blank" rel="noopener">tl.Dropout</a> </span>: dropout layer</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a> </span>: dense layer</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout" target="_blank" rel="noopener">tl.Dropout</a> </span>: dropout layer</li></ul><p>Finally once you implement the feedforward, you can go ahead and implement the entire block using: </p><ul><li><p><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual" target="_blank" rel="noopener">tl.Residual</a> </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout. </p></li><li><p><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual" target="_blank" rel="noopener">tl.Residual</a> </span>: takes in the feedforward block you will implement. </p></li></ul><p><a name="ex03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the transformer decoder block. Good luck!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: DecoderBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DecoderBlock</span><span class="params">(d_model, d_ff, n_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, mode, ff_activation)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a list of layers that implements a Transformer decoder block.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input is an activation tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model (int):  depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        mode (str): 'train' or 'eval'.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create masked multi-head attention block using CausalAttention function</span></span><br><span class="line">    causal_attention = CausalAttention( </span><br><span class="line">                        d_model,</span><br><span class="line">                        n_heads=n_heads,</span><br><span class="line">                        mode=mode</span><br><span class="line">                        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create feed-forward block (list) with two dense layers with dropout and input normalized</span></span><br><span class="line">    feed_forward = [ </span><br><span class="line">        <span class="comment"># Normalize layer inputs</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># Add first feed forward (dense) layer (don't forget to set the correct value for n_units)</span></span><br><span class="line">        tl.Dense(d_ff),</span><br><span class="line">        <span class="comment"># Add activation function passed in as a parameter (you need to call it!)</span></span><br><span class="line">        ff_activation(), <span class="comment"># Generally ReLU</span></span><br><span class="line">        <span class="comment"># Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)</span></span><br><span class="line">        tl.Dropout(rate=dropout, mode=mode),</span><br><span class="line">        <span class="comment"># Add second feed forward layer (don't forget to set the correct value for n_units)</span></span><br><span class="line">        tl.Dense(d_model),</span><br><span class="line">        <span class="comment"># Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)</span></span><br><span class="line">        tl.Dropout(rate=dropout, mode=mode)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">      tl.Residual(</span><br><span class="line">          <span class="comment"># Normalize layer input</span></span><br><span class="line">          tl.LayerNorm(),</span><br><span class="line">          <span class="comment"># Add causal attention block previously defined (without parentheses)</span></span><br><span class="line">          causal_attention,</span><br><span class="line">          <span class="comment"># Add dropout with rate and mode specified</span></span><br><span class="line">          tl.Dropout()</span><br><span class="line">        ),</span><br><span class="line">      tl.Residual(</span><br><span class="line">          <span class="comment"># Add feed forward block (without parentheses)</span></span><br><span class="line">          feed_forward</span><br><span class="line">        ),</span><br><span class="line">      ]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the decoder block</span></span><br><span class="line">print(DecoderBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, n_heads=<span class="number">8</span>, dropout=<span class="number">0.1</span>, mode=<span class="string">'train'</span>, ff_activation=tl.Relu))</span><br></pre></td></tr></table></figure><pre><code>[Serial[  Branch_out2[    None    Serial[      LayerNorm      Serial[        Branch_out3[          [Dense_512, AttnHeads]          [Dense_512, AttnHeads]          [Dense_512, AttnHeads]        ]        DotProductAttn_in3        AttnOutput        Dense_512      ]      Dropout    ]  ]  Add_in2], Serial[  Branch_out2[    None    Serial[      LayerNorm      Dense_2048      Relu      Dropout      Dense_512      Dropout    ]  ]  Add_in2]]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Serial[</span><br><span class="line">        Branch_out3[</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">        ]</span><br><span class="line">        DotProductAttn_in3</span><br><span class="line">        AttnOutput</span><br><span class="line">        Dense_512</span><br><span class="line">      ]</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">], Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Dense_2048</span><br><span class="line">      Relu</span><br><span class="line">      Dropout</span><br><span class="line">      Dense_512</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">]]</span><br></pre></td></tr></table></figure></p><p><a name="2.4"></a></p><h2 id="2-4-Transformer-Language-Model"><a href="#2-4-Transformer-Language-Model" class="headerlink" title="2.4 Transformer Language Model"></a>2.4 Transformer Language Model</h2><p>You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing.<br><img src="transformer_decoder.png" style="height:400px"></p><p><a name="ex04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need. </p><ul><li><p><span style="color:blue"> positional_enconder </span>- a list containing the following layers:</p><ul><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding" target="_blank" rel="noopener">tl.Embedding</a></span></li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout" target="_blank" rel="noopener">tl.Dropout</a></span></li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding" target="_blank" rel="noopener">tl.PositionalEncoding</a></span></li></ul></li><li><p>A list of <code>n_layers</code> <span style="color:blue"> decoder blocks</span>.</p></li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial" target="_blank" rel="noopener">tl.Serial</a>: </span> takes in the following layers or lists of layers:<ul><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight" target="_blank" rel="noopener">tl.ShiftRight</a>: </span>: shift the tensor to the right by padding on axis 1.</li><li><span style="color:blue"> positional_encoder </span>: encodes the text positions.</li><li><span style="color:blue"> decoder_blocks </span>: the ones you created.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm" target="_blank" rel="noopener">tl.LayerNorm</a> </span>: a layer norm.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense" target="_blank" rel="noopener">tl.Dense</a> </span>: takes in the vocab_size.</li><li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax" target="_blank" rel="noopener">tl.LogSoftmax</a> </span>: to predict.</li></ul></li></ul><p>Go go go!! You can do it :)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TransformerLM</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TransformerLM</span><span class="params">(vocab_size=<span class="number">33300</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  d_ff=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  n_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  max_len=<span class="number">4096</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  mode=<span class="string">'train'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  ff_activation=tl.Relu)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a Transformer language model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input to the model is a tensor of tokens. (This model uses only the</span></span><br><span class="line"><span class="string">    decoder part of the overall Transformer.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int): vocab size.</span></span><br><span class="line"><span class="string">        d_model (int):  depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        max_len (int): maximum symbol length for positional encoding.</span></span><br><span class="line"><span class="string">        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens</span></span><br><span class="line"><span class="string">        to activations over a vocab set.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Embedding inputs and positional encoder</span></span><br><span class="line">    positional_encoder = [ </span><br><span class="line">        <span class="comment"># Add embedding layer of dimension (vocab_size, d_model)</span></span><br><span class="line">        tl.Embedding(vocab_size,d_model),</span><br><span class="line">        <span class="comment"># Use dropout with rate and mode specified</span></span><br><span class="line">        tl.Dropout(rate = dropout, mode = mode),</span><br><span class="line">        <span class="comment"># Add positional encoding layer with maximum input length and mode specified</span></span><br><span class="line">        tl.PositionalEncoding()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span></span><br><span class="line">    decoder_blocks = [ </span><br><span class="line">        DecoderBlock(d_model, d_ff, n_heads,</span><br><span class="line">                 dropout, mode, ff_activation) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_layers)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the complete model as written in the figure</span></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        <span class="comment"># Use teacher forcing (feed output of previous step to current step)</span></span><br><span class="line">        tl.ShiftRight(), <span class="comment"># Specify the mode!</span></span><br><span class="line">        <span class="comment"># Add positional encoder</span></span><br><span class="line">        positional_encoder,</span><br><span class="line">        <span class="comment"># Add decoder blocks</span></span><br><span class="line">        decoder_blocks,</span><br><span class="line">        <span class="comment"># Normalize layer</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add dense layer of vocab_size (since need to select a word to translate to)</span></span><br><span class="line">        <span class="comment"># (a.k.a., logits layer. Note: activation already set by ff_activation)</span></span><br><span class="line">        tl.Dense(vocab_size),</span><br><span class="line">        <span class="comment"># Get probabilities with Logsoftmax</span></span><br><span class="line">        tl.LogSoftmax()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the Transformer</span></span><br><span class="line">print(TransformerLM(n_layers=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Serial[  ShiftRight(1)  Embedding_33300_512  Dropout  PositionalEncoding  Serial[    Branch_out2[      None      Serial[        LayerNorm        Serial[          Branch_out3[            [Dense_512, AttnHeads]            [Dense_512, AttnHeads]            [Dense_512, AttnHeads]          ]          DotProductAttn_in3          AttnOutput          Dense_512        ]        Dropout      ]    ]    Add_in2  ]  Serial[    Branch_out2[      None      Serial[        LayerNorm        Dense_2048        Relu        Dropout        Dense_512        Dropout      ]    ]    Add_in2  ]  LayerNorm  Dense_33300  LogSoftmax]</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  ShiftRight(<span class="number">1</span>)</span><br><span class="line">  Embedding_33300_512</span><br><span class="line">  Dropout</span><br><span class="line">  PositionalEncoding</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Serial[</span><br><span class="line">          Branch_out3[</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">          ]</span><br><span class="line">          DotProductAttn_in3</span><br><span class="line">          AttnOutput</span><br><span class="line">          Dense_512</span><br><span class="line">        ]</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Dense_2048</span><br><span class="line">        Relu</span><br><span class="line">        Dropout</span><br><span class="line">        Dense_512</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  LayerNorm</span><br><span class="line">  Dense_33300</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p><a name="3"></a></p><h1 id="Part-3-Training"><a href="#Part-3-Training" class="headerlink" title="Part 3: Training"></a>Part 3: Training</h1><p>Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a <code>gpu</code> or <code>cpu</code>. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words.</p><p><a name="3.1"></a></p><h3 id="3-1-Training-the-model"><a href="#3-1-Training-the-model" class="headerlink" title="3.1 Training the model"></a>3.1 Training the model</h3><p>You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an <code>epoch</code>. For each epoch, you have to go over all the data, using your training iterator.</p><p><a name="ex05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions:</strong> Implement the <code>train_model</code> program below to train the neural network above. Here is a list of things you should do:</p><ul><li>Create the train task by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask" target="_blank" rel="noopener"><code>trax.supervised.training.TrainTask</code></a> and pass in the following: <ul><li><span style="color:blue"> labeled_data </span> = train_gen</li><li><span style="color:blue"> loss_fn </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss" target="_blank" rel="noopener">tl.CrossEntropyLoss()</a></li><li><span style="color:blue"> optimizer </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam" target="_blank" rel="noopener">trax.optimizers.Adam(0.01)</a></li><li><span style="color:blue"> lr_schedule </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay" target="_blank" rel="noopener">lr_schedule</a></li></ul></li></ul><ul><li>Create the eval task by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask" target="_blank" rel="noopener"><code>trax.supervised.training.EvalTask</code></a> and pass in the following: <ul><li><span style="color:blue"> labeled_data </span> = eval_gen</li><li><span style="color:blue"> metrics </span> = tl.CrossEntropyLoss() and <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy" target="_blank" rel="noopener">tl.Accuracy()</a></li></ul></li></ul><ul><li>Create the training loop by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop" target="_blank" rel="noopener"><code>trax.supervised.Training.Loop</code></a> and pass in the following: <ul><li><span style="color:blue"> TransformerLM </span> </li><li><span style="color:blue"> train_task </span> </li><li><span style="color:blue"> eval_task </span> = [eval_task]</li><li><span style="color:blue"> output_dir</span> = output_dir</li></ul></li></ul><p>You will be using a cross entropy loss, with Adam optimizer. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Trax</a> documentation to get a full understanding. </p><p>The training loop that this function returns can be runned using the <code>run()</code> method by passing in the desired number of steps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"></span><br><span class="line"><span class="comment"># UNQ_C8</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span><span class="params">(TransformerLM, train_gen, eval_gen, output_dir = <span class="string">"~/model"</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        TransformerLM (trax.layers.combinators.Serial): The model you are building.</span></span><br><span class="line"><span class="string">        train_gen (generator): Training stream of data.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Evaluation stream of data.</span></span><br><span class="line"><span class="string">        output_dir (str): folder to save your file.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    output_dir = os.path.expanduser(output_dir)  <span class="comment"># trainer is an object</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    train_task = training.TrainTask( </span><br><span class="line">      labeled_data=train_gen, <span class="comment"># The training generator</span></span><br><span class="line">      loss_layer= tl.CrossEntropyLoss(), <span class="comment"># Loss function </span></span><br><span class="line">      optimizer= trax.optimizers.Adam(<span class="number">0.01</span>), <span class="comment"># Optimizer (Don't forget to set LR to 0.01)</span></span><br><span class="line">      lr_schedule= lr_schedule,</span><br><span class="line">      n_steps_per_checkpoint = <span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask( </span><br><span class="line">      labeled_data=eval_gen, <span class="comment"># The evaluation generator</span></span><br><span class="line">      metrics=[tl.CrossEntropyLoss(),tl.Accuracy()] <span class="comment"># CrossEntropyLoss and Accuracy</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    loop = training.Loop(TransformerLM(d_model=<span class="number">4</span>,</span><br><span class="line">                                       d_ff=<span class="number">16</span>,</span><br><span class="line">                                       n_layers=<span class="number">1</span>,</span><br><span class="line">                                       n_heads=<span class="number">2</span>,</span><br><span class="line">                                       mode=<span class="string">'train'</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure><p>Notice that the model will be trained for only 10 steps. </p><p>Even with this constraint the model with the original default arguments took a very long time to finish. Because of this some parameters are changed when defining the model that is fed into the training loop in the function above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Should take around 1.5 minutes</span></span><br><span class="line">!rm -f ~/model/model.pkl.gz</span><br><span class="line">loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>Step      1: Ran 1 train steps in 9.11 secsStep      1: train CrossEntropyLoss |  10.41297626Step      1: eval  CrossEntropyLoss |  10.41586781Step      1: eval          Accuracy |  0.00000000Step     10: Ran 9 train steps in 58.21 secsStep     10: train CrossEntropyLoss |  10.41278458Step     10: eval  CrossEntropyLoss |  10.41440201Step     10: eval          Accuracy |  0.00000000</code></pre><p> <a name="4"></a></p><h1 id="Part-4-Evaluation"><a href="#Part-4-Evaluation" class="headerlink" title="Part 4:  Evaluation"></a>Part 4:  Evaluation</h1><p><a name="4.1"></a></p><h3 id="4-1-Loading-in-a-trained-model"><a href="#4-1-Loading-in-a-trained-model" class="headerlink" title="4.1 Loading in a trained model"></a>4.1 Loading in a trained model</h3><p>In this part you will evaluate by loading in an almost exact version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model.</p><p>As you may have already noticed the model that you trained and the pretrained model share the same overall architecture but they have different values for some of the parameters:</p><p>   <code>Original (pretrained) model:</code>                                 </p><pre><code>TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8,                dropout=0.1, max_len=4096, ff_activation=tl.Relu)</code></pre><p>   <code>Your model:</code></p><pre><code>TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)</code></pre><p>   <strong>Only the parameters shown for your model were changed. The others stayed the same.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the model architecture</span></span><br><span class="line">model = TransformerLM(mode=<span class="string">'eval'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained weights</span></span><br><span class="line">model.init_from_file(<span class="string">'model.pkl.gz'</span>, weights_only=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><a name="5"></a></p><h1 id="Part-5-Testing-with-your-own-input"><a href="#Part-5-Testing-with-your-own-input" class="headerlink" title="Part 5: Testing with your own input"></a>Part 5: Testing with your own input</h1><p>You will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index. </p><p><a name="ex06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement the next symbol function that takes in the cur_output_tokens and the trained model to return the index of the next word. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_symbol</span><span class="params">(cur_output_tokens, model)</span>:</span></span><br><span class="line">    <span class="string">"""Returns the next symbol for a given sentence.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Serial): The transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        int: tokenized symbol.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current output tokens length</span></span><br><span class="line">    token_length = len(cur_output_tokens)</span><br><span class="line">    <span class="comment"># calculate the minimum power of 2 big enough to store token_length</span></span><br><span class="line">    <span class="comment"># HINT: use np.ceil() and np.log2()</span></span><br><span class="line">    <span class="comment"># add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0</span></span><br><span class="line">    padded_length = <span class="number">2</span>**int(np.ceil(np.log2(token_length + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fill cur_output_tokens with 0's until it reaches padded_length</span></span><br><span class="line">    padded = cur_output_tokens + [<span class="number">0</span>] * (padded_length - token_length)</span><br><span class="line">    padded_with_batch = np.array(padded)[<span class="keyword">None</span>, :] <span class="comment"># Don't replace this 'None'! This is a way of setting the batch dim</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># model expects a tuple containing two padded tensors (with batch)</span></span><br><span class="line">    output, _ = model((padded_with_batch, padded_with_batch)) </span><br><span class="line">    <span class="comment"># HINT: output has shape (1, padded_length, vocab_size)</span></span><br><span class="line">    <span class="comment"># To get log_probs you need to index output with 0 in the first dim</span></span><br><span class="line">    <span class="comment"># token_length in the second dim and all of the entries for the last dim.</span></span><br><span class="line">    log_probs = output[<span class="number">0</span>, token_length, :]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> int(np.argmax(log_probs))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out!</span></span><br><span class="line">sentence_test_nxt_symbl = <span class="string">"I want to fly in the sky."</span></span><br><span class="line">detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[<span class="number">0</span>], model)])</span><br></pre></td></tr></table></figure><pre><code>&#39;The&#39;</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">'The'</span><br></pre></td></tr></table></figure></p><p><a name="5.1"></a></p><h3 id="5-1-Greedy-decoding"><a href="#5-1-Greedy-decoding" class="headerlink" title="5.1 Greedy decoding"></a>5.1 Greedy decoding</h3><p>Now you will implement the greedy_decode algorithm that will call the <code>next_symbol</code> function. It takes in the input_sentence, the trained model and returns the decoded sentence. </p><p><a name="ex07"></a></p><h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p><strong>Instructions</strong>: Implement the greedy_decode algorithm. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10</span></span><br><span class="line"><span class="comment"># Decoding functions.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(input_sentence, model)</span>:</span></span><br><span class="line">    <span class="string">"""Greedy decode function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_sentence (string): a sentence or article.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Serial): Transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: summary of the input.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># Use tokenize()</span></span><br><span class="line">    cur_output_tokens = tokenize(input_sentence) + [<span class="number">0</span>]</span><br><span class="line">    generated_output = [] </span><br><span class="line">    cur_output = <span class="number">0</span> </span><br><span class="line">    EOS = <span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> cur_output != EOS:</span><br><span class="line">        <span class="comment"># Get next symbol</span></span><br><span class="line">        cur_output = next_symbol(cur_output_tokens, model)</span><br><span class="line">        <span class="comment"># Append next symbol to original sentence</span></span><br><span class="line">        cur_output_tokens.append(cur_output)</span><br><span class="line">        <span class="comment"># Append next symbol to generated sentence</span></span><br><span class="line">        generated_output.append(cur_output)</span><br><span class="line">        print(detokenize(generated_output))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> detokenize(generated_output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out on a sentence!</span></span><br><span class="line">test_sentence = <span class="string">"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips."</span></span><br><span class="line">print(wrapper.fill(test_sentence), <span class="string">'\n'</span>)</span><br><span class="line">print(greedy_decode(test_sentence, model))</span><br></pre></td></tr></table></figure><pre><code>It was a sunny day when I went to the market to buy some flowers. ButI only found roses, not tulips. :: I: I just: I just found: I just found ros: I just found roses: I just found roses,: I just found roses, not: I just found roses, not tu: I just found roses, not tulips: I just found roses, not tulips: I just found roses, not tulips.: I just found roses, not tulips.&lt;EOS&gt;: I just found roses, not tulips.&lt;EOS&gt;</code></pre><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">:</span><br><span class="line">: I</span><br><span class="line">: I just</span><br><span class="line">: I just found</span><br><span class="line">: I just found ros</span><br><span class="line">: I just found roses</span><br><span class="line">: I just found roses,</span><br><span class="line">: I just found roses, <span class="keyword">not</span></span><br><span class="line">: I just found roses, <span class="keyword">not</span> tu</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.&lt;EOS&gt;</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.&lt;EOS&gt;</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out with a whole article!</span></span><br><span class="line">article = <span class="string">"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students."</span></span><br><span class="line">print(wrapper.fill(article), <span class="string">'\n'</span>)</span><br><span class="line">print(greedy_decode(article, model))</span><br></pre></td></tr></table></figure><p><strong>Expected Output:</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Jordan</span><br><span class="line">Jordan Ful</span><br><span class="line">Jordan Fulcol</span><br><span class="line">Jordan Fulcoly</span><br><span class="line">Jordan Fulcoly,</span><br><span class="line">Jordan Fulcoly, Wayne</span><br><span class="line">Jordan Fulcoly, Wayne Dre</span><br><span class="line">Jordan Fulcoly, Wayne Drexe</span><br><span class="line">Jordan Fulcoly, Wayne Drexel</span><br><span class="line">Jordan Fulcoly, Wayne Drexel,</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line"></span><br><span class="line">Final summary:</span><br><span class="line"></span><br><span class="line">Jordan Fulcoly, Wayne Drexel, Tyler Carroll <span class="keyword">and</span> Connor Carroll were</span><br><span class="line">suspended <span class="keyword">for</span> one day. Four students were suspended <span class="keyword">for</span> one day</span><br><span class="line">because they allegedly did not heed to warnings that the 'Tebowing'</span><br><span class="line">craze was blocking the hallway <span class="keyword">and</span> presenting a safety hazard to</span><br><span class="line">students.&lt;EOS&gt;</span><br></pre></td></tr></table></figure></p><p><strong>Congratulations on finishing this week’s assignment!</strong> You did a lot of work and now you should have a better understanding of the encoder part of Transformers and how Transformers can be used for text summarization.</p><p><strong>Keep it up!</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Transformer-Summarizer&quot;&gt;&lt;a href=&quot;#Assignment-2-Transformer-Summarizer&quot; class=&quot;headerlink&quot; title=&quot;Assignment 2: Transfor
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Using RL to Solve Blackjack</title>
    <link href="https://zhangruochi.com/Using-RL-to-Solve-Blackjack/2020/09/17/"/>
    <id>https://zhangruochi.com/Using-RL-to-Solve-Blackjack/2020/09/17/</id>
    <published>2020-09-17T09:44:04.000Z</published>
    <updated>2020-09-17T09:45:27.584Z</updated>
    
    <content type="html"><![CDATA[<p><strong>转载自: <a href="https://github.com/zht007/tensorflow-practice/blob/master/7_Renforcement_Learning_blackjack/强化学习——MC(蒙特卡洛)玩21点扑克游戏.md" target="_blank" rel="noopener">https://github.com/zht007/tensorflow-practice/blob/master/7_Renforcement_Learning_blackjack/强化学习——MC(蒙特卡洛)玩21点扑克游戏.md</a></strong></p><h3 id="1-关于21点游戏"><a href="#1-关于21点游戏" class="headerlink" title="1. 关于21点游戏"></a>1. 关于21点游戏</h3><h4 id="1-1-规则简介"><a href="#1-1-规则简介" class="headerlink" title="1.1 规则简介"></a>1.1 规则简介</h4><p>21点的游戏规则详细很容易就能够找到，这里进行简单的介绍。</p><blockquote><ul><li><p>在这里<strong>智能体(Agent)</strong>扮演<strong>玩家(Player)</strong>，对方是<strong>庄家(Dealer)</strong>。</p></li><li><p><strong>点数(Score)</strong>：2-10的点数为牌面数字；J，Q，K是10点；<strong>A有两种算法</strong>，1或者11，算11总点数不超过21时则必须算成11(<strong>usable</strong>)，否则算作1。</p></li><li><p>庄家需要<strong>亮(Show)</strong>一张牌，玩家根据自己手中的牌和庄家亮的牌决定是<strong>要牌(hits)</strong>还是<strong>停牌(sticks)</strong>。</p></li><li><p>庄家要牌和停牌的规则是固定的，即点数小于17必须要牌，否则停牌。</p></li><li><p><strong>爆牌(goes bust)</strong>：牌总数操过21点，谁爆牌谁输，谁首先凑到21点谁赢，每有爆牌的时候谁大谁赢，同时凑到21为和局。</p></li></ul></blockquote><h4 id="1-2-转换成MDP"><a href="#1-2-转换成MDP" class="headerlink" title="1.2 转换成MDP"></a>1.2 转换成MDP</h4><p>了解规则后，我们将游戏转换成MDP，MDP的几大要素：状态(S: State)，行动(A: Action)，奖励(R: Reward)，策略Policy，状态值函数V(s): State-Value Function，行动值函数Q(s, a)Action-Value Function。</p><blockquote><p><strong>行动A</strong>：<strong>要牌(hits)</strong>还是<strong>停牌(sticks)</strong></p><p><strong>状态S</strong>：状态是由双方目前牌的点数决定的，但是当玩家点数小于等于11时，当然会毫不犹豫选择要牌，所以真正涉及到做选择的状态是12-21点的状态，此时庄家亮牌有A-10种情况，再加上是否有11的A(usable A)，所以21点游戏中所有的状态一<strong>共只有200个</strong>。</p><p><strong>奖励R</strong>：玩家赢牌奖励为1，输牌奖励为-1，和局和其他状态奖励为0。</p><p><strong>策略Policy</strong>：该状态下，要牌和停牌的概率</p></blockquote><h3 id="2-MC策略评估"><a href="#2-MC策略评估" class="headerlink" title="2. MC策略评估"></a>2. MC策略评估</h3><p>在<strong>给定策略</strong>下，为什么我们不用上一篇文章提到的DP方法进行策略评估呢？DP方法需要look one step ahead，假设玩家手里牌点数为14，庄家亮牌为10，你需要计算要牌和停牌之后所有可能性，下一张牌是什么？庄家可能抽到什么？离获得奖励有多远？等等，这几乎是不可能的。</p><p>MC可以通过抽样方式，直接根据策略实践，从而获取奖励和学习V(s)，克服了DP方法的限制。这里采用首次访问MC方法。大致分为三步：</p><p><strong>第一步</strong>：根据策略采样，直到游戏结束，获得一个episode的 (S0, A0, R1), (S1, A1, R2), . . . , (ST-1, AT-1, RT)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">episode = []</span><br><span class="line">state = env.reset()      </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    action = policy(state)</span><br><span class="line">    next_state, reward, done, _ = env.step(action)</span><br><span class="line">    episode.append((state, action, reward))</span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    state = next_state</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p><strong>第二步:</strong>   计算首次出现s状态的Reward，直到这个episode结束总共累积的Reward。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">states_in_episode = set([tuple(x[<span class="number">0</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> episode])</span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> states_in_episode:</span><br><span class="line">            <span class="comment"># Find the first occurance of the state in the episode</span></span><br><span class="line">            first_occurence_idx = next(i <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(episode) <span class="keyword">if</span> x[<span class="number">0</span>] == state)</span><br><span class="line">            <span class="comment"># Sum up all rewards since the first occurance</span></span><br><span class="line">            G = sum([x[<span class="number">2</span>]*(discount_factor**i) <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(episode[first_occurence_idx:])])</span><br><span class="line">            <span class="comment"># Calculate average return for this state over all sampled episodes</span></span><br><span class="line">            returns_sum[state] += G</span><br><span class="line">            returns_count[state] += <span class="number">1.0</span></span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p><strong>第三步</strong>：若干个epsoide之后，将累积的R平均就得到该s下的V(s)了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">V[state] = returns_sum[state] / returns_count[state]</span><br></pre></td></tr></table></figure><p>给定玩家的策略，当分数小于20则要牌，否则停牌</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_policy</span><span class="params">(observation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A policy that sticks if the player score is &gt;= 20 and hits otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    score, dealer_score, usable_ace = observation</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> score &gt;= <span class="number">20</span> <span class="keyword">else</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p>下图为500,000个epsoide之后的V(s)<img src="/Users/hongtao/Library/Application Support/typora-user-images/image-20190424164753631.png" alt="image-20190424164753631"></p><p>V(s)的分布只能告诉我们<strong>当前策略下</strong>每个<strong>状态</strong>(你的点数，庄家亮牌，是否有usable A)的<strong>价值</strong>，我们如何使用V(s)来改进我们的策略，从而获得最大几率获胜的可能性呢？这就是我们下一节要讨论的内容。</p><h3 id="3-MC控制"><a href="#3-MC控制" class="headerlink" title="3. MC控制"></a>3. MC控制</h3><p>当然我们的目的不仅仅是对当前策略进行评估，我们希望改进策略在游戏中获得最大的收益。与DP一样，MC可以采用评估加改进(Policy Evaluation and Policy Improvement)的方式，迭代更新策略，最终可以收敛到一个最佳的策略。</p><p>当然我们在MC控制中采用策略评估的时候，需要加入对行动的评估，即<strong>Q(s, a)行动值函数</strong>的评估。但是如果我们采用DP中Greedy的方式来改进策略会遇到问题。由于MC是用<strong>采样</strong>的方式更新<strong>Q(s, a)</strong>，这就意味着我们很可能错过一些状态和行动，而且永远也无法更新该状态和行动的Q函数了。这就是典型的<strong>探索利用困境(Explore Exploit Delima)</strong>。</p><p>解决探索利用困境，我们可以使用epsilon-greedy 方法，或者将探索和利用的policy分开，采用off-policy的方法更新策略。</p><h4 id="3-1-On-Policy-的-epsilon-greedy采样法"><a href="#3-1-On-Policy-的-epsilon-greedy采样法" class="headerlink" title="3.1 On-Policy 的 epsilon-greedy采样法"></a>3.1 On-Policy 的 epsilon-greedy采样法</h4><p>On-Policy即评估和改进的策略是同一个策略，为避免探索利用困境，我们采用 epsilon-greedy的方法。</p><p><strong>第一步</strong>：对于21点的游戏，我们定义 epsilon-greedy policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(Q, epsilon, nA)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></span><br><span class="line">        A = np.ones(nA, dtype=float) * epsilon / nA </span><br><span class="line">        best_action = np.argmax(Q[observation])</span><br><span class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</span><br><span class="line">        <span class="keyword">return</span> A </span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure><p>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</p><p>其中Q是一个dictionary，为该状态下对应的行动，这样定义epsilon greedy policy 既保证了最优行动的几率最大，同时也让采取其他行动几率为一个非零的小值(epsilon / nA )。这样就保证了智能体在采样的时候能够探索未知的状态和行动。</p><p><strong>第二步</strong>：与MC评估的第一步一致，根据策略采样，直到游戏结束，获得一个episode的 (S0, A0, R1), (S1, A1, R2), . . . , (ST-1, AT-1, RT)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</span><br><span class="line">       episode = []</span><br><span class="line">       state = env.reset()</span><br><span class="line">       <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">           probs = policy(state)</span><br><span class="line">           action = np.random.choice(np.arange(len(probs)), p=probs)</span><br><span class="line">           next_state, reward, done, _ = env.step(action)</span><br><span class="line">           episode.append((state, action, reward))</span><br><span class="line">           <span class="keyword">if</span> done:</span><br><span class="line">               <span class="keyword">break</span></span><br><span class="line">           state = next_state</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Control%20with%20Epsilon-Greedy%20Policies.ipynb" target="_blank" rel="noopener">github</a> MIT license</em></p><p>注意与MC评估不同的是，action无法从policy中直接得出，而是根据概率随机选择的，也就是有可能智能体会”探索”非最优行动。</p><p><strong>第三步</strong>：计算首次出现该s 和 a 的Reward，直到这个episode结束，总共累积的Reward。平均Reward并更新Q表。Q表更新的同时，Policy也就自动更新了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sa_in_episode = set([(tuple(x[<span class="number">0</span>]), x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> episode])</span><br><span class="line">        <span class="keyword">for</span> state, action <span class="keyword">in</span> sa_in_episode:</span><br><span class="line">            sa_pair = (state, action)</span><br><span class="line">            <span class="comment"># Find the first occurance of the (state, action) pair in the episode</span></span><br><span class="line">            first_occurence_idx = next(i <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(episode)</span><br><span class="line">                                       <span class="keyword">if</span> x[<span class="number">0</span>] == state <span class="keyword">and</span> x[<span class="number">1</span>] == action)</span><br><span class="line">            <span class="comment"># Sum up all rewards since the first occurance</span></span><br><span class="line">            G = sum([x[<span class="number">2</span>]*(discount_factor**i) <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(episode[first_occurence_idx:])])</span><br><span class="line">            <span class="comment"># Calculate average return for this state over all sampled episodes</span></span><br><span class="line">            returns_sum[sa_pair] += G</span><br><span class="line">            returns_count[sa_pair] += <span class="number">1.0</span></span><br><span class="line">            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The policy is improved implicitly by changing the Q dictionary</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Q, policy</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Control%20with%20Epsilon-Greedy%20Policies.ipynb" target="_blank" rel="noopener">github</a> MIT license</em></p><p>下图是500,000个episode之后Q表中各个状态对应的Action值，Action只有两个值0(停牌)和1(要牌)，读者就可以尝试用下图的策略指导玩21点的游戏啦。举个例子，比如你现在手上牌是14点，没有可作为11的A，庄家亮牌为8，那么根据左图所示，最好的策略就是要牌。</p><p><img src="/Users/hongtao/Library/Application Support/typora-user-images/image-20190429164937096.png" alt="image-20190429164937096"></p><h4 id="3-2-Off-Policy的-Weighted-Importance采样法"><a href="#3-2-Off-Policy的-Weighted-Importance采样法" class="headerlink" title="3.2 Off-Policy的 Weighted Importance采样法"></a>3.2 Off-Policy的 Weighted Importance采样法</h4><p>Off-Policy就是将最终想要得到的<strong>目标策略(Target Policy)</strong>和用于探索的<strong>行为策略(Behavior Policy)</strong>分离，对目标策略采取Greedy的改进方式，而对实际行动的行为策略采用随机探索的改进方式从而解决了探索利用困境。当然Off-Policy 还有很多其他的优点比如学习历史经验，学习别人的经验等等。</p><p>这部分涉及到的理论比较复杂，可参考<a href="http://incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">[1]</a> <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">[2]</a>中的相关内容。简单解释即首先用Behavior Policy指导智能体进行MC采样，然后用包含<strong>Importance Sampling Ratio</strong> 函数来更新Target Policy。Importance Sampling Ratio是Target Policy和Behavior Policy在同一路径下的概率比值。</p><p>Target Policy的Q(s, a)函数在MC采样下需要平均，这里采用加权平均的方法，包含Importatnce Sampling Ratio的权重简记为 W，最后，我们通过递推的方法更新 W 即可跟新Q(s, a)。</p><p><strong>第一步</strong>：生成两种policy方法，random policy 用于Behavior Policy，greedy policy用于Target Policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_random_policy</span><span class="params">(nA)</span>:</span></span><br><span class="line">    A = np.ones(nA, dtype=float) / nA</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_greedy_policy</span><span class="params">(Q)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(state)</span>:</span></span><br><span class="line">        A = np.zeros_like(Q[state], dtype=float)</span><br><span class="line">        best_action = np.argmax(Q[state])</span><br><span class="line">        A[best_action] = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p><strong>第二步</strong>：用Behavior Policy进行MC采样，这里与On-Policy 的方法类似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">target_policy = create_greedy_policy(Q)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        episode = []</span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            <span class="comment"># Sample an action from our policy</span></span><br><span class="line">            probs = behavior_policy(state)</span><br><span class="line">            action = np.random.choice(np.arange(len(probs)), p=probs)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            episode.append((state, action, reward))</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            state = next_state</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p>第三步**：递推的方法更新W和Q，Target Policy 也就自动更新了。注意由于是采用递推的方法，该episode是从后往前计算的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    G = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># The importance sampling ratio (the weights of the returns)</span></span><br><span class="line">    W = <span class="number">1.0</span></span><br><span class="line">    <span class="comment"># For each step in the episode, backwards</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(len(episode))[::<span class="number">-1</span>]:</span><br><span class="line">        state, action, reward = episode[t]</span><br><span class="line">        <span class="comment"># Update the total reward since step t</span></span><br><span class="line">        G = discount_factor * G + reward</span><br><span class="line">        <span class="comment"># Update weighted importance sampling formula denominator</span></span><br><span class="line">        C[state][action] += W</span><br><span class="line">        <span class="comment"># Update the action-value function using the incremental update formula (5.7)</span></span><br><span class="line">        <span class="comment"># This also improves our target policy which holds a reference to Q</span></span><br><span class="line">        Q[state][action] += (W / C[state][action]) * (G - Q[state][action])</span><br><span class="line">        <span class="comment"># If the action taken by the behavior policy is not the action </span></span><br><span class="line">        <span class="comment"># taken by the target policy the probability will be 0 and we can break</span></span><br><span class="line">        <span class="keyword">if</span> action !=  np.argmax(target_policy(state)):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        W = W * <span class="number">1.</span>/behavior_policy(state)[action]</span><br><span class="line">    </span><br><span class="line"><span class="keyword">return</span> Q, target_policy</span><br></pre></td></tr></table></figure><p><em>该部分代码参考<a href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb" target="_blank" rel="noopener">github</a> with MIT license</em></p><p>最后，经过500,000 个episod我们得到的最佳策略，与上一节采用On-Policy MC 方法的结果稍有差异，但基本一致。</p><p><img src="/Users/hongtao/Library/Application Support/typora-user-images/image-20190426125404672.png" alt="image-20190426125404672"></p><hr><p>参考资料</p><p>[1] <a href="http://incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">Reinforcement Learning: An Introduction (2nd Edition)</a></p><p>[2] <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">David Silver’s Reinforcement Learning Course (UCL, 2015)</a></p><p>[3] <a href="https://github.com/dennybritz/reinforcement-learning" target="_blank" rel="noopener">Github repo: Reinforcement Learning</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;转载自: &lt;a href=&quot;https://github.com/zht007/tensorflow-practice/blob/master/7_Renforcement_Learning_blackjack/强化学习——MC(蒙特卡洛)玩21点扑克游戏.
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Generalized Policy Iteration</title>
    <link href="https://zhangruochi.com/Generalized-Policy-Iteration/2020/09/10/"/>
    <id>https://zhangruochi.com/Generalized-Policy-Iteration/2020/09/10/</id>
    <published>2020-09-10T06:26:25.000Z</published>
    <updated>2020-09-10T07:20:31.481Z</updated>
    
    <content type="html"><![CDATA[<p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies.  As discussed there, we can easily obtain optimal policies once we have found the optimal value functions, $v_{\star}$ or $q_{\star}$, which satisfy the Bellman optimality equations:</p><script type="math/tex; mode=display">V_{\star} = max_a \sum_{s\prime,r} p(s\prime,r | s, a)[ r + \gamma v_{\star} (s\prime)]</script><script type="math/tex; mode=display">q_{\star}(s,a) = \sum_{s\prime,r} p(s\prime,r | s,a)[ r + \gamma max_{a\prime} q_{\star}(s\prime,a\prime)]</script><h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p>We consider how to compute the state-value function $V_{\pi}$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. </p><script type="math/tex; mode=display">V_{\pi}(s) = \sum_a \pi(a | s) \sum_{s\prime,r} p(s\prime, r | s, a) [ r + \gamma v_{\pi} (s\prime)]</script><p>Consider a sequence of approximate value functions $v_0,v_1, \cdots $,  each mapping $S+$ to $\mathbb{R}$ (the real numbers). The initial approximation, $v_0$, is chosen arbitrarily, and each successive approximation is obtained by using the Bellman equation for $V_{\pi}$ as an update rule:</p><script type="math/tex; mode=display">V_{k+1}(s) = \sum_a \pi(a | s) \sum_{s\prime,r} p(s\prime, r | s, a) [ r + \gamma v_{k} (s\prime)]</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>Our reason for computing the value function for a policy is to help find better policies. </p><p>Consider selecting a in s and thereafter following the existing policy, $\pi$,  The value of this way of behaving is</p><script type="math/tex; mode=display">q_{\pi}(s,a) = \sum_{s\prime,r}p(s\prime,r | s, a) [ r + \gamma  v_{\pi}(s\prime)]</script><p>The key criterion is whether this is greater than or less than $V_{\pi}(s)$.  If it is greater—that is, if it is better to select a once in s and thereafter follow ⇡ than it would be to follow $\pi$ all the time—then one would expect it to be better still to select a every time s is encountered, and that the new policy would in fact be a better one overall. </p><p>That this is true is a special case of a general result called the <strong>policy improvement theorem</strong>. Let $\pi$ and $\pi\prime$ be any pair of deterministic policies such that, for all $s\in S$, </p><script type="math/tex; mode=display">q_{\pi}(s, \pi\prime(s)) \geq v_{\pi}(s)</script><p>Then the policy $\pi\prime$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states $s\in S$:</p><script type="math/tex; mode=display">V_{\pi\prime}(s) \geq V_{\pi}(s)</script><h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>Once a policy, $\pi$, has been improved using $v_{\pi}$ to yield a better policy, $\pi\prime$, we can then compute $\pi\prime$ and  improve it again to yield an even better $\pi\prime\prime$. We can thus obtain a sequence of monotonically improving policies and value functions:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%"></center><p>This way of finding an optimal policy is called policy iteration. A complete algorithm is given in the box below.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%"></center><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><script type="math/tex; mode=display">V_{k+1}(s) = max_a \sum_{s\prime,r} p(s\prime,r | s, a)[ r + \gamma v_k (s\prime)]</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%"></center><h2 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h2><p>Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement). In policy iteration, these two processes alternate, each completing before the other begins, but this is not really necessary. In value iteration, for example, only a single iteration of policy evaluation is performed in between each policy improvement. In asynchronous DP methods, the evaluation and improvement processes are interleaved at an even finer grain. In some cases a single state is updated in one process before returning to the other. As long as both processes continue to update all states, the ultimate result is typically the same—convergence to the optimal value function and an optimal policy.</p><p>We use the term generalized policy iteration (GPI) to refer to the general idea of letting policy-evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram to the right. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the Bellman optimality equation holds, and thus that the policy and the value function are optimal.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="50%" height="50%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good 
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimal Policies with Dynamic Programming</title>
    <link href="https://zhangruochi.com/Optimal-Policies-with-Dynamic-Programming/2020/09/10/"/>
    <id>https://zhangruochi.com/Optimal-Policies-with-Dynamic-Programming/2020/09/10/</id>
    <published>2020-09-10T06:17:39.000Z</published>
    <updated>2020-09-10T06:18:14.325Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Optimal-Policies-with-Dynamic-Programming"><a href="#Assignment-2-Optimal-Policies-with-Dynamic-Programming" class="headerlink" title="Assignment 2: Optimal Policies with Dynamic Programming"></a>Assignment 2: Optimal Policies with Dynamic Programming</h1><p>Welcome to Assignment 2. This notebook will help you understand:</p><ul><li>Policy Evaluation and Policy Improvement.</li><li>Value and Policy Iteration.</li><li>Bellman Equations.</li></ul><h2 id="Gridworld-City"><a href="#Gridworld-City" class="headerlink" title="Gridworld City"></a>Gridworld City</h2><p>Gridworld City, a thriving metropolis with a booming technology industry, has recently experienced an influx of grid-loving software engineers. Unfortunately, the city’s street parking system, which charges a fixed rate, is struggling to keep up with the increased demand. To address this, the city council has decided to modify the pricing scheme to better promote social welfare. In general, the city considers social welfare higher when more parking is being used, the exception being that the city prefers that at least one spot is left unoccupied (so that it is available in case someone really needs it). The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. Now the city has hired you &mdash; an expert in dynamic programming &mdash; to help determine an optimal policy.</p><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><p>You’ll need two imports to complete this assigment:</p><ul><li>numpy: The fundamental package for scientific computing with Python.</li><li>tools: A module containing an environment and a plotting function.</li></ul><p>There are also some other lines in the cell below that are used for grading and plotting &mdash; you needn’t worry about them.</p><p>In this notebook, all cells are locked except those that you are explicitly asked to modify. It is up to you to decide how to implement your solution in these cells, <strong>but please do not import other libraries</strong> &mdash; doing so will break the autograder.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tools</span><br><span class="line"><span class="keyword">import</span> grader</span><br></pre></td></tr></table></figure><pre><code>&lt;Figure size 432x288 with 0 Axes&gt;</code></pre><p>In the city council’s parking MDP, states are nonnegative integers indicating how many parking spaces are occupied, actions are nonnegative integers designating the price of street parking, the reward is a real value describing the city’s preference for the situation, and time is discretized by hour. As might be expected, charging a high price is likely to decrease occupancy over the hour, while charging a low price is likely to increase it.</p><p>For now, let’s consider an environment with three parking spaces and three price points. Note that an environment with three parking spaces actually has four states &mdash; zero, one, two, or three spaces could be occupied.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">num_spaces = <span class="number">3</span></span><br><span class="line">num_prices = <span class="number">3</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">pi = np.ones((num_spaces + <span class="number">1</span>, num_prices)) / num_prices</span><br></pre></td></tr></table></figure><p>The value function is a one-dimensional array where the $i$-th entry gives the value of $i$ spaces being occupied.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">V</span><br></pre></td></tr></table></figure><pre><code>array([0., 0., 0., 0.])</code></pre><p>We can represent the policy as a two-dimensional array where the $(i, j)$-th entry gives the probability of taking action $j$ in state $i$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pi</span><br></pre></td></tr></table></figure><pre><code>array([[0.33333333, 0.33333333, 0.33333333],       [0.33333333, 0.33333333, 0.33333333],       [0.33333333, 0.33333333, 0.33333333],       [0.33333333, 0.33333333, 0.33333333]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pi[<span class="number">0</span>] = [<span class="number">0.75</span>, <span class="number">0.11</span>, <span class="number">0.14</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s, pi_s <span class="keyword">in</span> enumerate(pi):</span><br><span class="line">    <span class="keyword">for</span> a, p <span class="keyword">in</span> enumerate(pi_s):</span><br><span class="line">        print(<span class="string">f'pi(A=<span class="subst">&#123;a&#125;</span>|S=<span class="subst">&#123;s&#125;</span>) = <span class="subst">&#123;p.round(<span class="number">2</span>)&#125;</span>    '</span>, end=<span class="string">''</span>)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure><pre><code>pi(A=0|S=0) = 0.75    pi(A=1|S=0) = 0.11    pi(A=2|S=0) = 0.14    pi(A=0|S=1) = 0.33    pi(A=1|S=1) = 0.33    pi(A=2|S=1) = 0.33    pi(A=0|S=2) = 0.33    pi(A=1|S=2) = 0.33    pi(A=2|S=2) = 0.33    pi(A=0|S=3) = 0.33    pi(A=1|S=3) = 0.33    pi(A=2|S=3) = 0.33    </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">V[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><p>We can visualize a value function and policy with the <code>plot</code> function in the <code>tools</code> module. On the left, the value function is displayed as a barplot. State zero has an expected return of ten, while the other states have an expected return of zero. On the right, the policy is displayed on a two-dimensional grid. Each vertical strip gives the policy at the labeled state. In state zero, action zero is the darkest because the agent’s policy makes this choice with the highest probability. In the other states the agent has the equiprobable policy, so the vertical strips are colored uniformly.</p><p>You can access the state space and the action set as attributes of the environment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.S</span><br></pre></td></tr></table></figure><pre><code>[0, 1, 2, 3]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.A</span><br></pre></td></tr></table></figure><pre><code>[0, 1, 2]</code></pre><p>You will need to use the environment’s <code>transitions</code> method to complete this assignment. The method takes a state and an action and returns a 2-dimensional array, where the entry at $(i, 0)$ is the reward for transitioning to state $i$ from the current state and the entry at $(i, 1)$ is the conditional probability of transitioning to state $i$ given the current state and action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">state = <span class="number">3</span></span><br><span class="line">action = <span class="number">1</span></span><br><span class="line">transitions = env.transitions(state, action)</span><br><span class="line">transitions</span><br></pre></td></tr></table></figure><pre><code>array([[1.        , 0.12390437],       [2.        , 0.15133714],       [3.        , 0.1848436 ],       [2.        , 0.53991488]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sp, (r, p) <span class="keyword">in</span> enumerate(transitions):</span><br><span class="line">    print(<span class="string">f'p(S\'=<span class="subst">&#123;sp&#125;</span>, R=<span class="subst">&#123;r&#125;</span> | S=<span class="subst">&#123;state&#125;</span>, A=<span class="subst">&#123;action&#125;</span>) = <span class="subst">&#123;p.round(<span class="number">2</span>)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>p(S&#39;=0, R=1.0 | S=3, A=1) = 0.12p(S&#39;=1, R=2.0 | S=3, A=1) = 0.15p(S&#39;=2, R=3.0 | S=3, A=1) = 0.18p(S&#39;=3, R=2.0 | S=3, A=1) = 0.54</code></pre><h2 id="Section-1-Policy-Evaluation"><a href="#Section-1-Policy-Evaluation" class="headerlink" title="Section 1: Policy Evaluation"></a>Section 1: Policy Evaluation</h2><p>You’re now ready to begin the assignment! First, the city council would like you to evaluate the quality of the existing pricing scheme. Policy evaluation works by iteratively applying the Bellman equation for $v_{\pi}$ to a working value function, as an update rule, as shown below.</p><script type="math/tex; mode=display">\large v(s) \leftarrow \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a)[r + \gamma v(s')]</script><p>This update can either occur “in-place” (i.e. the update rule is sequentially applied to each state) or with “two-arrays” (i.e. the update rule is simultaneously applied to each state). Both versions converge to $v_{\pi}$ but the in-place version usually converges faster. <strong>In this assignment, we will be implementing all update rules in-place</strong>, as is done in the pseudocode of chapter 4 of the textbook. </p><p>We have written an outline of the policy evaluation algorithm described in chapter 4.1 of the textbook. It is left to you to fill in the <code>bellman_update</code> function to complete the algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_policy</span><span class="params">(env, V, pi, gamma, theta)</span>:</span></span><br><span class="line">    delta = float(<span class="string">'inf'</span>)</span><br><span class="line">    <span class="keyword">while</span> delta &gt; theta:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            bellman_update(env, V, pi, s, gamma)</span><br><span class="line">            delta = max(delta, abs(v - V[s]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> V</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bellman_update</span><span class="params">(env, V, pi, s, gamma)</span>:</span></span><br><span class="line">    <span class="string">"""Mutate ``V`` according to the Bellman update equation."""</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    actions = pi[s]</span><br><span class="line">    G = [<span class="number">0</span>] * len(actions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, action)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> s_, (r,p) <span class="keyword">in</span> enumerate(transitions):</span><br><span class="line">            G[action] += p * (r + gamma * V[s_])</span><br><span class="line">            </span><br><span class="line">    V[s] = np.sum(G * actions)</span><br></pre></td></tr></table></figure><p>The cell below uses the policy evaluation algorithm to evaluate the city’s policy, which charges a constant price of one.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set up test environment</span></span><br><span class="line">num_spaces = <span class="number">10</span></span><br><span class="line">num_prices = <span class="number">4</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build test policy</span></span><br><span class="line">city_policy = np.zeros((num_spaces + <span class="number">1</span>, num_prices))</span><br><span class="line">city_policy[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">V = evaluate_policy(env, V, city_policy, gamma, theta)</span><br><span class="line"></span><br><span class="line">print(V)</span><br></pre></td></tr></table></figure><pre><code>[80.04173399 81.65532303 83.37394007 85.12975566 86.87174913 88.55589131 90.14020422 91.58180605 92.81929841 93.78915889 87.77792991]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set up test environment</span></span><br><span class="line">num_spaces = <span class="number">10</span></span><br><span class="line">num_prices = <span class="number">4</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build test policy</span></span><br><span class="line">city_policy = np.zeros((num_spaces + <span class="number">1</span>, num_prices))</span><br><span class="line">city_policy[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">V = evaluate_policy(env, V, city_policy, gamma, theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test the value function</span></span><br><span class="line">answer = [<span class="number">80.04</span>, <span class="number">81.65</span>, <span class="number">83.37</span>, <span class="number">85.12</span>, <span class="number">86.87</span>, <span class="number">88.55</span>, <span class="number">90.14</span>, <span class="number">91.58</span>, <span class="number">92.81</span>, <span class="number">93.78</span>, <span class="number">87.77</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the value function is within 2 decimal places of the correct answer</span></span><br><span class="line"><span class="keyword">assert</span> grader.near(V, answer, <span class="number">1e-2</span>)</span><br></pre></td></tr></table></figure><p>You can use the <code>plot</code> function to visualize the final value function and policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line">tools.plot(V, city_policy)</span><br></pre></td></tr></table></figure><p><img src="output_26_0.png" alt="png"></p><p>Observe that the value function qualitatively resembles the city council’s preferences &mdash; it monotonically increases as more parking is used, until there is no parking left, in which case the value is lower. Because of the relatively simple reward function (more reward is accrued when many but not all parking spots are taken and less reward is accrued when few or all parking spots are taken) and the highly stochastic dynamics function (each state has positive probability of being reached each time step) the value functions of most policies will qualitatively resemble this graph. However, depending on the intelligence of the policy, the scale of the graph will differ. In other words, better policies will increase the expected return at every state rather than changing the relative desirability of the states. Intuitively, the value of a less desirable state can be increased by making it less likely to remain in a less desirable state. Similarly, the value of a more desirable state can be increased by making it more likely to remain in a more desirable state. That is to say, good policies are policies that spend more time in desirable states and less time in undesirable states. As we will see in this assignment, such a steady state distribution is achieved by setting the price to be low in low occupancy states (so that the occupancy will increase) and setting the price high when occupancy is high (so that full occupancy will be avoided).</p><h2 id="Section-2-Policy-Iteration"><a href="#Section-2-Policy-Iteration" class="headerlink" title="Section 2: Policy Iteration"></a>Section 2: Policy Iteration</h2><p>Now the city council would like you to compute a more efficient policy using policy iteration. Policy iteration works by alternating between evaluating the existing policy and making the policy greedy with respect to the existing value function. We have written an outline of the policy iteration algorithm described in chapter 4.3 of the textbook. We will make use of the policy evaluation algorithm you completed in section 1. It is left to you to fill in the <code>q_greedify_policy</code> function, such that it modifies the policy at $s$ to be greedy with respect to the q-values at $s$, to complete the policy improvement algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">improve_policy</span><span class="params">(env, V, pi, gamma)</span>:</span></span><br><span class="line">    policy_stable = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        old = pi[s].copy()</span><br><span class="line">        q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.array_equal(pi[s], old):</span><br><span class="line">            policy_stable = <span class="keyword">False</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> pi, policy_stable</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_iteration</span><span class="params">(env, gamma, theta)</span>:</span></span><br><span class="line">    V = np.zeros(len(env.S))</span><br><span class="line">    pi = np.ones((len(env.S), len(env.A))) / len(env.A)</span><br><span class="line">    policy_stable = <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> policy_stable:</span><br><span class="line">        V = evaluate_policy(env, V, pi, gamma, theta)</span><br><span class="line">        pi, policy_stable = improve_policy(env, V, pi, gamma)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_greedify_policy</span><span class="params">(env, V, pi, s, gamma)</span>:</span></span><br><span class="line">    <span class="string">"""Mutate ``pi`` to be greedy with respect to the q-values induced by ``V``."""</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    G = [<span class="number">0</span>] * len(env.A)</span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, action)</span><br><span class="line">        <span class="keyword">for</span> s_, (r,p) <span class="keyword">in</span> enumerate(transitions):</span><br><span class="line">            G[action] += p * (r + gamma * V[s_])</span><br><span class="line">            </span><br><span class="line">    best_a = np.argmax(G)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,_ <span class="keyword">in</span> enumerate(pi[s]):</span><br><span class="line">        <span class="keyword">if</span> i == best_a:</span><br><span class="line">            pi[s][i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pi[s][i] = <span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">6</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">pi = np.ones((<span class="number">7</span>, <span class="number">4</span>)) / <span class="number">4</span></span><br><span class="line"></span><br><span class="line">new_pi, stable = improve_policy(env, V, pi, gamma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># expect first call to greedify policy</span></span><br><span class="line">expected_pi = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.all(new_pi == expected_pi)</span><br><span class="line"><span class="keyword">assert</span> stable == <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the value function has not changed, so the greedy policy should not change</span></span><br><span class="line">new_pi, stable = improve_policy(env, V, new_pi, gamma)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.all(new_pi == expected_pi)</span><br><span class="line"><span class="keyword">assert</span> stable == <span class="keyword">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V, pi = policy_iteration(env, gamma, theta)</span><br><span class="line"></span><br><span class="line">V_answer = [<span class="number">81.60</span>, <span class="number">83.28</span>, <span class="number">85.03</span>, <span class="number">86.79</span>, <span class="number">88.51</span>, <span class="number">90.16</span>, <span class="number">91.70</span>, <span class="number">93.08</span>, <span class="number">94.25</span>, <span class="number">95.25</span>, <span class="number">89.45</span>]</span><br><span class="line">pi_answer = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure value function is within 2 decimal places of answer</span></span><br><span class="line"><span class="keyword">assert</span> grader.near(V, V_answer, <span class="number">1e-2</span>)</span><br><span class="line"><span class="comment"># make sure policy is exactly correct</span></span><br><span class="line"><span class="keyword">assert</span> np.all(pi == pi_answer)</span><br></pre></td></tr></table></figure><p>When you are ready to test the policy iteration algorithm, run the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = policy_iteration(env, gamma, theta)</span><br></pre></td></tr></table></figure><p>You can use the <code>plot</code> function to visualize the final value function and policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure><p><img src="output_36_0.png" alt="png"></p><p>You can check the value function (rounded to one decimal place) and policy against the answer below:<br><br>State $\quad\quad$    Value $\quad\quad$ Action<br><br>0 $\quad\quad\quad\;$        81.6 $\quad\quad\;$ 0<br><br>1 $\quad\quad\quad\;$        83.3 $\quad\quad\;$ 0<br><br>2 $\quad\quad\quad\;$        85.0 $\quad\quad\;$ 0<br><br>3 $\quad\quad\quad\;$        86.8 $\quad\quad\;$ 0<br><br>4 $\quad\quad\quad\;$        88.5 $\quad\quad\;$ 0<br><br>5 $\quad\quad\quad\;$        90.2 $\quad\quad\;$ 0<br><br>6 $\quad\quad\quad\;$        91.7 $\quad\quad\;$ 0<br><br>7 $\quad\quad\quad\;$        93.1 $\quad\quad\;$ 0<br><br>8 $\quad\quad\quad\;$        94.3 $\quad\quad\;$ 0<br><br>9 $\quad\quad\quad\;$        95.3 $\quad\quad\;$ 3<br><br>10 $\quad\quad\;\;\,\,$      89.5 $\quad\quad\;$ 3<br></p><h2 id="Section-3-Value-Iteration"><a href="#Section-3-Value-Iteration" class="headerlink" title="Section 3: Value Iteration"></a>Section 3: Value Iteration</h2><p>The city has also heard about value iteration and would like you to implement it. Value iteration works by iteratively applying the Bellman optimality equation for $v_{\ast}$ to a working value function, as an update rule, as shown below.</p><script type="math/tex; mode=display">\large v(s) \leftarrow \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v(s')]</script><p>We have written an outline of the value iteration algorithm described in chapter 4.4 of the textbook. It is left to you to fill in the <code>bellman_optimality_update</code> function to complete the value iteration algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_iteration</span><span class="params">(env, gamma, theta)</span>:</span></span><br><span class="line">    V = np.zeros(len(env.S))</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            bellman_optimality_update(env, V, s, gamma)</span><br><span class="line">            delta = max(delta, abs(v - V[s]))</span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    pi = np.ones((len(env.S), len(env.A))) / len(env.A)</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bellman_optimality_update</span><span class="params">(env, V, s, gamma)</span>:</span></span><br><span class="line">    <span class="string">"""Mutate ``V`` according to the Bellman optimality update equation."""</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    G = np.zeros(len(env.A))    </span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, a)</span><br><span class="line">        <span class="keyword">for</span> s_, (r, p) <span class="keyword">in</span> enumerate(transitions):</span><br><span class="line">            G[a] += p*(r + gamma*V[s_])</span><br><span class="line">    V[s] = np.max(G)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">6</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># only state 0 updated</span></span><br><span class="line">bellman_optimality_update(env, V, <span class="number">0</span>, gamma)</span><br><span class="line"><span class="keyword">assert</span> list(V) == [<span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># only state 2 updated</span></span><br><span class="line">bellman_optimality_update(env, V, <span class="number">2</span>, gamma)</span><br><span class="line"><span class="keyword">assert</span> list(V) == [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        bellman_optimality_update(env, V, s, gamma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure value function is exactly correct</span></span><br><span class="line">answer = [<span class="number">61</span>, <span class="number">63</span>, <span class="number">65</span>, <span class="number">67</span>, <span class="number">69</span>, <span class="number">71</span>, <span class="number">72</span>, <span class="number">74</span>, <span class="number">75</span>, <span class="number">76</span>, <span class="number">71</span>]</span><br><span class="line"><span class="keyword">assert</span> np.all(V == answer)</span><br></pre></td></tr></table></figure><p>When you are ready to test the value iteration algorithm, run the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = value_iteration(env, gamma, theta)</span><br></pre></td></tr></table></figure><p>You can use the <code>plot</code> function to visualize the final value function and policy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure><p><img src="output_46_0.png" alt="png"></p><p>You can check your value function (rounded to one decimal place) and policy against the answer below:<br><br>State $\quad\quad$    Value $\quad\quad$ Action<br><br>0 $\quad\quad\quad\;$        81.6 $\quad\quad\;$ 0<br><br>1 $\quad\quad\quad\;$        83.3 $\quad\quad\;$ 0<br><br>2 $\quad\quad\quad\;$        85.0 $\quad\quad\;$ 0<br><br>3 $\quad\quad\quad\;$        86.8 $\quad\quad\;$ 0<br><br>4 $\quad\quad\quad\;$        88.5 $\quad\quad\;$ 0<br><br>5 $\quad\quad\quad\;$        90.2 $\quad\quad\;$ 0<br><br>6 $\quad\quad\quad\;$        91.7 $\quad\quad\;$ 0<br><br>7 $\quad\quad\quad\;$        93.1 $\quad\quad\;$ 0<br><br>8 $\quad\quad\quad\;$        94.3 $\quad\quad\;$ 0<br><br>9 $\quad\quad\quad\;$        95.3 $\quad\quad\;$ 3<br><br>10 $\quad\quad\;\;\,\,$      89.5 $\quad\quad\;$ 3<br></p><p>In the value iteration algorithm above, a policy is not explicitly maintained until the value function has converged. Below, we have written an identically behaving value iteration algorithm that maintains an updated policy. Writing value iteration in this form makes its relationship to policy iteration more evident. Policy iteration alternates between doing complete greedifications and complete evaluations. On the other hand, value iteration alternates between doing local greedifications and local evaluations. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_iteration2</span><span class="params">(env, gamma, theta)</span>:</span></span><br><span class="line">    V = np.zeros(len(env.S))</span><br><span class="line">    pi = np.ones((len(env.S), len(env.A))) / len(env.A)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">            bellman_update(env, V, pi, s, gamma)</span><br><span class="line">            delta = max(delta, abs(v - V[s]))</span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure><p>You can try the second value iteration algorithm by running the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = value_iteration2(env, gamma, theta)</span><br><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure><p><img src="output_51_0.png" alt="png"></p><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations, you’ve completed assignment 2! In this assignment, we investigated policy evaluation and policy improvement, policy iteration and value iteration, and Bellman updates. Gridworld City thanks you for your service!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Optimal-Policies-with-Dynamic-Programming&quot;&gt;&lt;a href=&quot;#Assignment-2-Optimal-Policies-with-Dynamic-Programming&quot; class=&quot;hea
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Markov Decision Processes II</title>
    <link href="https://zhangruochi.com/Markov-Decision-Processes-II/2020/09/06/"/>
    <id>https://zhangruochi.com/Markov-Decision-Processes-II/2020/09/06/</id>
    <published>2020-09-06T11:36:56.000Z</published>
    <updated>2020-09-08T10:08:41.368Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lesson-1-Policies-and-Value-Functions"><a href="#Lesson-1-Policies-and-Value-Functions" class="headerlink" title="Lesson 1: Policies and Value Functions"></a>Lesson 1: Policies and Value Functions</h2><h3 id="Recognize-that-a-policy-is-a-distribution-over-actions-for-each-possible-state"><a href="#Recognize-that-a-policy-is-a-distribution-over-actions-for-each-possible-state" class="headerlink" title="Recognize that a policy is a distribution over actions for each possible state."></a>Recognize that a policy is a distribution over actions for each possible state.</h3><p>a policy is a mapping from states to probabilities of selecting each possible action.  If the agent is following policy $\pi$ at time $t$, then $\pi(a | s)$ is the probability that $A_t = a$ if $S_t = s$</p><h3 id="Describe-the-similarities-and-differences-between-stochastic-and-deterministic-policies"><a href="#Describe-the-similarities-and-differences-between-stochastic-and-deterministic-policies" class="headerlink" title="Describe the similarities and differences between stochastic and deterministic policies"></a>Describe the similarities and differences between stochastic and deterministic policies</h3><p>Deterministic policies: a policy assigns probabilities to each action in each state.</p><p>Stochastic policy: a policy where multiple actions may be selected with non-zero probability.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="50%" height="50%"></center><h3 id="Identify-the-characteristics-of-a-well-defined-policy"><a href="#Identify-the-characteristics-of-a-well-defined-policy" class="headerlink" title="Identify the characteristics of a well-defined policy"></a>Identify the characteristics of a well-defined policy</h3><ul><li>An agent’s behavior is specified by a policy that maps the state to a probability distribution over actions</li><li>The policy can depend only on the current state, and not other things like time or previous states. See you next time.</li></ul><h3 id="Describe-the-roles-of-state-value-and-action-value-functions-in-reinforcement-learning"><a href="#Describe-the-roles-of-state-value-and-action-value-functions-in-reinforcement-learning" class="headerlink" title="Describe the roles of state-value and action-value functions in reinforcement learning"></a>Describe the roles of state-value and action-value functions in reinforcement learning</h3><p>Similarly, we define the value of taking action a in state s under a policy $\pi$, denoted $q_{\pi}(s,a)$,  as the expected return starting from $s$,  taking the action a, and thereafter following policy $\pi$:</p><script type="math/tex; mode=display">q_{\pi}(s) = \mathbb{E}_{\pi}[ G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi}[ \sum_{k=0}^{\infty} \eta^k R_{t+k+1} | S_t = s, A_t = a ]</script><p>We call $q_{\pi}$ the action-value function for policy $\pi$.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">the roles of state-value and action-value functions</div></center><h3 id="Describe-the-relationship-between-value-functions-and-policies"><a href="#Describe-the-relationship-between-value-functions-and-policies" class="headerlink" title="Describe the relationship between value functions and policies"></a>Describe the relationship between value functions and policies</h3><p>Value function enable us to judge the quality of different policies.</p><p>The value function of a state s under a policy $\pi$, denoted $V_\pi(s)$, is the expected return when starting in s and following $\pi$ thereafter. For MDPs, we can define $V_{\pi}$ formally by</p><script type="math/tex; mode=display">V_{\pi}(s) = \mathbb{E}[ G_t | S_t = s] = \mathbb{E}_{\pi}[ \sum_{k=0}^{\infty} \eta^k R_{t+k+1} | S_t = s ], \text{for all} \quad s \in \mathbb{S}</script><p>where $\mathbb{E}[\dot]$ denotes the expected value of a random variable given that the agent follows policy $\pi$, and t is any time step.</p><h3 id="Create-examples-of-valid-value-functions-for-a-given-MDP"><a href="#Create-examples-of-valid-value-functions-for-a-given-MDP" class="headerlink" title="Create examples of valid value functions for a given MDP"></a>Create examples of valid value functions for a given MDP</h3><h2 id="Lesson-2-Bellman-Equations"><a href="#Lesson-2-Bellman-Equations" class="headerlink" title="Lesson 2: Bellman Equations"></a>Lesson 2: Bellman Equations</h2><h3 id="Derive-the-Bellman-equation-for-state-value-functions"><a href="#Derive-the-Bellman-equation-for-state-value-functions" class="headerlink" title="Derive the Bellman equation for state-value functions"></a>Derive the Bellman equation for state-value functions</h3><script type="math/tex; mode=display">V_{\pi}(s) = \mathbb{E}[ G_t | S_t = s] = \sum_a \pi (a | s)\sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\pi}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}</script><h3 id="Derive-the-Bellman-equation-for-action-value-functions"><a href="#Derive-the-Bellman-equation-for-action-value-functions" class="headerlink" title="Derive the Bellman equation for action-value functions"></a>Derive the Bellman equation for action-value functions</h3><script type="math/tex; mode=display">q_{\pi}(s,a) = \mathbb{E}_{\pi}[ G_t | S_t = s, A_t = a] = \sum_{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta\sum_{a\prime} \pi (a\prime | q_{\pi}(s\prime, a\prime))]</script><h3 id="Understand-how-Bellman-equations-relate-current-and-future-values"><a href="#Understand-how-Bellman-equations-relate-current-and-future-values" class="headerlink" title="Understand how Bellman equations relate current and future values"></a>Understand how Bellman equations relate current and future values</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><p>The current time-step’s state/action values can be written recursivelu in terms of future state/action values</p><p>Bellman equation for $V_{\pi}$,  It expresses a relationship between the value of a state and the values of its successor states. Think of looking ahead from a state to its possible successor states, as suggested by the diagram to the right. Each open circle represents a state and each solid circle represents a state–action pair. Starting from state s, the root node at the top, the agent could take any of some set of actions—three are shown in the diagram—based on its policy $\pi$.  From each of these, the environment could respond with one of several next states, $s\prime$ (two are shown in the figure), along with a reward, r, depending on its dynamics given by the function $p$. The Bellman equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the(discounted) value of the expected next state, plus the reward expected along the way.</p><p>We call diagrams like that above backup diagrams because they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. These operations transfer value information back to a state (or a state–action pair) from its successor states (or state–action pairs)</p><h3 id="Use-the-Bellman-equations-to-compute-value-functions"><a href="#Use-the-Bellman-equations-to-compute-value-functions" class="headerlink" title="Use the Bellman equations to compute value functions"></a>Use the Bellman equations to compute value functions</h3><p>The value function  is the unique solution to its Bellman equation.</p><h2 id="Lesson-3-Optimality-Optimal-Policies-amp-Value-Functions"><a href="#Lesson-3-Optimality-Optimal-Policies-amp-Value-Functions" class="headerlink" title="Lesson 3: Optimality (Optimal Policies &amp; Value Functions)"></a>Lesson 3: Optimality (Optimal Policies &amp; Value Functions)</h2><h3 id="Define-an-optimal-policy-understand-how-a-policy-can-be-at-least-as-good-as-every-other-policy-in-every-state"><a href="#Define-an-optimal-policy-understand-how-a-policy-can-be-at-least-as-good-as-every-other-policy-in-every-state" class="headerlink" title="Define an optimal policy, understand how a policy can be at least as good as every other policy in every state"></a>Define an optimal policy, understand how a policy can be at least as good as every other policy in every state</h3><p>For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies.  A policy $\pi$ is defined to be better than or equal to a policy $\pi\prime$ if its expected return is greater than or equal to that of $\pi\prime$ for all states.  In other words, $\pi \geq \pi\prime$ if and only if $V_{\pi}(s) \geq V_{\pi\prime}(s)$ for all $s \in \mathbb{S}$. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the optimal state-value function, denoted $V_{\star}$, and defined as</p><script type="math/tex; mode=display">V_{\star} = max_{\pi}V_{\pi}(s) \quad \text{for all} \quad s \in \mathbb{S}</script><p>Optimal policies also share the same optimal action-value function, denoted $q_{\star}$, and defined as</p><script type="math/tex; mode=display">q_{\star}(s,a) = max_{\pi} q_{\pi}(s,a) \quad \text{for all} \quad s \in \mathbb{S} \, \text{and} \, a \in \mathbb{A}(s)</script><p>For the state–action pair (s, a), this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_{\star}$ in terms of $V_{\star}$ as follows:</p><script type="math/tex; mode=display">q_{\star}(s,a) = \mathbb{E}[ R_{t+1} + \eta v_{\star}(S_{t+1}) | S_t = s, A_t = a]</script><h3 id="Derive-the-Bellman-optimality-equation-for-state-value-functions"><a href="#Derive-the-Bellman-optimality-equation-for-state-value-functions" class="headerlink" title="Derive the Bellman optimality equation for state-value functions"></a>Derive the Bellman optimality equation for state-value functions</h3><script type="math/tex; mode=display">V_{\star}(s) = \sum_a \pi_{\star} (a | s)\sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\star}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}</script><script type="math/tex; mode=display">V_{\star}(s) = max_a \sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\star}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}</script><h3 id="Derive-the-Bellman-optimality-equation-for-action-value-functions"><a href="#Derive-the-Bellman-optimality-equation-for-action-value-functions" class="headerlink" title="Derive the Bellman optimality equation for action-value functions"></a>Derive the Bellman optimality equation for action-value functions</h3><script type="math/tex; mode=display">q_{\star}(s,a) = \sum_{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta\sum_{a\prime} \pi_{\star} (a\prime | q_{\star}(s\prime, a\prime))]</script><script type="math/tex; mode=display">q_{\star}(s,a) = \sum_{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta max_{a\prime}q_{\star}(s\prime, a\prime))]</script><h3 id="Understand-the-connection-between-the-optimal-value-function-and-optimal-policies"><a href="#Understand-the-connection-between-the-optimal-value-function-and-optimal-policies" class="headerlink" title="Understand the connection between the optimal value function and optimal policies"></a>Understand the connection between the optimal value function and optimal policies</h3><p>Once we had the optimal state-value function, it’s relatively easy to work out the optimal policy. If we have the optimal action-value function, working out the optimal policy is even easier. This correspondence between optimal-value functions and optimal-policies will help us to derive many of the reinforced learning algorithms we will explore later in this specialization.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lesson-1-Policies-and-Value-Functions&quot;&gt;&lt;a href=&quot;#Lesson-1-Policies-and-Value-Functions&quot; class=&quot;headerlink&quot; title=&quot;Lesson 1: Policies
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Markov Decision Processes I</title>
    <link href="https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/"/>
    <id>https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/</id>
    <published>2020-09-04T03:37:10.000Z</published>
    <updated>2020-09-07T05:55:54.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lesson-1-Introduction-to-Markov-Decision-Processes"><a href="#Lesson-1-Introduction-to-Markov-Decision-Processes" class="headerlink" title="Lesson 1: Introduction to Markov Decision Processes"></a>Lesson 1: Introduction to Markov Decision Processes</h2><h3 id="Understand-Markov-Decision-Processes-or-MDPs"><a href="#Understand-Markov-Decision-Processes-or-MDPs" class="headerlink" title="Understand Markov Decision Processes, or MDPs"></a>Understand Markov Decision Processes, or MDPs</h3><p>MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards.</p><h3 id="Understand-the-graphical-representation-of-a-Markov-Decision-Process"><a href="#Understand-the-graphical-representation-of-a-Markov-Decision-Process" class="headerlink" title="Understand the graphical representation of a Markov Decision Process"></a>Understand the graphical representation of a Markov Decision Process</h3><p>MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent.1 The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time<br>through its choice of actions.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">The agent–environment interaction in a Markov decision process.</div></center><h3 id="Describe-how-the-dynamics-of-an-MDP-are-defined"><a href="#Describe-how-the-dynamics-of-an-MDP-are-defined" class="headerlink" title="Describe how the dynamics of an MDP are defined"></a>Describe how the dynamics of an MDP are defined</h3><p>In a finite MDP, the sets of states, actions, and rewards (S, A and R) all  have a finite number of elements. In this case, the random variables $R_t$ and $S_t$ have well defined<br>discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s\prime \in S$ and $r \in R$, there is a probability of those values occurring at time t, given particular values of the preceding state and action:</p><script type="math/tex; mode=display">p(s\prime, r | s, a) = Pr\{S_t = s\prime, R_t = r | S_{t-1} = s, A_{t-1} = a \}</script><script type="math/tex; mode=display">\sum_{s\prime \in s}\sum_{r \in R} p(s\prime, r | s, a) = 1, \text{for all} \, s\in S, a \in A_{(s)}</script><p>for all $s\prime, s \in S, r \in R \text{and} a \in A_{(s)}$.  The function p defines the dynamics of the MDP. The dot over the equals sign in the equation reminds us that it is a definition (in this case of the function p) rather than a fact that follows from previous definitions. The dynamics function $p: S x R x S x A \rightarrow [0, 1]$ is an ordinary deterministic function of four arguments.</p><p>In a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for $S_t$ and $A_t$, and, given them, not at all on earlier states and actions. This is best viewed a restriction not on the decision process, but on the state.  The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property.</p><p>From the four-argument dynamics function, p, one can compute anything else one might want to know about the environment, such as the <strong>state-transition probabilities</strong> (which we denote, with a slight abuse of notation, as a three-argument function<br>$p: S x S x A \rightarrow [0, 1]$)</p><script type="math/tex; mode=display">p(s\prime | s, a) = Pr\{S_t = s\prime | S_{t-1} = s, A_{t-1} = a \} = \sum_{r \in R}p(s\prime, r | s, a)</script><p>We can also compute the expected rewards for state–action pairs as a two-argument function $r : S x A \rightarrow \mathbb{R}$</p><script type="math/tex; mode=display">r(s,a) = \mathbb{R} [R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R} r \sum_{s\prime \in S} p(s\prime, r | s, a)</script><p>and the expected rewards for <strong>state–action–next-state</strong> triples as a three-argument function $r: S x A x S \rightarrow \mathbb{R}$</p><script type="math/tex; mode=display">r(s,a,s\prime) = \mathbb{E} [R_t | S_{t-1} = s, A_{t-1} = a, S_t = s\prime] = \sum_{r \in R} r \frac{ p(s\prime, r | s, a) }{p(s\prime | s, a)}</script><h3 id="Explain-how-many-diverse-processes-can-be-written-in-terms-of-the-MDP-framework"><a href="#Explain-how-many-diverse-processes-can-be-written-in-terms-of-the-MDP-framework" class="headerlink" title="Explain how many diverse processes can be written in terms of the MDP framework"></a>Explain how many diverse processes can be written in terms of the MDP framework</h3><p>The MDP framework is abstract and flexible and can be applied to many di↵erent problems in many di↵erent ways.  </p><ul><li>In general, actions can be any decisions we want to learn how to make, and<br>the states can be anything we can know that might be useful in making them. </li><li>The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.  </li><li>The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.</li></ul><p>The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the <strong>actions</strong>), one signal to represent the basis on which the choices are made (the <strong>states</strong>), and one signal to define the agent’s goal (the <strong>rewards</strong>).</p><h2 id="Lesson-2-Goal-of-Reinforcement-Learning"><a href="#Lesson-2-Goal-of-Reinforcement-Learning" class="headerlink" title="Lesson 2: Goal of Reinforcement Learning"></a>Lesson 2: Goal of Reinforcement Learning</h2><h3 id="Describe-how-rewards-relate-to-the-goal-of-an-agent"><a href="#Describe-how-rewards-relate-to-the-goal-of-an-agent" class="headerlink" title="Describe how rewards relate to the goal of an agent"></a>Describe how rewards relate to the goal of an agent</h3><p>Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis:</p><blockquote><p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p></blockquote><h3 id="Understand-episodes-and-identify-episodic-tasks"><a href="#Understand-episodes-and-identify-episodic-tasks" class="headerlink" title="Understand episodes and identify episodic tasks"></a>Understand episodes and identify episodic tasks</h3><p>when the agent–environment interaction breaks naturally into <strong>subsequences</strong>, which we call episodes,7 such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the <strong>terminal state</strong>, followed by a <strong>reset</strong> to a standard starting state or to a sample from a standard distribution of starting states.</p><p>In general, we seek to maximize the expected return, where the return, denoted $G_t$,  is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:</p><script type="math/tex; mode=display">G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_{T}</script><p>where T is a final time step.</p><h2 id="Lesson-3-Continuing-Tasks"><a href="#Lesson-3-Continuing-Tasks" class="headerlink" title="Lesson 3: Continuing Tasks"></a>Lesson 3: Continuing Tasks</h2><h3 id="Formulate-returns-for-continuing-tasks-using-discounting"><a href="#Formulate-returns-for-continuing-tasks-using-discounting" class="headerlink" title="Formulate returns for continuing tasks using discounting"></a>Formulate returns for continuing tasks using discounting</h3><p>In many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit.  For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation is problematic for continuing tasks because the final time step would be $T = \infty$ and the return, which is what we are trying to maximize, could itself easily be infinite.</p><p>The additional concept that we need is that of <strong>discounting</strong>. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_t$ to maximize the expected discounted return:</p><script type="math/tex; mode=display">G_t = R_{t+1} + \eta R_{t+2} + \eta^2 R_{t+3} + \cdots  = \sum_{k=0}^{\infty} \eta^k R_{t+k+1}</script><p>where \eta is a parameter, $ 0 \leq \eta \leq 1$, called the discount rate.</p><p>The discount rate determines the present value of future rewards: a reward received k time steps in the future is worth only $\eta^{k-1}$ times what it would be worth if it were received immediately. If $\eta &lt; 1$, the infinite sum has a finite value as long as the reward sequence $\{ R_k \}$ is <strong>bounded</strong>.</p><p>Returns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning:</p><script type="math/tex; mode=display">G_t =  R_{t+1} + \eta (R_{t+2} + \eta R_{t+3} + \eta^2 R_{t+4} + \cdots) = R_{t+1} + \eta G_{t+1}</script><h3 id="Describe-how-returns-at-successive-time-steps-are-related-to-each-other"><a href="#Describe-how-returns-at-successive-time-steps-are-related-to-each-other" class="headerlink" title="Describe how returns at successive time steps are related to each other"></a>Describe how returns at successive time steps are related to each other</h3><p>If $\eta = 0$, the agent is “myopic” in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose At so as to maximize only $R_{t+1}$. If each of the agent’s actions happened to influence only the immediate reward, not future rewards as well, then a myopic agent could maximize by separately maximizing each immediate reward. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced. As $\eta$ approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lesson-1-Introduction-to-Markov-Decision-Processes&quot;&gt;&lt;a href=&quot;#Lesson-1-Introduction-to-Markov-Decision-Processes&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>The K-Armed Bandit Problem</title>
    <link href="https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/"/>
    <id>https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/</id>
    <published>2020-09-03T10:55:55.000Z</published>
    <updated>2020-09-03T11:10:47.100Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lesson-1-The-K-Armed-Bandit-Problem"><a href="#Lesson-1-The-K-Armed-Bandit-Problem" class="headerlink" title="Lesson 1: The K-Armed Bandit Problem"></a>Lesson 1: The K-Armed Bandit Problem</h2><h3 id="Define-reward"><a href="#Define-reward" class="headerlink" title="Define reward"></a>Define reward</h3><p>In the k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the <strong>value</strong> of that action. We denote the action selected on time step t as $A_t$, and the corresponding reward as $R_t$. The value then<br>of an arbitrary action a, denoted $q\ast(a)$, is the expected reward given that a is selected:</p><script type="math/tex; mode=display">q\ast(a) = \mathbb{E} [R_t | A_t = a ]</script><p>We denote the estimated value of action a at time step t as $Q_t(a)$. We would like $Q_t(a)$ to be close<br>to $q\ast(a)$.</p><h3 id="Understand-the-temporal-nature-of-the-bandit-problem"><a href="#Understand-the-temporal-nature-of-the-bandit-problem" class="headerlink" title="Understand the temporal nature of the bandit problem"></a>Understand the temporal nature of the bandit problem</h3><h3 id="Define-k-armed-bandit"><a href="#Define-k-armed-bandit" class="headerlink" title="Define k-armed bandit"></a>Define k-armed bandit</h3><p>Consider the following learning problem. You are faced repeatedly with a choice among k di↵erent options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p><h3 id="Define-action-values"><a href="#Define-action-values" class="headerlink" title="Define action-values"></a>Define action-values</h3><p>We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods.</p><h2 id="Lesson-2-What-to-Learn-Estimating-Action-Values"><a href="#Lesson-2-What-to-Learn-Estimating-Action-Values" class="headerlink" title="Lesson 2: What to Learn? Estimating Action Values"></a>Lesson 2: What to Learn? Estimating Action Values</h2><h3 id="Define-action-value-estimation-methods"><a href="#Define-action-value-estimation-methods" class="headerlink" title="Define action-value estimation methods"></a>Define action-value estimation methods</h3><p>One natural way to estimate this is by averaging the rewards<br>actually received:</p><script type="math/tex; mode=display">Q_t(a) = \frac{\text{sum of rewards when a taken prior to t} }{\text{ number of times a taken prior to t} }</script><h3 id="Define-exploration-and-exploitation"><a href="#Define-exploration-and-exploitation" class="headerlink" title="Define exploration and exploitation"></a>Define exploration and exploitation</h3><p>If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are <strong>exploiting</strong> your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are <strong>exploring</strong>, because this enables you to improve your estimate of the nongreedy action’s value.</p><p>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</p><h3 id="Select-actions-greedily-using-an-action-value-function"><a href="#Select-actions-greedily-using-an-action-value-function" class="headerlink" title="Select actions greedily using an action-value function"></a>Select actions greedily using an action-value function</h3><script type="math/tex; mode=display">A_t = argmax_a Q_t(a)</script><h3 id="Define-online-learning"><a href="#Define-online-learning" class="headerlink" title="Define online learning"></a>Define online learning</h3><script type="math/tex; mode=display">Q_n = \frac{ R_1 + R_2 + \cdots + R_{n-1} }{ n - 1 }</script><p>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if this is done, then the memory and computational requirements would grow over time as more rewards are seen. Each additional reward would require additional memory to store it and additional computation to compute the sum in the numerator.</p><p>As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward.</p><script type="math/tex; mode=display">Q_(n+1) = Q_n + \frac{1}{n}[ R_n - Q_n ]</script><h3 id="Define-the-general-online-update-equation"><a href="#Define-the-general-online-update-equation" class="headerlink" title="Define the general online update equation"></a>Define the general online update equation</h3><script type="math/tex; mode=display">\text{NewEstimate} = \text{OldEstimate} + \text{StepSize} [ \text{target} - \text{OldEstimate} ]</script><h3 id="Understand-why-we-might-use-a-constant-stepsize-in-the-case-of-non-stationarity"><a href="#Understand-why-we-might-use-a-constant-stepsize-in-the-case-of-non-stationarity" class="headerlink" title="Understand why we might use a constant stepsize in the case of non-stationarity"></a>Understand why we might use a constant stepsize in the case of non-stationarity</h3><p>The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter.</p><script type="math/tex; mode=display">Q_(n+1) = Q_n + \alpha [ R_n - Q_n ]</script><p>where the step-size parameter $ \alpha \in (0, 1]$ is constant.</p><script type="math/tex; mode=display">Q_{n+1} = (1 - \alpha)^n Q_1 + \sum^{n}_{i=1}\alpha(1 - \alpha)^{n-i}R_i</script><p>Note that the weight, $\alpha(1 - \alpha)^{n-i}$ given the reward $R_i$ depends on how many rewards ago, $n-1$ it was observed. The quantity $1-\alpha$<br>is less than 1, and thus the weight given to $R_i$ decreases as the number of intervening rewards increases.</p><h2 id="Lesson-3-Exploration-vs-Exploitation-Tradeoff"><a href="#Lesson-3-Exploration-vs-Exploitation-Tradeoff" class="headerlink" title="Lesson 3: Exploration vs. Exploitation Tradeoff"></a>Lesson 3: Exploration vs. Exploitation Tradeoff</h2><h3 id="Define-epsilon-greedy"><a href="#Define-epsilon-greedy" class="headerlink" title="Define epsilon-greedy"></a>Define epsilon-greedy</h3><p>Greedy action selection always exploits current<br>knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability $\alpha$ instead select randomly from among all the actions with equal probability, independently of the action-value estimates.  We call methods using this near-greedy action selection rule $\alpha$-greedy methods.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,len(self.q_values))</span><br></pre></td></tr></table></figure><h3 id="Understand-optimistic-initial-values-Describe-the-benefits-of-optimistic-initial-values-for-early-exploration"><a href="#Understand-optimistic-initial-values-Describe-the-benefits-of-optimistic-initial-values-for-early-exploration" class="headerlink" title="Understand optimistic initial values, Describe the benefits of optimistic initial values for early exploration"></a>Understand optimistic initial values, Describe the benefits of optimistic initial values for early exploration</h3><p>Initial action values can also be used as a simple way to encourage exploration. Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed,we set them all to +5. Recall that the $q\ast(a)$ in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism encourages action-value methods to explore. Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being <strong>disappointed</strong> with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time.</p><p><img src="initial_values.png" alt></p><h3 id="Explain-the-criticisms-of-optimistic-initial-values"><a href="#Explain-the-criticisms-of-optimistic-initial-values" class="headerlink" title="Explain the criticisms of optimistic initial values"></a>Explain the criticisms of optimistic initial values</h3><p>We call this technique for encouraging exploration optimistic initial values. We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial conditions in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much. This criticism applies as well to the sample-average methods, which also treat the beginning of time as a special event, averaging all subsequent rewards with equal weights.</p><h3 id="Describe-the-upper-confidence-bound-action-selection-method"><a href="#Describe-the-upper-confidence-bound-action-selection-method" class="headerlink" title="Describe the upper confidence bound action selection method"></a>Describe the upper confidence bound action selection method</h3><p>It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.One effective way of doing this is to select actions according to</p><script type="math/tex; mode=display">A_t = argmax_a [ Q_t(a) + c\sqrt{\frac{\ln t}{ N_t(a)}} ]</script><h3 id="Define-optimism-in-the-face-of-uncertainty"><a href="#Define-optimism-in-the-face-of-uncertainty" class="headerlink" title="Define optimism in the face of uncertainty"></a>Define optimism in the face of uncertainty</h3><p>The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value  The quantity being max’ed over is thus a sort of upper bound on the possible true value of action $a$, with $c$ determining the confidence level. Each time $a$ is selected the uncertainty is presumably reduced: $N_t(a)$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, $t$ increases but $N_t(a)$ does not; because $t$ appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates,or that have already been selected frequently, will be selected with decreasing frequency over time.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lesson-1-The-K-Armed-Bandit-Problem&quot;&gt;&lt;a href=&quot;#Lesson-1-The-K-Armed-Bandit-Problem&quot; class=&quot;headerlink&quot; title=&quot;Lesson 1: The K-Armed 
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Bandits and Exploration/Exploitation</title>
    <link href="https://zhangruochi.com/Bandits-and-Exploration-Exploitation/2020/09/03/"/>
    <id>https://zhangruochi.com/Bandits-and-Exploration-Exploitation/2020/09/03/</id>
    <published>2020-09-03T09:59:35.000Z</published>
    <updated>2020-09-03T10:00:48.172Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-1-Bandits-and-Exploration-Exploitation"><a href="#Assignment-1-Bandits-and-Exploration-Exploitation" class="headerlink" title="Assignment 1: Bandits and Exploration/Exploitation"></a>Assignment 1: Bandits and Exploration/Exploitation</h1><p>Welcome to Assignment 1. This notebook will:</p><ul><li>Help you create your first bandit algorithm</li><li>Help you understand the effect of epsilon on exploration and learn about the exploration/exploitation tradeoff</li><li>Introduce you to some of the reinforcement learning software we are going to use for this specialization</li></ul><p>This class uses RL-Glue to implement most of our experiments. It was originally designed by Adam White, Brian Tanner, and Rich Sutton. This library will give you a solid framework to understand how reinforcement learning experiments work and how to run your own. If it feels a little confusing at first, don’t worry - we are going to walk you through it slowly and introduce you to more and more parts as you progress through the specialization.</p><p>We are assuming that you have used a Jupyter notebook before. But if not, it is quite simple. Simply press the run button, or shift+enter to run each of the cells. The places in the code that you need to fill in will be clearly marked for you.</p><h2 id="Section-0-Preliminaries"><a href="#Section-0-Preliminaries" class="headerlink" title="Section 0: Preliminaries"></a>Section 0: Preliminaries</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rlglue.rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">import</span> main_agent</span><br><span class="line"><span class="keyword">import</span> ten_arm_env</span><br><span class="line"><span class="keyword">import</span> test_env</span><br></pre></td></tr></table></figure><p>In the above cell, we import the libraries we need for this assignment. We use numpy throughout the course and occasionally provide hints for which methods to use in numpy. Other than that we mostly use vanilla python and the occasional other library, such as matplotlib for making plots.</p><p>You might have noticed that we import ten_arm_env. This is the <strong>10-armed Testbed</strong> introduced in <a href="http://www.incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">section 2.3</a> of the textbook. We use this throughout this notebook to test our bandit agents. It has 10 arms, which are the actions the agent can take. Pulling an arm generates a stochastic reward from a Gaussian distribution with unit-variance. For each action, the expected value of that action is randomly sampled from a normal distribution, at the start of each run. If you are unfamiliar with the 10-armed Testbed please review it in the textbook before continuing.</p><p><strong>DO NOT IMPORT OTHER LIBRARIES as this will break the autograder.</strong></p><p><strong>DO NOT SET A RANDOM SEED as this will break the autograder.</strong></p><h2 id="Section-1-Greedy-Agent"><a href="#Section-1-Greedy-Agent" class="headerlink" title="Section 1: Greedy Agent"></a>Section 1: Greedy Agent</h2><p>We want to create an agent that will find the action with the highest expected reward. One way an agent could operate is to always choose the action with  the highest value based on the agent’s current estimates. This is called a greedy agent as it greedily chooses the action that it thinks has the highest value. Let’s look at what happens in this case.</p><p>First we are going to implement the argmax function, which takes in a list of action values and returns an action with the highest value. Why are we implementing our own instead of using the argmax function that numpy uses? Numpy’s argmax function returns the first instance of the highest value. We do not want that to happen as it biases the agent to choose a specific action in the case of ties. Instead we want to break ties between the highest values randomly. So we are going to implement our own argmax function. You may want to look at <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html" target="_blank" rel="noopener">np.random.choice</a> to randomly select from a list of values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(q_values)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Takes in a list of q_values and returns the index of the item </span></span><br><span class="line"><span class="string">    with the highest value. Breaks ties randomly.</span></span><br><span class="line"><span class="string">    returns: int - the index of the highest value in q_values</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    top_value = float(<span class="string">"-inf"</span>)</span><br><span class="line">    ties = []</span><br><span class="line">    </span><br><span class="line">    top_value = max(q_values)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(q_values)):</span><br><span class="line">        <span class="comment"># if a value in q_values is greater than the highest value update top and reset ties to zero</span></span><br><span class="line">        <span class="comment"># if a value is equal to top value add the index to ties</span></span><br><span class="line">        <span class="comment"># return a random selection from ties.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top_value:</span><br><span class="line">            ties.append(i)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.random.choice(ties)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">test_array = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">8</span>, <span class="string">"Check your argmax implementation returns the index of the largest value"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure np.random.choice is called correctly</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">test_array = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">test_array = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">8</span>, <span class="string">"Check your argmax implementation returns the index of the largest value"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seed so results are deterministic</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">test_array = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">counts = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    a = argmax(test_array)</span><br><span class="line">    counts[a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure argmax does not always choose first entry</span></span><br><span class="line"><span class="keyword">assert</span> counts[<span class="number">0</span>] != <span class="number">100</span>, <span class="string">"Make sure your argmax implementation randomly choooses among the largest values."</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure argmax does not always choose last entry</span></span><br><span class="line"><span class="keyword">assert</span> counts[<span class="number">3</span>] != <span class="number">100</span>, <span class="string">"Make sure your argmax implementation randomly choooses among the largest values."</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the random number generator is called exactly once whenver `argmax` is called</span></span><br><span class="line">expected = [<span class="number">44</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">56</span>] <span class="comment"># &lt;-- notice not perfectly uniform due to randomness</span></span><br><span class="line"><span class="keyword">assert</span> counts == expected</span><br></pre></td></tr></table></figure><p>Now we introduce the first part of an RL-Glue agent that you will implement. Here we are going to create a GreedyAgent and implement the agent_step method. This method gets called each time the agent takes a step. The method has to return the action selected by the agent. This method also ensures the agent’s estimates are updated based on the signals it gets from the environment.</p><p>Fill in the code below to implement a greedy agent.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GreedyAgent</span><span class="params">(main_agent.Agent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : The action that the agent took on the previous time step</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update Q values Hint: Look at the algorithm in section 2.4 of the textbook.</span></span><br><span class="line">        <span class="comment"># increment the counter in self.arm_count for the action from the previous time step</span></span><br><span class="line">        <span class="comment"># update the step size using self.arm_count</span></span><br><span class="line">        <span class="comment"># update self.q_values for the action from the previous time step</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] + (<span class="number">1.0</span> / self.arm_count[self.last_action]) * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># current action = ? # Use the argmax function you created above</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        current_action = argmax(self.q_values)</span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">greedy_agent = GreedyAgent()</span><br><span class="line">greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.last_action = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the q_values were updated correctly</span></span><br><span class="line"><span class="keyword">assert</span> greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the agent is using the argmax that breaks ties randomly</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">greedy_agent = GreedyAgent()</span><br><span class="line">greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.last_action = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># take a fake agent step</span></span><br><span class="line">action = greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure agent took greedy action</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure q_values were updated correctly</span></span><br><span class="line"><span class="keyword">assert</span> greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>Let’s visualize the result. Here we run an experiment using RL-Glue to test our agent. For now, we will set up the experiment code; in future lessons, we will walk you through running experiments so that you can create your own.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">num_runs = <span class="number">200</span>                    <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">num_steps = <span class="number">1000</span>                  <span class="comment"># The number of pulls of each arm the agent takes</span></span><br><span class="line">env = ten_arm_env.Environment     <span class="comment"># We set what environment we want to use to test</span></span><br><span class="line">agent = GreedyAgent               <span class="comment"># We choose what agent we want to use</span></span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>&#125;  <span class="comment"># We pass the agent the information it needs. Here how many arms there are.</span></span><br><span class="line">env_info = &#123;&#125;                     <span class="comment"># We pass the environment the information it needs. In this case nothing.</span></span><br><span class="line"></span><br><span class="line">all_averages = []</span><br><span class="line"></span><br><span class="line">average_best = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):           <span class="comment"># tqdm is what creates the progress bar below</span></span><br><span class="line">    np.random.seed(run)</span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(env, agent)          <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">    rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line">    rl_glue.rl_start()                    <span class="comment"># We start the experiment</span></span><br><span class="line"></span><br><span class="line">    average_best += np.max(rl_glue.environment.arms)</span><br><span class="line">    </span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    averages = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        reward, _, action, _ = rl_glue.rl_step() <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                                                 <span class="comment"># the reward, and action taken.</span></span><br><span class="line">        scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">        averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    all_averages.append(averages)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot([average_best / num_runs <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend([<span class="string">"Best Possible"</span>, <span class="string">"Greedy"</span>])</span><br><span class="line">plt.title(<span class="string">"Average Reward of Greedy Agent"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">greedy_scores = np.mean(all_averages, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 63.68it/s]</code></pre><p><img src="output_15_1.png" alt="png"></p><p>How did our agent do? Is it possible for it to do better?</p><h2 id="Section-2-Epsilon-Greedy-Agent"><a href="#Section-2-Epsilon-Greedy-Agent" class="headerlink" title="Section 2: Epsilon-Greedy Agent"></a>Section 2: Epsilon-Greedy Agent</h2><p>We learned about <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/tHDck/what-is-the-trade-off" target="_blank" rel="noopener">another way for an agent to operate</a>, where it does not always take the greedy action. Instead, sometimes it takes an exploratory action. It does this so that it can find out what the best action really is. If we always choose what we think is the current best action is, we may miss out on taking the true best action, because we haven’t explored enough times to find that best action.</p><p>Implement an epsilon-greedy agent below. Hint: we are implementing the algorithm from <a href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=52" target="_blank" rel="noopener">section 2.4</a> of the textbook. You may want to use your greedy code from above and look at <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random.html" target="_blank" rel="noopener">np.random.random</a>, as well as <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html" target="_blank" rel="noopener">np.random.randint</a>, to help you select random actions. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedyAgent</span><span class="params">(main_agent.Agent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : The action that the agent took on the previous time step</span></span><br><span class="line">        <span class="comment"># self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update Q values - this should be the same update as your greedy agent above</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] + (<span class="number">1.0</span> / self.arm_count[self.last_action]) * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy</span></span><br><span class="line">        <span class="comment"># Randomly choose a number between 0 and 1 and see if it's less than self.epsilon</span></span><br><span class="line">        <span class="comment"># (hint: look at np.random.random()). If it is, set current_action to a random action.</span></span><br><span class="line">        <span class="comment"># otherwise choose current_action greedily as you did above.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line"></span><br><span class="line">        _ = np.random.random()</span><br><span class="line">        </span><br><span class="line">        current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,len(self.q_values))</span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0.0</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># given this random seed, we should see a greedy action (action 2) here</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"><span class="comment"># we'll try to guess a few of the trickier places</span></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure to update for the *last_action* not the current action</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values != [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">"A"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the stepsize is based on the *last_action* not the current action</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values != [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">"B"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the agent is using the argmax that breaks ties randomly</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span>, <span class="string">"C"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># let's see what happens for another action</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># given this random seed, we should see a random action (action 4) here</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The agent saw a reward of 1, so should increase the value for *last_action*</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">"D"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the agent should have picked a random action for this particular random seed</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">4</span>, <span class="string">"E"</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># manipulate the random seed so the agent takes a random action</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">0</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check to make sure we update value for action 4</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.0</span>, <span class="number">0</span>, <span class="number">1.0</span>]</span><br></pre></td></tr></table></figure><p>Now that we have our epsilon greedy agent created. Let’s compare it against the greedy agent with epsilon of 0.1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot Epsilon greedy results and greedy results</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">agent = EpsilonGreedyAgent</span><br><span class="line">env = ten_arm_env.Environment</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: epsilon&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">all_averages = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">    np.random.seed(run)</span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">    rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    averages = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        reward, _, action, _ = rl_glue.rl_step() <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                                                 <span class="comment"># the reward, and action taken.</span></span><br><span class="line">        scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">        averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    all_averages.append(averages)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.plot(greedy_scores)</span><br><span class="line">plt.title(<span class="string">"Average Reward of Greedy Agent vs. E-Greedy Agent"</span>)</span><br><span class="line">plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend((<span class="string">"Best Possible"</span>, <span class="string">"Greedy"</span>, <span class="string">"Epsilon: 0.1"</span>))</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 62.89it/s]</code></pre><p><img src="output_23_1.png" alt="png"></p><p>Notice how much better the epsilon-greedy agent did. Because we occasionally choose a random action we were able to find a better long term policy. By acting greedily before our value estimates are accurate, we risk settling on a suboptimal action.</p><h2 id="Section-2-1-Averaging-Multiple-Runs"><a href="#Section-2-1-Averaging-Multiple-Runs" class="headerlink" title="Section 2.1 Averaging Multiple Runs"></a>Section 2.1 Averaging Multiple Runs</h2><p>Did you notice that we averaged over 2000 runs? Why did we do that?</p><p>To get some insight, let’s look at the results of two individual runs by the same agent.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot runs of e-greedy agent</span></span><br><span class="line">agent = EpsilonGreedyAgent</span><br><span class="line">env = ten_arm_env.Environment</span><br><span class="line">agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: <span class="number">0.1</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">all_averages = []</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> (<span class="number">0</span>, <span class="number">1</span>):</span><br><span class="line">    np.random.seed(run) <span class="comment"># Here we set the seed so that we can compare two different runs</span></span><br><span class="line">    averages = []</span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">    rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">        scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">        averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.plot(averages)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"Comparing two independent runs"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_27_0.png" alt="png"></p><p>Notice how the two runs were different? But, if this is the exact same algorithm, why does it behave differently in these two runs?</p><p>The answer is that it is due to randomness in the environment and in the agent. Depending on what action the agent randomly starts with, or when it randomly chooses to explore, it can change the results of the runs. And even if the agent chooses the same action, the reward from the environment is randomly sampled from a Gaussian. The agent could get lucky, and see larger rewards for the best action early on and so settle on the best action faster. Or, it could get unlucky and see smaller rewards for best action early on and so take longer to recognize that it is in fact the best action.</p><p>To be more concrete, let’s look at how many times an exploratory action is taken, for different seeds. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">print(<span class="string">"Random Seed 1"</span>)</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    <span class="keyword">if</span> np.random.random() &lt; <span class="number">0.1</span>:</span><br><span class="line">        print(<span class="string">"Exploratory Action"</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">print()</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Random Seed 2"</span>)</span><br><span class="line">np.random.seed(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    <span class="keyword">if</span> np.random.random() &lt; <span class="number">0.1</span>:</span><br><span class="line">        print(<span class="string">"Exploratory Action"</span>)</span><br></pre></td></tr></table></figure><pre><code>Random Seed 1Exploratory ActionExploratory ActionExploratory ActionRandom Seed 2Exploratory Action</code></pre><p>With the first seed, we take an exploratory action three times out of 15, but with the second, we only take an exploratory action once. This can significantly affect the performance of our agent because the amount of exploration has changed significantly.</p><p>To compare algorithms, we therefore report performance averaged across many runs. We do this to ensure that we are not simply reporting a result that is due to stochasticity, as explained <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/PtVBs/sequential-decision-making-with-evaluative-feedback" target="_blank" rel="noopener">in the lectures</a>. Rather, we want statistically significant outcomes. We will not use statistical significance tests in this course. Instead, because we have access to simulators for our experiments, we use the simpler strategy of running for a large number of runs and ensuring that the confidence intervals do not overlap. </p><h2 id="Section-3-Comparing-values-of-epsilon"><a href="#Section-3-Comparing-values-of-epsilon" class="headerlink" title="Section 3: Comparing values of epsilon"></a>Section 3: Comparing values of epsilon</h2><p>Can we do better than an epsilon of 0.1? Let’s try several different values for epsilon and see how they perform. We try different settings of key performance parameters to understand how the agent might perform under different conditions.</p><p>Below we run an experiment where we sweep over different values for epsilon:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment code for different e-greedy</span></span><br><span class="line">epsilons = [<span class="number">0.0</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.4</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line"></span><br><span class="line">n_q_values = []</span><br><span class="line">n_averages = []</span><br><span class="line">n_best_actions = []</span><br><span class="line"></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epsilon <span class="keyword">in</span> epsilons:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        agent = EpsilonGreedyAgent</span><br><span class="line">        agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: epsilon&#125;</span><br><span class="line">        env_info = &#123;<span class="string">"random_seed"</span>: run&#125;</span><br><span class="line"></span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line">        </span><br><span class="line">        best_arm = np.argmax(rl_glue.environment.arms)</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        best_action_chosen = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">            averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> action == best_arm:</span><br><span class="line">                best_action_chosen.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                best_action_chosen.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> epsilon == <span class="number">0.1</span> <span class="keyword">and</span> run == <span class="number">0</span>:</span><br><span class="line">                n_q_values.append(np.copy(rl_glue.agent.q_values))</span><br><span class="line">        <span class="keyword">if</span> epsilon == <span class="number">0.1</span>:</span><br><span class="line">            n_averages.append(averages)</span><br><span class="line">            n_best_actions.append(best_action_chosen)</span><br><span class="line">        all_averages.append(averages)</span><br><span class="line">        </span><br><span class="line">    plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.legend([<span class="string">"Best Possible"</span>] + epsilons)</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 58.80it/s]100%|██████████| 200/200 [00:03&lt;00:00, 59.62it/s]100%|██████████| 200/200 [00:03&lt;00:00, 61.00it/s]100%|██████████| 200/200 [00:02&lt;00:00, 70.13it/s]</code></pre><p><img src="output_33_1.png" alt="png"></p><p>Why did 0.1 perform better than 0.01?</p><p>If exploration helps why did 0.4 perform worse that 0.0 (the greedy agent)?</p><p>Think about these and how you would answer these questions. They are questions in the practice quiz. If you still have questions about it, retake the practice quiz.</p><h2 id="Section-4-The-Effect-of-Step-Size"><a href="#Section-4-The-Effect-of-Step-Size" class="headerlink" title="Section 4: The Effect of Step Size"></a>Section 4: The Effect of Step Size</h2><p>In Section 1 of this assignment, we decayed the step size over time based on action-selection counts. The step-size was 1/N(A), where N(A) is the number of times action A was selected. This is the same as computing a sample average. We could also set the step size to be a constant value, such as 0.1. What would be the effect of doing that? And is it better to use a constant or the sample average method? </p><p>To investigate this question, let’s start by creating a new agent that has a constant step size. This will be nearly identical to the agent created above. You will use the same code to select the epsilon-greedy action. You will change the update to have a constant step size instead of using the 1/N(A) update.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedyAgentConstantStepsize</span><span class="params">(main_agent.Agent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, observation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : An int of the action that the agent took on the previous time step.</span></span><br><span class="line">        <span class="comment"># self.step_size : A float which is the current step size for the agent.</span></span><br><span class="line">        <span class="comment"># self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update q_values for action taken at previous time step </span></span><br><span class="line">        <span class="comment"># using self.step_size intead of using self.arm_count</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] +  self.step_size * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy. This is the same as you implemented above.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        _ = np.random.random()</span><br><span class="line">        current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,len(self.q_values))</span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>]:</span><br><span class="line">    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()</span><br><span class="line">    e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">    e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">    e_greedy_agent.epsilon = <span class="number">0.0</span></span><br><span class="line">    e_greedy_agent.step_size = step_size</span><br><span class="line">    action = e_greedy_agent.agent_step(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, step_size, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">"Check that you are updating q_values correctly using the stepsize."</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Check Epsilon Greedy with Different Constant Stepsizes</span></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>]:</span><br><span class="line">    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()</span><br><span class="line">    e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">    e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">    e_greedy_agent.epsilon = <span class="number">0.0</span></span><br><span class="line">    e_greedy_agent.step_size = step_size</span><br><span class="line">    </span><br><span class="line">    action = e_greedy_agent.agent_step(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, step_size, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment code for different step sizes</span></span><br><span class="line">step_sizes = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="string">'1/N(A)'</span>]</span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line"></span><br><span class="line">q_values = &#123;step_size: [] <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line">true_values = &#123;step_size: <span class="keyword">None</span> <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line">best_actions = &#123;step_size: [] <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        np.random.seed(run)</span><br><span class="line">        agent = EpsilonGreedyAgentConstantStepsize <span class="keyword">if</span> step_size != <span class="string">'1/N(A)'</span> <span class="keyword">else</span> EpsilonGreedyAgent</span><br><span class="line">        agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: epsilon, <span class="string">"step_size"</span>: step_size, <span class="string">"initial_value"</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">        env_info = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line">        </span><br><span class="line">        best_arm = np.argmax(rl_glue.environment.arms)</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> run == <span class="number">0</span>:</span><br><span class="line">            true_values[step_size] = np.copy(rl_glue.environment.arms)</span><br><span class="line">            </span><br><span class="line">        best_action_chosen = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">            averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> action == best_arm:</span><br><span class="line">                best_action_chosen.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                best_action_chosen.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> run == <span class="number">0</span>:</span><br><span class="line">                q_values[step_size].append(np.copy(rl_glue.agent.q_values))</span><br><span class="line">        best_actions[step_size].append(best_action_chosen)</span><br><span class="line">    ax.plot(np.mean(best_actions[step_size], axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.legend(step_sizes)</span><br><span class="line">plt.title(<span class="string">"% Best Arm Pulled"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"% Best Arm Pulled"</span>)</span><br><span class="line">vals = ax.get_yticks()</span><br><span class="line">ax.set_yticklabels([<span class="string">'&#123;:,.2%&#125;'</span>.format(x) <span class="keyword">for</span> x <span class="keyword">in</span> vals])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 63.14it/s]100%|██████████| 200/200 [00:03&lt;00:00, 62.84it/s]100%|██████████| 200/200 [00:03&lt;00:00, 62.57it/s]100%|██████████| 200/200 [00:03&lt;00:00, 62.70it/s]100%|██████████| 200/200 [00:03&lt;00:00, 61.82it/s]</code></pre><p><img src="output_40_1.png" alt="png"></p><p>Notice first that we are now plotting the amount of time that the best action is taken rather than the average reward. To better  understand the performance of an agent, it can be useful to measure specific behaviors, beyond just how much reward is accumulated. This measure indicates how close the agent’s behaviour is to optimal.</p><p>It seems as though 1/N(A) performed better than the others, in that it reaches a solution where it takes the best action most frequently. Now why might this be? Why did a step size of 0.5 start out better but end up performing worse? Why did a step size of 0.01 perform so poorly?</p><p>Let’s dig into this further below. Let’s plot how well each agent tracks the true value, where each agent has a different step size method. You do not have to enter any code here, just follow along.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">largest = <span class="number">0</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes:</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">    largest = np.argmax(true_values[step_size])</span><br><span class="line">    plt.plot([true_values[step_size][largest] <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line">    plt.title(<span class="string">"Step Size: &#123;&#125;"</span>.format(step_size))</span><br><span class="line">    plt.plot(np.array(q_values[step_size])[:, largest])</span><br><span class="line">    plt.legend([<span class="string">"True Expected Value"</span>, <span class="string">"Estimated Value"</span>])</span><br><span class="line">    plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Value"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_42_0.png" alt="png"></p><p><img src="output_42_1.png" alt="png"></p><p><img src="output_42_2.png" alt="png"></p><p><img src="output_42_3.png" alt="png"></p><p><img src="output_42_4.png" alt="png"></p><p>These plots help clarify the performance differences between the different step sizes. A step size of 0.01 makes such small updates that the agent’s value estimate of the best action does not get close to the actual value. Step sizes of 0.5 and 1.0 both get close to the true value quickly, but are very susceptible to stochasticity in the rewards. The updates overcorrect too much towards recent rewards, and so oscillate around the true value. This means that on many steps, the action that pulls the best arm may seem worse than it actually is.  A step size of 0.1 updates fairly quickly to the true value, and does not oscillate as widely around the true values as 0.5 and 1.0. This is one of the reasons that 0.1 performs quite well. Finally we see why 1/N(A) performed well. Early on while the step size is still reasonably high it moves quickly to the true expected value, but as it gets pulled more its step size is reduced which makes it less susceptible to the stochasticity of the rewards.</p><p>Does this mean that 1/N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment.</p><p>Let’s look at how a sudden change in the reward distributions affects a step size like 1/N(A). This time we will run the environment for 2000 steps, and after 1000 steps we will randomly change the expected value of all of the arms. We compare two agents, both using epsilon-greedy with epsilon = 0.1. One uses a constant step size of 0.1, the other a step size of 1/N(A) that reduces over time. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">num_steps = <span class="number">2000</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">'w'</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_steps)], linestyle=<span class="string">"--"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> agent <span class="keyword">in</span> [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(num_runs)):</span><br><span class="line">        agent_info = &#123;<span class="string">"num_actions"</span>: <span class="number">10</span>, <span class="string">"epsilon"</span>: epsilon, <span class="string">"step_size"</span>: step_size&#125;</span><br><span class="line">        np.random.seed(run)</span><br><span class="line">        </span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[<span class="number">-1</span>] + reward)</span><br><span class="line">            averages.append(scores[<span class="number">-1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">1000</span>:</span><br><span class="line">                rl_glue.environment.arms = np.random.randn(<span class="number">10</span>)</span><br><span class="line">        all_averages.append(averages)</span><br><span class="line">        </span><br><span class="line">    plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend([<span class="string">"Best Possible"</span>, <span class="string">"1/N(A)"</span>, <span class="string">"0.1"</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Steps"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Average reward"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 200/200 [00:06&lt;00:00, 29.43it/s]100%|██████████| 200/200 [00:06&lt;00:00, 30.10it/s]</code></pre><p><img src="output_44_1.png" alt="png"></p><p>Now the agent with a step size of 1/N(A) performed better at the start but then performed worse when the environment changed! What happened?</p><p>Think about what the step size would be after 1000 steps. Let’s say the best action gets chosen 500 times. That means the step size for that action is 1/500 or 0.002. At each step when we update the value of the action and the value is going to move only 0.002 * the error. That is a very tiny adjustment and it will take a long time for it to get to the true value.</p><p>The agent with step size 0.1, however, will always update in 1/10th of the direction of the error. This means that on average it will take ten steps for it to update its value to the sample mean.</p><p>These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarity—-and the related concept of partial observability—-is a common feature of reinforcement learning problems and when learning online.  </p><h2 id="Section-5-Conclusion"><a href="#Section-5-Conclusion" class="headerlink" title="Section 5: Conclusion"></a>Section 5: Conclusion</h2><p>Great work! You have:</p><ul><li>Implemented your first agent</li><li>Learned about the effect of epsilon, an exploration parameter, on the performance of an agent</li><li>Learned about the effect of step size on the performance of the agent</li><li>Learned about a good experiment practice of averaging across multiple runs</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-1-Bandits-and-Exploration-Exploitation&quot;&gt;&lt;a href=&quot;#Assignment-1-Bandits-and-Exploration-Exploitation&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Reinforcement Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Reinforcement-Learning/"/>
    
    
  </entry>
  
</feed>
