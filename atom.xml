<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-07-18T17:42:08.653Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Parts-of-Speech Tagging</title>
    <link href="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/"/>
    <id>https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/</id>
    <published>2020-07-18T17:39:21.000Z</published>
    <updated>2020-07-18T17:42:08.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Parts-of-Speech-Tagging-POS"><a href="#Assignment-2-Parts-of-Speech-Tagging-POS" class="headerlink" title="Assignment 2: Parts-of-Speech Tagging (POS)"></a>Assignment 2: Parts-of-Speech Tagging (POS)</h1><p>Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective…) to each word in an input text.  Tagging is difficult because some words can represent more than one part of speech at different times. They are  <strong>Ambiguous</strong>. Let’s look at the following example: </p><ul><li>The whole team played <strong>well</strong>. [adverb]</li><li>You are doing <strong>well</strong> for yourself. [adjective]</li><li><strong>Well</strong>, this assignment took me forever to complete. [interjection]</li><li>The <strong>well</strong> is dry. [noun]</li><li>Tears were beginning to <strong>well</strong> in her eyes. [verb]</li></ul><p>Distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will: </p><ul><li>Learn how parts-of-speech tagging works</li><li>Compute the transition matrix A in a Hidden Markov Model</li><li>Compute the transition matrix B in a Hidden Markov Model</li><li>Compute the Viterbi algorithm </li><li>Compute the accuracy of your own model </li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">0 Data Sources</a></li><li><a href="#1">1 POS Tagging</a><ul><li><a href="#1.1">1.1 Training</a><ul><li><a href="#ex-01">Exercise 01</a></li></ul></li><li><a href="#1.2">1.2 Testing</a><ul><li><a href="#ex-02">Exercise 02</a></li></ul></li></ul></li><li><a href="#2">2 Hidden Markov Models</a><ul><li><a href="#2.1">2.1 Generating Matrices</a><ul><li><a href="#ex-03">Exercise 03</a></li><li><a href="#ex-04">Exercise 04</a></li></ul></li></ul></li><li><a href="#3">3 Viterbi Algorithm</a><ul><li><a href="#3.1">3.1 Initialization</a><ul><li><a href="#ex-05">Exercise 05</a></li></ul></li><li><a href="#3.2">3.2 Viterbi Forward</a><ul><li><a href="#ex-06">Exercise 06</a></li></ul></li><li><a href="#3.3">3.3 Viterbi Backward</a><ul><li><a href="#ex-07">Exercise 07</a></li></ul></li></ul></li><li><a href="#4">4 Predicting on a data set</a><ul><li><a href="#ex-08">Exercise 08</a></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Importing packages and loading in the data set </span></span><br><span class="line"><span class="keyword">from</span> utils_pos <span class="keyword">import</span> get_word_tag, preprocess  </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><p><a name="0"></a></p><h2 id="Part-0-Data-Sources"><a href="#Part-0-Data-Sources" class="headerlink" title="Part 0: Data Sources"></a>Part 0: Data Sources</h2><p>This assignment will use two tagged data sets collected from the <strong>Wall Street Journal (WSJ)</strong>. </p><p><a href="http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html" target="_blank" rel="noopener">Here</a> is an example ‘tag-set’ or Part of Speech designation describing the two or three letter tag and their meaning. </p><ul><li>One data set (<strong>WSJ-2_21.pos</strong>) will be used for <strong>training</strong>.</li><li>The other (<strong>WSJ-24.pos</strong>) for <strong>testing</strong>. </li><li>The tagged training data has been preprocessed to form a vocabulary (<strong>hmm_vocab.txt</strong>). </li><li>The words in the vocabulary are words from the training set that were used two or more times. </li><li>The vocabulary is augmented with a set of ‘unknown word tokens’, described below. </li></ul><p>The training set will be used to create the emission, transmission and tag counts. </p><p>The test set (WSJ-24.pos) is read in to create <code>y</code>. </p><ul><li>This contains both the test text and the true tag. </li><li>The test set has also been preprocessed to remove the tags to form <strong>test_words.txt</strong>. </li><li>This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in <strong>utils_pos.py</strong>. </li><li>This forms the list <code>prep</code>, the preprocessed text used to test our  POS taggers.</li></ul><p>A POS tagger will necessarily encounter words that are not in its datasets. </p><ul><li>To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. </li><li>For example, the suffix ‘ize’ is a hint that the word is a verb, as in ‘final-ize’ or ‘character-ize’. </li><li>A set of unknown-tokens, such as ‘—unk-verb—‘ or ‘—unk-noun—‘ will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.</li></ul><p><img src="DataSources1.PNG"></p><p>Implementation note: </p><ul><li>For python 3.6 and beyond, dictionaries retain the insertion order. </li><li>Furthermore, their hash-based lookup makes them suitable for rapid membership tests. <ul><li>If _di_ is a dictionary, <code>key in di</code> will return <code>True</code> if _di_ has a key _key_, else <code>False</code>. </li></ul></li></ul><p>The dictionary <code>vocab</code> will utilize these features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the training corpus</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"WSJ_02-21.pos"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    training_corpus = f.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"A few items of the training corpus list"</span>)</span><br><span class="line">print(training_corpus[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>A few items of the training corpus list[&#39;In\tIN\n&#39;, &#39;an\tDT\n&#39;, &#39;Oct.\tNNP\n&#39;, &#39;19\tCD\n&#39;, &#39;review\tNN\n&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read the vocabulary data, split by each line of text, and save the list</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"hmm_vocab.txt"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    voc_l = f.read().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"A few items of the vocabulary list"</span>)</span><br><span class="line">print(voc_l[<span class="number">0</span>:<span class="number">50</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"A few items at the end of the vocabulary list"</span>)</span><br><span class="line">print(voc_l[<span class="number">-50</span>:])</span><br></pre></td></tr></table></figure><pre><code>A few items of the vocabulary list[&#39;!&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;, &#39;&amp;&#39;, &quot;&#39;&quot;, &quot;&#39;&#39;&quot;, &quot;&#39;40s&quot;, &quot;&#39;60s&quot;, &quot;&#39;70s&quot;, &quot;&#39;80s&quot;, &quot;&#39;86&quot;, &quot;&#39;90s&quot;, &quot;&#39;N&quot;, &quot;&#39;S&quot;, &quot;&#39;d&quot;, &quot;&#39;em&quot;, &quot;&#39;ll&quot;, &quot;&#39;m&quot;, &quot;&#39;n&#39;&quot;, &quot;&#39;re&quot;, &quot;&#39;s&quot;, &quot;&#39;til&quot;, &quot;&#39;ve&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;-&#39;, &#39;--&#39;, &#39;--n--&#39;, &#39;--unk--&#39;, &#39;--unk_adj--&#39;, &#39;--unk_adv--&#39;, &#39;--unk_digit--&#39;, &#39;--unk_noun--&#39;, &#39;--unk_punct--&#39;, &#39;--unk_upper--&#39;, &#39;--unk_verb--&#39;, &#39;.&#39;, &#39;...&#39;, &#39;0.01&#39;, &#39;0.0108&#39;, &#39;0.02&#39;, &#39;0.03&#39;, &#39;0.05&#39;, &#39;0.1&#39;, &#39;0.10&#39;, &#39;0.12&#39;, &#39;0.13&#39;, &#39;0.15&#39;]A few items at the end of the vocabulary list[&#39;yards&#39;, &#39;yardstick&#39;, &#39;year&#39;, &#39;year-ago&#39;, &#39;year-before&#39;, &#39;year-earlier&#39;, &#39;year-end&#39;, &#39;year-on-year&#39;, &#39;year-round&#39;, &#39;year-to-date&#39;, &#39;year-to-year&#39;, &#39;yearlong&#39;, &#39;yearly&#39;, &#39;years&#39;, &#39;yeast&#39;, &#39;yelled&#39;, &#39;yelling&#39;, &#39;yellow&#39;, &#39;yen&#39;, &#39;yes&#39;, &#39;yesterday&#39;, &#39;yet&#39;, &#39;yield&#39;, &#39;yielded&#39;, &#39;yielding&#39;, &#39;yields&#39;, &#39;you&#39;, &#39;young&#39;, &#39;younger&#39;, &#39;youngest&#39;, &#39;youngsters&#39;, &#39;your&#39;, &#39;yourself&#39;, &#39;youth&#39;, &#39;youthful&#39;, &#39;yuppie&#39;, &#39;yuppies&#39;, &#39;zero&#39;, &#39;zero-coupon&#39;, &#39;zeroing&#39;, &#39;zeros&#39;, &#39;zinc&#39;, &#39;zip&#39;, &#39;zombie&#39;, &#39;zone&#39;, &#39;zones&#39;, &#39;zoning&#39;, &#39;{&#39;, &#39;}&#39;, &#39;&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocab: dictionary that has the index of the corresponding words</span></span><br><span class="line">vocab = &#123;&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the index of the corresponding words. </span></span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sorted(voc_l)): </span><br><span class="line">    vocab[word] = i       </span><br><span class="line">    </span><br><span class="line">print(<span class="string">"Vocabulary dictionary, key is the word, value is a unique integer"</span>)</span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> vocab.items():</span><br><span class="line">    print(<span class="string">f"<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;v&#125;</span>"</span>)</span><br><span class="line">    cnt += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> cnt &gt; <span class="number">20</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>Vocabulary dictionary, key is the word, value is a unique integer:0!:1#:2$:3%:4&amp;:5&#39;:6&#39;&#39;:7&#39;40s:8&#39;60s:9&#39;70s:10&#39;80s:11&#39;86:12&#39;90s:13&#39;N:14&#39;S:15&#39;d:16&#39;em:17&#39;ll:18&#39;m:19&#39;n&#39;:20</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the test corpus</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"WSJ_24.pos"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    y = f.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"A sample of the test corpus"</span>)</span><br><span class="line">print(y[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>A sample of the test corpus[&#39;The\tDT\n&#39;, &#39;economy\tNN\n&#39;, &quot;&#39;s\tPOS\n&quot;, &#39;temperature\tNN\n&#39;, &#39;will\tMD\n&#39;, &#39;be\tVB\n&#39;, &#39;taken\tVBN\n&#39;, &#39;from\tIN\n&#39;, &#39;several\tJJ\n&#39;, &#39;vantage\tNN\n&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#corpus without tags, preprocessed</span></span><br><span class="line">_, prep = preprocess(vocab, <span class="string">"test.words"</span>)     </span><br><span class="line"></span><br><span class="line">print(<span class="string">'The length of the preprocessed test corpus: '</span>, len(prep))</span><br><span class="line">print(<span class="string">'This is a sample of the test_corpus: '</span>)</span><br><span class="line">print(prep[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>The length of the preprocessed test corpus:  34199This is a sample of the test_corpus: [&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;, &#39;from&#39;, &#39;several&#39;, &#39;--unk--&#39;]</code></pre><p><a name="1"></a></p><h1 id="Part-1-Parts-of-speech-tagging"><a href="#Part-1-Parts-of-speech-tagging" class="headerlink" title="Part 1: Parts-of-speech tagging"></a>Part 1: Parts-of-speech tagging</h1><p><a name="1.1"></a></p><h2 id="Part-1-1-Training"><a href="#Part-1-1-Training" class="headerlink" title="Part 1.1 - Training"></a>Part 1.1 - Training</h2><p>You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. </p><p>In this section, you will find the words that are not ambiguous. </p><ul><li>For example, the word <code>is</code> is a verb and it is not ambiguous. </li><li>In the <code>WSJ</code> corpus, $86$% of the token are unambiguous (meaning they have only one tag) </li><li>About $14\%$ are ambiguous (meaning that they have more than one tag)</li></ul><p><img src="pos.png" style="width:400px;height:250px;"></p><p>Before you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables. </p><h4 id="Transition-counts"><a href="#Transition-counts" class="headerlink" title="Transition counts"></a>Transition counts</h4><ul><li>The first dictionary is the <code>transition_counts</code> dictionary which computes the number of times each tag happened next to another tag. </li></ul><p>This dictionary will be used to compute: </p><script type="math/tex; mode=display">P(t_i |t_{i-1}) \tag{1}</script><p>This is the probability of a tag at position $i$ given the tag at position $i-1$.</p><p>In order for you to compute equation 1, you will create a <code>transition_counts</code> dictionary where </p><ul><li>The keys are <code>(prev_tag, tag)</code></li><li>The values are the number of times those two tags appeared in that order. </li></ul><h4 id="Emission-counts"><a href="#Emission-counts" class="headerlink" title="Emission counts"></a>Emission counts</h4><p>The second dictionary you will compute is the <code>emission_counts</code> dictionary. This dictionary will be used to compute:</p><script type="math/tex; mode=display">P(w_i|t_i)\tag{2}</script><p>In other words, you will use it to compute the probability of a word given its tag. </p><p>In order for you to compute equation 2, you will create an <code>emission_counts</code> dictionary where </p><ul><li>The keys are <code>(tag, word)</code> </li><li>The values are the number of times that pair showed up in your training set. </li></ul><h4 id="Tag-counts"><a href="#Tag-counts" class="headerlink" title="Tag counts"></a>Tag counts</h4><p>The last dictionary you will compute is the <code>tag_counts</code> dictionary. </p><ul><li>The key is the tag </li><li>The value is the number of times each tag appeared.</li></ul><p><a name="ex-01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Write a program that takes in the <code>training_corpus</code> and returns the three dictionaries mentioned above <code>transition_counts</code>, <code>emission_counts</code>, and <code>tag_counts</code>. </p><ul><li><code>emission_counts</code>: maps (tag, word) to the number of times it happened. </li><li><code>transition_counts</code>: maps (prev_tag, tag) to the number of times it has appeared. </li><li><code>tag_counts</code>: maps (tag) to the number of times it has occured. </li></ul><p>Implementation note: This routine utilises <em>defaultdict</em>, which is a subclass of <em>dict</em>. </p><ul><li>A standard Python dictionary throws a <em>KeyError</em> if you try to access an item with a key that is not currently in the dictionary. </li><li>In contrast, the <em>defaultdict</em> will create an item of the type of the argument, in this case an integer with the default value of 0. </li><li>See <a href="https://docs.python.org/3.3/library/collections.html#defaultdict-objects" target="_blank" rel="noopener">defaultdict</a>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_dictionaries</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dictionaries</span><span class="params">(training_corpus, vocab)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        training_corpus: a corpus where each line has a word followed by its tag.</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary where the keys are the tags and the values are the counts</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the dictionaries using defaultdict</span></span><br><span class="line">    emission_counts = defaultdict(int)</span><br><span class="line">    transition_counts = defaultdict(int)</span><br><span class="line">    tag_counts = defaultdict(int)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "prev_tag" (previous tag) with the start state, denoted by '--s--'</span></span><br><span class="line">    prev_tag = <span class="string">'--s--'</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use 'i' to track the line number in the corpus</span></span><br><span class="line">    i = <span class="number">0</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Each item in the training corpus contains a word and its POS tag</span></span><br><span class="line">    <span class="comment"># Go through each word and its tag in the training corpus</span></span><br><span class="line">    <span class="keyword">for</span> word_tag <span class="keyword">in</span> training_corpus:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the word_tag count</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Every 50,000 words, print the word count</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"word count = <span class="subst">&#123;i&#125;</span>"</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># get the word and tag using the get_word_tag helper function (imported from utils_pos.py)</span></span><br><span class="line">        word, tag = get_word_tag(word_tag,vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the transition count for the previous word and tag</span></span><br><span class="line">        transition_counts[(prev_tag, tag)] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the emission count for the tag and word</span></span><br><span class="line">        emission_counts[(tag, word)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Increment the tag count</span></span><br><span class="line">        tag_counts[tag] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set the previous tag to this tag (for the next iteration of the loop)</span></span><br><span class="line">        prev_tag = tag</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> emission_counts, transition_counts, tag_counts</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)</span><br></pre></td></tr></table></figure><pre><code>word count = 50000word count = 100000word count = 150000word count = 200000word count = 250000word count = 300000word count = 350000word count = 400000word count = 450000word count = 500000word count = 550000word count = 600000word count = 650000word count = 700000word count = 750000word count = 800000word count = 850000word count = 900000word count = 950000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get all the POS states</span></span><br><span class="line">states = sorted(tag_counts.keys())</span><br><span class="line">print(<span class="string">f"Number of POS tags (number of 'states'): <span class="subst">&#123;len(states)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">"View these POS tags (states)"</span>)</span><br><span class="line">print(states)</span><br></pre></td></tr></table></figure><pre><code>Number of POS tags (number of &#39;states&#39;): 46View these POS tags (states)[&#39;#&#39;, &#39;$&#39;, &quot;&#39;&#39;&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;--s--&#39;, &#39;.&#39;, &#39;:&#39;, &#39;CC&#39;, &#39;CD&#39;, &#39;DT&#39;, &#39;EX&#39;, &#39;FW&#39;, &#39;IN&#39;, &#39;JJ&#39;, &#39;JJR&#39;, &#39;JJS&#39;, &#39;LS&#39;, &#39;MD&#39;, &#39;NN&#39;, &#39;NNP&#39;, &#39;NNPS&#39;, &#39;NNS&#39;, &#39;PDT&#39;, &#39;POS&#39;, &#39;PRP&#39;, &#39;PRP$&#39;, &#39;RB&#39;, &#39;RBR&#39;, &#39;RBS&#39;, &#39;RP&#39;, &#39;SYM&#39;, &#39;TO&#39;, &#39;UH&#39;, &#39;VB&#39;, &#39;VBD&#39;, &#39;VBG&#39;, &#39;VBN&#39;, &#39;VBP&#39;, &#39;VBZ&#39;, &#39;WDT&#39;, &#39;WP&#39;, &#39;WP$&#39;, &#39;WRB&#39;, &#39;``&#39;]</code></pre><h5 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Number of POS tags (number of 'states'46</span><br><span class="line">View these states</span><br><span class="line">['#', '$', "''", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']</span><br></pre></td></tr></table></figure><p>The ‘states’ are the Parts-of-speech designations found in the training data. They will also be referred to as ‘tags’ or POS in this assignment. </p><ul><li>“NN” is noun, singular, </li><li>‘NNS’ is noun, plural. </li><li>In addition, there are helpful tags like ‘—s—‘ which indicate a start of a sentence.</li><li>You can get a more complete description at <a href="https://www.clips.uantwerpen.be/pages/mbsp-tags" target="_blank" rel="noopener">Penn Treebank II tag set</a>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"transition examples: "</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> list(transition_counts.items())[:<span class="number">3</span>]:</span><br><span class="line">    print(ex)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"emission examples: "</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> list(emission_counts.items())[<span class="number">200</span>:<span class="number">203</span>]:</span><br><span class="line">    <span class="keyword">print</span> (ex)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"ambiguous word example: "</span>)</span><br><span class="line"><span class="keyword">for</span> tup,cnt <span class="keyword">in</span> emission_counts.items():</span><br><span class="line">    <span class="keyword">if</span> tup[<span class="number">1</span>] == <span class="string">'back'</span>: <span class="keyword">print</span> (tup, cnt)</span><br></pre></td></tr></table></figure><pre><code>transition examples: ((&#39;--s--&#39;, &#39;IN&#39;), 5050)((&#39;IN&#39;, &#39;DT&#39;), 32364)((&#39;DT&#39;, &#39;NNP&#39;), 9044)emission examples: ((&#39;DT&#39;, &#39;any&#39;), 721)((&#39;NN&#39;, &#39;decrease&#39;), 7)((&#39;NN&#39;, &#39;insider-trading&#39;), 5)ambiguous word example: (&#39;RB&#39;, &#39;back&#39;) 304(&#39;VB&#39;, &#39;back&#39;) 20(&#39;RP&#39;, &#39;back&#39;) 84(&#39;JJ&#39;, &#39;back&#39;) 25(&#39;NN&#39;, &#39;back&#39;) 29(&#39;VBP&#39;, &#39;back&#39;) 4</code></pre><h5 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">transition examples: </span><br><span class="line">(('--s--', 'IN'), 5050)</span><br><span class="line">(('IN', 'DT'), 32364)</span><br><span class="line">(('DT', 'NNP'), 9044)</span><br><span class="line"></span><br><span class="line">emission examples: </span><br><span class="line">(('DT', 'any'), 721)</span><br><span class="line">(('NN', 'decrease'), 7)</span><br><span class="line">(('NN', 'insider-trading'), 5)</span><br><span class="line"></span><br><span class="line">ambiguous word example: </span><br><span class="line">('RB', 'back') 304</span><br><span class="line">('VB', 'back') 20</span><br><span class="line">('RP', 'back') 84</span><br><span class="line">('JJ', 'back') 25</span><br><span class="line">('NN', 'back') 29</span><br><span class="line">('VBP', 'back') 4</span><br></pre></td></tr></table></figure><p><a name="1.2"></a></p><h3 id="Part-1-2-Testing"><a href="#Part-1-2-Testing" class="headerlink" title="Part 1.2 - Testing"></a>Part 1.2 - Testing</h3><p>Now you will test the accuracy of your parts-of-speech tagger using your <code>emission_counts</code> dictionary. </p><ul><li>Given your preprocessed test corpus <code>prep</code>, you will assign a parts-of-speech tag to every word in that corpus. </li><li>Using the original tagged test corpus <code>y</code>, you will then compute what percent of the tags you got correct. </li></ul><p><a name="ex-02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement <code>predict_pos</code> that computes the accuracy of your model. </p><ul><li>This is a warm up exercise. </li><li>To assign a part of speech to a word, assign the most frequent POS for that word in the training set. </li><li>Then evaluate how well this approach works.  Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same.  If so, the prediction was correct!</li><li>Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: predict_pos</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_pos</span><span class="params">(prep, y, emission_counts, vocab, states)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.</span></span><br><span class="line"><span class="string">        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">        states: a sorted list of all possible tags for this assignment</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: Number of times you classified a word correctly</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the number of correct predictions to zero</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the (tag, word) tuples, stored as a set</span></span><br><span class="line">    all_words = set(emission_counts.keys())  <span class="comment"># (tag, word)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of (word, POS) tuples in the corpus 'y'</span></span><br><span class="line">    total = len(y)</span><br><span class="line">    <span class="keyword">for</span> word, y_tup <span class="keyword">in</span> zip(prep, y): </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the (word, POS) string into a list of two items</span></span><br><span class="line">        y_tup_l = y_tup.split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Verify that y_tup contain both word and POS</span></span><br><span class="line">        <span class="keyword">if</span> len(y_tup_l) == <span class="number">2</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the true POS label for this word</span></span><br><span class="line">            true_label = y_tup_l[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># If the y_tup didn't contain word and POS, go to next word</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">        count_final = <span class="number">0</span></span><br><span class="line">        pos_final = <span class="string">''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the word is in the vocabulary...</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> states:</span><br><span class="line"></span><br><span class="line">            <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># define the key as the tuple containing the POS and word</span></span><br><span class="line">                key = (pos, word)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if the (pos, word) key exists in the emission_counts dictionary</span></span><br><span class="line">                <span class="keyword">if</span> key <span class="keyword">in</span> emission_counts: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># get the emission count of the (pos,word) tuple </span></span><br><span class="line">                    count = emission_counts[key]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># keep track of the POS with the largest count</span></span><br><span class="line">                    <span class="keyword">if</span> count &gt; count_final: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final count (largest count)</span></span><br><span class="line">                        count_final = count</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final POS</span></span><br><span class="line">                        pos_final = pos</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the final POS (with the largest count) matches the true POS:</span></span><br><span class="line">            <span class="keyword">if</span> pos_final == true_label: <span class="comment"># complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Update the number of correct predictions</span></span><br><span class="line">                num_correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    accuracy = num_correct / total</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)</span><br><span class="line">print(<span class="string">f"Accuracy of prediction using predict_pos is <span class="subst">&#123;accuracy_predict_pos:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Accuracy of prediction using predict_pos is 0.8889</code></pre><h5 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of prediction <span class="keyword">using</span> predict_pos is <span class="number">0.8889</span></span><br></pre></td></tr></table></figure><p>88.9% is really good for this warm up exercise. With hidden markov models, you should be able to get <strong>95% accuracy.</strong></p><p><a name="2"></a></p><h1 id="Part-2-Hidden-Markov-Models-for-POS"><a href="#Part-2-Hidden-Markov-Models-for-POS" class="headerlink" title="Part 2: Hidden Markov Models for POS"></a>Part 2: Hidden Markov Models for POS</h1><p>Now you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder</p><ul><li>The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization. </li><li>In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. </li><li>By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in Part 1.</li></ul><p>The Markov Model contains a number of states and the probability of transition between those states. </p><ul><li>In this case, the states are the parts-of-speech. </li><li>A Markov Model utilizes a transition matrix, <code>A</code>. </li><li>A Hidden Markov Model adds an observation or emission matrix <code>B</code> which describes the probability of a visible observation when we are in a particular state. </li><li>In this case, the emissions are the words in the corpus</li><li>The state, which is hidden, is the POS tag of that word.</li></ul><p><a name="2.1"></a></p><h2 id="Part-2-1-Generating-Matrices"><a href="#Part-2-1-Generating-Matrices" class="headerlink" title="Part 2.1 Generating Matrices"></a>Part 2.1 Generating Matrices</h2><h3 id="Creating-the-‘A’-transition-probabilities-matrix"><a href="#Creating-the-‘A’-transition-probabilities-matrix" class="headerlink" title="Creating the ‘A’ transition probabilities matrix"></a>Creating the ‘A’ transition probabilities matrix</h3><p>Now that you have your <code>emission_counts</code>, <code>transition_counts</code>, and <code>tag_counts</code>, you will start implementing the Hidden Markov Model. </p><p>This will allow you to quickly construct the </p><ul><li><code>A</code> transition probabilities matrix.</li><li>and the <code>B</code> emission probabilities matrix. </li></ul><p>You will also use some smoothing when computing these matrices. </p><p>Here is an example of what the <code>A</code> transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):</p><div class="table-container"><table><thead><tr><th><strong>A</strong></th><th>…</th><th>RBS</th><th>RP</th><th>SYM</th><th>TO</th><th>UH</th><th>…</th></tr></thead><tbody><tr><td><strong>RBS</strong></td><td>…</td><td>2.217069e-06</td><td>2.217069e-06</td><td>2.217069e-06</td><td>0.008870</td><td>2.217069e-06</td><td>…</td></tr><tr><td><strong>RP</strong></td><td>…</td><td>3.756509e-07</td><td>7.516775e-04</td><td>3.756509e-07</td><td>0.051089</td><td>3.756509e-07</td><td>…</td></tr><tr><td><strong>SYM</strong></td><td>…</td><td>1.722772e-05</td><td>1.722772e-05</td><td>1.722772e-05</td><td>0.000017</td><td>1.722772e-05</td><td>…</td></tr><tr><td><strong>TO</strong></td><td>…</td><td>4.477336e-05</td><td>4.472863e-08</td><td>4.472863e-08</td><td>0.000090</td><td>4.477336e-05</td><td>…</td></tr><tr><td><strong>UH</strong></td><td>…</td><td>1.030439e-05</td><td>1.030439e-05</td><td>1.030439e-05</td><td>0.061837</td><td>3.092348e-02</td><td>…</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table></div><p>Note that the matrix above was computed with smoothing. </p><p>Each cell gives you the probability to go from one part of speech to another. </p><ul><li>In other words, there is a 4.47e-8 chance of going from parts-of-speech <code>TO</code> to <code>RP</code>. </li><li>The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.</li></ul><p>The smoothing was done as follows: </p><script type="math/tex; mode=display">P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_{i}) + \alpha }{C(t_{i-1}) +\alpha * N}\tag{3}</script><ul><li>$N$ is the total number of tags</li><li>$C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in <code>transition_counts</code> dictionary.</li><li>$C(t_{i-1})$ is the count of the previous POS in the <code>tag_counts</code> dictionary.</li><li>$\alpha$ is a smoothing parameter.</li></ul><p><a name="ex-03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the <code>create_transition_matrix</code> below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix <code>A</code>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_transition_matrix</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_transition_matrix</span><span class="params">(alpha, tag_counts, transition_counts)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: number used for smoothing</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        transition_counts: transition count for the previous word and tag</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        A: matrix of dimension (num_tags,num_tags)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get a sorted list of unique POS tags</span></span><br><span class="line">    all_tags = sorted(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Count the number of unique POS tags</span></span><br><span class="line">    num_tags = len(all_tags)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the transition matrix 'A'</span></span><br><span class="line">    A = np.zeros((num_tags,num_tags))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the unique transition tuples (previous POS, current POS)</span></span><br><span class="line">    trans_keys = set(transition_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Return instances of 'None' with your code) ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row of the transition matrix A</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column of the transition matrix A</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_tags):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the count of the (prev POS, current POS) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># Define the tuple (prev POS, current POS)</span></span><br><span class="line">            <span class="comment"># Get the tag at position i and tag at position j (from the all_tags list)</span></span><br><span class="line">            key = (all_tags[i],all_tags[j])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check if the (prev POS, current POS) tuple </span></span><br><span class="line">            <span class="comment"># exists in the transition counts dictionaory</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> trans_keys: <span class="comment">#complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get count from the transition_counts dictionary </span></span><br><span class="line">                <span class="comment"># for the (prev POS, current POS) tuple</span></span><br><span class="line">                count = transition_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the previous tag (index position i) from tag_counts</span></span><br><span class="line">            count_prev_tag = tag_counts[all_tags[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Apply smoothing using count of the tuple, alpha, </span></span><br><span class="line">            <span class="comment"># count of previous tag, alpha, and number of total tags</span></span><br><span class="line">            A[i,j] = (count + alpha ) / ( count_prev_tag + alpha * num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line">A = create_transition_matrix(alpha, tag_counts, transition_counts)</span><br><span class="line"><span class="comment"># Testing your function</span></span><br><span class="line">print(<span class="string">f"A at row 0, col 0: <span class="subst">&#123;A[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"A at row 3, col 1: <span class="subst">&#123;A[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"View a subset of transition matrix A"</span>)</span><br><span class="line">A_sub = pd.DataFrame(A[<span class="number">30</span>:<span class="number">35</span>,<span class="number">30</span>:<span class="number">35</span>], index=states[<span class="number">30</span>:<span class="number">35</span>], columns = states[<span class="number">30</span>:<span class="number">35</span>] )</span><br><span class="line">print(A_sub)</span><br></pre></td></tr></table></figure><pre><code>A at row 0, col 0: 0.000007040A at row 3, col 1: 0.1691View a subset of transition matrix A              RBS            RP           SYM        TO            UHRBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02</code></pre><h5 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A at row <span class="number">0</span>, col <span class="number">0</span>: <span class="number">0.000007040</span></span><br><span class="line">A at row <span class="number">3</span>, col <span class="number">1</span>: <span class="number">0.1691</span></span><br><span class="line">View a subset of transition matrix A</span><br><span class="line">              RBS            RP           SYM        TO            UH</span><br><span class="line">RBS  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">0.008870</span>  <span class="number">2.217069e-06</span></span><br><span class="line">RP   <span class="number">3.756509e-07</span>  <span class="number">7.516775e-04</span>  <span class="number">3.756509e-07</span>  <span class="number">0.051089</span>  <span class="number">3.756509e-07</span></span><br><span class="line">SYM  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">0.000017</span>  <span class="number">1.722772e-05</span></span><br><span class="line">TO   <span class="number">4.477336e-05</span>  <span class="number">4.472863e-08</span>  <span class="number">4.472863e-08</span>  <span class="number">0.000090</span>  <span class="number">4.477336e-05</span></span><br><span class="line">UH   <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">0.061837</span>  <span class="number">3.092348e-02</span></span><br></pre></td></tr></table></figure><h3 id="Create-the-‘B’-emission-probabilities-matrix"><a href="#Create-the-‘B’-emission-probabilities-matrix" class="headerlink" title="Create the ‘B’ emission probabilities matrix"></a>Create the ‘B’ emission probabilities matrix</h3><p>Now you will create the <code>B</code> transition matrix which computes the emission probability. </p><p>You will use smoothing as defined below: </p><script type="math/tex; mode=display">P(w_i | t_i) = \frac{C(t_i, word_i)+ \alpha}{C(t_{i}) +\alpha * N}\tag{4}</script><ul><li>$C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in <code>emission_counts</code> dictionary).</li><li>$C(t_i)$ is the number of times $tag_i$ was in the training data (stored in <code>tag_counts</code> dictionary).</li><li>$N$ is the number of words in the vocabulary</li><li>$\alpha$ is a smoothing parameter. </li></ul><p>The matrix <code>B</code> is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. </p><p>Here is an example of the matrix, only a subset of tags and words are shown: </p><p style="text-align: center;"> <b>B Emissions Probability Matrix (subset)</b>  </p><div class="table-container"><table><thead><tr><th><strong>B</strong></th><th>…</th><th>725</th><th>adroitly</th><th>engineers</th><th>promoted</th><th>synergy</th><th>…</th></tr></thead><tbody><tr><td><strong>CD</strong></td><td>…</td><td><strong>8.201296e-05</strong></td><td>2.732854e-08</td><td>2.732854e-08</td><td>2.732854e-08</td><td>2.732854e-08</td><td>…</td></tr><tr><td><strong>NN</strong></td><td>…</td><td>7.521128e-09</td><td>7.521128e-09</td><td>7.521128e-09</td><td>7.521128e-09</td><td><strong>2.257091e-05</strong></td><td>…</td></tr><tr><td><strong>NNS</strong></td><td>…</td><td>1.670013e-08</td><td>1.670013e-08</td><td><strong>4.676203e-04</strong></td><td>1.670013e-08</td><td>1.670013e-08</td><td>…</td></tr><tr><td><strong>VB</strong></td><td>…</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>…</td></tr><tr><td><strong>RB</strong></td><td>…</td><td>3.226454e-08</td><td><strong>6.456135e-05</strong></td><td>3.226454e-08</td><td>3.226454e-08</td><td>3.226454e-08</td><td>…</td></tr><tr><td><strong>RP</strong></td><td>…</td><td>3.723317e-07</td><td>3.723317e-07</td><td>3.723317e-07</td><td><strong>3.723317e-07</strong></td><td>3.723317e-07</td><td>…</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table></div><p><a name="ex-04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement the <code>create_emission_matrix</code> below that computes the <code>B</code> emission probabilities matrix. Your function takes in $\alpha$, the smoothing parameter, <code>tag_counts</code>, which is a dictionary mapping each tag to its respective count, the <code>emission_counts</code> dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix <code>B</code>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_emission_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_emission_matrix</span><span class="params">(alpha, tag_counts, emission_counts, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: tuning parameter used in smoothing </span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        B: a matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get the number of POS tag</span></span><br><span class="line">    num_tags = len(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a list of all POS tags</span></span><br><span class="line">    all_tags = sorted(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the total number of unique words in the vocabulary</span></span><br><span class="line">    num_words = len(vocab)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the emission matrix B with places for</span></span><br><span class="line">    <span class="comment"># tags in the rows and words in the columns</span></span><br><span class="line">    B = np.zeros((num_tags, num_words))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a set of all (POS, word) tuples </span></span><br><span class="line">    <span class="comment"># from the keys of the emission_counts dictionary</span></span><br><span class="line">    emis_keys = set(list(emission_counts.keys()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row (POS tags)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column (words)</span></span><br><span class="line">        <span class="keyword">for</span> j,word <span class="keyword">in</span> enumerate(vocab): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the emission count for the (POS tag, word) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># Define the (POS tag, word) tuple for this row and column</span></span><br><span class="line">            key =  (all_tags[i], word)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the (POS tag, word) tuple exists as a key in emission counts</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> emis_keys: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">                <span class="comment"># Get the count of (POS tag, word) from the emission_counts d</span></span><br><span class="line">                count = emission_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the POS tag</span></span><br><span class="line">            count_tag = tag_counts[all_tags[i]]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Apply smoothing and store the smoothed value </span></span><br><span class="line">            <span class="comment"># into the emission matrix B for this row and column</span></span><br><span class="line">            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> B</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># creating your emission probability matrix. this takes a few minutes to run. </span></span><br><span class="line">B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"View Matrix position at row 0, column 0: <span class="subst">&#123;B[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"View Matrix position at row 3, column 1: <span class="subst">&#123;B[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Try viewing emissions for a few words in a sample dataframe</span></span><br><span class="line">cidx  = [<span class="string">'725'</span>,<span class="string">'adroitly'</span>,<span class="string">'engineers'</span>, <span class="string">'promoted'</span>, <span class="string">'synergy'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the integer ID for each word</span></span><br><span class="line">cols = [vocab[a] <span class="keyword">for</span> a <span class="keyword">in</span> cidx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose POS tags to show in a sample dataframe</span></span><br><span class="line">rvals =[<span class="string">'CD'</span>,<span class="string">'NN'</span>,<span class="string">'NNS'</span>, <span class="string">'VB'</span>,<span class="string">'RB'</span>,<span class="string">'RP'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># For each POS tag, get the row number from the 'states' list</span></span><br><span class="line">rows = [states.index(a) <span class="keyword">for</span> a <span class="keyword">in</span> rvals]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the emissions for the sample of words, and the sample of POS tags</span></span><br><span class="line">B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )</span><br><span class="line">print(B_sub)</span><br></pre></td></tr></table></figure><pre><code>View Matrix position at row 0, column 0: 0.000006032View Matrix position at row 3, column 1: 0.000000720              725      adroitly     engineers      promoted       synergyCD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07</code></pre><h5 id="Expected-Output-4"><a href="#Expected-Output-4" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">View Matrix position at row <span class="number">0</span>, column <span class="number">0</span>: <span class="number">0.000006032</span></span><br><span class="line">View Matrix position at row <span class="number">3</span>, column <span class="number">1</span>: <span class="number">0.000000720</span></span><br><span class="line">              <span class="number">725</span>      adroitly     engineers      promoted       synergy</span><br><span class="line">CD   <span class="number">8.201296e-05</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span></span><br><span class="line">NN   <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">2.257091e-05</span></span><br><span class="line">NNS  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span>  <span class="number">4.676203e-04</span>  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span></span><br><span class="line">VB   <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span></span><br><span class="line">RB   <span class="number">3.226454e-08</span>  <span class="number">6.456135e-05</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span></span><br><span class="line">RP   <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span></span><br></pre></td></tr></table></figure><p><a name="3"></a></p><h1 id="Part-3-Viterbi-Algorithm-and-Dynamic-Programming"><a href="#Part-3-Viterbi-Algorithm-and-Dynamic-Programming" class="headerlink" title="Part 3: Viterbi Algorithm and Dynamic Programming"></a>Part 3: Viterbi Algorithm and Dynamic Programming</h1><p>In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, <code>A</code> and <code>B</code> to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. </p><ul><li><strong>Initialization</strong> - In this part you initialize the <code>best_paths</code> and <code>best_probabilities</code> matrices that you will be populating in <code>feed_forward</code>.</li><li><strong>Feed forward</strong> - At each step, you calculate the probability of each path happening and the best paths up to that point. </li><li><strong>Feed backward</strong>: This allows you to find the best path with the highest probabilities. </li></ul><p><a name="3.1"></a></p><h2 id="Part-3-1-Initialization"><a href="#Part-3-1-Initialization" class="headerlink" title="Part 3.1:  Initialization"></a>Part 3.1:  Initialization</h2><p>You will start by initializing two matrices of the same dimension. </p><ul><li><p>best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.</p></li><li><p>best_paths: A matrix that helps you trace through the best possible path in the corpus. </p></li></ul><p><a name="ex-05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions</strong>:<br>Write a program below that initializes the <code>best_probs</code> and the <code>best_paths</code> matrix. </p><p>Both matrices will be initialized to zero except for column zero of <code>best_probs</code>.  </p><ul><li>Column zero of <code>best_probs</code> is initialized with the assumption that the first word of the corpus was preceded by a start token (“—s—“). </li><li>This allows you to reference the <strong>A</strong> matrix for the transition probability</li></ul><p>Here is how to initialize column 0 of <code>best_probs</code>:</p><ul><li>The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\textrm{best_probs}[s_{idx}, i]$.</li><li>This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\mathbf{B}[i, vocab[corpus[0]]]$.</li><li>Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). </li><li><strong>vocab</strong> is a dictionary that returns the unique integer that refers to that particular word.</li></ul><p>Conceptually, it looks like this:<br>$\textrm{best_probs}[s_{idx}, i] = \mathbf{A}[s_{idx}, i] \times \mathbf{B}[i, corpus[0] ]$</p><p>In order to avoid multiplying and storing small values on the computer, we’ll take the log of the product, which becomes the sum of two logs:</p><p>$best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$</p><p>Also, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set $best_probs[i,0] = float(‘-inf’)$ when $A[s_{idx}, i] == 0$</p><p>So the implementation to initialize $best_probs$ looks like this:</p><p>$ if A[s_{idx}, i] &lt;&gt; 0 : best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])$</p><p>$ if A[s_{idx}, i] == 0 : best_probs[i,0] = float(‘-inf’)$</p><p>Please use <a href="https://docs.python.org/3/library/math.html" target="_blank" rel="noopener">math.log</a> to compute the natural logarithm.</p><p>The example below shows the initialization assuming the corpus starts with the phrase “Loss tracks upward”.</p><p><img src="Initialize4.PNG"></p><p>Represent infinity and negative infinity like this:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">float('inf')</span><br><span class="line">float('-inf')</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: initialize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(states, tag_counts, A, B, corpus, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        states: a list of all possible parts-of-speech</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        A: Transition Matrix of dimension (num_tags, num_tags)</span></span><br><span class="line"><span class="string">        B: Emission Matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">        corpus: a sequence of words whose POS is to be identified in a list </span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        best_probs: matrix of dimension (num_tags, len(corpus)) of floats</span></span><br><span class="line"><span class="string">        best_paths: matrix of dimension (num_tags, len(corpus)) of integers</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the total number of unique POS tags</span></span><br><span class="line">    num_tags = len(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_probs matrix </span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as the columns</span></span><br><span class="line">    best_probs = np.zeros((num_tags, len(corpus)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_paths matrix</span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as columns</span></span><br><span class="line">    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the start token</span></span><br><span class="line">    s_idx = states.index(<span class="string">"--s--"</span>)</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each of the POS tags</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags) : <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Handle the special case when the transition from start token to POS tag i is zero</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] == <span class="number">0</span>: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag 'i', column 0, to negative infinity</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = float(<span class="string">"-inf"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># For all other cases when transition from start token to POS tag i is non-zero:</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag 'i', column 0</span></span><br><span class="line">            <span class="comment"># Check the formula in the instructions above</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = math.log(A[s_idx,i])  +  math.log(B[i,vocab[corpus[<span class="number">0</span>]]])</span><br><span class="line">            </span><br><span class="line">                         </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the function</span></span><br><span class="line">print(<span class="string">f"best_probs[0,0]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>"</span>) </span><br><span class="line">print(<span class="string">f"best_paths[2,3]: <span class="subst">&#123;best_paths[<span class="number">2</span>,<span class="number">3</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>best_probs[0,0]: -22.6098best_paths[2,3]: 0.0000</code></pre><h5 id="Expected-Output-5"><a href="#Expected-Output-5" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">0</span>]: <span class="number">-22.6098</span></span><br><span class="line">best_paths[<span class="number">2</span>,<span class="number">3</span>]: <span class="number">0.0000</span></span><br></pre></td></tr></table></figure><p><a name="3.2"></a></p><h2 id="Part-3-2-Viterbi-Forward"><a href="#Part-3-2-Viterbi-Forward" class="headerlink" title="Part 3.2 Viterbi Forward"></a>Part 3.2 Viterbi Forward</h2><p>In this part of the assignment, you will implement the <code>viterbi_forward</code> segment. In other words, you will populate your <code>best_probs</code> and <code>best_paths</code> matrices.</p><ul><li>Walk forward through the corpus.</li><li>For each word, compute a probability for each possible tag. </li><li>Unlike the previous algorithm <code>predict_pos</code> (the ‘warm-up’ exercise), this will include the path up to that (word,tag) combination. </li></ul><p>Here is an example with a three-word corpus “Loss tracks upward”:</p><ul><li>Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. </li><li>In the diagram below, the first word “Loss” is already initialized. </li><li>The algorithm will compute a probability for each of the potential tags in the second and future words. </li></ul><p>Compute the probability that the tag of the second work (‘tracks’) is a verb, 3rd person singular present (VBZ).  </p><ul><li>In the <code>best_probs</code> matrix, go to the column of the second word (‘tracks’), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.</li><li>Examine each of the paths from the tags of the first word (‘Loss’) and choose the most likely path.  </li><li>An example of the calculation for <strong>one</strong> of those paths is the path from (‘Loss’, NN) to (‘tracks’, VBZ).</li><li>The log of the probability of the path up to and including the first word ‘Loss’ having POS tag NN is $-14.32$.  The <code>best_probs</code> matrix contains this value -14.32 in the column for ‘Loss’ and row for ‘NN’.</li><li>Find the probability that NN transitions to VBZ.  To find this probability, go to the <code>A</code> transition matrix, and go to the row for ‘NN’ and the column for ‘VBZ’.  The value is $4.37e-02$, which is circled in the diagram, so add $-14.32 + log(4.37e-02)$. </li><li>Find the log of the probability that the tag VBS would ‘emit’ the word ‘tracks’.  To find this, look at the ‘B’ emission matrix in row ‘VBZ’ and the column for the word ‘tracks’.  The value $4.61e-04$ is circled in the diagram below.  So add $-14.32 + log(4.37e-02) + log(4.61e-04)$.</li><li>The sum of $-14.32 + log(4.37e-02) + log(4.61e-04)$ is $-25.13$. Store $-25.13$ in the <code>best_probs</code> matrix at row ‘VBZ’ and column ‘tracks’ (as seen in the cell that is highlighted in light orange in the diagram).</li><li>All other paths in best_probs are calculated.  Notice that $-25.13$ is greater than all of the other values in column ‘tracks’ of matrix <code>best_probs</code>, and so the most likely path to ‘VBZ’ is from ‘NN’.  ‘NN’ is in row 20 of the <code>best_probs</code> matrix, so $20$ is the most likely path.</li><li>Store the most likely path $20$ in the <code>best_paths</code> table.  This is highlighted in light orange in the diagram below.</li></ul><p>The formula to compute the probability and path for the $i^{th}$ word in the $corpus$, the prior word $i-1$ in the corpus, current POS tag $j$, and previous POS tag $k$ is:</p><p>$\mathrm{prob} = \mathbf{best_prob}_{k, i-1} + \mathrm{log}(\mathbf{A}_{k, j}) + \mathrm{log}(\mathbf{B}_{j, vocab(corpus_{i})})$</p><p>where $corpus_{i}$ is the word in the corpus at index $i$, and $vocab$ is the dictionary that gets the unique integer that represents a given word.</p><p>$\mathrm{path} = k$</p><p>where $k$ is the integer representing the previous POS tag.</p><p><a name="ex-06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p>Instructions: Implement the <code>viterbi_forward</code> algorithm and store the best_path and best_prob for every possible tag for each word in the matrices <code>best_probs</code> and <code>best_tags</code> using the pseudo code below.</p><p>`for each word in the corpus</p><pre><code>for each POS tag type that this word may be    for POS tag type that the previous word could be        compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.        retain the highest probability computed for the current word        set best_probs to this highest probability        set best_paths to the index &#39;k&#39;, representing the POS tag of the previous word which produced the highest probability `</code></pre><p>Please use <a href="https://docs.python.org/3/library/math.html" target="_blank" rel="noopener">math.log</a> to compute the natural logarithm.</p><p><img src="Forward4.PNG"></p><h2 id><a href="#" class="headerlink" title></a><details></details></h2><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember that when accessing emission matrix B, the column index is the unique integer ID associated with the word.  It can be accessed by using the 'vocab' dictionary, where the key is the word, and the value is the unique integer ID for that word.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_forward</span><span class="params">(A, B, test_corpus, best_probs, best_paths, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        A, B: The transiton and emission matrices respectively</span></span><br><span class="line"><span class="string">        test_corpus: a list containing a preprocessed corpus</span></span><br><span class="line"><span class="string">        best_probs: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        best_probs: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the number of unique POS tags (which is the num of rows in best_probs)</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through every word in the corpus starting from word 1</span></span><br><span class="line">    <span class="comment"># Recall that word 0 was initialized in `initialize()`</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(test_corpus)): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print number of words processed, every 5000 words</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">5000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Words processed: &#123;:&gt;8&#125;"</span>.format(i))</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###</span></span><br><span class="line">        <span class="comment"># For each unique POS tag that the current word can be</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_prob for word i to negative infinity</span></span><br><span class="line">            best_prob_i = float(<span class="string">"-inf"</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_path for current word i to None</span></span><br><span class="line">            best_path_i = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># For each POS tag that the previous word can be:</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Calculate the probability = </span></span><br><span class="line">                <span class="comment"># best probs of POS tag k, previous word i-1 + </span></span><br><span class="line">                <span class="comment"># log(prob of transition from POS k to POS j) + </span></span><br><span class="line">                <span class="comment"># log(prob that emission of POS j is word i)</span></span><br><span class="line">                prob = best_probs[k, i<span class="number">-1</span>] + math.log(A[k,j]) + math.log(B[j, vocab[test_corpus[i]]])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if this path's probability is greater than</span></span><br><span class="line">                <span class="comment"># the best probability up to and before this point</span></span><br><span class="line">                <span class="keyword">if</span> prob &gt; best_prob_i: <span class="comment"># complete this line</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Keep track of the best probability</span></span><br><span class="line">                    best_prob_i = prob</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># keep track of the POS tag of the previous word</span></span><br><span class="line">                    <span class="comment"># that is part of the best path.  </span></span><br><span class="line">                    <span class="comment"># Save the index (integer) associated with </span></span><br><span class="line">                    <span class="comment"># that previous word's POS tag</span></span><br><span class="line">                    best_path_i = k</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Save the best probability for the </span></span><br><span class="line">            <span class="comment"># given current word's POS tag</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus</span></span><br><span class="line">            best_probs[j,i] = best_prob_i</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the unique integer ID of the previous POS tag</span></span><br><span class="line">            <span class="comment"># into best_paths matrix, for the POS tag of the current word</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus.</span></span><br><span class="line">            best_paths[j,i] = best_path_i</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure><p>Run the <code>viterbi_forward</code> function to fill in the <code>best_probs</code> and <code>best_paths</code> matrices.</p><p><strong>Note</strong> that this will take a few minutes to run.  There are about 30,000 words to process.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this will take a few minutes to run =&gt; processes ~ 30,000 words</span></span><br><span class="line">best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)</span><br></pre></td></tr></table></figure><pre><code>Words processed:     5000Words processed:    10000Words processed:    15000Words processed:    20000Words processed:    25000Words processed:    30000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test this function </span></span><br><span class="line">print(<span class="string">f"best_probs[0,1]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>) </span><br><span class="line">print(<span class="string">f"best_probs[0,4]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">4</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>best_probs[0,1]: -24.7822best_probs[0,4]: -49.5601</code></pre><h5 id="Expected-Output-6"><a href="#Expected-Output-6" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">1</span>]: <span class="number">-24.7822</span></span><br><span class="line">best_probs[<span class="number">0</span>,<span class="number">4</span>]: <span class="number">-49.5601</span></span><br></pre></td></tr></table></figure><p><a name="3.3"></a></p><h2 id="Part-3-3-Viterbi-backward"><a href="#Part-3-3-Viterbi-backward" class="headerlink" title="Part 3.3 Viterbi backward"></a>Part 3.3 Viterbi backward</h2><p>Now you will implement the Viterbi backward algorithm.</p><ul><li>The Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the <code>best_paths</code> and the <code>best_probs</code> matrices.</li></ul><p>The example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: “Loss tracks upward”.</p><p>POS tag for ‘upward’ is <code>RB</code></p><ul><li>Select the the most likely POS tag for the last word in the corpus, ‘upward’ in the <code>best_prob</code> table.</li><li>Look for the row in the column for ‘upward’ that has the largest probability.</li><li>Notice that in row 28 of <code>best_probs</code>, the estimated probability is -34.99, which is larger than the other values in the column.  So the most likely POS tag for ‘upward’ is <code>RB</code> an adverb, at row 28 of <code>best_prob</code>. </li><li>The variable <code>z</code> is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus.  In array z, at position 2, store the value 28 to indicate that the word ‘upward’ (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is <code>RB</code>).</li><li>The variable <code>pred</code> contains the POS tags in string form.  So <code>pred</code> at index 2 stores the string <code>RB</code>.</li></ul><p>POS tag for ‘tracks’ is <code>VBZ</code></p><ul><li>The next step is to go backward one word in the corpus (‘tracks’).  Since the most likely POS tag for ‘upward’ is <code>RB</code>, which is uniquely identified by integer ID 28, go to the <code>best_paths</code> matrix in column 2, row 28.  The value stored in <code>best_paths</code>, column 2, row 28 indicates the unique ID of the POS tag of the previous word.  In this case, the value stored here is 40, which is the unique ID for POS tag <code>VBZ</code> (verb, 3rd person singular present).</li><li>So the previous word at index 1 of the corpus (‘tracks’), most likely has the POS tag with unique ID 40, which is <code>VBZ</code>.</li><li>In array <code>z</code>, store the value 40 at position 1, and for array <code>pred</code>, store the string <code>VBZ</code> to indicate that the word ‘tracks’ most likely has POS tag <code>VBZ</code>.</li></ul><p>POS tag for ‘Loss’ is <code>NN</code></p><ul><li>In <code>best_paths</code> at column 1, the unique ID stored at row 40 is 20.  20 is the unique ID for POS tag <code>NN</code>.</li><li>In array <code>z</code> at position 0, store 20.  In array <code>pred</code> at position 0, store <code>NN</code>.</li></ul><p><img src="Backwards5.PNG"></p><p><a name="ex-07"></a></p><h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Implement the <code>viterbi_backward</code> algorithm, which returns a list of predicted POS tags for each word in the corpus.</p><ul><li>Note that the numbering of the index positions starts at 0 and not 1. </li><li><code>m</code> is the number of words in the corpus.  <ul><li>So the indexing into the corpus goes from <code>0</code> to <code>m - 1</code>.</li><li>Also, the columns in <code>best_probs</code> and <code>best_paths</code> are indexed from <code>0</code> to <code>m - 1</code></li></ul></li></ul><p><strong>In Step 1:</strong><br>Loop through all the rows (POS tags) in the last entry of <code>best_probs</code> and find the row (POS tag) with the maximum value.<br>Convert the unique integer ID to a tag (a string representation) using the dictionary <code>states</code>.  </p><p>Referring to the three-word corpus described above:</p><ul><li><code>z[2] = 28</code>: For the word ‘upward’ at position 2 in the corpus, the POS tag ID is 28.  Store 28 in <code>z</code> at position 2.</li><li>states(28) is ‘RB’: The POS tag ID 28 refers to the POS tag ‘RB’.</li><li><code>pred[2] = &#39;RB&#39;</code>: In array <code>pred</code>, store the POS tag for the word ‘upward’.</li></ul><p><strong>In Step 2:</strong>  </p><ul><li>Starting at the last column of best_paths, use <code>best_probs</code> to find the most likely POS tag for the last word in the corpus.</li><li>Then use <code>best_paths</code> to find the most likely POS tag for the previous word. </li><li>Update the POS tag for each word in <code>z</code> and in <code>preds</code>.</li></ul><p>Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.<br><code>z[1] = best_paths[z[2],2]</code>  </p><p>The small test following the routine prints the last few words of the corpus and their states to aid in debug.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_backward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_backward</span><span class="params">(best_probs, best_paths, corpus, states)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function returns the best path.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the number of words in the corpus</span></span><br><span class="line">    <span class="comment"># which is also the number of columns in best_probs, best_paths</span></span><br><span class="line">    m = best_paths.shape[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize array z, same length as the corpus</span></span><br><span class="line">    z = [<span class="keyword">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of unique POS tags</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the best probability for the last word</span></span><br><span class="line">    best_prob_for_last_word = float(<span class="string">'-inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize pred array, same length as corpus</span></span><br><span class="line">    pred = [<span class="keyword">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment">## Step 1 ##</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each POS tag for the last word (last column of best_probs)</span></span><br><span class="line">    <span class="comment"># in order to find the row (POS tag integer ID) </span></span><br><span class="line">    <span class="comment"># with highest probability for the last word</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the probability of POS tag at row k </span></span><br><span class="line">        <span class="comment"># is better than the previosly best probability for the last word:</span></span><br><span class="line">        <span class="keyword">if</span> best_probs[k,<span class="number">-1</span>] &gt; best_prob_for_last_word: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Store the new best probability for the lsat word</span></span><br><span class="line">            best_prob_for_last_word = best_probs[k,<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># Store the unique integer ID of the POS tag</span></span><br><span class="line">            <span class="comment"># which is also the row number in best_probs</span></span><br><span class="line">            z[m - <span class="number">1</span>] = k</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># Convert the last word's predicted POS tag</span></span><br><span class="line">    <span class="comment"># from its unique integer ID into the string representation</span></span><br><span class="line">    <span class="comment"># using the 'states' dictionary</span></span><br><span class="line">    <span class="comment"># store this in the 'pred' array for the last word</span></span><br><span class="line">    pred[m - <span class="number">1</span>] = states[k]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2 ##</span></span><br><span class="line">    <span class="comment"># Find the best POS tags by walking backward through the best_paths</span></span><br><span class="line">    <span class="comment"># From the last word in the corpus to the 0th word in the corpus</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the unique integer ID of</span></span><br><span class="line">        <span class="comment"># the POS tag for the word at position 'i' in the corpus</span></span><br><span class="line">        pos_tag_for_word_i = best_paths[z[i], i]</span><br><span class="line"><span class="comment">#         print(z[i])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># In best_paths, go to the row representing the POS tag of word i</span></span><br><span class="line">        <span class="comment"># and the column representing the word's position in the corpus</span></span><br><span class="line">        <span class="comment"># to retrieve the predicted POS for the word at position i-1 in the corpus</span></span><br><span class="line">        z[i - <span class="number">1</span>] = pos_tag_for_word_i</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the previous word's POS tag in string form</span></span><br><span class="line">        <span class="comment"># Use the 'states' dictionary, </span></span><br><span class="line">        <span class="comment"># where the key is the unique integer ID of the POS tag,</span></span><br><span class="line">        <span class="comment"># and the value is the string representation of that POS tag</span></span><br><span class="line">        pred[i - <span class="number">1</span>] = states[pos_tag_for_word_i]        </span><br><span class="line">     <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run and test your function</span></span><br><span class="line">pred = viterbi_backward(best_probs, best_paths, prep, states)</span><br><span class="line">m=len(pred)</span><br><span class="line">print(<span class="string">'The prediction for pred[-7:m-1] is: \n'</span>, prep[<span class="number">-7</span>:m<span class="number">-1</span>], <span class="string">"\n"</span>, pred[<span class="number">-7</span>:m<span class="number">-1</span>], <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">'The prediction for pred[0:8] is: \n'</span>, pred[<span class="number">0</span>:<span class="number">7</span>], <span class="string">"\n"</span>, prep[<span class="number">0</span>:<span class="number">7</span>])</span><br></pre></td></tr></table></figure><pre><code>The prediction for pred[-7:m-1] is:  [&#39;see&#39;, &#39;them&#39;, &#39;here&#39;, &#39;with&#39;, &#39;us&#39;, &#39;.&#39;]  [&#39;VB&#39;, &#39;PRP&#39;, &#39;RB&#39;, &#39;IN&#39;, &#39;PRP&#39;, &#39;.&#39;] The prediction for pred[0:8] is:  [&#39;DT&#39;, &#39;NN&#39;, &#39;POS&#39;, &#39;NN&#39;, &#39;MD&#39;, &#39;VB&#39;, &#39;VBN&#39;]  [&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;]</code></pre><p><strong>Expected Output:</strong>   </p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The prediction <span class="keyword">for</span> prep[<span class="number">-7</span>:m<span class="number">-1</span>] is:  </span><br><span class="line"> ['see', 'them', 'here', 'with', 'us', '.']  </span><br><span class="line"> ['VB', 'PRP', 'RB', 'IN', 'PRP', '.']   </span><br><span class="line">The prediction <span class="keyword">for</span> pred[<span class="number">0</span>:<span class="number">8</span>] is:    </span><br><span class="line"> ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN']   </span><br><span class="line"> ['The', 'economy', "'s", 'temperature', 'will', 'be', 'taken']</span><br></pre></td></tr></table></figure><p>Now you just have to compare the predicted labels to the true labels to evaluate your model on the accuracy metric!</p><p><a name="4"></a></p><h1 id="Part-4-Predicting-on-a-data-set"><a href="#Part-4-Predicting-on-a-data-set" class="headerlink" title="Part 4: Predicting on a data set"></a>Part 4: Predicting on a data set</h1><p>Compute the accuracy of your prediction by comparing it with the true <code>y</code> labels. </p><ul><li><code>pred</code> is a list of predicted POS tags corresponding to the words of the <code>test_corpus</code>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The third word is:'</span>, prep[<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'Your prediction is:'</span>, pred[<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'Your corresponding label y is: '</span>, y[<span class="number">3</span>])</span><br></pre></td></tr></table></figure><pre><code>The third word is: temperatureYour prediction is: NNYour corresponding label y is:  temperature NN</code></pre><p><a name="ex-08"></a></p><h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p>Implement a function to compute the accuracy of the viterbi algorithm’s POS tag predictions.</p><ul><li>To split y into the word and its tag you can use <code>y.split()</code>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        pred: a list of the predicted parts-of-speech </span></span><br><span class="line"><span class="string">        y: a list of lines where each word is separated by a '\t' (i.e. word \t tag)</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zip together the prediction and the labels</span></span><br><span class="line">    <span class="keyword">for</span> prediction, y <span class="keyword">in</span> zip(pred, y):</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># Split the label into the word and the POS tag</span></span><br><span class="line">        word_tag_tuple = y.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check that there is actually a word and a tag</span></span><br><span class="line">        <span class="comment"># no more and no less than 2 items</span></span><br><span class="line">        <span class="keyword">if</span> len(word_tag_tuple) &lt; <span class="number">2</span>: <span class="comment"># complete this line</span></span><br><span class="line">            <span class="keyword">continue</span> </span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># store the word and tag separately</span></span><br><span class="line">        word, tag = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> word_tag_tuple]</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(tag, prediction)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if the POS tag label matches the prediction</span></span><br><span class="line">        <span class="keyword">if</span> tag == prediction: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># count the number of times that the prediction</span></span><br><span class="line">            <span class="comment"># and label match</span></span><br><span class="line">            num_correct += <span class="number">1.0</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># keep track of the total number of examples (that have valid labels)</span></span><br><span class="line">        total += <span class="number">1.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> num_correct/total</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Accuracy of the Viterbi algorithm is <span class="subst">&#123;compute_accuracy(pred, y):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Accuracy of the Viterbi algorithm is 0.9531</code></pre><h5 id="Expected-Output-7"><a href="#Expected-Output-7" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of the Viterbi algorithm is <span class="number">0.9531</span></span><br></pre></td></tr></table></figure><p>Congratulations you were able to classify the parts-of-speech with 95% accuracy. </p><h3 id="Key-Points-and-overview"><a href="#Key-Points-and-overview" class="headerlink" title="Key Points and overview"></a>Key Points and overview</h3><p>In this assignment you learned about parts-of-speech tagging. </p><ul><li>In this assignment, you predicted POS tags by walking forward through a corpus and knowing the previous word.</li><li>There are other implementations that use bidirectional POS tagging.</li><li>Bidirectional POS tagging requires knowing the previous word and the next word in the corpus when predicting the current word’s POS tag.</li><li>Bidirectional POS tagging would tell you more about the POS instead of just knowing the previous word. </li><li>Since you have learned to implement the unidirectional approach, you have the foundation to implement other POS taggers used in industry.</li></ul><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul><li><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">“Speech and Language Processing”, Dan Jurafsky and James H. Martin</a></li><li>We would like to thank Melanie Tosik for her help and inspiration</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Parts-of-Speech-Tagging-POS&quot;&gt;&lt;a href=&quot;#Assignment-2-Parts-of-Speech-Tagging-POS&quot; class=&quot;headerlink&quot; title=&quot;Assignment 2
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>ML-Interview-Computer-Vision</title>
    <link href="https://zhangruochi.com/ML-Interview-Computer-Vision/2020/05/29/"/>
    <id>https://zhangruochi.com/ML-Interview-Computer-Vision/2020/05/29/</id>
    <published>2020-05-28T23:41:05.000Z</published>
    <updated>2020-05-29T04:43:09.902Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>为什么输入网络前要对图像做归一化？</p><blockquote><ol><li>把不同的图片映射到同一坐标系，使其具有相同的尺度及相似的特征分布。</li><li>一定程度上消除了过度曝光，质量不佳或者噪声等各种原因对模型权值更新的影响。</li><li>加快gradient更新的收敛速度。</li></ol></blockquote></li><li><p>权重初始化方法有哪些？</p><blockquote><ol><li>Small random numbers (gaussian with zero mean and 1e-2 standard deviation):  Works okay for small networks, but problems with deeper networks.<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%"></center></li><li><strong>Xavier</strong>: 基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0. 初始化方差为:  <code>std = sqrt(node_in)</code>. 参考 <a href="https://zhuanlan.zhihu.com/p/27919794" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27919794</a></li><li><strong>Kaiming</strong>: 在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2. 初始化方差为: <code>std = sqrt(node_in / 2)</code></li></ol></blockquote></li><li><p>说说 FCN 的基本思想.</p><blockquote><p>FCN对图像进行像素级的分类，从而解决了语义级别的图像分割问题。与经典的CNN在卷积层使用全连接层得到固定长度的特征向量进行分类不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷基层的特征图（feature map）进行上采样，使它恢复到输入图像相同的尺寸，从而可以对每一个像素都产生一个预测，同时保留了原始输入图像中的空间信息，最后在上采样的特征图进行像素的分类。</p></blockquote></li><li><p>什么是转置卷积?</p><blockquote><p>事实上，卷积运算还可以通过矩阵乘法来实现.假设我们定义高和宽分别为4的输入X，以及高和宽分别为3的卷积核K, 卷积运算输出高和宽分别为2.</p><script type="math/tex; mode=display">\frac{h(w) - k + 2p}{ s } + 1</script><p>我们将卷积核K改写成含有大量零元素的稀疏矩阵W，即权重矩阵。权重矩阵的形状为(4, 16)， 其中的非零元素来自卷积核K中的元素。将输入X逐行连结，得到⻓度为16的向量。然后将W与向量化的X做矩阵乘法，得到⻓度为4的向量。对其变形后，我们可以得到和上面卷积运算相同的结 果。可⻅，我们在这个例子中使用矩阵乘法实现了卷积运算。<br>现在我们从<strong>矩阵乘法</strong>的⻆度来描述卷积运算。设输入向量为$x$，权重矩阵为$W$，卷积的前向计算 函数的实现可以看作将函数输入乘以权重矩阵，并输出向量$y=Wx$.们知道，反向传播需要依据链式法则。由于$\triangledown_x y = W^T$，卷积的反向传播函数的实现可以看作将函数输入乘以转置后的权重矩阵$W^T$。而转置卷积层正好交换了卷积层的前向计算函数与反向传播函数:转置卷积层的这两个函数可以看作将函数输入向量分别乘以$W^T$ 和 $W$.<br>不难想象，转置卷积层可以用来交换卷积层输入和输出的形状。让我们继续用矩阵乘法描述卷积。设权重矩阵是形状为4 × 16的矩阵，对于⻓度为16的输入向量，卷积前向计算输出⻓度为4的向量。假如输入向量的⻓度为4，转置权重矩阵的形状为16 × 4，那么转置卷积层将输出⻓度为16的向量。在模型设计中，转置卷积层常用于将较小的特征图变换为更大的特征图。在全卷积网络中，当输入是高和宽较小的特征图时，转置卷积层可以用来将高和宽放大到输入图像的尺寸。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%"></center></blockquote></li><li><p>什么是空洞卷积（Dilated convolution）?</p><blockquote><ol><li>从kernel（卷积核）角度：相当于在标准概念的kernel（卷积核）中，相邻点之间添加rate-1个0，然后使用扩张后的kernel（卷积核）与原图进行卷积。如下图rate=2，相当于标准的3<em>3卷积核变为5</em>5卷积核，每一行中间添加2-1个0</li><li>从原图角度：使用标准概念的kernel（卷积核）在原图中每隔rate-1进行像素点卷积采样。如下图rate=2，在原图中每隔rate-1进行卷积。<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%"></center></li></ol></blockquote></li><li><p>解释下 Unet 的 Architecture.</p><blockquote><p>Unet 使用 encoder 和 decoder 的架构，在encoder下采样4次，一共下采样16倍。对称地，其decoder也相应上采样4次，将encoder得到的高级语义特征图恢复到原图片的分辨率。相比于FCN和Deeplab等，UNet共进行了4次上采样，并在同一个stage使用了skip connection，而不是直接在高级语义特征上进行监督和loss反传，这样就保证了最后恢复出来的特征图融合了更多的low-level的feature，也使得不同scale的feature得到了的融合。</p></blockquote></li><li><p>解释下 FPN 网络.</p><blockquote><p>一个自底向上的线路，一个自顶向下的线路，横向连接（lateral connection）。图中放大的区域就是横向连接，这里1 * 1的卷积核的主要作用是减少channel的数量，也就是减少了feature map的个数，并不改变feature map的尺寸大小。</p><ol><li><strong>自底向上</strong>其实就是网络的前向过程。在前向过程中，feature map的大小在经过某些层后会改变，而在经过其他一些层的时候不会改变，作者将不改变feature map大小的层归为一个stage，因此每次抽取的特征都是每个stage的最后一个层输出，这样就能构成特征金字塔。 </li><li><strong>自顶向下</strong>的过程采用上采样（upsampling）进行，而横向连接则是将上采样的结果和自底向上生成的相同大小的feature map进行融合（merge）。在融合之后还会再采用3 * 3的卷积核对每个融合结果进行卷积，目的是消除上采样的混叠效应（aliasing effect）。并假设生成的feature map结果是P2，P3，P4，P5，和原来自底向上的卷积结果C2，C3，C4，C5一一对应。<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%"></center></li></ol></blockquote></li><li><p>什么是Anchors？</p><blockquote><p>参考 <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a></p></blockquote></li><li><p>解释 ROI Pooling 和 ROI Align 的区别.</p><blockquote><p>对于一个region proposal，首先从原图经过全卷积网络到特征图，得到的候选框位置可能存在浮点数，进行取整操作从而出现第一次量化；其次，在ROI Pooling求取每个小网格的位置时也同样存在浮点数取整的情况。这两次量化的结果都使得候选框的位置会出现偏差，在论文里，作者把它总结为“不匹配问题（misalignment）<br>。为了解决ROI Pooling的上述缺点，ROI Align提出改进的方法。ROI Align的思路是：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作</p></blockquote></li><li><p>请解释下two stage object detection 的发展脉络。</p><blockquote><p>参考 <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a><br>讲解 R-CNN, Fast R-CNN, Faster RCNN, Mask R-CNN 的发展轨迹。</p></blockquote></li><li><p>请解释下one stage object detection 的发展脉络。</p><blockquote><p>参考 <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a><br>讲解YOLO1,YOLO2,YOLO3,YOLO4</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      FCN, Unet, Mask R-CNN, YOLO
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Natural-Language-Processing</title>
    <link href="https://zhangruochi.com/ML-Interview-Natural-Language-Processing/2020/05/28/"/>
    <id>https://zhangruochi.com/ML-Interview-Natural-Language-Processing/2020/05/28/</id>
    <published>2020-05-28T07:14:07.000Z</published>
    <updated>2020-05-28T22:50:59.616Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><ol><li><p>简述 Transformer 提出的背景。</p><blockquote><ol><li>seq2seq 处理长期依赖仍然是一个挑战。</li><li>seq2seq 模型架构的顺序特性阻止了并行化。</li></ol></blockquote></li><li><p>什么是self-attention机制？ 请举例来说明。</p><blockquote><p>参考 <a href="https://mp.weixin.qq.com/s/8Kic83oCoiKzAKe-dvRUvw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/8Kic83oCoiKzAKe-dvRUvw</a></p><ol><li>输入X, 通过3个线性转换把X转换为Q,K,V. 有了K,Q,V 三个特征向量，就可以做attention. </li><li>用每个单词的 query 向量与其自身及其他单词的 key 向量做dot product. 得到权重的分布表示。<br>假设有两个单词，Thinking, Machines. 通过嵌入变换会$X_1$,$X_2$两个向量。分别与$Wq$, $W_k$,$W_v$三个矩阵想做点乘得到，{q1,q2},{k1,k2},{v1,v2} 6个向量。 然后{q1,k1} 做点乘得到得分$score_1$, {q1,k2}做点乘得到$score_2$。对上述 socre 进行规范化，然后 softmax 得到权重 $[w_1,w_2]$. </li><li>对 valuex 向量求加权平均。用权重向量$[w_1,w_2]$ 乘以$[v1,v2]$值得到一个加权后的值. <script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{Q,K^T}{\sqrt{d_k}})V</script><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></li></ol></blockquote></li><li><p>encoder 中的self-attention 与 decoder 中 masked self-attention 有什么区别？</p><blockquote><p>encoder 中，$QK^T$ 会组成一个word2word的attention map. 是一个方阵. 比如说你的输入是一句话 “i have a dream” 总共4个单词， 这里就会形成一张4x4的注意力机制的图.</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center>这里的masked就是要在做language modelling（或者像翻译）的时候，不给模型看到未来的信息。此时的 attention map 是一个下三角矩阵。<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%"></center></blockquote></li><li><p>什么是 Multi-Head Attention？</p><blockquote><p>Multi-Head Attention就是把self-attention 做 N 次，然后把 N 个 heads concatenate 在一起。最后再做一个线性变换。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="50%" height="50%"></center></blockquote></li><li><p>Batch Norm 和 Layer Norm 的区别是什么？</p><blockquote><p>BN并不适用于RNN等动态网络和batchsize较小的时候效果不好。Layer Normalization的提出有效的解决BN的这两个问题。LN和BN不同点是归一化的维度是互相垂直的. N表示样本轴，C 表示通道轴，F 是每个通道的特征数量。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="50%" height="50%"></center></blockquote></li><li><p>什么是transformer 中的 Encoder-Decoder Attention？</p><blockquote><p>在decoder中，Transformer block比编码器中多了个encoder-cecoder attention。在encoder-decoder attention中，Q 来自于 decoder 的上一个输出，K,V 来自于 encoder 的输出.</p></blockquote></li><li><p>什么是Positional Encoding？</p><blockquote><p>参考 <a href="https://zhuanlan.zhihu.com/p/95079337" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/95079337</a><br>Transformer抛弃了RNN，而RNN最大的优点就是在时间序列上对数据的抽象，所以文章中作者提出两种Positional Encoding的方法，将encoding后的数据与embedding数据求和，加入了相对位置信息。</p></blockquote></li><li><p>简述 Transformer 的架构。</p><blockquote><p>Transformer 包含 Encoder Part and Decoder Part. Encoder Part 比较重要的是 Self-Attention. Decoder Part 比较重要的是 Masked Multi-Head Attention，Encoder-Decoder Attention. 同时，encoder 和 decoder 都包含 Feed Forward, Residuals 以及 Layer Norm. </p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="80%" height="80%"></center></blockquote></li></ol><h2 id="ELMo-OpenAI-GPT-BERT"><a href="#ELMo-OpenAI-GPT-BERT" class="headerlink" title="ELMo,OpenAI GPT,BERT"></a>ELMo,OpenAI GPT,BERT</h2><blockquote><p>参考 <a href="https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/">https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/</a></p></blockquote><ol><li><p>解释下 ELMO 的思想.</p><blockquote><p>ELMO 的意思是 embedding from language model. word2vec 最大的问题是在训练好后，词向量在任何context的情况下都是不变的。而实际上我们知道单词的意思随着语境的变化而变化。ELMO 训练了一个双向的 LSTM, 然后将 hidden layer concate 在一起。对于L层的双向lstm语言模型，每个单词一共有2L+1个表征（representations）.最后根据具体的任务，将2L+1个表征加权平均在一起。</p></blockquote></li><li><p>解释下OpenAI Transformer.</p><blockquote><p>OpenAI Pre-training a Transformer Decoder for Language Modeling.</p></blockquote></li><li><p>Bert 是怎样实现 mask 的？</p><blockquote><ul><li>MLM：将完整句子中的部分字mask，预测该mask词</li><li>NSP：为每个训练前的例子选择句子 A 和 B 时，50% 的情况下 B 是真的在 A 后面的下一个句子， 50% 的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句</li></ul></blockquote></li><li><p>在数据中随机mask15%的token，其中80%被换位[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？</p><blockquote><p>Bert随机mask语料中15%的token，然后预测masked token，那么masked token 位置输出的final hidden vectors喂给softmax网络即可得到maskedtoken的预测结果。这样操作存在一个问题，fine-tuning的时候没有[MASK]token，因此存在pre-training和fine-tuning之间的mismatch，为了解决这个问题，采用了下面的策略：</p><ol><li>80%的时间中：将选中的词用[MASK]token来代替，例如<br>my dog is hairy → my dog is [MASK]</li><li>10%的时间中：将选中的词用任意的词来进行代替，例如<br>my dog is hairy → my dog is apple</li><li>10%的时间中：选中的词不发生变化，例如<br>my dog is hairy → my dog is hairy</li></ol></blockquote></li><li><p>为什么BERT有3个嵌入层，它们都是如何实现的？</p><blockquote><ul><li>input_id是语义表达，和传统的w2v一样，方法也一样的lookup</li><li>segment_id是辅助BERT区别句子对中的两个句子的向量表示，从[1,embedding_size]里面lookup</li><li>position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从[511,embedding_size]里面lookup</li></ul></blockquote></li><li><p>Bert的损失函数？</p><blockquote><ol><li>MLM:在 encoder 的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用 softmax 计算mask中每个单词的概率</li><li>NSP:用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量,用 softmax 计算 IsNextSequence 的概率</li><li>MLM+NSP即为最后的损失</li></ol></blockquote></li><li><p>elmo、GPT、bert三者之间有什么区别？</p><blockquote><ol><li>特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。</li><li>单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。</li><li>GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      Natural language processing
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Ensemble</title>
    <link href="https://zhangruochi.com/ML-Interview-Ensemble/2020/05/28/"/>
    <id>https://zhangruochi.com/ML-Interview-Ensemble/2020/05/28/</id>
    <published>2020-05-28T02:07:46.000Z</published>
    <updated>2020-05-28T22:45:39.171Z</updated>
    
    <content type="html"><![CDATA[<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><ol><li><p>集成学习分哪几种，他们有何异同？</p><blockquote><ol><li>Boosting: 采用串行的方式，各个基础分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终的结果。</li><li>Bagging: 采用并行的方式，各个基分类器之间无依赖。其中比较著名的算法之一是基于决策树的随机森林。为了让及分类器之间互相独立，将训练集分成若干子集（当训练样本较少时，子集之间有重叠）。在最终的决策阶段，每个个体单独做出判断，然后通过投票的方式做出最后的集体决策。</li></ol></blockquote></li><li><p>集成学习是如何提高基分类器的性能的？</p><blockquote><p>基分类器的误差，是方差和偏差两种错误之和。偏差源于underfitting，方差源于overfitting. Boosting 方法通过逐步聚焦于基分类器分错的样本，减少集成分类器的偏差。Bagging通过对训练样本进行多次采样，分别训练多个不同的模型，然后做综合，来减少分类器的方差。</p></blockquote></li><li><p>集成学习的有哪些基本步骤？请以 Adaboosting 来举例。</p><blockquote><ol><li>initialize equal weights for all samples<script type="math/tex; mode=display">\alpha_{i} = \frac{1}{N}</script></li><li>Repeat t = 1,…,T<ul><li>learn $f_{t}(x)$ with data weights $\alpha_{i}$</li><li>compute weighted error<script type="math/tex; mode=display">weighted_{error_{t}} = \sum_{i=1}^{m}\alpha_{i}I(y_{i} \neq f_{t}(x_{i}))</script></li><li>compute coefficient <script type="math/tex; mode=display">\hat{w_{t}} = \frac{1}{2}\ln(\frac{1 - weighted_{error_{t}} }{weighted_{error_{t}}})</script><ul><li>$\hat{w_{t}}$ is higher when weighted_error is larger</li></ul></li><li>recomputed weights $\alpha_{i}$<script type="math/tex; mode=display">\alpha_{i} =      \begin{equation}    \left\{     \begin{array}{lr}      \alpha_{i}e^{-\hat{w_{t}}} \quad if \ f_t(x_i) = y_i & \\       \alpha_{i}e^{\hat{w_{t}}}  \quad if \ f_t(x_i) \neq y_i  &     \end{array}     \right.     \end{equation}</script></li><li>Normalize weights $\alpha_{i}$<ul><li>if $x_{i}$ often mistake, weight $\alpha_{i}$ gets very large</li><li>if $x_{i}$ often correct, weight $\alpha_{i}$ gets very small<script type="math/tex; mode=display">\alpha_{i} = \frac{\alpha_{i}}{\sum_{i}^{m}\alpha_{i}}</script></li></ul></li></ul></li><li>In the testing time, the final prediction is:<script type="math/tex; mode=display">\hat{y_{t}} = sign( \sum_1^T \hat{w_{t}} f_t(x) )</script></li></ol></blockquote></li><li><p>常用的基分类器是什么？</p><blockquote><p>常用决策树作为基分类器，主要有以下几方面的原因</p><ol><li>决策树的表达能力和泛化能力，可以通过调节树的层数来方便实现。</li><li>数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树随机性较大，这样”不稳定的学习器”更适合作为基分类器。</li><li>决策树在节点分裂时，随机地选择一个特征子集，从中找出最优分裂特征，很好地引入了随机性。<br>神经网络模型也适合作为基分类器，因为神经网络也是比较”不稳定的”。还可以通过调整神经元的数量，连接方式，网络层数，初始权重引入随机性。</li></ol></blockquote></li><li><p>在随机森林中，可否使用线性分类器或者K-近邻作为基分类器？</p><blockquote><p>随机森林是属于 Bagging类的集成学习。Bagging的主要好处是集成后的分类器的方差，比基分类器的反差小。Bagging 所采用的基分类器，最好是本身不稳定的分类器，这样才能获得更优的性能。线性分类器或者K-近邻都是较为稳定的分类器，本身反差就不大，所以不适合。</p></blockquote></li><li><p>随机森林的随机性体现在哪里？</p><blockquote><ol><li>每棵树的样本是随机抽样得到的</li><li>每课数生长时分裂的属性集合不同</li></ol></blockquote></li><li><p>什么是bias 什么是 variance?</p><blockquote><p>Bias 是 underfitting 造成的。Bias是指由所有采样得到的大小为m的训练数据训练出的所有模型的输出的平均值和真实模型输出之间的偏差。<br>Variance 是 overfitting 造成的。Variance是之由所有采样得到的大小为m的训练数据集训练出的所有模型的输出方差。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></blockquote></li><li><p>GBDT的基本原理是什么？</p><blockquote><p>Gradient Boosting 是 Boosting 中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的的弱分类器以累加的形式结合到现有模型中。</p></blockquote></li><li><p>GBDT 和 Adatboost 的关系和区别是什么？</p><blockquote><p>和AdaBoost一样，Gradient Boosting也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过提升错分数据点的权重来定位模型的不足而Gradient Boosting是通过算梯度（gradient）来定位模型的不足。因此相比AdaBoost, Gradient Boosting可以使用更多种类的目标函数,而当目标函数是均方误差时，计算损失函数的负梯度值在当前模型的值即为残差。当目标函数不是 square loss 时残差并不一定等于负梯度。Adaboost 是 GBDT 的一个特例，GBDT 是 Adaboost的推广。</p><script type="math/tex; mode=display">\left\{ \begin{aligned}& L(y_i, F(x_i)) = \frac{1}{2} * (y_i - F(x_i))^2 \\& - \frac{\partial(y_i, F(x_i))}{\partial F(x_i)} = (y_i - F(x_i))\end{aligned}\right.</script></blockquote></li><li><p>GBDT 为什么要拟合上一次模型的负梯度？</p><blockquote><p>我们要拟合损失函数的负梯度，可以看做拟合一个方向为负梯度方向，步长为单位长度的值，所以拟合的过程相当于我们沿着负梯度方向走了一个步长，具体走多少步（多少步可以理解为训练多少个决策树来拟合该分类器，使得损失函数最低）达到终止的条件，即走到最优点的附近。</p></blockquote></li><li><p>梯度提升和梯度下降的区别和联系？</p><blockquote><p>两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向信息来对当前模型进行更新。在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新。在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中。</p><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center></blockquote></li><li><p>GBDT和局限性有哪些？</p><blockquote><p>GBDT 在高纬度稀疏数据集上，表现不如支持向量机或者神经网络。<br>训练过程需要串行训练。</p></blockquote></li><li><p>XGBoost 与 GBDT 的联系和区别？</p><blockquote><ol><li>原始的 GBDT 算法基于损失函数的负梯度来构造信贷决策树，只是在决策树构建完成构造新的决策树，只是在决策树构建完成后进行剪枝。而 XGBoost在决策树构建阶段就加入了正则项。</li><li>不同的决策树算法采用不同的准则来进行树的构建，比如 IC3 采用信息增益，C4.5 为了克服特征中取值较多的特征而采用信息增益比， CART 采用基尼系数和平方误差。XGBoost 将预测值带入到损失函数中求得损失函数的当前最小值，然后计算出分裂前后损失函数的差值，利用最大化这个差值来作为准则完成树的构建。<br>总的来说，两者的区别和联系可以总结为:<br>a. GBDT是机器学习算法，XGBoost 是其工程实现。<br>b. 在使用 CART 作为基分类器时，XGBoost 显式加入正则项来控制模型的复杂度，有利于防止过拟合。<br>c. GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。<br>d. 传统的 GBDT 采用 CART 作为基分类器，XGBoost 支持多种基分类器。<br>e. 传统的 GBDT 在每轮迭代时使用全部的数据，XGBoost 则采用了与随机森林相似的策略，支持对数据进行采样。<br>f. XGBoost能够自动学习出缺失值的处理策略</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      集成学习,Bagging,Boosting, Bias,Variance
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Deep-Learning</title>
    <link href="https://zhangruochi.com/ML-Interview-Deep-Learning/2020/05/27/"/>
    <id>https://zhangruochi.com/ML-Interview-Deep-Learning/2020/05/27/</id>
    <published>2020-05-27T09:46:36.000Z</published>
    <updated>2020-05-28T23:09:19.497Z</updated>
    
    <content type="html"><![CDATA[<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ol><li><p>写出常用激活函数及其导数</p><blockquote><p>Sigmod </p><script type="math/tex; mode=display">f(z) = \frac{1}{1+exp(-z)}</script><script type="math/tex; mode=display">f\prime(z) = f(z)(1 - f(z))</script><p>Tanh</p><script type="math/tex; mode=display">f(z) = tanh(z) = \frac{e^z - e^{-z}}{ e^z + e^{-z}}</script><script type="math/tex; mode=display">f\prime(z) = 1 - (f(z))^2</script><p>Relu</p><script type="math/tex; mode=display">f(z) = max(0,z)</script><script type="math/tex; mode=display">f\prime(z) = \left\{\begin{aligned}& 1, z > 0 \\& 0, z \leq 0\end{aligned}\right.</script></blockquote></li><li><p>为什么 Sigmoid 和 Tanh 激活函数会导致梯度消失现象？</p><blockquote><p>Sigmoid 函数将输入映射到区间(0,1)，当 z 较大和较小时，f(z) 趋近于 1. 此时的梯度趋近于0. Tanh 实际相当于 Sigmoid 的平移。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></blockquote></li><li><p>ReLU 系列的激活函数相对于Sigmoid 和 Tanh 激活函数的优点是什么？局限性是什么？</p><blockquote><p>优点: 1. 计算简便 2. 有效地解决梯度消失的问题 3. ReLU 单侧抑制提供了网络的稀疏表达能力<br>局限性: 神经元死亡的问题。因为$f(z) = max(0,z)$ 导致负梯度在经过该 ReLU单元时被置为 0，且之后也不被任何数据激活。实际训练时，如果 learning rate 过大，会导致一定比例的 neuron 不可逆死亡，使得整个训练过程失败。Leaky ReLU 可以有效地解决该问题。</p><script type="math/tex; mode=display">f(z) = \left\{\begin{aligned} & z, z > 0 \\& az, z \leq 0\end{aligned}\right.</script></blockquote></li><li><p>平方误差损失函数和交叉熵损失函数分别适用什么场景？</p><blockquote><p>一般来说，平方损失函数适合于连续输出，并且最后一层不含 Sigmoid 或者 Softmax 激活函数的神经网络。交叉熵损失则更适合二分类和多分类场景。</p></blockquote></li></ol><h2 id="神经网络训练技巧"><a href="#神经网络训练技巧" class="headerlink" title="神经网络训练技巧"></a>神经网络训练技巧</h2><ol><li><p>神经网路训练时是否可以将全部参数初始化为0.</p><blockquote><p>同一层的神经元都是同构的，他们拥有相同的输入，如果将参数全部初始化为相同的值，那么无论 forward 还是 backward 都会拥有完全相同的值。因此，我们需要随机地初始化神经网络的参数，以打破这种对称性。</p></blockquote></li><li><p>为什么 Dropout 可以抑制过拟合，它的工作原理是什么？</p><blockquote><p>Dropout作用与每份小批量训练数据，由于其随机丢弃神经元的机制，相当于每次迭代都在训练不同结构的神经网络。类比于Bagging方法，Dropout可被认为是一种实用的大规模神经网络的模型继承算法。对于包含 N 个神经元结点的网络，在 Dropout 的作用下可看做为$2^N$个模型的集成。这$2^N$个模型可认为是原始网络的子网络。应用Dropout包括训练和预测两个阶段，在训练阶段，每个神经元需要增加一个概率系数.</p><script type="math/tex; mode=display">\left\{ \begin{aligned} & r_j^{(l)} \sim Bernoulli(p) \\& \tilde{y}^{(l)} = r^{(l)} * y^{(l)}\end{aligned}\right.</script><p>测试阶段是前向传播过程，每个神经元的参数要预先乘以概率系数p，以恢复在训练时该神经元只有p的概率被用于整个神经网络的前向传播计算</p></blockquote></li><li><p>BatchNorm 的基本动机与原理是什么？ 在卷积网络中如何使用?</p><blockquote><ol><li>神经网络训练的本质是学习数据分布，因此我们常假设训练数据与测试数据是独立同分布的。如果分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有数据进行归一化处理。随着网络训练的进行，每个hidden layer的参数变化使得后一层的输入发生变化，从而每一批训练数据的分布也随之发生变化，使得网络在每次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合风险。</li><li>BatchNorm 是针对每一批数据，在网络的每一层输入之前增加归一化处理，将所有batch数据强制统一在统一的数据分布下。<script type="math/tex; mode=display">\hat{x}^{k} = \frac{x^{(k) - E[x^{(k)}]}}{\sqrt{Var[x^{(k)}]}}</script>其中x^{(k)}为该层第 K 个神经元的原始输入数据，$E[x^{(k)}]$为这一个batch在第k个神经元的均值，$\sqrt{Var[x^{(k)}]}$为这一批数据在第k个神经元的标准差。</li><li>但是均值为 0，方差为1 这个限制太严格了，降低了神经网络的拟合能力。因此加入了两个可学习参数 $\beta$ 和 $\eta$<script type="math/tex; mode=display">y_i = \eta \hat{x}^{k} + \beta</script>在测试阶段，没有batch mean 和 var. 我们使用训练阶段的 running average.</li><li>BatchNorm usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.</li></ol></blockquote></li></ol><h2 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h2><ol><li><p>说说卷及操作的本质。</p><blockquote><ol><li>Sparse Interaction（稀疏交互）： 卷积操作中，每个输出神经元仅仅与前一层特定局部区域的神经元存在连接权重。时间复杂度得到优化，过拟合的情况也得到改善。<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center></li><li>Hierarchical feature representation   ：通常来说，图像，文本，语音等现实世界中的数据都是具有局部的特征结构，我们可以先学习局部的特征，再将局部特征组合起来形成更加复杂的和抽象的特征。这与人类视觉感知物体的共通的。</li><li>Parameter Sharing （参数共享）：给定一个 feature map, 我们使用一个 filter 去扫这个 feature map. filter 里面的参数叫权重，这张图里每个位置都是被同样的 filter 扫描的，所以权重是相同的。参数共享的物理意义是使得卷积层具有平移不变性。例如，在猫的图片上先进行 convolution，再平移l 像素输出，与现将图片平移l 像素再进行卷积操作的输出结果是相等的。</li></ol></blockquote></li><li><p>常用的池化操作有哪些？池化的作用是什么？</p><blockquote><p>Mean Pooling 和 Max pooling. 池化操作除了能显著降低参数数量，还能够保持对平移、伸缩、旋转操作的不变性。Mean Pooling 对背景的保留效果较好，Max pooling 对纹理的提取效果更好。<br>特殊的池化方式有，Global Average Pooling，Spatial Pyramid Pooling(空间金字塔池化). Global Average Pooling 可以将 feature map 转换到特定的维度。SPP 主要考虑多尺度信息，例如计算1x1、2x2、4x4的池化并将结果拼接在一起作为下一层的输入。还可以使得我们构建的网络能够输入任意大小的图片，而不需要提前经过裁剪缩放等预处理操作</p></blockquote></li><li><p>CNN 如何用于文本分类任务？</p><blockquote><p>对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于 N-Gram. CNN 的作用就是能够自动地对 N-gram 特征进行组合和筛选，获得不同抽象层次的语义信息。常用的应用如 char-based model, 把每个char 的 vector concat 在一起，然后使用 conv1d提取特殊的pattern 和 semantic.</p></blockquote></li><li><p>ResNet 的核心理论是什么？</p><blockquote><p>ResNet提出的背景是缓解深层的神经网络中梯度消失的问题。直观来讲，一个 L+1 层的网络不会比 L 层的网络效果差，因为我们简单地设最后一层为一个恒等映射即可。然而实际上深层网络反而会有更大的训练误差，这很大程度上归结于深度神经网络中的梯度消失问题。<br>如下图所示，输入$x$经过两个神经网络变换得到$F(x)$,同时 $x$ 短接到两层之后，最后这个包含两层的神经网络的输出为 $H(x) = F(x) + x$. 这样一来，$F(x)$被设计为只需要拟合x与目标输出H(x)的残差 $H(x) - x$. 如果某一层的效果足够好，那么多加层不会使得模型变差，因为该层的输出短接到了后面的层。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center></blockquote></li><li><p>DenseNet 的核心理论是什么？</p><blockquote><p>既将 $x_0$ 到 $l_1$ 层的所有输出feature map 通过 Channel concat在一起.由于在DenseNet中需要对不同层的feature map进行cat操作,所以需要不同层的feature map保持相同的feature size,这就限制了网络中Down sampling的实现.为了使用Down sampling,作者将DenseNet分为多个Denseblock. 在同一个Denseblock中要求feature size保持相同大小,在不同Denseblock之间设置transition layers实现Down sampling, 在作者的实验中transition layer由BN + Conv(1×1) ＋2×2 average-pooling组成.</p></blockquote></li></ol><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><ol><li><p>处理文本数据时，循环神经网络与前馈神经网络相比有什么特点？</p><blockquote><p>一个长度为T的序列用RNN建模，展开之后可以看作是一个 T 层的前馈神经网络。其中，第$t$层的隐含状态$h_t$ 编码了序列前$t$个输入信息，可以通过当前的输入$x_t$ 和上一层神经网络的状态$h_{t-1}$计算得到. $h_t$和y的计算公式为:</p><script type="math/tex; mode=display">\left\{ \begin{aligned}& net_t = Ux_t + Wh_{t-1} \\& h_t = f(net_t) \\& y = g(Vh_t)\end{aligned}\right.</script><p>其中，$f$ 和 $g$ 为激活函数，U 为输入层到隐藏层的权重矩阵，W 为隐藏层从上一时刻到一下时刻状态转移的权重矩阵。在文本分类中，$f$可以选取Tanh函数或者ReLU函数，$g$可以采用 softmax 函数。相比于CNN, RNN 由于具备对序列信息的刻画能力，往往能够得到更准确的结果。</p></blockquote></li><li><p>循环神经网络为什么会出现梯度消失和梯度爆炸？有哪些改进方案？</p><blockquote><p>RNN 求解采用 BPTT(back propagation through time) 算法实现，实际上是 back propagation 算法的变种。使用 BPTT算法学习的RNN 并不能捕捉长距离的依赖关系，这种现象主要源于神经网络中的梯度消失。因为RNN 的梯度可以写成连乘的形式。详细可参考 <a href="https://zhangruochi.com/BackPropagation-through-time/2019/10/12/">https://zhangruochi.com/BackPropagation-through-time/2019/10/12/</a><br>梯度爆照可以通过梯度裁剪来环节，当梯度大于某个给定值时，对梯度进行收缩。梯度消失可通过 LSTM， GRU 等模型加入门控机制来弥补。</p></blockquote></li><li><p>LSTM 是如何实现长短期记忆功能的？</p><blockquote><p><a href="https://zhangruochi.com/LSTM-Mxnet-Implementation/2019/04/13/">https://zhangruochi.com/LSTM-Mxnet-Implementation/2019/04/13/</a><br>经典的 LSTM，第 t 步的更新公式为：</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{I}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xi} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}_i),\\\boldsymbol{F}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xf} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hf} + \boldsymbol{b}_f),\\\boldsymbol{O}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xo} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{ho} + \boldsymbol{b}_o),\\\tilde{\boldsymbol{C}}_t &= \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xc} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c),\\\boldsymbol{C}_t &= \boldsymbol{F}_t \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t.\end{aligned}</script><p>  与传统的 RNN 相比，LSTM 依然是基于$x_t$和$h_{t-1}$ 来计算$h_t$，只不过对内部的结构进行了更加精心的设计，加入了 input gate $i_t$, forget gate $f_t$, output gate $o_t$. input gate控制当前计算的新状态多大程度更新到当前momery cell 中，forget cell控制前一步的memory cell中的信息有多大程度被遗忘掉，输出门控制当前输出有多程度取决与当前的 memory cell.<br>  当输入的序列中没有重要信息时，LSTM 的遗忘门的值接近于 1，输入门接近于0. 此时过去的记忆会被保留下来，从而实现长期记忆功能。当输入的序列中有重要信息时，LSTM 应当把其存记忆中，此时输入门的值会接近于 1，而遗忘门的值接近于0。经过这样的设计，整个网络更容易学习到序列之间的长期依赖。</p></blockquote></li><li><p>LSTM 里各模块分别适用什么激活函数，可以使用别的激活函数激活吗？</p><blockquote><p>三个门控单元使用Sigmoid作为激活函数,生成候选记忆时，使用tanh作为激活函数。Sigmoid函数的输出在(0, 1)之间，符合门控的物理定义。使用 Tanh函数，是因为其输出在(-1,1)之间，这与大多数场景下特征分布是 0 中心的吻合，此外，Tanh函数在输入为0附近相比Sigmoid函数有更大的梯度，收敛更快。</p></blockquote></li></ol><h2 id="Seq2Seq-模型"><a href="#Seq2Seq-模型" class="headerlink" title="Seq2Seq 模型"></a>Seq2Seq 模型</h2><blockquote><p><a href="https://zhangruochi.com/Attention/2019/12/16/">https://zhangruochi.com/Attention/2019/12/16/</a></p></blockquote><ol><li><p>什么是 Seq2Seq 模型，Seq2Seq 模型有哪些优点？</p><blockquote><p>Seq2Seq模型的核心思想是，通过深度神经网络将输入序列映射为输出序列，这一过程由encoder 与 decoder 两个环节组成。在经典实现中，encoder 和 decoder 都是sequence model. encoder将序列编码成 context vector，decoder 将 context vector 解码成序列。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center></blockquote></li><li><p>Seq2Seq 模型在解码时，有哪些常用的办法？</p><blockquote><p>Seq2Seq 最基础的解码方法是贪心法，即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果，知道遇到结束符。但是贪心算法往往只能得到局部最优解。<br><strong>Beam search</strong> 是贪心算法的改进。改方法会保存beam size 个当前较好的选择，然后解码时每一步根据保存的选择进行下一步的扩展和排序，接着选择前b个进行保存，循环迭代，知道结束后选择最佳的一个座位解码结果。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center></blockquote></li><li><p>Seq2Seq 引入注意力机制是为了解决什么问题？为什么选用了双向循环神经网络模型？</p><blockquote><ol><li>随着输入序列的增长，Seq2Seq的性能发生显著性下降。这是因为编码时输入序列的全部信息压缩到一个 context vector。随着输入序列的增长，句子越前面的词丢失就越严重。Attention机制的引入就是为了解决这个问题。</li><li>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</li><li>机器翻译中，使用双向RNN是因为当前词的状态不仅决定于这个词之前的词，还决定于这个词之后的词。比如 I was a student two years ago.</li></ol></blockquote></li><li><p>如何计算attention score.</p><blockquote><ol><li>利用RNN结构得到encoder中的hidden state $(h_1,h_2,\cdots, h_n)$</li><li>假设当前decoder的hidden state 是$s_{t-1}$, 我们可以计算每一个输入位置j的 hidden state 与当前输出位置的关联性，$e_{ij} = a(s_{t-1}, h_j)$，其中 [公式] 是一种相关性的算符，例如常见的有dot product. 输出位置与所有的输入位置的关联性写成向量形式有 $\vec{e_t} = a(s_{t-1}, h_i), \cdots, a(s_{t-1}, h_T)$</li><li>对$\vec{e_t}$进行softmax操作，然后将其normalize得到attenion score分布$\alpha_{tj}$</li><li>利用 attention score 得到加权的context vector. $\vec{c_t} =\sum_{j=1}^{T}\alpha_{tj} h_j$<br>将加权的context vector 与 decoder 的 $h_t^{dec}$ 拼接在一起。</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      MLP, 神经网络训练技巧, CNN, RNN, Seq2Seq
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Optimization</title>
    <link href="https://zhangruochi.com/ML-Interview-Optimization/2020/05/27/"/>
    <id>https://zhangruochi.com/ML-Interview-Optimization/2020/05/27/</id>
    <published>2020-05-26T23:19:28.000Z</published>
    <updated>2020-05-28T23:04:38.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="监督学习的损失函数"><a href="#监督学习的损失函数" class="headerlink" title="监督学习的损失函数"></a>监督学习的损失函数</h2><ol><li>有监督学习涉及的损失函数有哪些？请列举并简述它们的特点。<blockquote><ol><li>MSE<script type="math/tex; mode=display">L = \sum( Y - f(x))^2</script></li><li>MAE<script type="math/tex; mode=display">L = \sum|Y - f(x)|</script></li><li>Hinge：Hinge损失不仅会惩罚错误的预测，还会惩罚不自信的正确预测。用于支持向量机(SVM)中。<script type="math/tex; mode=display">L_{hinghe}(f,y) = max\{0, 1-f_y\}</script></li><li>Binary Cross Entropy<script type="math/tex; mode=display">L = -y * log(p) - (1-y) * log(1-p)</script></li><li>Cross Entropy <script type="math/tex; mode=display">L(x_i,y_i) = -\sum_{j=1}^{e} y_{ij} * log(p_{ij})</script>where $Y_i$ is one-hot encoded target vector $(y_{i1},\cdots, y_{i2})$.<script type="math/tex; mode=display">y_{ij} = \left\{\begin{aligned}& 1 \quad \text{if i element is in class j} \\ & 0 \quad \text{otherwise}\end{aligned}\right.</script></li><li>Kullback-Leibler Divergence：表示两个概率分布的差异。Variational Auto-Encoder中使用。<script type="math/tex; mode=display">D_{KL}(p||q) = \sum_{i=1}^{N}p(x_i)\dot(log p(x_i) - log q(x_i))</script></li><li>Huber：结合 MSE 和 MAE 的优点<script type="math/tex; mode=display">L = \left\{ \begin{aligned}& \frac{1}{2}(y - f(x))^2, \quad if \ | y - f(x)| \leq \delta, \\ & \delta|y - f(x)| - \frac{1}{2}\delta^2, otherwise\end{aligned}\right.</script></li><li>Dice loss： 两个轮廓的相似度，应用在图像分割领域<script type="math/tex; mode=display">DL(A,B) = 2 \frac{A \cap B }{ |A| + |B|}</script></li></ol></blockquote></li></ol><h2 id="机器学习中的优化问题"><a href="#机器学习中的优化问题" class="headerlink" title="机器学习中的优化问题"></a>机器学习中的优化问题</h2><ol><li><p>机器学习中，哪些是凸优化问题？</p><blockquote><p>凸函数曲面上任意两点连接而成的线段，其上的任意一点都不会处于改函数曲面的下方。一个常用的机器学习模型，逻辑回归，对应的优化问题就是凸优化问题。因为我们可以求得优化函数的 Hessian矩阵是半正定的。</p><script type="math/tex; mode=display">L(\lambda x + (1-\lambda) y) \leq \lambda L(x) + (1 - \lambda)L(y)</script><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></blockquote></li><li><p>当数据量特别大时，经典的梯度下降算法有什么问题？</p><blockquote><p>经典的梯度下降算法在每次模型参数进行更新时，需要遍历所有的训练数据。当 M 很大时，这需要进行很大的计算。为了计算这个问题，随机梯度下降法用单个样本损失来近似所有样本的平均损失。为了降低随机梯度的方差，使得迭代更加稳定，一般使用小批量梯度下降算法。对于小批量下降法的使用，需要注意以下几点：</p><ol><li>不同应用中，每个 batch 的大小通常会不一样。一般选择 2 的幂次可以充分利用矩阵运算。</li><li>为了避免数据的特定顺序给算法收敛带来的影响，一般会在每次遍历数据之前，先对所有数据进行shuffle。</li><li>为了加快收敛速度，同时提高求解精度，通常采用衰减学习速率的方案：一开始算法采用较大的学习速率，当误差曲线进入平台期后，减小学习速率做更精细的调整。</li></ol></blockquote></li><li><p>请给出随机梯度下降算法失效的原因。</p><blockquote><p>随机梯度下降算法放弃了对梯度准确性的追求，每步仅仅采用一个（或少量）样本来估计当前梯度。但是由于每步接受的信息量有限，随机梯度下降法对梯度的估计常常出现偏差，造成目标函数收敛很不稳定，伴有剧烈的波动，有时甚至出现不收敛的情况。对于随机梯度下降法来说，最可怕的不是局部最优点，而是山谷和鞍点。鞍点就是一片平摊的区域，在梯度几乎为零的区域，随机梯度下降法无法计算出梯度的微小变化，导致在来回震荡。</p></blockquote></li><li><p>如何改进随机梯度下降法？（动量和环境感知）</p><blockquote><ol><li>Momentum（动量）：当来到鞍点处，在惯性作用下继续前行，则有机会冲出平坦的陷阱。动量法的收敛速度更快，收敛曲线也更稳定。实际上是对 gradient 做 moving average.<script type="math/tex; mode=display">\begin{aligned}& v_t = \eta v_{t-1} + \gamma g_t \\& \theta_{t+1} = \theta_t - v_t \end{aligned}</script></li></ol></blockquote></li><li><p>AdaGrad 方法</p><blockquote><p>随机梯度下降法对环境的感知是指在参数空间中，根据不同参数的一些经验性判断，自适应地确定参数的学习速率。例如在文本处理中训练 word embedding，有写词频繁出现，有些词极少出现，我们希望极少出现的词更新的步幅大一些。AdaGrad 采用<code>历史梯度平方和</code>来衡量不同参数的稀疏性，取值越小说明越稀疏。 具体的更新公式为:</p><script type="math/tex; mode=display">\theta_{t+1,i} = \theta_{t,i} - \frac{\gamma}{\sqrt{\sum_{k=0}^{t} g_{k,i}^2 + \epsilon}}</script></blockquote></li><li><p>Adam 方法</p><blockquote><p>Adam方法集惯性保持和环境感知两个优点于一身。一方面，Adam 记录梯度的 first moment，即过往梯度与当前梯度的平均，这体现了惯性保持；另一方面，Adam 还记录了梯度的 second moment，即过往梯度平方与当前梯度平方的平均，这类似 AdaGrad方法，体现环境感知能力,为不同参数产生自适应的学习速率。first and second monent 采用exponential decay average，使得时间久远的梯度对当前平均值的贡献呈指数衰减。</p><script type="math/tex; mode=display">\begin{aligned}& m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\& v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2 \\ & \hat{m_t} = \frac{m_t}{1 - \beta^t_1}   \\& \hat{v_t} = \frac{v_t}{1 - \beta^t_2} \\& \theta_{t+1} = \theta_t - \frac{ \gamma \cdot \hat{m_t} }{ \sqrt{\hat{v_t} + \epsilon} }\end{aligned}</script><p>其中$\beta_1$, $\beta_2$ 为衰减系数，$m_t$是 first moment, $v_t$ 是second moment.</p></blockquote></li></ol><p><a href="https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/">https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/</a></p><ol><li>L1 正则化与稀疏性原理是什么？<blockquote><p>带正则项和带约束条件是等价的，为了约束w的可能取值空间从而防止过拟合，我们为该最优化问题加上一个约束，就是w的 L2 范数不能大于m</p><script type="math/tex; mode=display">\begin{aligned} & \min sum_{i=1}^N(y_i - w^T x_i)^2 \\ & s.t. ||w||^2_2 \leq m\end{aligned}</script><p>为了求解带约束条件的凸优化问题，写出拉格朗日函数</p><script type="math/tex; mode=display">sum_{i=1}^N(y_i - w^T x_i)^2 + \lambda(||w||^2_2 - m)</script><p>L2正则化相当于为参数定义了一个圆形的解空间(因为必须保证L2范数不能大于m), 而 L1 正则化想当于定义了一个菱形的解空间。L1 的解空间显然更容易与目标函数的等高线在角点碰撞，从而产生稀疏解。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      损失函数，优化算法
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Unsupervised-Learning</title>
    <link href="https://zhangruochi.com/ML-Interview-Unsupervised-Learning/2020/05/26/"/>
    <id>https://zhangruochi.com/ML-Interview-Unsupervised-Learning/2020/05/26/</id>
    <published>2020-05-26T01:14:23.000Z</published>
    <updated>2020-05-28T22:49:19.611Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%"></center><h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><ol><li><p>简述 K-Means算法的具体步骤<br>输入是样本集$D=\{x_1,x_2,…x_m\}$,聚类的簇树k,最大迭代次数N。输出是簇划分$C=\{C_1,C_2,…C_k\}$</p><blockquote><ol><li>数据预处理,如归一化、离群点处理等<br>2.从数据集D中随机选择k个样本作为初始的k个质心向量：$\{\mu_1,\mu_2,…,\mu_k\}$</li><li>对于n=1,2,…,N<ul><li>将簇划分C初始化为 $C_t = \varnothing \;\; t =1,2…k$</li><li>计算样本$x_i$和各个质心向量$\mu_j(j=1,2,…k)$的距离: $d_{ij} = ||x_i - \mu_j||_2^2$，将$x_i$标记最小的为$d_{ij}$所对应的类别$\lambda_i$, 此时更新 $C_{\lambda_i} = C_{\lambda_i} \cup \{x_i\}$</li><li>对于j=1,2,…,k,对 $C_j$中所有的样本点重新计算新的质心$\mu_j = \frac{1}{|C_j|}\sum\limits_{x \in C_j}x$</li><li>如果所有的k个质心向量都没有发生变化，则转到步骤3）</li></ul></li><li>输出簇划分$C=\{C_1,C_2,…C_k\}$</li></ol></blockquote></li><li><p>简述K-Means++与 K-Means的区别</p><blockquote><p>K-Means中k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心。如果仅仅是完全随机的选择，有可能导致算法收敛很慢。K-Means++算法就是对K-Means随机初始化质心的方法的优化。K-Means++的对于初始化质心的优化策略也很简单，如下：</p><ol><li>从输入的数据点集合中随机选择一个点作为第一个聚类中心$u_1$</li><li>对于数据集中的每一个点$x_i$,计算它与已选择的聚类中心中最近聚类中心的距离<script type="math/tex; mode=display">D(x_i) = arg\;min||x_i- \mu_r||_2^2\;\;r=1,2,...k_{selected}</script></li><li>选择一个新的数据点作为新的聚类中心，选择的原则是：$D(x)$较大的点，被选取作为聚类中心的概率较大.</li><li>重复b和c直到选择出k个聚类质心</li><li>利用这k个质心来作为初始化质心去运行标准的K-Means算法</li></ol></blockquote></li><li><p>K-Means均值算法的缺点是什么？</p><blockquote><ol><li>K值的选取不好把握</li><li>对于不是凸的或者球形的数据集比较难收敛</li><li>如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。</li><li>采用迭代方法，得到的结果只是局部最优。</li><li>对噪音和异常点比较的敏感。</li></ol></blockquote></li><li><p>如何选取 K-Means 的 K 值？</p><blockquote><p>K 值的选择一般基于经验和多次试验结果。比如可以采用手肘法，我们可以尝试不同的 K 值，并将不同的 K 值所对应的损失函数画成折线。拐点就是 K 的最佳值。</p></blockquote></li><li><p>什么是 Kernel K- Means ?</p><blockquote><p>还童的欧式距离度量方式，使得 K 均值算法本质上假设了各个数据簇的数据呈现球形或者高维球形，这种分布在实际生活中不常见。面对非凸的数据分布时，引入核函数来进行非线性映射，将输入空间中的数据点映射到高维的特征空间，并在新的特征中空间进行聚类。非线性映射增加了数据点线性可分的概率。</p></blockquote></li></ol><h3 id="DBSCANS-（密度聚类）"><a href="#DBSCANS-（密度聚类）" class="headerlink" title="DBSCANS （密度聚类）"></a>DBSCANS （密度聚类）</h3><blockquote><p><a href="https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/">https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/</a></p></blockquote><h3 id="Birch-层次聚类"><a href="#Birch-层次聚类" class="headerlink" title="Birch (层次聚类)"></a>Birch (层次聚类)</h3><ol><li>什么是层次聚类? 层次聚类的步骤是什么？<blockquote><p>层次聚类不指定具体的簇数，而只关注簇之间的远近，最终会形成一个树形图。<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%"></center><br>根据聚类簇之间距离的计算方法的不同，层次聚类算法可以大致分为：单链接（Single-link）算法，全链接算法（complete-link）或均链接算法（average-link）。单链接算法用两个聚类簇中最近的样本距离作为两个簇之间的距离；而全链接使用计算两个聚类簇中最远的样本距离；均链接算法中两个聚类之间的距离由两个簇中所有的样本共同决定。</p><ol><li>每一个样本点视为一个簇；</li><li>计算各个簇之间的距离，最近的两个簇聚合成一个新簇；</li><li>重复以上过程直至最后只有一簇。</li></ol></blockquote></li></ol><h3 id="Gaussian-Mixed-Model-概率聚类"><a href="#Gaussian-Mixed-Model-概率聚类" class="headerlink" title="Gaussian Mixed Model (概率聚类)"></a>Gaussian Mixed Model (概率聚类)</h3><blockquote><p><a href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/">https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/</a></p></blockquote><ol><li>高斯混合模型的核心思想是什么？它是如何迭代计算的？<blockquote><p>高斯混合模型假设数据可以看作是从多个高斯分布中生成出来的。求解步骤如下:</p><ol><li>E step: 根据当前参数，计算每个点属于各个高斯分布的概率</li><li>M step: 使用上述 E step 求得的概率，计算每个高斯分布的加权平均参数。</li></ol></blockquote></li></ol><h3 id="聚类算法的评估"><a href="#聚类算法的评估" class="headerlink" title="聚类算法的评估"></a>聚类算法的评估</h3><ol><li>以聚类算法为例，假设没有外部标签数据，如何评估两个聚类算法的优劣？<blockquote><p>在无监督的情况下，我们可通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。</p><ol><li>轮廓系数：给定一个点p，改点的轮廓系数定义为<script type="math/tex; mode=display">s(p) = \frac{b(p) - a(p)}{max{a(p), b(p)}}</script>其中，$a(p)$是点$p$与同一簇中其他点$p\prime$之间的平均距离；$b(p)$是点$p$与另一不同簇中的点之间的最小平均距离（如果有n个簇，则只计算和点p最接近的一簇中的点与该点的平均距离). $a(p)$反应的是$p$所属的簇中数据的紧密程度，$b(p)$反应的是该簇与其他临近簇的分离程度。显然，$b(p)$越大，$a(p)$越小，对应的聚类的质量越好。</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      无监督学习，K-Means，DBSCANS，Birch，GMM
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Decomposition</title>
    <link href="https://zhangruochi.com/ML-Interview-Decomposition/2020/05/26/"/>
    <id>https://zhangruochi.com/ML-Interview-Decomposition/2020/05/26/</id>
    <published>2020-05-25T22:56:23.000Z</published>
    <updated>2020-05-28T22:52:58.124Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PCA-最大方差理论"><a href="#PCA-最大方差理论" class="headerlink" title="PCA 最大方差理论"></a>PCA 最大方差理论</h2><ol><li><p>如何定义主成分？从这种定义出发，如何设计目标函数使得降维达到提取主成分的目的？针对这个目标函数，如何对 PCA 问题进行求解？</p><blockquote><p>在信号处理领域，我们认为信号具有较大的方差，噪声具有较小的方差，信号与噪声之比成为信噪比。信噪比越大意味着数据的质量越好。x 投影之后的方差就是协方差矩阵的特征值，最佳投影方向也就是协方差矩阵最大的特征值。至此，<br>PCA 的求解方法为：</p><ol><li>对样本数据进行中心化处理</li><li>求样本的协方差矩阵</li><li>对协方差矩阵进行特征值分解，将特征值从大到小排列</li><li>去特征值前$d$大对应的特征向量$w_1,w_2,…,w_d$,<br>通过以下映射将n维样本映射到$d$维度。<script type="math/tex; mode=display">x_i\prime = \left[\begin{matrix}& w_1^{T}x_i \\& w_2^{T}x_i \\& w_3^{T}x_i \\& \cdots \\& w_d^{T}x_i \end{matrix}\right]</script></li></ol></blockquote></li><li><p>PCA 的缺点是什么？<br>在 PCA 中，算法没有考虑数据的标签（类别），只是把数据映射到一些方差比较大的方向而已。如下图，PCA 算法会把两个类别的数据映射到y轴，使得分类效果特别差。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></li></ol><h2 id="LDA-线性判别分析"><a href="#LDA-线性判别分析" class="headerlink" title="LDA 线性判别分析"></a>LDA 线性判别分析</h2><ol><li><p>对于具有类别标签的数据，映带如何设计目标函数使得降维的过程中不损失类别信息？在这种目标下，应当如何求解？</p><blockquote><p>投影后每类内部方差最小，类间方差最大<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><br>类内散度矩阵$s_w$:</p><script type="math/tex; mode=display">S_w = \Sigma_0 + \Sigma_1 = \sum\limits_{x \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum\limits_{x \in X_1}(x-\mu_1)(x-\mu_1)^T</script><p>类间散度矩阵$s_b$:</p><script type="math/tex; mode=display">S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T</script><p>LDA 的优化目标：</p><script type="math/tex; mode=display">\underbrace{arg\;max}_w\;\;J(w) = \frac{w^TS_bw}{w^TS_ww}</script></blockquote></li><li><p>LDA 算法的步骤是什么？</p><blockquote><ol><li>计算类内散度矩阵$S_w$</li><li>计算类间散度矩阵$S_b$</li><li>计算矩阵$S_w^{-1}S_b$</li><li>计算$S_w^{-1}S_b$的最大的d个特征值和对应的d个特征向量$(w_1,w_2,…w_d)$得到投影矩阵$W$.</li><li>对样本集中的每一个样本特征$x_i$,转化为新的样本$z_i=W^Tx_i$.</li></ol></blockquote></li><li><p>LDA 与 PCA 作为经典的降维算法，如何从应用的角度分析其原理的异同？</p><blockquote><p>从目标出发，PCA 选择的是投影后数据方差最大的方向，由于它是无监督的，因此 PCA 假设方差越大，信息量越多，用主成分来表示原始数据可以去除用于的维度，达到降维。而 LDA选择的是投影后类内方差小、类间方差大的方向。其用到了类别信息，为了找到数据中具有判别性的维度，使得原始数据在这些方向上投影后，不同类 jin尽可能区分开。举例来说，我们想从一段音频中提取人的语音信号，这时可以使用 PCA 先进行降维，过滤掉一些固定频率的北京噪声。但如果我们的需求是从这段音频中区分出声音属于哪个人，那么我们应该使用 LDA 对数据进行降维，使得每个人的语音信号具有区分性。<br><strong>从应用的角度，我们可以掌握一个基本的原则—对无监督的任务使用 PCA 进行降维，对有监督的则应用 LDA</strong></p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      Decomposition, PCA,LDA
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Classicial-Algorithms</title>
    <link href="https://zhangruochi.com/ML-Interview-Classicial-Algorithms/2020/05/25/"/>
    <id>https://zhangruochi.com/ML-Interview-Classicial-Algorithms/2020/05/25/</id>
    <published>2020-05-25T00:28:33.000Z</published>
    <updated>2020-05-28T22:48:42.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h2><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><blockquote><p>参考 <a href="https://zhuanlan.zhihu.com/p/35755150" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35755150</a></p></blockquote><p>SVM的基本型：</p><script type="math/tex; mode=display">min_{w,b} = \frac{1}{2} ||w||^{2}</script><script type="math/tex; mode=display">s.t. \quad y_i(w^T x_i + b) \leq 1, i=1,2,3...m</script><ol><li><p>空间上线性可分的两点，分别向svm的超平面做投影，投影的点在超平面上依然线性可分吗？</p><blockquote><p>一定线性不可分</p></blockquote></li><li><p>硬间隔和软间隔是指什么？</p><blockquote><p>SVM的基本形态是一个硬间隔分类器，它要求所有样本都满足硬间隔约束(即函数间隔要大于1)，所以当数据集有噪声点时，SVM为了把噪声点也划分正确，超平面就会向另外一个类的样本靠拢，这就使得划分超平面的几何间距变小，降低模型的泛化性能。除此之外，当噪声点混入另外一个类时，对于硬间隔分类器而言，这就变成了一个线性不可分的问题，于是就使用核技巧，通过将样本映射到高维特征空间使得样本线性可分，这样得到一个复杂模型，并由此导致过拟合（原样本空间得到的划分超平面会是弯弯曲曲的，它确实可以把所有样本都划分正确，但得到的模型只对训练集有效）。<br>为了解决上述问题，SVM通过引入松弛变量构造了软间隔分类器，它允许分类器对一些样本犯错，允许一些样本不满足硬间隔约束条件，这样做可以避免SVM分类器过拟合，于是也就避免了模型过于复杂，降低了模型对噪声点的敏感性，提升了模型的泛化性能。<br>因为松弛变量是非负的，因此样本的函数间隔可以比1小。函数间隔比1小的样本被叫做离群点，我们放弃了对离群点的精确分类，这对我们的分类器来说是种损失。但是放弃这些点也带来了好处，那就是超平面不必向这些点的方向移动，因而可以得到更大的几何间隔（在低维空间看来，分类边界也更平滑）。显然我们必须权衡这种损失和好处。</p></blockquote></li><li><p>松弛变量和惩罚因子是什么？</p><blockquote><p>松弛变量：松弛变量表示样本离群的程度，松弛变量越大，离群越远，松弛变量为零，则样本没有离群。<br>惩罚因子：惩罚因子表示我们有多重视离群点带来的损失，当C取无穷大时，会迫使超平面将所有的样本都划分正确，这就退化成了硬间隔分类器。</p></blockquote></li><li><p>拉格朗日乘子法是什么？</p><blockquote><p>拉格朗日乘数法是一种优化算法，主要运用于解决优化问题，它的基本思想就是用拉格朗日乘子构造一个新的优化函数将原本的约束优化问题转换成等价的无约束优化问题。</p></blockquote></li><li><p>什么是对偶问题?</p><blockquote><p>常一个优化问题可以从两个角度来考虑，即主问题(primal problem)和对偶问题(dual problem)。在约束最优化问题中，常常利用拉格朗日对偶性将原始问题（主问题）转换成对偶问题，通过解对偶问题来得到原始问题的解。这样做是因为对偶问题的复杂度往往低于主问题。</p></blockquote></li><li><p>什么是 kernel trick?</p><blockquote><p>$x_i$ 和 $x_j$ 在特征空间的內积等于它们在原始的样本空间通过 $k(x_i,x_j)$ 计算的结果。有了这样的函数，我们不必去计算高维甚至无穷维特征空间中的內积。 d<br>SVM 基本式的对偶问题为: </p><script type="math/tex; mode=display">max_{\alpha} \sum_{i=1}^{m}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_i y_j \Phi{x_i}^{T}\Phi_{x_j}</script><script type="math/tex; mode=display">s.t. \sum_{i=1}^{m}\alpha_i y_i = 0</script><script type="math/tex; mode=display">\alpha_i \geq 0, i = 1,2,...,m.</script></blockquote></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><ol><li><p>什么是逻辑回归？</p><blockquote><p>对数几率回归。对逻辑回归的公式进行整理，得到:</p><script type="math/tex; mode=display">log\frac{p}{1-p} = \theta^{T}x</script><script type="math/tex; mode=display">p = P(y=1 | x)</script><p>逻辑回归通过极大似然来得到最佳参数</p><script type="math/tex; mode=display">L(\theta) = \prod_{i:y_{i}=1}p(x_{i})\prod_{i^{\prime}:y_{i^{\prime}}=0}(1-p(x_{i^{\prime}}))</script></blockquote></li><li><p>使用逻辑回归处理多标签的分类问题时，有哪些常用做法？</p><blockquote><ol><li>如果一个样本只对应一个标签，那么可以使用 sofmax regression</li><li>当存在样本属于多个标签的情况，可以训练$i$个分类器，第$i$个分类器用以区分每个样本是否可以归为第i类。可以训练 softmax regression. 设定一个 threshold，判断每个类别的概率是否高于 threshold.</li></ol></blockquote></li></ol><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><ol><li><p>决策树有哪些启发函数？</p><blockquote><p>ID3（最大信息增益） 计算每个特征的信息增益，然后选择信息增益最大的特征来划分样本，完成决策树的增长。<br>C4.5（最大信息增益比）。<br>CART(最大基尼指数)</p></blockquote></li><li><p>信息熵、信息增益、信息增益比、最大基尼系数是什么？</p><blockquote><ol><li><strong>信息熵</strong> 是度量样本集合不确定度（纯度）的最常用的指标。<br>当前样本集合 D 中第 k 类样本所占的比例为 pk ，则 D 的信息熵定义为<script type="math/tex; mode=display">Ent(D) = - \sum_{K=1}^{|y|}p_k log_2^{p_k}</script></li><li><strong>信息增益</strong> 表示得知属性 a 的信息而使得样本集合不确定度减少的程度<br>假设离散属性 a 有 V 个可能的取值 {a1,a2,…,aV}；样本集合中，属性 a 上取值为 av 的样本集合，记为 Dv。<script type="math/tex; mode=display">Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{D^v}{D}Ent(D^v)</script>信息增益率 = 信息增益/IV(a),说明信息增益率是信息增益除了一个属性a的固有值得来的。<script type="math/tex; mode=display">IV(a) = -sum_{v=1}^{v}\frac{D^v}{D}log_2\frac{D^v}{D}</script><strong>Gini</strong>描述的是数据的纯度<script type="math/tex; mode=display">Gini(D) = 1 - sum_{k=1}^{n}(\frac{|C_k|}{|D|})^2</script>特征 A 的 Gini指数定义为:<script type="math/tex; mode=display">Gini(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}Gini(D_i)</script></li></ol></blockquote></li><li><p>ID3,C4.5,CART 各自的优缺点是什么？</p><blockquote><p>ID3倾向于取值较多的特征,因为信息增益放映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性越高，也就是条件熵越小、信息增益越大。<br>C4.5实际上是对 ID3 进行优化，通过引入信息增益比，一定程度上对取值较多的特征进行惩罚、避免 ID3 出现过拟合。<br>CART 与 ID3,C4.5不同，它是一颗二叉树，采用二元分割法，每一步将数据按照特征 A 的取值切成两份，分别进入左右子树。</p></blockquote></li><li><p>Cart 在做 regression 和 classification 的区别是？</p><blockquote><p> 在分类问题中，CART 使用基尼指数（Gini index）作为选择特征（feature）和划分（split）的依据；在回归问题中，CART 使用 mse（mean square error）或者 mae（mean absolute error）作为选择 feature 和 split 的 criteria。<br>在分类问题中，CART的每一片叶子都代表的是一个class；在回归问题中，CART 的每一片叶子表示的是一个预测值，取值是连续的。预测值一般是该片叶子所含训练集元素输出的均值。</p></blockquote></li><li><p>决策树如何进行剪枝？</p><blockquote><p>预剪枝：1. 当树到达一定深度时，停止树的生长；2.当到达当前节点的样本数量小于某个阈值时，停止树的生长；3. 计算每次分裂时测试集的准确度提升，当小于某个阈值时不再继续扩展。<br>后剪枝：后剪枝的方法有很多，比如代价复杂度剪枝、悲观剪枝、最小误差剪枝等。</p></blockquote></li></ol><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><ol><li><p>简述朴素贝叶斯的原理。</p><blockquote><p>朴素贝叶斯采用<code>属性条件独立性</code>的假设，对于给定的待分类观测数据X, 计算在X出现的条件下，各个目标类出现的概率（即后验概率）， 将该后验概率最大的类作为X所属的类。方法是根据已有样本进行贝叶斯估计学习出先验概率$P(Y)$和条件概率$P(X|Y)$，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解后验概率P(Y|X).</p></blockquote></li><li><p>朴素贝叶斯“朴素”在哪里？</p><blockquote><p>利用贝叶斯定理求解联合概率P(XY)时，需要计算条件概率P(X|Y)。在计算P(X|Y)时，朴素贝叶斯做了一个很强的条件独立假设（当Y确定时，X的各个分量取值之间相互独立），即</p><script type="math/tex; mode=display">P(X_1=x_1,X_2=x_2,\cdots,X_j=x_j|Y=y_k) = P(X_1=x_1|Y=y_k) * P(X_2=x_2|Y=y_k),\cdots,P(X_j=x_j|Y=y_k)</script></blockquote></li><li><p>什么是拉普拉斯平滑法?</p><blockquote><p>拉普拉斯平滑法是朴素贝叶斯中处理零概率问题的一种修正方式。在进行分类的时候，可能会出现某个属性在训练集中没有与某个类同时出现过的情况，如果直接基于朴素贝叶斯分类器的表达式进行计算的话就会出现零概率现象。为了避免其他属性所携带的信息被训练集中未出现过的属性值“抹去”，所以才使用拉普拉斯估计器进行修正。具体的方法是：在分子上加1,对于先验概率，在分母上加上训练集中可能的类别数；对于条件概率，则在分母上加上第i个属性可能的取值数</p></blockquote></li><li><p>朴素贝叶斯中有哪些不同的模型？</p><blockquote><p>朴素贝叶斯含有3种模型，分别是<strong>高斯模型</strong>，对连续型数据进行处理；<strong>多项式模型</strong>，对离散型数据进行处理，计算数据的条件概率(使用拉普拉斯估计器进行平滑的一个模型)；<strong>伯努利模型</strong>，伯努利模型的取值特征是布尔型，即出现为ture,不出现为false,在进行文档分类时，就是一个单词有没有在一个文档中出现过。</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      SVM, Logistic Regression, Decision Tree
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Feature-Engineering-and-Evaluation</title>
    <link href="https://zhangruochi.com/ML-Interview-Feature-Engineering-and-Evaluation/2020/05/24/"/>
    <id>https://zhangruochi.com/ML-Interview-Feature-Engineering-and-Evaluation/2020/05/24/</id>
    <published>2020-05-24T08:02:06.000Z</published>
    <updated>2020-05-29T00:18:35.014Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><ol><li><p>为什么需要对数值类型的特征做归一化？</p><blockquote><p>常用的归一化有：Min Max Scaler / Z-Score<br>当特征的 range 不同时，归一化特征可以加快梯度下降收敛的速度。PCA 等算法的假设有数据是均值均值为0,方差为1. </p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></blockquote></li><li><p>应该怎样处理类别特征？</p><blockquote><p>Ordinal Encoding<br>One-hot Encoding<br>Binary Encoding</p></blockquote></li><li><p>什么是特征组合，如何处理高维组合特征？</p><blockquote><p>两个或多个特征组合在一起$(x1,\cdots, xn)$ 等形成组合特征.<br>特征选择，矩阵分解，PCA.</p></blockquote></li><li><p>怎样有效地找到组合特征</p><blockquote><ol><li>决策树从根节点到叶子结点的路径可以看成一种特征组合的方式<br><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><br>Filter, wrapper, embedding 等方法进行特征选择，形成组合特征。</li></ol></blockquote></li><li><p>有哪些文本表示模型，它们各自的优缺点是什么？</p><blockquote><p>Bag of Word, 常用 TF-IDF表示词的权重（term frequency and inverse document frequency）; N-gram. 提取词组; 因为相同的词可能有多种表示，经常会做词干提取word stemming; 主题模型（得到每个主题上词的分布特征）; Word embedding ; Contextual word embeddings</p></blockquote></li><li><p>word2vec是如何工作的</p><blockquote><p>参考 <a href="https://zhangruochi.com/Word-Vectors/2019/12/04/">https://zhangruochi.com/Word-Vectors/2019/12/04/</a><br>CBOW 根据上下文来预测中心词，Skip-gram根据中心词来预测上下文。 CBOW 和 Skip-gram 都是由三层的神经网络组成。输入层为N维 one-hot encoding，隐藏层为 K 维。则输入层和隐藏层的 weight matrix （N*K）就是 embedding vector. word vector 可以由one-hot encoding 与 weight matrix 相乘得到。隐藏层到输出层的weightg matrix 为 （K*N）.输出也是一个N维向量，则可以根据softmax来求每个词的概率，然后应用梯度下降。<br>由于softmax需要对所有词进行遍历，计算量大。此时可以使用negtive sampling 或者 hierarchical softmax.</p></blockquote></li><li><p>LSA（Latent Semantic Analysis） 算法是怎样工作的？</p><blockquote><p>LSA 算法先统计 term-document矩阵（矩阵的每个元素为tf-idf）进行奇异值分解，从而得到term的向量表示和document的向量表示. 其算法的基本流程是：</p><ol><li>分析文档集合，建立词汇-文本矩阵A</li><li>对词汇-文本矩阵进行奇异值分解</li><li>对SVD分解后的矩阵进行降维</li><li>使用降维后的矩阵构建潜在语义空间</li></ol></blockquote></li><li><p>Glove 是怎样工作的？</p><blockquote><ol><li>Construct co-occurrence Matrix</li><li>Construct relationships between word vectors and co-occurrence Matrix<ul><li>Let X denote the word-word co-occurrence matrix, where $X_{ij}$ indicates the number of times word j occur in the context of word i</li><li>$w_{i}$,$\tilde{w_{j}}$ is the word vector</li><li>$b_i,b_j$ is the bias term<script type="math/tex; mode=display">w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} = \log(X_{ij}) \tag{1}</script></li></ul></li><li>Construct loss function: Mean Square Loss<script type="math/tex; mode=display">J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2</script><script type="math/tex; mode=display">f(x)=\begin{equation} \begin{cases} (x/x_{max})^{\alpha}  & \text{if} \ x < x_{max} \\ 1 & \text{otherwise} \end{cases} \end{equation}</script></li></ol></blockquote></li><li><p>LSA, word2vec, 以及 Glove 的区别于联系？</p><blockquote><p>LSA和word2vec作为两大类方法的代表，一个是利用了全局特征的矩阵分解方法，一个是利用局部上下文的方法。GloVe模型就是将这两中特征合并到一起的，即使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。</p></blockquote></li><li><p>图像分类时，训练数据不足如何处理。</p><blockquote><p>数据不足有过拟合风险，或者模型不能收敛。</p><ol><li>可以使用降低过拟合风险的措施。如l1/l2,继承学习,dropout 等</li><li>Data augmentation （旋转、平移、缩放、像素扰动、颜色变换、清晰度、对比度等）</li><li>Fine tuing or transfer learning</li><li>生成对抗模型生成新样本</li><li>对图像进行特征提取，使用传统的机器学习模型。</li></ol></blockquote></li></ol><h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><ol><li><p>准确率的局限性是什么？</p><blockquote><p>当正负数据不平衡时会失去意义</p></blockquote></li><li><p>Precision 和 Recall 怎样权衡？</p><blockquote><p>Precision 是指分类正确的正样本/模型预测的正样本, Recall 是指分类正确的正样本/实际的正样本。P-R 曲线横轴是recall，纵轴是precision。P-R 曲线是将阙值从高到低滑动画出的。<br>使用 P-R 曲线来综合判定两个模型的好坏。 F1 和 ROC 也能反应排序模型的好坏。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center></blockquote></li><li><p>RMSE 的局限是什么？怎样解决？</p><blockquote><p>如果存在个别偏离程度大的异常值，RMSE的效果会很差。</p><ol><li>数据预处理清理 outlier； </li><li>建模考虑异常机制，如异常点检测；</li><li>使用更合适的指标如 MAPE</li></ol></blockquote></li><li><p>什么是 ROC 曲线？</p><blockquote><p>横轴是 FPR（FP/N）, 纵轴是 TPR（TP/P）。绘制 ROC 曲线，需要将模型的输出概率从大到小排序，然后动态地选择阈值。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center></blockquote></li><li><p>AUC如何计算？</p><blockquote><p>AUC 是 ROC 曲线下的面积大小，计算时只用沿着ROC 曲线做积分就行了。AUC取值一般在[0.5,1]之间，越大越好。</p></blockquote></li><li><p>ROC 曲线相比 P-R 曲线有什么特点</p><blockquote><p>当正负样本的分布发生明显变化时，ROC曲线基本不变。因此 ROC 适用的场景更多。如下图是将负样本的数量增加 10 倍之后的结果。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center></blockquote></li><li><p>为什么在一些场景中需要使用余弦相似度而不是欧氏距离。</p><blockquote><p>余弦相似度只关注向量的夹角，并不关心向量的绝对值大小，范围为[-1,1]。比如在度量两个文本的相似度时，以词频和词向量最为特征。文本越长则欧式距离一定越大，但是余弦相似度则可以保持不变。总的来说，关注相对差异，使用余弦相似度。关注数值绝对差异，使用欧式距离。</p></blockquote></li><li><p>如何进行线上 A/B 测试？</p><blockquote><p>用户分桶，在分桶过程中一定要保证独立性和采样的无偏性。</p></blockquote></li><li><p>为什么在进行了离线评估后还要进行线上评估？</p><blockquote><ol><li>离线评估无法完全消除过拟合的影响。</li><li>离线评估无法完全还原线上的工程环境。</li><li>线上系统的某些商业指标无法在离线环境中还原，如用户点击率，留存时长等。</li></ol></blockquote></li><li><p>模型评估时，有哪些主要的验证方法，他们的优缺点是什么？</p><blockquote><ol><li>holdout. 在验证集上计算出的评估指标与原始分组有很大的关系。</li><li>k-fold. 把k次评估的平均值作为最终的评估指标。</li><li>留一法. 每次留下 1 个样本作为验证集。开销大，实际工程中较少使用。</li><li>自助法. 基于自助采样的方法，对于总数为n的样本集合，进行n次有放回的随机采样，得到大小为n的训练集，没有被采样的样本作为测试集。</li></ol></blockquote></li><li><p>超参数有哪些调优方法？</p><blockquote><ol><li>Grid Search。 十分消耗计算资源和时间，一般先使用较广的搜索范围和较大的步长，或者先确定对模型影响最大的参数。</li><li>随机搜索。 业界公认的Random search效果会比Grid search好。 例如前面的场景A有2种选择、B有3种、C有5种、连续值随机采样，那么每次分别在A、B、C中随机取值组合成新的超参数组合来训练。虽然有随机因素，但随机搜索可能出现效果特别差、也可能出现效果特别好，在尝试次数和Grid search相同的情况下一般最值会更大，当然variance也更大但这不影响最终结果。</li><li>贝叶斯优化算法. 是基于数据使用贝叶斯定理估计目标函数的后验分布，然后再根据分布选择下一个采样的超参数组合。它充分利用了前一个采样点的信息，其优化的工作方式是通过对目标函数形状的学习，并找到使结果向全局最大提升的参数</li></ol></blockquote></li><li><p>过拟合、欠拟合具体是指什么现象？</p><blockquote><p>过拟合是指数据拟合过当，模型在训练集上表现好，但是测试集和新数据上表现差。欠拟合是模型在训练集和测试集上都表现不好。</p></blockquote></li><li><p>能否说出集中降低过拟合和欠拟合风险的方法？</p><blockquote><p>降低过拟合：获取更多数据、降低模型复杂度、正则化、集成学习<br>降低欠拟合：添加新特征、增加模型负责度、减少正则化系数</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      特征工程,模型评估相关问题
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Time and Ordering</title>
    <link href="https://zhangruochi.com/Time-and-Ordering/2020/05/09/"/>
    <id>https://zhangruochi.com/Time-and-Ordering/2020/05/09/</id>
    <published>2020-05-08T17:10:01.000Z</published>
    <updated>2020-05-09T05:36:27.341Z</updated>
    
    <content type="html"><![CDATA[<p>分布式系统和传统的单机系统不同，彼此是通过网络而不是”主板”连接、消息通讯是不可靠的。因此如果没有任何同步机制，同一系统的成员之间无法确保时间戳的误差控制在某个范围内。这个基本条件的缺失，会给上层应用的设计带来很多的麻烦。比如，一个业务流程的两个阶段分别在两台机器上处理，而后在第三台机器上将处理记录join起来，就可能因为时间戳的问题引发混乱。如何做好时间同步的协议，成为了分布式系统中的一个基本的问题。</p><p>在系统对时的时候，有两类基本的协议，第一个是外部对时，简单的说，就是整个分布式系统中的所有成员，与外部某个指定的源头进行时间同步，确保与源头的时间的diff在某个误差范围$D$内; 另一种是内部对时，即内部通过广播等各种手段，确保系统内的成员俩俩间的时间误差在一定范围内。从这里可以看出，如果一个集群使用了外部对时，控制误差在$D$以内，那么这个集群内部的时间的误差，也一定能够控制在$2D$的范围内。但反过来不一定，因为有可能整个集群与外部的时间存在很大的整体偏差，尽管在内部彼此的偏差很小。</p><p>那么如何进行时间的同步呢？这里介绍两个经典的协议：Cristian和NTP。</p><h2 id="Cristian"><a href="#Cristian" class="headerlink" title="Cristian"></a>Cristian</h2><p>Cristian的基本过程是这样的，假定现在P进程要从授时服务器S获取时间，那么最朴素的做法就是P向S发送请求，S将自己的时间t返回给P，而后P设置自己的时间为t。这个做法存在一个很关键的问题，就是由于网络的通讯时间是不确定的，P拿到t的时候，已经经过了不确定多久了，无法估计结束后P与S的时间误差范围。因此，我们需要将网络通讯的时间，即RTT(Rount Trip Time)也考虑进来。在这个场景下，RTT指的是P进程发出请求，到得到S的回应消息的时间差，这个时间差是P进程自己可以记录求得的。假定我们知道从 $P \to S$的最小延时是 $min_1$, $S \to P$的最小延时是$min_2$,那么，我们可以推断，真实的时间在$[t+min_2, t+RTT-min_1]$间内，Cristian的做法就将对时结果设置为：$t’=t+\frac{RTT+min_2-min_1}{2}$ 这个中间位置上。那么，其误差就能控制在$\pm \frac{RTT-min_1-min_2}{2}$ 的范围内。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><h2 id="NTP"><a href="#NTP" class="headerlink" title="NTP"></a>NTP</h2><p>另外一个知名的时间同步协议是NTP，全称Network Time Protocol。NTP协议一般在某个大的机构内部署，将机构内的设备组织成树形结构，每个节点都从父节点处获取时间。整个同步过程分为两轮，第一轮父节点记录自己发送返回的时间点$ts_1$，子节点记录自己接收到返回消息的时间 $tr_1$；而后第二轮，子节点记录自己的发送时间$ts_2$；；父节点记录收到请求的时间$tr_2$后将$ts_1$ 和 $tr_2$返回。那么子节点可以计算出自己和父节点之间的时间偏差为: $o=\frac{(tr_{1}-tr_{2}+ts_{2}-ts{1})}{2}$，并以此为依据进行修正(一般需要确保时间不能“倒流”)。那么为什么$o$是这么计算呢？假定子节点与父节点的时间偏差(offset)为$o\prime$、父节点往子节点的通讯时延为$L_1$、子节点往父节点的通讯时延为$L_2$，那么:</p><script type="math/tex; mode=display">\begin{align*} & tr_{1}=ts_{1}+L_{1}+o' \\& tr_{2}=ts_{2}+L_{2}-o' \\\end{align*}</script><p>相减可以得到:</p><script type="math/tex; mode=display">o'= \frac{(tr_1-tr_2+ts_2-ts_1)}{2} + \frac{(L_2 - L1)}{2} = o + \frac{(L_2-L1)}{2}</script><p>因此:</p><script type="math/tex; mode=display">\lvert o'-o \rvert \leqslant \lvert \frac{(L_2-L_1)}{2}\rvert < \frac{(L_{1}+L_{2})}{2} = \frac{RTT}{2}</script><p>由此可知o的这个值也在RTT相关的一个误差范围内，是可估计的。<br>从上面两个协议可以看出，对时的误差是与RTT强相关的。由于消息的传递受制于光速、距离越远时间准确度的保证就越差。对于那些假定了时间误差在某个范围内的分布式协议，在跨越距离很大的时候，我们就必须要将这个误差对系统的影响考虑在内，这将显著增加分布式系统设计的复杂度、或者影响设计出来的系统的吞吐(尤其是有高一致性要求的事务型系统)。</p><p>最后，不论是Cristian还是NTP，都只描述了一次对时如何将时间的偏移(clow skew)控制在一定范围内。由于不同机器的时钟的行进速度(clock drift)是不同的，因此我们需要每隔一段时间，进行一次修正，以消除时钟节奏不同的影响。多久需要做一次同步呢? 这个做一个简单的计算就可以得到。假定系统整体时钟的行进速率与标准时钟的速率小于MDR(Max Drift Rate, 一般由时钟的实现方式决定)，那么系统内俩俩时钟的行进速率差小于2MDR。如果我们要求系统内时间差不能超过M，那就必须以不低于$\delta = \frac{M}{2 \times {MDR}}$的间隔进行时间同步。在现实的系统中，我们需要计算合理的M，以避免系统内出现过多的时间同步消息。</p><p>在上面部分，我们谈到了分布式系统里进程彼此的物理时间是如何进行同步的，并介绍了一些经典的时间同步算法。但静下心来仔细想想，我们希望进行时间同步，很多时候是希望不同的进程，对系统内事件的顺序达成一致。至于是否是使用真实世界的那个时间来排序，往往并不是那么重要。<br>那么，如何在一个分布式系统中，对发生在众多节点上的事件进行定序呢？目前已知的做法包括以下几种：</p><ul><li>使用物理时间同步的方法，确保众多节点的时间偏差在某个范围内。而后记录事件的发生时间及理论误差范围，比如将每个事件的发生时间登记为$(t \pm \Delta)$如果两个事件的时间范围没有overlap，那么就自然的可以排序判断；否则，则需要引入一个新的排序规则(比如以节点id)，对这两个事件约定一个排序。spanner中采用了这种方式。</li><li>采用Lamport Timestamp及其引申算法进行定序，确保事件满足causality consistency的性质，成为后续更高层次的分布式算法设计的基础。本文后面主要将展开这类算法，并引出分布式系统中一些基础概念。这些基础概念是理解分布式共识问题(consensus problem)的基础。</li></ul><h2 id="Lamport"><a href="#Lamport" class="headerlink" title="Lamport"></a>Lamport</h2><p>为明确这个问题，我们首先需要先对事件的序(happen-before)做出一个定义。在Lamport的体系中，事件的先后关系是按照如下原则设定的：</p><ul><li>规则一：如果A、B两个事件都发生在同一个进程内，那么，A、B之间的序自然可以由这个进程给出。假如进程先执行了A后执行了B，那么可以说A在B之前发生，记为$A \prec B$;</li><li>规则二：如果进程x往进程y发送了一条消息M；设在进程x的消息发送事件为A，进程y收到消息的事件为B，则显然我们应当认为A在B之前发生，同样记为$A \prec B$.</li></ul><p>由此引出了Lamport timestamp的算法，这个算法就是一种给事件打上逻辑时间戳、确保其满足causality的基本属性。这个算法的基本过程为：</p><ul><li>每个进程都记录自己的一个当前时间戳，初始的时候，大家都是0</li><li>如果进程内部发生了一个新的事件，那么将当前时间戳记为 $t’=t+1$，并认为事件发生于$t’$时刻</li><li>如果进程A向进程B通讯，则发送消息的时候，进程A的时间戳$t’_A = t_A + 1$并随消息发送到B，B更新自己的时间戳为$t’_B = max(t’_B, t’_A) + 1$.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><h3 id="Concurrent-Events"><a href="#Concurrent-Events" class="headerlink" title="Concurrent Events"></a>Concurrent Events</h3><ul><li>A pair of concurrent events doesn’t have a causal path from one event to another (either way, in the pair)</li><li>Lamport timestamps not guaranteed to be ordered or unequal for concurrent events</li><li>Ok, since concurrent events are not causality related!</li><li>Remember</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">E1 -&gt; E2 =&gt; timestamp(E1) &lt; timestamp (E2), </span><br><span class="line">BUT timestamp(E1) &lt; timestamp (E2) =&gt; &#123;E1 -&gt; E2&#125; OR &#123;E1 and E2 concurrent&#125;</span><br></pre></td></tr></table></figure><h2 id="Vector-timestamps"><a href="#Vector-timestamps" class="headerlink" title="Vector timestamps"></a>Vector timestamps</h2><ul><li>Used in key-value stores like Riak</li><li>Each process uses a vector of integer clocks</li><li>Suppose there are N processes in the group 1…N</li><li>Each vector has N elements</li><li>Process i maintains vector Vi[1…N]</li><li>$j_{th}$ element of vector clock at process $i$, $V_i[j]$, is $i’s$ knowledge of latest events at process $j$</li></ul><p>Incrementing vector clocks</p><ol><li>On an instruction or send event at process $i$, it increments only its $i_{th}$ element of its vector clock.</li><li>Each message carries the send-event’s vector timestamp V_{message}[1…N]</li><li>On receiving a message at process $i$:</li></ol><script type="math/tex; mode=display">\begin{align*} &V_i[i] = V_i[i] + 1 \\& V_i[j] = max(V_{message}[j], V_i[j]) \quad for \ quad j \neq i \\\end{align*}</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center><h3 id="Causality"><a href="#Causality" class="headerlink" title="Causality"></a>Causality</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://lvsizhe.github.io/course/2018/09/time-in-distributed-systems-part1.html" target="_blank" rel="noopener">https://lvsizhe.github.io/course/2018/09/time-in-distributed-systems-part1.html</a></li><li>lecture slide from <a href="https://www.coursera.org/learn/cloud-computing/lecture/dy8wf/2-5-vector-clocks" target="_blank" rel="noopener">https://www.coursera.org/learn/cloud-computing/lecture/dy8wf/2-5-vector-clocks</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;分布式系统和传统的单机系统不同，彼此是通过网络而不是”主板”连接、消息通讯是不可靠的。因此如果没有任何同步机制，同一系统的成员之间无法确保时间戳的误差控制在某个范围内。这个基本条件的缺失，会给上层应用的设计带来很多的麻烦。比如，一个业务流程的两个阶段分别在两台机器上处理，而
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Distributed &amp; Cloud Computing" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Distributed-Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>Programming Language: New Types, Pattern Matching, Tail Recursion</title>
    <link href="https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/05/03/"/>
    <id>https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/05/03/</id>
    <published>2020-05-02T19:29:05.000Z</published>
    <updated>2020-05-03T07:29:53.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Conceptual-Ways-to-Build-New-Types"><a href="#Conceptual-Ways-to-Build-New-Types" class="headerlink" title="Conceptual Ways to Build New Types"></a>Conceptual Ways to Build New Types</h2><p>To create a compound type, there are really only three essential building blocks. Any decent programming language provides these building blocks in some way:</p><ul><li><strong>Each-of</strong>: A compound type t describes values that contain each of values of type t1, t2, …, and tn. Tuples are an example: int * bool describes values that contain an int and a bool. A <strong>Java class</strong> with fields is also an each-of sort of thing.</li><li><strong>One-of</strong>: A compound type t describes values that contain a value of one of the types t1, t2, …, or tn. For a type that contains an int or a bool in ML, we need <code>datatype bindings</code>. In object-oriented languages with classes like Java, one-of types are achieved with <strong>subclassing</strong>, but that is a topic for much later in the course.</li><li><strong>Self-reference</strong>: A compound type t may refer to itself in its definition in order to describe recursive data structures like lists and trees. This is useful in combination with each-of and one-of types. For example, int list describes values that either contain nothing or contain an int and another int list. </li></ul><h2 id="Records-Another-Approach-to-Each-of-Types"><a href="#Records-Another-Approach-to-Each-of-Types" class="headerlink" title="Records: Another Approach to Each-of Types"></a>Records: Another Approach to <strong>Each-of</strong> Types</h2><p>Record types are “each-of” types where each component is a <code>named field</code>.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;foo : <span class="built_in">int</span>, bar : <span class="built_in">int</span>*<span class="built_in">bool</span>, baz : <span class="built_in">bool</span>*<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><p>In ML, we do not have to declare that we want a record type with particular field names and field types — we just write down a record expression and the type-checker gives it the right type.</p><p>Now that we know how to build record values, we need a way to access their pieces. For now, we will use <code>#foo e</code> where <code>foo</code> is a field name. </p><h3 id="The-truth-of-tuple"><a href="#The-truth-of-tuple" class="headerlink" title="The truth of tuple"></a>The truth of tuple</h3><p>In fact, this is how ML actually defines tuples: A tuple is a record. That is, all the syntax for tuples is just a convenient way to write down and use records. The REPL just always uses the tuple syntax where possible, so if you evaluate {2=1+2, 1=3+4} it will print the result as (7,3). Using the tuple syntax is better style, but we did not need to give tuples their own semantics: we can instead use the “another way of writing” rules above and then reuse the semantics for records.</p><p>This is the first of many examples we will see of <code>syntactic sugar</code>. We say, Tuples are just syntactic sugar for records with fields named 1, 2, …, n.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> z = (<span class="number">3</span>,<span class="number">7</span>) : <span class="built_in">int</span> * <span class="built_in">int</span></span><br><span class="line"><span class="keyword">val</span> z = &#123;<span class="number">1</span>=<span class="number">3</span>,<span class="number">3</span>=<span class="number">7</span>&#125; : &#123;<span class="number">1</span>:<span class="built_in">int</span>, <span class="number">3</span>:<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="Datatype-Bindings-Our-Own-One-of-Types"><a href="#Datatype-Bindings-Our-Own-One-of-Types" class="headerlink" title="Datatype Bindings: Our Own One-of Types"></a>Datatype Bindings: Our Own <strong>One-of</strong> Types</h2><p>We now introduce datatype bindings, our third kind of binding after variable bindings and function bindings.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">datatype mytype = TwoInts of int * int</span><br><span class="line">                | Str of string</span><br><span class="line">                | Pizza</span><br></pre></td></tr></table></figure><p>Roughly, this defines a new type where values have an int <em> int or a string or nothing. Any value will also be <code>tagged</code> with information that lets us know which variant it is: These tags, which we will call <em>*constructors</em></em>, are <code>TwoInts</code>, <code>Str</code>, and <code>Pizza</code>.</p><p>More precisely, the example above adds four things to the environment:</p><ul><li>A new type mytype that we can now use just like any other type</li><li>Three constructors TwoInts, Str, and Pizza</li></ul><p>A constructor is two different things. First, it is either a function for creating values of the new type (if the variant has of t for some type t) or it is actually a value of the new type (otherwise). In our example, TwoInts is a function of type int*int -&gt; mytype, Str is a function of type string-&gt;mytype, and Pizza is a value of type mytype. Second, we use constructors in case-expressions as described further below.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> mytype = <span class="type">TwoInts</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span> </span><br><span class="line">                | <span class="type">Str</span> <span class="keyword">of</span> <span class="built_in">string</span> </span><br><span class="line">                | <span class="type">Pizza</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a = <span class="type">Str</span> <span class="string">"hi"</span></span><br><span class="line"><span class="keyword">val</span> b = <span class="type">Str</span></span><br><span class="line"><span class="keyword">val</span> c = <span class="type">Pizza</span></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">TwoInts</span>(<span class="number">1</span>+<span class="number">2</span>,<span class="number">3</span>+<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> e = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">(* val a = Str "hi" : mytype</span></span><br><span class="line"><span class="comment">val b = fn : string -&gt; mytype</span></span><br><span class="line"><span class="comment">val c = Pizza : mytype</span></span><br><span class="line"><span class="comment">val d = TwoInts (3,7) : mytype</span></span><br><span class="line"><span class="comment">val e = Str "hi" : mytype *)</span></span><br></pre></td></tr></table></figure><h2 id="How-ML-Provides-Access-to-Datatype-Values-Case-Expressions"><a href="#How-ML-Provides-Access-to-Datatype-Values-Case-Expressions" class="headerlink" title="How ML Provides Access to Datatype Values: Case Expressions"></a>How ML Provides Access to Datatype Values: Case Expressions</h2><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> f x = <span class="comment">(* f has type mytype -&gt; int*)</span> </span><br><span class="line">    <span class="keyword">case</span> x <span class="keyword">of</span></span><br><span class="line">      <span class="type">Pizza</span> =&gt; <span class="number">3</span></span><br><span class="line">    | <span class="type">TwoInts</span>(i1,i2) =&gt; i1 + i2 </span><br><span class="line">    | <span class="type">Str</span> s =&gt; <span class="type">String</span>.size s</span><br></pre></td></tr></table></figure><p>In one sense, a case-expression is like a more powerful if-then-else expression: Like a conditional expression, it evaluates two of its subexpressions: first the expression between the case and of keywords and second the expression in the first branch that matches. But instead of having two branches (one for true and one for false), we can have one branch for each variant of our datatype (and we will generalize this further below). Like conditional expressions, each branch’s expression must have the same type (int in the example above) because the type-checker cannot know what branch will be used.<br>Each branch has the form <code>p =&gt; e</code> where p is a pattern and e is an expression, and we separate the branches with the | character. Patterns look like expressions, but do not think of them as expressions. Instead they are used to match against the result of evaluating the case’s first expression (the part after case). This is why evaluating a case-expression is called pattern-matching.</p><h3 id="Datatype-Bindings-and-Case-Expressions-So-Far-Precisely"><a href="#Datatype-Bindings-and-Case-Expressions-So-Far-Precisely" class="headerlink" title="Datatype Bindings and Case Expressions So Far, Precisely"></a>Datatype Bindings and Case Expressions So Far, Precisely</h3><p>We can summarize what we know about datatypes and pattern matching so far as follows: The binding</p><blockquote><p>datatype t = C1 of t1 | C2 of t2 | … | Cn of tn</p></blockquote><p>introduces a new type t and each constructor Ci is a function of type ti-&gt;t. One omits the “of ti” for a variant that “carries nothing” and such a constructor just has type t. To “get at the pieces” of a t we use a case expression:</p><blockquote><p>case e of p1 =&gt; e1 | p2 =&gt; e2 | … | pn =&gt; en</p></blockquote><p>A case expression evaluates e to a value v, finds the first pattern pi that matches v, and evaluates ei to produce the result for the whole case expression. So far, patterns have looked like Ci(x1,…,xn) where Ci is a constructor of type t1 <em> … </em> tn -&gt; t (or just Ci if Ci carries nothing). Such a pattern matches a value of the form Ci(v1,…,vn) and binds each xi to vi for evaluating the corresponding ei.</p><h2 id="Type-Synonyms"><a href="#Type-Synonyms" class="headerlink" title="Type Synonyms"></a>Type Synonyms</h2><p>A <strong>type synonym</strong> simply creates another name for an existing type that is entirely interchangeable with the existing type.</p><p>For example, if we write:<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> foo = <span class="built_in">int</span></span><br></pre></td></tr></table></figure></p><p>then we can write foo wherever we write int and vice-versa.</p><p>for more complicated types, it can be convenient to create type synonyms. Here are some examples for types we created above:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> card = suit * rank</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> name_record = &#123; student_num : <span class="built_in">int</span> <span class="built_in">option</span>,</span><br><span class="line">                    first : <span class="built_in">string</span>,</span><br><span class="line">                    middle : <span class="built_in">string</span> <span class="built_in">option</span>,</span><br><span class="line">                    last : <span class="built_in">string</span> &#125;</span><br></pre></td></tr></table></figure><h2 id="Lists-and-Options-are-Datatypes"><a href="#Lists-and-Options-are-Datatypes" class="headerlink" title="Lists and Options are Datatypes"></a>Lists and Options are Datatypes</h2><p>Because datatype definitions can be recursive, we can use them to create our own types for lists. For example, this binding works well for a linked list of integers:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> my_int_list = <span class="type">Empty</span></span><br><span class="line">                        | <span class="type">Cons</span> <span class="keyword">of</span> <span class="built_in">int</span> * my_int_list</span><br></pre></td></tr></table></figure><p>We can use the constructors Empty and Cons to make values of my_int_list and we can use case expressions to use such values:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> one_two_three = <span class="type">Cons</span>(<span class="number">1</span>,<span class="type">Cons</span>(<span class="number">2</span>,<span class="type">Cons</span>(<span class="number">3</span>,<span class="type">Empty</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> append_mylist (xs,ys) = </span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">        <span class="type">Empty</span> =&gt; ys</span><br><span class="line">    | <span class="type">Cons</span>(x,xs’) =&gt; <span class="type">Cons</span>(x, append_mylist(xs’,ys))</span><br></pre></td></tr></table></figure><p>For options, all you need to know is SOME and NONE are constructors, which we use to create values (just like before) and in patterns to access the values. Here is a short example of the latter:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> inc_or_zero intoption = <span class="keyword">case</span> intoption <span class="keyword">of</span></span><br><span class="line">        <span class="type">NONE</span> =&gt; <span class="number">0</span></span><br><span class="line">      | <span class="type">SOME</span> i =&gt; i+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">&lt;!-- <span class="keyword">val</span> inc_or_zero = <span class="keyword">fn</span> : <span class="built_in">int</span> <span class="built_in">option</span> -&gt; <span class="built_in">int</span> --&gt;</span><br></pre></td></tr></table></figure><p>The story for lists is similar with a few convenient syntactic peculiarities: [] really is a constructor that carries nothing and :: really is a constructor that carries two things, but :: is unusual because it is an infix operator (it is placed between its two operands), both when creating things and in patterns:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_list xs = </span><br><span class="line">    <span class="keyword">case</span> xs:</span><br><span class="line">        <span class="literal">[]</span> =&gt; <span class="number">0</span></span><br><span class="line">        | x::xs' =&gt; x + sum_list xs'</span><br></pre></td></tr></table></figure><p>Notice here x and xs’ are nothing but local variables introduced via pattern-matching. We can use any names for the variables we want. </p><h2 id="Pattern-Matching-for-Each-Of-Types-The-Truth-About-Val-Bindings"><a href="#Pattern-Matching-for-Each-Of-Types-The-Truth-About-Val-Bindings" class="headerlink" title="Pattern-Matching for Each-Of Types: The Truth About Val-Bindings"></a>Pattern-Matching for Each-Of Types: The Truth About Val-Bindings</h2><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> full_name (r : &#123;first:<span class="built_in">string</span>,middle:<span class="built_in">string</span>,last:<span class="built_in">string</span>&#125;) = <span class="keyword">case</span> r <span class="keyword">of</span></span><br><span class="line">        &#123;first=x,middle=y,last=z&#125; =&gt; x ^ <span class="string">" "</span> ^ y ^ <span class="string">" "</span> ^z</span><br></pre></td></tr></table></figure><p>However, a case-expression with one branch is poor style — it looks strange because the purpose of such expressions is to distinguish cases, plural. So how should we use pattern-matching for each-of types, when we know that a single pattern will definitely match so we are using pattern-matching just for the convenient <strong>extraction of values</strong>? It turns out you can use patterns in val-bindings too! So this approach is better style:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">fun</span> full_name (r : &#123;first:<span class="built_in">string</span>,middle:<span class="built_in">string</span>,last:<span class="built_in">string</span>&#125;) = </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">val</span> &#123;first=x,middle=y,last=z&#125; = r</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        x ^ <span class="string">" "</span> ^ y ^ <span class="string">" "</span> ^z </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum_triple (triple : <span class="built_in">int</span>*<span class="built_in">int</span>*<span class="built_in">int</span>) = </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">val</span> (x,y,z) = triple</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        x+y+z</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_triple (x,y,z) = x+y+z</span><br></pre></td></tr></table></figure><p>This version of sum_triple should intrigue you: It takes a triple as an argument and uses pattern-matching to bind three variables to the three pieces for use in the function body. But it looks exactly like a function that takes three arguments of type int. Indeed, is the type int<em>int</em>int-&gt;int for three-argument functions or for one argument functions that take triples?<br>It turns out we have been basically lying: There is no such thing as a multi-argument function in ML: <strong>Every function in ML takes exactly one argument!</strong> Every time we write a multi-argument function, we are really writing a one-argument function that takes a tuple as an argument and uses pattern-matching to extract the pieces. This is such a common idiom that it is easy to forget about and it is totally fine to talk about “multi-argument functions” when discussing your ML code with friends. But in terms of the actual language definition, it really is a one-argument function: syntactic sugar for expanding out to the first version of sum_triple with a one-arm case expression.</p><h2 id="Digression-Type-inference"><a href="#Digression-Type-inference" class="headerlink" title="Digression: Type inference"></a>Digression: Type inference</h2><p>In ML, every variable and function has a type (or your program fails to type-check) — type inference only means you do not need to write down the type.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_triple triple = <span class="keyword">case</span> triple <span class="keyword">of</span></span><br><span class="line">    (x,y,z) =&gt; z + y + x</span><br></pre></td></tr></table></figure><p>In fact, type inference sometimes reveals that functions are more general than you might have thought. Consider this code, which does use part of a tuple/record:<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> partial_sum (x,y,z) = x + z</span><br><span class="line"><span class="keyword">fun</span> partial_name &#123;first=x, middle=y, last=z&#125; = x ^ <span class="string">" "</span> ^ z</span><br></pre></td></tr></table></figure></p><p>In both cases, the inferred function types reveal that the type of y can be any type, so we can call partial_sum (3,4,5) or partial_sum (3,false,5). This is okay because the polymorphism indicates that partial_sum has a more gen- eral type. If you can take a type containing ’a, ’b, ’c, etc. and replace each of these type variables consistently to get the type you “want,” then you have a more general type than the one you want.</p><h2 id="Nested-Patterns"><a href="#Nested-Patterns" class="headerlink" title="Nested Patterns"></a>Nested Patterns</h2><p>It turns out the definition of patterns is recursive: anywhere we have been putting a variable in our patterns, we can instead put another pattern. Roughly speaking, the semantics of pattern-matching is that the value being matched must have the same “shape” as the pattern and variables are bound to the “right pieces.” (This is very hand-wavy explanation which is why a precise definition is described below.) For example, the pattern a::(b::(c::d)) would match any list with at least 3 elements and it would bind a to the first element, b to the second, c to the third, and d to the list holding all the other elements (if any). The pattern a::(b::(c::[])) on the other hand, would match only lists with exactly three elements. Another nested patterns is (a,b,c)::d, which matches any non-empty list of triples, binding a to the first component of the head, b to the second component of the head, c to the third component of the head, and d to the tail of the list.</p><p>In general, pattern-matching is about taking a value and a pattern and (1) deciding if the pattern matches the value and (2) if so, binding variables to the right parts of the value. Here are some key parts to the elegant recursive definition of pattern matching:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">BadTriple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> zip3 list_triple = <span class="keyword">case</span> list_triple <span class="keyword">of</span></span><br><span class="line">    (<span class="literal">[]</span>,<span class="literal">[]</span>,<span class="literal">[]</span>) =&gt; <span class="literal">[]</span></span><br><span class="line">        | (hd1::tl1,hd2::tl2,hd3::tl3) =&gt; (hd1,hd2,hd3)::zip3(tl1,tl2,tl3) </span><br><span class="line">        | _ =&gt; <span class="keyword">raise</span> <span class="type">BadTriple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> unzip3 lst =</span><br><span class="line">    <span class="keyword">case</span> lst <span class="keyword">of</span></span><br><span class="line">        <span class="literal">[]</span> =&gt; (<span class="literal">[]</span>,<span class="literal">[]</span>,<span class="literal">[]</span>)</span><br><span class="line">      | (a,b,c)::tl =&gt; <span class="keyword">let</span> <span class="keyword">val</span> (l1,l2,l3) = unzip3 tl</span><br><span class="line">                       <span class="keyword">in</span></span><br><span class="line">                           (a::l1,b::l2,c::l3)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="Exceptions"><a href="#Exceptions" class="headerlink" title="Exceptions"></a>Exceptions</h2><p>ML has a built-in notion of exception. You can raise (also known as throw) an exception with the raise primitive. For example, the hd function in the standard library raises the List.Empty exception when called with []:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> hd xs =</span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line"><span class="literal">[]</span> =&gt; <span class="keyword">raise</span> <span class="type">List</span>.<span class="type">Empty</span> | x::_ =&gt; x</span><br></pre></td></tr></table></figure><p>You can create your own kinds of exceptions with an exception binding. Exceptions can optionally carry values with them, which let the code raising the exception provide more information:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">MyUndesirableCondition</span></span><br><span class="line"><span class="keyword">exception</span> <span class="type">MyOtherException</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span></span><br></pre></td></tr></table></figure><p>Kinds of exceptions are a lot like constructors of a datatype binding. Indeed, they are functions (if they carry values) or values (if they don’t) that create values of type exn rather than the type of a datatype. So Empty, MyUndesirableCondition, and MyOtherException(3,9) are all values of type exn, whereas MyOtherException has type int*int-&gt;exn.</p><h2 id="Tail-Recursion-and-Accumulators"><a href="#Tail-Recursion-and-Accumulators" class="headerlink" title="Tail Recursion and Accumulators"></a>Tail Recursion and Accumulators</h2><p>This topic involves new programming idioms, but no new language constructs. It defines tail recursion, describes how it relates to writing efficient recursive functions in functional languages like ML, and presents how to use accumulators as a technique to make some functions tail recursive.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum1 xs =</span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">        <span class="literal">[]</span> =&gt; <span class="number">0</span></span><br><span class="line">      | i::xs’ =&gt; i + sum1 xs’</span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum2 xs =</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">fun</span> f (xs,acc) =</span><br><span class="line">            <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">                <span class="literal">[]</span> =&gt; acc</span><br><span class="line">              | i::xs’ =&gt; f(xs’,i+acc)</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        f(xs,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Why might sum2 be preferred when it is clearly more complicated? To answer, we need to understand a little bit about how function calls are implemented. Conceptually, there is a <strong>call stack</strong>, which is a stack (the data structure with push and pop operations) with one element for each function call that has been started but has not yet completed. Each element stores things like the value of local variables and what part of the function has not been evaluated yet. When the evaluation of one function body calls another function, a new element is pushed on the call stack and it is popped off when the called function completes.</p><blockquote><p> there is nothing more for the caller to do after the callee returns except return the callee’s result.</p></blockquote><p>This situation is called a tail call (let’s not try to figure out why it’s called this) and functional languages like ML typically promise an essential optimization: When a call is a tail call, the caller’s stack-frame is popped before the call — the callee’s stack-frame just replaces the caller’s. This makes sense: the caller was just going to return the callee’s result anyway. Therefore, calls to sum2 never use more than 1 stack frame.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Conceptual-Ways-to-Build-New-Types&quot;&gt;&lt;a href=&quot;#Conceptual-Ways-to-Build-New-Types&quot; class=&quot;headerlink&quot; title=&quot;Conceptual Ways to Build
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>EDA Summary</title>
    <link href="https://zhangruochi.com/EDA-Summary/2020/04/30/"/>
    <id>https://zhangruochi.com/EDA-Summary/2020/04/30/</id>
    <published>2020-04-30T11:55:33.000Z</published>
    <updated>2020-05-01T00:02:49.967Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-main-goals-of-EDA-are"><a href="#The-main-goals-of-EDA-are" class="headerlink" title="The main goals of EDA are:"></a>The main goals of EDA are:</h2><ul><li>Provide summary level insight into a data set</li><li>Uncover underlying patterns and structure in the data</li><li>Identify outliers, missing data and class balance issues</li><li>Carry out quality control checks</li></ul><h2 id="The-principal-steps-in-the-process-of-EDA-are"><a href="#The-principal-steps-in-the-process-of-EDA-are" class="headerlink" title="The principal steps in the process of EDA are:"></a>The principal steps in the process of EDA are:</h2><ol><li>Summarize the data - Generally done using dataframes in R or Python</li><li>Tell the Story - Summarize the details of what connects the dataset to the business opportunity</li><li>Deal with missing data - Identify the strategy for dealing with missing data</li><li>Investigate - Using data visualization and hypothesis testing delve into the relationship between the dataset and the business opportunity</li><li>Communicate - Communicate the findings from the above steps</li></ol><h2 id="Data-visualization"><a href="#Data-visualization" class="headerlink" title="Data visualization"></a>Data visualization</h2><ol><li>Jupyter notebooks in combination with pandas and simple plots are the basis for modern EDA when using Python as a principal language</li></ol><h3 id="Advantages-of-Jupyter-notebooks"><a href="#Advantages-of-Jupyter-notebooks" class="headerlink" title="Advantages of Jupyter notebooks:"></a>Advantages of Jupyter notebooks:</h3><ul><li>They are portable: then can be used locally on private servers, public cloud, and as part of IBM Watson Studio</li><li>They work with dozens of languages</li><li>They mix markdown with executable code in a way that works naturally with storytelling and investigation</li><li>matplotlib itself and its numerous derivative works like seaborn are the core of the Python data visualization landscape</li><li>pandas and specifically the dataframe class works naturally with Jupyter, matplotlib and downstream modeling frameworks like sklearn</li></ul><h3 id="EDA-and-Data-Visualization-best-practices"><a href="#EDA-and-Data-Visualization-best-practices" class="headerlink" title="EDA and Data Visualization best practices"></a>EDA and Data Visualization best practices</h3><ol><li>The majority of code for any data science project should be contained within text files. This is a software engineering best practice that ensures re-usability, allows for unit testing and works naturally with version control. &gt;In Python the text files can be executable scripts, modules, a full Python package or some combination of these.</li><li>Keep a record of plots and visualization code that you create. It is difficult to remember all of the details of how visualizations were created. Extracting the visualization code to a specific place will ensure that similar plots for future projects will be quick to create.</li><li>Use you plots as a quality assurance tool. Given what you know about the data it can be useful to make an educated guess before you execute the cell or run the script. This habit is surprisingly useful for quality assurance of both data and code.</li></ol><h2 id="Missing-values"><a href="#Missing-values" class="headerlink" title="Missing values"></a>Missing values</h2><ul><li>Dealing with missing data sits at the intersection of EDA and data ingestion in the AI enterprise workflow</li><li>Ignoring missing data may have unintended consequences in terms of model performance that may not be easy to detect</li><li>Removing either complete rows or columns in a feature matrix that contain missing values is called complete case analysis</li><li>Complete case analysis, although commonly used, can lead to undesirable results—the extent to which depends on the category of missingness</li></ul><h3 id="The-categories-of-missingness-are"><a href="#The-categories-of-missingness-are" class="headerlink" title="The categories of missingness are:"></a>The categories of missingness are:</h3><ol><li>Missing completely at random or MCAR:</li></ol><p>When data are MCAR, missing cases are, on average, identical to non-missing cases, with respect to the feature matrix. Complete case analysis will reduce the power of the analysis, but will not affect bias.</p><ol><li>Missing at random or MAR:</li></ol><p>When data are MAR the missing data often have some dependence on measured values, and models can be used to help impute what the likely data would be. For example, in an MLB survey, there may be a gender bias when it comes to completing all of the questions.</p><ol><li>Missing not at random or MNAR:</li></ol><p>In this case the missing data depend on unmeasured or unknown variables. There is no information available to account for the missingness.</p><ul><li>The best case scenario is that the data are MCAR. It should be noted that imputing values under the other two types of missingness can result in an increase in bias.</li><li>In statistics the process of replacing missing data with substituted values is known as imputation.</li><li>It is a common practice to perform multiple imputations.</li><li>The practice of imputing missing values introduces uncertainty into the results of a data science project.</li><li>One way to deal with that additional uncertainty is to try a range of different values for imputation and measure how the results vary between each set of imputations. This technique is known as multiple imputation.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;The-main-goals-of-EDA-are&quot;&gt;&lt;a href=&quot;#The-main-goals-of-EDA-are&quot; class=&quot;headerlink&quot; title=&quot;The main goals of EDA are:&quot;&gt;&lt;/a&gt;The main g
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="EDA" scheme="https://zhangruochi.com/categories/AI-Workflow/EDA/"/>
    
    
  </entry>
  
  <entry>
    <title>Model Training Tricks (2)</title>
    <link href="https://zhangruochi.com/Model-Training-Tricks-2/2020/04/28/"/>
    <id>https://zhangruochi.com/Model-Training-Tricks-2/2020/04/28/</id>
    <published>2020-04-27T20:04:20.000Z</published>
    <updated>2020-04-28T21:09:54.664Z</updated>
    
    <content type="html"><![CDATA[<p>If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a really long time. The reason that deep learning is not straightforward is because your data, memory, and time is limited. If you are running out of memory or time, then the solution is to train a smaller model. If you are not able to train for long enough to overfit, then you are not taking advantage of the capacity of your model.</p><p>So step one is to get to the point that you can overfit. Then, the question is how to reduce that overfitting.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>Many practitioners when faced with an overfitting model start at exactly the wrong end of this diagram. Their starting point is to use a smaller model, or more regularisation. Using a smaller model should be absolutely the last step you take, unless your model is taking up too much time or memory. Reducing the size of your model as reducing the ability of your model to learn subtle relationships in your data.<br>Instead, your first step should be to seek to create more data. That could involve adding more labels to data that you already have in your organisation, finding additional tasks that your model could be asked to solve (or to think of it another way, identifying different kinds of labels that you could model), or creating additional synthetic data via using more or different data augmentation. Thanks to the development of mixup and similar approaches, effective data augmentation is now available for nearly all kinds of data.<br>Once you’ve got as much data as you think you can reasonably get a hold of, and are using it as effectively as possible by taking advantage of all of the labels that you can find, and all of the augmentation that make sense, if you are still overfitting and you should think about using more generalisable architectures. For instance, adding batch normalisation may improve generalisation.<br>If you are still overfitting after doing the best you can at using your data and tuning your architecture, then you can take a look at regularisation. Generally speaking, adding dropout to the last layer or two will do a good job of regularising your model. However, as we learnt from the story of the development of AWD-LSTM, it is often the case that adding dropout of different types throughout your model can help regularise even better. Generally speaking, a larger model with more regularisation is more flexible, and can therefore be more accurate than a smaller model with less regularisation.<br>Only after considering all of these options would be recommend that you try using smaller versions of your architectures.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/fastai/fastbook/blob/master/15_arch_details.ipynb" target="_blank" rel="noopener">https://github.com/fastai/fastbook/blob/master/15_arch_details.ipynb</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a r
      
    
    </summary>
    
    
      <category term="Competition" scheme="https://zhangruochi.com/categories/Competition/"/>
    
      <category term="Tricks" scheme="https://zhangruochi.com/categories/Competition/Tricks/"/>
    
    
  </entry>
  
  <entry>
    <title>Model Training Tricks (1)</title>
    <link href="https://zhangruochi.com/Model-Training-Tricks-1/2020/04/27/"/>
    <id>https://zhangruochi.com/Model-Training-Tricks-1/2020/04/27/</id>
    <published>2020-04-27T08:21:11.000Z</published>
    <updated>2020-04-28T08:04:35.236Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dls</span><span class="params">(bs, size)</span>:</span></span><br><span class="line">    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),</span><br><span class="line">                   get_items=get_image_files,</span><br><span class="line">                   get_y=parent_label,</span><br><span class="line">                   item_tfms=Resize(<span class="number">460</span>),</span><br><span class="line">                   batch_tfms=[*aug_transforms(size=size, min_scale=<span class="number">0.75</span>),</span><br><span class="line">                               Normalize.from_stats(*imagenet_stats)])</span><br><span class="line">    <span class="keyword">return</span> dblock.dataloaders(path, bs=bs)</span><br></pre></td></tr></table></figure><p>Normalization becomes especially important when using pretrained models. The pretrained model only knows how to work with data of the type that it has seen before. If the average pixel was zero in the data it was trained with, but your data has zero as the minimum possible value of a pixel, then the model is going to be seeing something very different to what is intended.</p><p>This means that when you distribute a model, you need to also distribute the statistics used for normalization, since anyone using it for inference, or transfer learning, will need to use the same statistics. By the same token, if you’re using a model that someone else has trained, make sure you find out what normalization statistics they used, and match them.</p><h2 id="Progressive-resizing"><a href="#Progressive-resizing" class="headerlink" title="Progressive resizing"></a>Progressive resizing</h2><blockquote><p>Gradually using larger and larger images as you train</p></blockquote><p>Start training using small images, and end training using large images. By spending most of the epochs training with small images, training completed much faster. By completing training using large images, the final accuracy was much higher. We call this approach progressive resizing.</p><p>Note that for transfer learning, progressive resizing may actually hurt performance. This would happen if your pretrained model was quite <code>similar</code> to your transfer learning task and dataset, and was trained on similar sized images, so the weights don’t need to be changed much. In that case, training on smaller images may damage the pretrained weights. On the other hand, if the transfer learning task is going to be on images that are of different sizes, shapes, or style to those used in the pretraining tasks, progressive resizing will probably help. As always, the answer to “does it help?” is “try it!”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">dls = get_dls(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">learn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(), </span><br><span class="line">                metrics=accuracy)</span><br><span class="line">learn.fit_one_cycle(<span class="number">4</span>, <span class="number">3e-3</span>)</span><br><span class="line"></span><br><span class="line">learn.dls = get_dls(<span class="number">64</span>, <span class="number">224</span>)</span><br><span class="line">learn.fine_tune(<span class="number">5</span>, <span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><h2 id="Test-time-augmentation"><a href="#Test-time-augmentation" class="headerlink" title="Test time augmentation"></a>Test time augmentation</h2><blockquote><p>During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image</p></blockquote><p>Select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we could do this not just for different crops, but for different values across all of our test time augmentation parameters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds,targs = learn.tta()</span><br><span class="line">accuracy(preds, targs).item()</span><br></pre></td></tr></table></figure><h2 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h2><p>Mixup works as follows, for each image:</p><ol><li>Select another image from your dataset at random</li><li>Pick a weight at random</li><li>Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable</li><li>Take a weighted average (with the same weight) of this image’s labels with your image’s labels; this will be your dependent variable</li></ol><p>In pseudo-code, we’re doing (where t is the weight for our weighted average):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image2,target2 = dataset[randint(<span class="number">0</span>,len(dataset)]</span><br><span class="line">t = random_float(<span class="number">0.5</span>,<span class="number">1.0</span>)</span><br><span class="line">new_image = t * image1 + (<span class="number">1</span>-t) * image2</span><br><span class="line">new_target = t * target1 + (<span class="number">1</span>-t) * target2</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>The third image is built by adding 0.3 times the first one and 0.7 times the second. In this example, should the model predict church? gas station? The right answer is 30% church and 70% gas station since that’s what we’ll get if we take the linear combination of the one-hot encoded targets. For instance, if church has for index 2 and gas station as for index 7, the one-hot-encoded representations are</p><blockquote><p>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]<br>[0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0]</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = xresnet50()</span><br><span class="line">learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), </span><br><span class="line">                metrics=accuracy, cbs=Mixup)</span><br><span class="line">learn.fit_one_cycle(<span class="number">5</span>, <span class="number">3e-3</span>)</span><br></pre></td></tr></table></figure><h2 id="Label-smoothing"><a href="#Label-smoothing" class="headerlink" title="Label smoothing"></a>Label smoothing</h2><p>In the theoretical expression of the loss, in classification problems, our targets are one-hot encoded (in practice we tend to avoid doing it to save memory, but what we compute is the same loss as if we had used one-hot encoding). That means the model is trained to return 0 for all categories but one, for which it is trained to return 1. Even 0.999 is not good enough, the model will get gradients and learn to predict activations that are even more confident. This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category even if it’s not too sure, just because it was trained this way. <strong>It can become very harmful if your data is not perfectly labeled.</strong></p><p>This is how label smoothing works in practice: we start with one-hot encoded labels, then replace all zeros by</p><script type="math/tex; mode=display">\frac{\epsilon}{N}</script><p>where $N$ is the number of classes and $\epsilon$ is a parameter (usually 0.1, which would mean we are 10% unsure of our labels). Since you want the labels to add up to 1, replace the 1 by </p><p><script type="math/tex">1-\epsilon + \frac{\epsilon}{N}</script>. </p><p>This way, we don’t encourage the model to predict something overconfident: in our Imagenette example where we have 10 classes, the targets become something like:</p><blockquote><p>[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Normalization&quot;&gt;&lt;a href=&quot;#Normalization&quot; class=&quot;headerlink&quot; title=&quot;Normalization&quot;&gt;&lt;/a&gt;Normalization&lt;/h2&gt;&lt;figure class=&quot;highlight pyth
      
    
    </summary>
    
    
      <category term="Competition" scheme="https://zhangruochi.com/categories/Competition/"/>
    
      <category term="Tricks" scheme="https://zhangruochi.com/categories/Competition/Tricks/"/>
    
    
  </entry>
  
  <entry>
    <title>Empathize Stage</title>
    <link href="https://zhangruochi.com/Empathize-Stage/2020/04/21/"/>
    <id>https://zhangruochi.com/Empathize-Stage/2020/04/21/</id>
    <published>2020-04-21T15:47:10.000Z</published>
    <updated>2020-04-22T05:30:34.864Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Empathize-Process"><a href="#Empathize-Process" class="headerlink" title="Empathize Process"></a>Empathize Process</h2><ol><li>Get as close to the source of data as possible usually by interviewing the people involved</li><li>Identify the business problem</li><li>Obtain all of the relevant the data</li><li>Translate the business problem into a testable hypothesis or hypotheses</li></ol><h2 id="Identifying-the-business-opportunity-Through-the-eyes"><a href="#Identifying-the-business-opportunity-Through-the-eyes" class="headerlink" title="Identifying the business opportunity: Through the eyes"></a>Identifying the business opportunity: Through the eyes</h2><h3 id="Our-story"><a href="#Our-story" class="headerlink" title="Our story"></a>Our story</h3><p><strong>The first stage of any project in a large enterprise is to identify the business opportunity. In the world of design thinking, this begins with the Empathize stage</strong>. During this time, you and your team are gathering as much information as possible to understand the challenges faced by your AAVAIL.</p><p>You are suprised by the fact that you, a data scientist, are being asked to help out with interviews, observations, process mapping, and various design thinking sessions. These techniques as well as many others are used during the empathize stage to gather <strong>as much information as possible</strong> so that a problem may be defined.</p><p>As a data scientist, this process should be used to guide your investigative process. Ultimately, your top priority is to analyze the data coming out of Singapore, understand the problem and fix the situation. <strong>The involved parties are subscribers, data engineers, data scientists, marketing and management</strong>. You are going to need to talk everyone involved in the data generation process. This is why you’re spending time on interviews and observations.</p><p><strong>Asking questions is a critical part of getting the process started</strong>. You will want to be naturally curious gathering details about the product, the subscriber, and the interaction between the two. This information gathering stage provides both a perspective on the situation and it will help you formulate the business question.</p><p>In the short sections below, we provide guidelines for asking questions and beginning with an investigative mindset.</p><h3 id="Articulate-the-business-question"><a href="#Articulate-the-business-question" class="headerlink" title="Articulate the business question"></a>Articulate the business question</h3><p>There are generally many business questions that can be derived from a given situation. It is an important thought exercise to enumerate the possible questions, that way it makes the discussion easier when you work with the involved stakeholders in order to focus and prioritize. In this situation here are some ways of articulating the business case.</p><ul><li>Can we use marketing to reduce the rate of churn?</li><li>Can we salvage the Singapore market with new products?</li><li>Are there factors outside of our influence that caused the situation in Singapore and is it temporary?</li><li>Can we identify the underlying variables in Singapore that are related to churn and can we use the knowledge to remedy the situations?</li><li>The business problem in all of these examples is written shown in terms of the data we have.</li></ul><p>NOTE: This case study can be approached in many different ways and there may not be a clear right or wrong. During the various modules of this course, we will provide guidance when there are multiple paths to choose from.</p><h3 id="Prioritize"><a href="#Prioritize" class="headerlink" title="Prioritize"></a>Prioritize</h3><p>It is logical, but there is a need to prioritize If there are several distinct business objectives. In this case maybe one is related to reducing chrun directly and another is about profitability.</p><p>There are three major contributing factors when it comes to priority.</p><h4 id="Stakeholder-or-domain-expert-opinion"><a href="#Stakeholder-or-domain-expert-opinion" class="headerlink" title="Stakeholder or domain expert opinion"></a>Stakeholder or domain expert opinion</h4><p>In situations where considerable domain expertise is required to effectively prioritize (e.g. Physics, Medicine and Finance) prioritization will likely be driven by the people closest to the domain.</p><h4 id="Feasibility"><a href="#Feasibility" class="headerlink" title="Feasibility"></a>Feasibility</h4><ul><li>Do we have the necessary data to address the business questions?</li><li>Do we have clean enough data to address the business questions?</li><li>Do we have the technology infrastructure to deploy a solution once the data are modeled?</li></ul><h4 id="Impact"><a href="#Impact" class="headerlink" title="Impact"></a>Impact</h4><p>When looking at Impact we’re purely looking at expected dollar contribution and added value from a monetary perspective. When possible, calculating the back-of-the-envelope ROI is a crucial step that you can do. This is an expectation and not real ROI calculation, but it is a guiding principle nonetheless.</p><p>The ROI calculation should be an expected dollar value that you can generate based on all available information you currently have in your organization combined with any domain insight you can collect.</p><p>Measuring the back-of-the-envelope ROI calculation could make use of any of the following:</p><ul><li>Estimates for fully-loaded salaries of employees involved</li><li>Cost per unit item and/or time required to produce</li><li>Number of customers, clients, or users</li><li>Revenue and more</li></ul><h2 id="Scientific-Thinking-for-Business"><a href="#Scientific-Thinking-for-Business" class="headerlink" title="Scientific Thinking for Business"></a>Scientific Thinking for Business</h2><h3 id="Our-Story"><a href="#Our-Story" class="headerlink" title="Our Story"></a>Our Story</h3><p>Data science involves lots of investigation via trial and error. The investigations are based on evidence and this is one of the strongest reasons why data science is considered a “real” science.</p><p>You will be using a scientific process with your work at AAVAIL. This will help you to organize your work as well as be able to clearly explain everything you are doing to the AAVAIL leadership.</p><p>Let’s take a look now at some guidance and best practices for engaging with a <strong>scientific mindset</strong>.</p><h3 id="Science-is-a-process-and-the-route-to-solving-problems-is-not-always-direct"><a href="#Science-is-a-process-and-the-route-to-solving-problems-is-not-always-direct" class="headerlink" title="Science is a process and the route to solving problems is not always direct"></a>Science is a process and the route to solving problems is not always direct</h3><p>A common argument made by statisticians and mathematicians is that data science is not really a science. This is untrue, mainly because data science involves a lot of <strong>investigations</strong> through sometimes chaotic data sets, in search of meaningful patterns that might help in solving particular problems.</p><p>Since data science implies a scientific approach, it is important that all data scientists learn to adopt and use a scientific thought process. <strong>A scientific thought process of observation, developing hypotheses, testing hypotheses, and modifying hypotheses is critical to your success as a data scientist</strong>.</p><p>Pulling in data and jumping right into exploratory data analysis can make your work prone to exactly the types of negative issues that plague data science today. There are a number of well-discussed issues revolving around data science and data science teams not living up to promised potential.</p><p>At the heart of this problem is the process of communicating results to leadership. It should begin with a meaningful and well-articulated business opportunity. If that opportunity is stated too simply, as say, increasing overall revenue then the central talking point for communication is too vague to be meaningful from the data side.</p><blockquote><p>The business scenario needs to be communicated in a couple of ways:</p><ol><li>Stated in a testable way in terms of data</li><li>Stated in a clear way that minimizes the influence of confounding factors</li></ol></blockquote><h3 id="Testable-hypotheses"><a href="#Testable-hypotheses" class="headerlink" title="Testable hypotheses"></a>Testable hypotheses</h3><p>There is no one single best way to articulate a business opportunity as a testable hypothesis. In some cases the statement will be intuitive, but in other cases there will be some back and forth with stakeholders and domain experts.</p><h3 id="Guidelines-for-creating-testable-hypotheses"><a href="#Guidelines-for-creating-testable-hypotheses" class="headerlink" title="Guidelines for creating testable hypotheses"></a>Guidelines for creating testable hypotheses</h3><ol><li>Become a scientist of the business</li></ol><p>Spend a little bit less time learning new algorithms and Python packages and more time learning the levers that make your specific business go up or down and the variables that impact those levers.</p><ol><li>Make an effort to understand how the data are produced</li></ol><p>If it comes down to it, sources of variation can be explicitly accounted for in many types of models. If the data come from a database you should ask about the process by which the data are stored. If the data are compiled by another person then dig into the details and find out about the compiling process as well as the details of what happened before the data arrived on their desk.</p><ol><li>Make yourself part of the business</li></ol><p>Do not under any circumstances become siloed. Proactively get involved with the business unit as a partner, not a support function.</p><ol><li>Think about how to measure success</li></ol><p>When thinking about what course of action might be most appropriate, keep at the forefront of your mind how you will measure business value when said action is complete.</p><p><strong>IMPORTANT</strong>: Data Science is NOT Business Intelligence. BI analysts serve to derive business insights out of data. There is without a doubt some overlap, but the job of a data scientist is to investigate the business opportunity and solve it.</p><p>There is a balancing act to maintain between directly addressing the business need and ensuring that you have thoughtfully studied the problem enough to ensure that you can account for most of the likely contingencies. The scientific method can be of some guidance here.</p><h3 id="Thinking-scientifically-about-the-business-scenario"><a href="#Thinking-scientifically-about-the-business-scenario" class="headerlink" title="Thinking scientifically about the business scenario"></a>Thinking scientifically about the business scenario</h3><p>A major goal of this process is to make the business objectives clear to leadership. Some of these individuals are technical and some are not, so as a good rule-of-thumb get in the habit of articulating the business problem at a level that everyone can understand. Stakeholders and leadership need to know what you are trying to accomplish before you begin work. They also need to be aware from the start what success would look like. Science is an iterative process and many experiments produce results that some might consider a failure. However, experiments that are properly setup will not fail no matter the result–the result may not useful but you have gained valuable information along the way.</p><p>Experiments in this context could refer to an actual scientific experiment (e.g. A/B testing) or it could be more subtle. Let’s say you work for a company that collects tolls in an automated way, and you want to identify the make and model of each car in order to modify pricing models based on predicted vehicle weight. After talking with the stakeholders and the folks who implemented the image storage solution you are ready to begin. The experiment here has to do with how you begin. You may think that there is enough training data to implement a huge multi-class model and just solve most of the problem. If you approach it that way then you are hypothesizing that the solution will work.</p><p>For those of you who have done much image analysis work, you could guess that approach would likely result in a significant loss of time. If we take a step back and think scientifically, we could approach the solution from an evidence driven perspective. Before investing a significant amount of time you may try to see if you can distinguish one make and model from the rest before adding more classes. You may want to first pipe the images through an image segmentation algorithm to identify the make of the car. There are many possible ways to build towards a comprehensive solution, but it is important to determine if either of these piecemeal approaches would have any immediate business value.</p><p>This might be a good time for a reminder about the steps in the scientific method.</p><h3 id="The-Scientific-Method"><a href="#The-Scientific-Method" class="headerlink" title="The Scientific Method"></a>The Scientific Method</h3><p>It is the process by which science is carried out. The general idea is to build on previous knowledge to in order to improve an understanding of a given topic.</p><ol><li>Formulate the question</li><li>Generate a hypothesis to address the question</li><li>Make a prediction</li><li>Conduct an experiment</li><li>Analyze the data and draw a conclusion</li></ol><p>We will continue with an interactive example, but first it is important to note that <strong>Scientific experiments must be repeatable in order to become reliable evidence.</strong></p><h4 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h4><p>The question can be open-ended and generally it summarizes your business opportunity. Let’s say you work for a small business that manufactures sleds and other winter gear and you are not sure which cities to build your next retail locations. You have heard that Utah, Colorado and Vermont are all states that have high rates of snowfall, but it is unclear which one has the highest rate of snowfall.</p><h4 id="Hypothesis"><a href="#Hypothesis" class="headerlink" title="Hypothesis"></a>Hypothesis</h4><p>Because the Rocky mountains are higher in elevation and they are well-known for fresh powder on the ski slopes you hypothesize that both Utah and Colorado have more snow than Vermont.</p><h4 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h4><p>If you were to run a hypothesis test Vermont would have significantly less snow fall than Colorado or Utah</p><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><p>You hit the NOAA weather API to get average annual snowfall by city. We have compiled these data for you in snowfall.csv. You could use a 1-way ANOVA to test the validity of your prediction, but let’s start by looking at the data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First we read in the data</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"../data/snowfall.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Next, subset the data to focus only on the states of interest</span></span><br><span class="line"></span><br><span class="line">mask = [<span class="keyword">True</span> <span class="keyword">if</span> s <span class="keyword">in</span> [<span class="string">'CO'</span>,<span class="string">'UT'</span>,<span class="string">'VT'</span>] <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">'state'</span>].values]</span><br><span class="line">df1 = df[mask]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, create a pivot of the data that focuses only on the relevant summary data</span></span><br><span class="line"></span><br><span class="line">pivot = df1.groupby([<span class="string">'state'</span>])[<span class="string">'snowfall'</span>].describe()</span><br><span class="line">df1_pivot = pd.DataFrame(&#123;<span class="string">'count'</span>: pivot[<span class="string">'count'</span>],</span><br><span class="line">                          <span class="string">'avg_snowfall'</span>: pivot[<span class="string">'mean'</span>],</span><br><span class="line">                          <span class="string">'max_snowfall'</span>: pivot[<span class="string">'max'</span>]&#125;)</span><br><span class="line">print(df1_pivot)</span><br><span class="line"></span><br><span class="line"><span class="comment">#        count  avg_snowfall  max_snowfall</span></span><br><span class="line"><span class="comment"># state</span></span><br><span class="line"><span class="comment"># CO       5.0         37.76          59.6</span></span><br><span class="line"><span class="comment"># UT       2.0         51.65          58.2</span></span><br><span class="line"><span class="comment"># VT       1.0         80.90          80.9</span></span><br></pre></td></tr></table></figure><h4 id="Analyze"><a href="#Analyze" class="headerlink" title="Analyze"></a>Analyze</h4><p>There is not enough data to do a 1-way ANOVA. The experiment is not a failure; it has a few pieces of information.</p><p>There is not enough data<br>There is a small possibility that VT gets more snow on average than either CO or UT<br>Our degree of belief in the conclusion drawn from (2) is very small because of (1)<br>The notion of degree of belief is central to scientific thinking. It is somehow a part of our human nature to believe statements that have little to no supporting evidence. <strong>In science the word belief, with respect to a hypothesis is proportional to the evidence</strong>. With more evidence available, ideally, from repeated experiments, one’s degree of belief should change. Evidence is derived from the process described above and if we have none then we are stuck at the question stage and a proper scientific hypothesiscannot be made.</p><p>The other important side to degree of belief is that it never caps out at 100 percent certainty. Some hypotheses have become laws like Newton’s Law of Gravitation, but most natural phenomena in the world outside of physics cannot be explained as a law.</p><p>A hypothesis is the simplest explanation of a phenomenon. A scientific theory is an in-depth explanation of the observed phenomenon. Do not be mistaken with the word theory, there can be sufficient evidence that your degree of belief all but touches 100%, and is plenty for decision making purposes. A built-in safeguard for scientific thought is that our degree of belief does not reach 100%, which leaves some room to find new evidence that could move the dial in the other direction.</p><p>There are additional factors like external peer review that help ensure the integrity of the scientific method and in the case of implementing a model for a specific business task this could mean assigning reviewers for a pull request or simply asking other qualified individuals to check over your work.</p><h2 id="Gathering-Data"><a href="#Gathering-Data" class="headerlink" title="Gathering Data"></a>Gathering Data</h2><h3 id="Our-Story-1"><a href="#Our-Story-1" class="headerlink" title="Our Story"></a>Our Story</h3><p>Your first step at AAVAIL, just like everywhere else, it to look at the data sources. You soon discover that AAVAIL has data everywhere! There is no shortage of data. It looks like they have managed to store every type of transaction with their subscribers.</p><p>You will need a smart way of managing all of this data. Let’s take a look now at some best practices for managing data in a large enterprise.</p><h3 id="Documenting-your-data"><a href="#Documenting-your-data" class="headerlink" title="Documenting your data"></a>Documenting your data</h3><p>Too often data scientists will find themselves deep in the process of developing a solution, based on the data that was provided to them, before they realize that the data itself is flawed, inaccurate or in some other way non-ideal. Developing the habit of creating a simple document with at least a description of the ideal data needed to test the hypotheses around the business problem may seem like an unnecessary step, but it has potential to both:</p><ul><li>Streamline the modeling process</li><li>Help ensure that all future data come in an improved form</li></ul><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><p>The process of gathering data is often referred to as Extract, Transform, Load (ETL). Data is generally gathered (or extracted) from heterogeneous sources, cleaned (transformed) and loaded into a single place that facilitates analysis. Before the advent of the modern data scientist’s toolkit data was often staged in a database,data lake or a data warehouse. Still today data is frequently staged to facilitate collaboration, but there are tools now that enable more possibilities today than ever before. Jupyter Lab has an extension called data grid that allows it to read delimited files with millions or even billions of rows. Then tools like Dask help you scale your analyses. To ensure that projects are completed in a reasonable amount of time the initial pass at ETL should use a simple format like CSV, then a more complex system can be built out once you have accomplished the Minimum Viable Product (MVP).</p><h3 id="Common-methods-of-gathering-data"><a href="#Common-methods-of-gathering-data" class="headerlink" title="Common methods of gathering data"></a>Common methods of gathering data</h3><h4 id="Plain-text-files"><a href="#Plain-text-files" class="headerlink" title="Plain text files"></a>Plain text files</h4><p>Plain text file can come in many forms and generally the open function is used to bring the data into a Python environment. This is a flexible format, but because no structure is imposed, custom scripts are generally needed to parse these files and these scripts do not always generalize to new files.</p><p>The large majority of data science projects involve a modeling step that requires input data in a tabular numeric format. In order to extract data from a plain text file you may need to identify patterns in the text and use regular expressions (regex) to pull out the relevant information. Python’s built-in regex library is known as re.</p><p>On the other hand if the data you are working with consists of natural language, then there are a number of libraries that can work directly with the text files. The two main libraries are:</p><ul><li>spaCy</li><li>NLTK</li></ul><p>Also, scikit-learn has become a standard tool in the overall workflow when processing these files.</p><ul><li>scikit-learn’s text tutorial</li></ul><p>These tools can be applied to unstructured text to generate things like word counts, and word frequencies. We saw an example of this in the Data science workflow combined with design thinking example.</p><h4 id="Delimited-files"><a href="#Delimited-files" class="headerlink" title="Delimited files"></a>Delimited files</h4><p>One of the most commonly encountered ways of storing structured data is in delimited files, where rows of tabular data are stored in lines of a text file and the columns within each row are separated by a special “delimiter” character such as a comma or a tab character.</p><p>This simple structure helps account for the popularity of these formats, with probably the most widely used being Comma-Separated Values (CSV). CSV files are both human and machine readable, and have minimal overhead in terms of the proportion of the file devoted to defining the structure of the data when compared to most other file formats. As such Pandas comes with methods for both reading and writingCSV files. (Note that these functions can also handle other delimiters like tab or the pipe character “|”, but commas are the default.)</p><p>Spreadsheet programs like Microsoft Excel that are used for analyzing tabular data also read from and write to files in CSV format. The native Excel file format (often with file extensions .xls or .xlsx) can also be considered a delimited file type. Though these files also contain a significant amount of extra information related to things like styling that are separate from the actual data. Nonetheless, since these files are commonly used to save datasets, Pandas also has a method for reading them: pandas.read_excel.</p><p>HINT: A best practice when loading data from plain text or delimited files is to separate the code for parsing into its own script. Because the files are read line by line in a separate Python call, it is more memory efficient and this separation of tasks helps with automation and maintenance.</p><p>It is a common mistake to try to read large files into pandas then use the date frame environment to parse. If your parsing (cleaning) task is simple then use a parser. Here is a simple example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/evn/python</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">simple example of a parser</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment">## specify the files</span></span><br><span class="line">file_in = os.path.join(<span class="string">".."</span>,<span class="string">"data"</span>,<span class="string">"snowfall.csv"</span>)</span><br><span class="line">file_out = os.path.join(<span class="string">".."</span>,<span class="string">"data"</span>,<span class="string">"snowfall_parsed.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## create an outfile handle (needs to be closed)</span></span><br><span class="line">fidout = open(file_out,<span class="string">"w"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## use the csv module to read/write</span></span><br><span class="line">writer = csv.writer(fidout)</span><br><span class="line"></span><br><span class="line"><span class="comment">## generic parsing function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> line[<span class="number">3</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">"HI"</span>,<span class="string">"NC"</span>,<span class="string">"OR"</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> line + [<span class="string">'new_data'</span>]</span><br><span class="line">    </span><br><span class="line"><span class="comment">## for each line in the file read in the first file that you need to reference</span></span><br><span class="line"><span class="keyword">with</span> open(file_in) <span class="keyword">as</span> csvfile:</span><br><span class="line">    reader = csv.reader(csvfile, delimiter=<span class="string">','</span>)</span><br><span class="line">    header_in = reader.__next__()</span><br><span class="line">    writer.writerow(header_in + [<span class="string">"new_column"</span>])</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> reader:</span><br><span class="line">        line = parse_line(line)</span><br><span class="line">        <span class="keyword">if</span> line:</span><br><span class="line">            writer.writerow(line)</span><br><span class="line">   </span><br><span class="line">fidout.close()</span><br><span class="line">print(<span class="string">"done parsing"</span>)</span><br></pre></td></tr></table></figure><p>The highlighted lines show where this parser changes the original data by filtering and adding an additional column.</p><h4 id="JSON-files"><a href="#JSON-files" class="headerlink" title="JSON files"></a>JSON files</h4><p>While delimited files are well suited for housing data in flat tables, datasets with more complex structures require different formats. The JavaScript Object Notation (JSON) file format can accommodate quite complex data hierarchies. Python’s built-in library handles reading/writing JSON files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">data = json.load(open(<span class="string">'some_file.json'</span>))</span><br></pre></td></tr></table></figure><p>In addition, pandas.read_json is also available for loading JSON files.</p><p>At its base a JSON object can be thought of as analogous to a Python dictionary or list of dictionaries. For example a table of data from a JSON file could be read into Python as a list of dictionaries where each dictionary represented a row of the table, and the keys of each dictionary were the column names. This formatting is somewhat inefficient for simple tabular data, with column information explicitly repeated with each row, but is useful when representing more intricate relationships in the data. JSON objects often have a highly nested structure that you can think of as dictionaries within dictionaries within dictionaries.</p><p>For example, modern websites track a great deal of information about users’ interactions with the site and the varied nature of these interactions make a table structure too rigid for recording them. In practice, most sites send JSON objects back and forth between the user’s computer and the website’s server. Many data scientists’ primary source of data are ultimately these JSON objects.</p><h4 id="Relational-databases"><a href="#Relational-databases" class="headerlink" title="Relational databases"></a>Relational databases</h4><p>Relational databases, i.e. those that impose a schema on datasets are a major source of data for data science projects. Database tables can naturally be converted into Python objects like Pandas DataFrames. Reading data into a Python environment requires opening a connection to a database and there are various libraries for managing this connection, depending on the type of database to be accessed. Some Relational DataBase Management System (RDBMS) and their corresponding interface utilities for Python:</p><div class="table-container"><table><thead><tr><th style="text-align:left">RDBMS</th><th style="text-align:left">Python Connector</th></tr></thead><tbody><tr><td style="text-align:left">MySQL</td><td style="text-align:left">MySQL Connector</td></tr><tr><td style="text-align:left">PostgreSQL</td><td style="text-align:left">Psycopg</td></tr><tr><td style="text-align:left">SQLite</td><td style="text-align:left">sqlite3</td></tr><tr><td style="text-align:left">Microsoft SQL</td><td style="text-align:left">pyodbc</td></tr></tbody></table></div><p>Each of these tools are designed with maintaining the integrity of the database in mind, including methods for rolling back updates, and ways to safeguard against SQL Injection vulnerabilities. As such, the process of querying the database and ingesting the results can seem fairly involved. For example, here is a basic flow for getting the contents of a table from a PostgreSQL database using psycopg2.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> psycopg2 <span class="keyword">as</span> pg2</span><br><span class="line">conn = pg2.connect(database=<span class="string">'my_db'</span>, user=<span class="string">'my_username'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a cursor to traverse the database</span></span><br><span class="line">cur = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># cursor object executes a query, but does not automatically return results</span></span><br><span class="line">cur.execute(<span class="string">"SELECT * FROM my_table"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Return all query results</span></span><br><span class="line">results = cur.fetchall()</span><br><span class="line"><span class="comment"># WARNING: If the result set is large, it may overwhelm the memory</span></span><br><span class="line"><span class="comment"># resources on your machine.</span></span><br><span class="line"></span><br><span class="line">cur.close()</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><p>While the steps required to connect, query, and disconnect from a relational database are more involved than when loading in data from a file on your local machine, the table structure from the database schema basically guarantees that the data will fit cleanly into a Pandas DataFrame or NumPy Array.</p><h4 id="NoSQL-databases"><a href="#NoSQL-databases" class="headerlink" title="NoSQL databases"></a>NoSQL databases</h4><p>“NoSQL” is a catch-all term referring to “non SQL” or “non relational”, or more recently “not only SQL”. Usually meaning that the method for housing data does not impose a schema on it (or at least not as tightly constrained as in relational databases). This tradeoff gives greater flexibility in what and how data are stored at the cost of increased traversal times when searching the database. This tradeoff is similar to the one we encountered when working with delimited files like CSVs and with JSON files. When loading or dumping data between a file and a database, CSVs are a good match for tables in a relational database, whereas JSONs are more aligned with NoSQL databases.</p><p>The are many examples of NoSQL databases, each with different use cases, and most of which can be accessed with Python.</p><p>One flexible and popular example is MongoDB. MongoDB is a document-oriented database, where a “document” encapsulates and encodes data in a standard format. In the case of MongoDB, that format is JSON-like. Like the relational databases mentioned above, MongoDB has a client for querying it directly, as well as a connector for querying from within Python. These queries are constructed usingMongoDB’s query syntax.</p><p>The Python connector to MongoDB is PyMongo.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"><span class="comment"># By default a Mongo db running locally is accessible via port 27017</span></span><br><span class="line">client = MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'database_name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Within a db, documents are grouped into "collections" -- roughly equivalent</span></span><br><span class="line"><span class="comment"># to tables in a relational db.</span></span><br><span class="line">coll = db[<span class="string">'collection_name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Return all the documents within the collection</span></span><br><span class="line">docs = coll.find()</span><br></pre></td></tr></table></figure><h4 id="Web-scraping-and-APIs"><a href="#Web-scraping-and-APIs" class="headerlink" title="Web scraping and APIs"></a>Web scraping and APIs</h4><p>Automating the process of downloading content from websites is known as web scraping.</p><p><strong> IMPORTANT </strong></p><blockquote><p>Web scraping can be done in legitimate ways, but just as easily web scraping tools do not stop you from violating a websites terms of service. If a website encourages the sharing of their data then they will create a specific API endpoint that you will use. More often than not the API will require to have an identifying key.</p></blockquote><p>Various tools in Python are available for accessing and parsing webpage data.Requests is a user-friendly library for downloading web pages. It can also be used to retrieve files that are exposed through a URL. For a webpage the data returned from a call using Requests is the HyperText Markup Language (HTML) code that instructs a client such as a web browser how to display a page. This HTML code will often (but not always) contain the data you want to collect from the particular webpage.</p><p>Modern webpages tend to have a great deal of information in their HTML beyond what is shown to the user, so parsing through it all to collect the relevant data can be a daunting task. Fortunately, if a page is readable in your browser, then its HTML must have a coherent structure. Beautiful Soup is a Python library that provides tools for walking through that structure in a systematic and repeatable way. Thus, in the context of web scraping Beautiful Soup can be used to extract the relevant data from the soup of all the other information contained in an HTML file.</p><p>Many websites’ contents are dynamically rendered in such a way that the information displayed on a page never makes it directly into the page’s HTML. In such cases it may not be possible to download the data of interest with a tool like Requests. One option in this scenario is to move to a tool for browser automation, such as Selenium. Selenium’s Python interface is described here.</p><p>Another tool, specifically designed for web scraping in Python, is Scrapy.</p><p>Depending on your website of interest you may have to try several of these tools to successfully collect the relevant data in a scalable way. But a general rule of thumb is that if you can see what you want to collect in your browser, the the website sent it to you, so it should be retrievable.</p><h4 id="Streaming-data"><a href="#Streaming-data" class="headerlink" title="Streaming data"></a>Streaming data</h4><p>In the modern landscape of business data streams are becoming more common. A data stream is a sequence of digitally encoded signals. Data can be streamed for many purposes including storage and further processing (like modeling). Data streams become important when the data of a project or company becomes mature and the AI pipeline is connected to it. As we move into the portions of the AI enterprise workflow that focus on models in production we will be using Apache Spark’s streaming to connect deployed models with streaming data. Data collected from sensors or devices connected via the internet of things are oftent setup to produce streaming data. We will work specifically with these types of data in module 5.</p><h4 id="Apache-Hadoop-File-Share-HDFS"><a href="#Apache-Hadoop-File-Share-HDFS" class="headerlink" title="Apache Hadoop File Share (HDFS)"></a>Apache Hadoop File Share (HDFS)</h4><p>Apache Hadoop File Share (HDFS) is the core of Apache Hadoop , an open source system that is designed to use arrays of commodity hardware to store and manage very large datasets.</p><p>HDFS is the storage component of the system. Large datasets are divided into blocks, and those blocks are distributed and stored across the nodes in an HDFS cluster. Any code that is created to analyze the datasets stored in a Hadoop cluster is executed locally for each block of data, and in parallel. This parallel analysis of data blocks means that Hadoop can process very large data sets rapidly.</p><p>The Hadoop framework itself is written mostly in Java. However, any language, including Python, may be used to analyze the data stored in a Hadoop cluster. The Apache Foundation provides a number of other packages that may be installed alongside Hadoop to add additional relational database functionality and improve scalability.</p><p>IMPORTANT: Apache Hadoop is a de facto standard in many large enterprises today. It is often used with Apache Spark and a NoSQL database engine to provide data storage and management of data pipelines used by machine learning models.</p><h4 id="Other-sources-of-data-formats"><a href="#Other-sources-of-data-formats" class="headerlink" title="Other sources of data formats"></a>Other sources of data formats</h4><div class="table-container"><table><thead><tr><th style="text-align:left">Format</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">HDF5</td><td style="text-align:left">There is a is a hierarchical format HDF5 used to store complex scientific data. The format is useful for storing and sharing large amounts of data.</td></tr><tr><td style="text-align:left">NumPy’s *.npy and *.nzp formats</td><td style="text-align:left">NumPy has its own binary format (NPY) and the NPZ format is an extension of it that allows multiple arrays and compression.</td></tr></tbody></table></div><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this module you should have learned:</p><ul><li>Stakeholder or domain expert opinion, feasibility and impact are three of the most important factors when prioritizing business opportunities</li><li>The practice of articulating a business opportunity, with the data in mind, as a testable hypothesis helps keep the overall project linked to the business needs</li><li>The notion of degree of belief is important when making statements both in science and in business. No statement has 100% degree of belief, it is some percentage of 100% that is a reflection of accumulated evidence</li><li>The scientific method helps formalize a process for rationalizing business decisions through experimentation and evidence</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Empathize-Process&quot;&gt;&lt;a href=&quot;#Empathize-Process&quot; class=&quot;headerlink&quot; title=&quot;Empathize Process&quot;&gt;&lt;/a&gt;Empathize Process&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Get 
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Process Model" scheme="https://zhangruochi.com/categories/AI-Workflow/Process-Model/"/>
    
    
  </entry>
  
  <entry>
    <title>Decision Thinking and Data Science Process</title>
    <link href="https://zhangruochi.com/Decision-Thinking-and-Data-Science-Process/2020/04/21/"/>
    <id>https://zhangruochi.com/Decision-Thinking-and-Data-Science-Process/2020/04/21/</id>
    <published>2020-04-21T09:18:18.000Z</published>
    <updated>2020-04-21T21:40:13.008Z</updated>
    
    <content type="html"><![CDATA[<p>Between design thinking and the above mentioned process models there are several 1:1 relationships between stages. The other relationships are generally straightforward to delineate. The design thinking process is consists of five stages and it has the distinct advantage of being applied outside of data science.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Design Thinking</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Process Models</div></center><p>It is the details that keep you flowing from one stage to the next, while iterating in ways that are business driven that makes the contents of this course new. Let’s use a simple example to illustrate the basic process.</p><blockquote><p>A friend of yours just opened a new Sherlock Holmes themed café. Her café is state-of-the-art complete with monitors built into the tables. The business is off to a good start, but she has gotten some feedback that the games could use improvement. She knows that good games keep the customers around a little longer. The games are a way to keep customers entertained while they drink coffee and buy food items. She has some games already, but wants your help to create a few more games to keep customers both informed and entertained.</p></blockquote><p>Being a data scientist you would not just sit down and create a game—you are, of course, going to create based on your initial investigation of the business scenario.</p><h3 id="Empathize"><a href="#Empathize" class="headerlink" title="Empathize"></a>Empathize</h3><p><strong>In this stage time is dedicated to understanding the business opportunities.</strong></p><p>In this setting the frequency and duration of customer visits are going to be related to overall sales. The initial business opportunity here is How do you ensure new games drive revenue?. There are many other business opportunities, like what is the optimal menu for the customer-base and do seasonal variations of offerings help the business?, but lets focus on the initial one for this example. As part of this stage you would talk with your friend, her employees and some customers to do your best to fully understand the experience of the customer. The important thing here is to spend time on-site simulating the experience of a customer to obtain as genuine an understanding of the problem as possible. You may realize that most customers are there to work or most of them are just passing through. This domain knowledge is useful when making decisions like which new types of new games to create. After you have gathered your information and studied it you will generally articulate the business scenario using a scientific thought process—this means a statement that can be tested. The business opportunity should be stated in a way that minimizes the presence of confounding factors.</p><p>There are logical follow-up questions to ask to fully understand the problem, but the next two stages are the more appropriate places to get into these details. Now that you understand the problem it is time to gather the data.</p><p><strong>HINT</strong>: This is the stage where we gather all of the data and we make note of what would be ideal data.</p><ol><li><p>The data here are mostly sales and customer profiles. There are two important aspects of the data that would be ideal:</p></li><li><p>The data are at a transaction level (each purchase and its associated data are recorded)<br>We can associate game usage with transactions.</p></li></ol><p>Fortunately for us this is a modern cafe so customers order and play games through the same interface. Additionally, they are incentivized to login to the system and generate a customer profile. In this stage we go through the process of gathering the raw data. This may involve querying a database, gathering files, web-scraping and other mechanisms. It is important to gather <strong>all of the relevant data</strong> in this stage, because access and quality of the data may force you to modify the business question. It is very difficult to assess the quality of data when it is not in hand. If possible effort should be made to collect even marginally related data.</p><p>Lets assume that your initial investigation led you to understand that games that used quotations from the books in an interactive way were the most effective. So you have come up with the idea to develop a game that is built on a chatbot that has been trained to talk like Sherlock. This would involve Natural Language Processing (NLP) and we would need a corpus. As a start you might download The Adventures of Sherlock Holmes, by Arthur Conan Doyle from Project Gutenberg.</p><p><strong>HINT</strong>: This is a live coding example and we suggest that you open a Jupyter notebook either locally or within Watson Studio so that you may annotate and expand on the example freely.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">text = requests.get(<span class="string">'https://www.gutenberg.org/files/1661/1661-0.txt'</span>).text</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"sherlock-holmes.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> text_file:</span><br><span class="line">    text_file.write(text)</span><br></pre></td></tr></table></figure><h3 id="Define"><a href="#Define" class="headerlink" title="Define"></a>Define</h3><p><strong>This is the data wrangling stage</strong></p><p>Given the data, an understanding of the business scenario and your gathered domain knowledge you will next perform your data cleaning and preliminary exploratory data analysis. To get to the point of preliminary investigation into the findings from the empathize stage it is frequently the case that we need to clean our data.</p><p>This could involve parsing JSON, manipulating SQL queries, reading CSV, cleaning a corpus of text, sifting through images, and so much more. One common goal of this part of the process is the creation of one or more Pandas dataframes or NumPy arrays that will be used for initial exploratory data analysis (EDA).</p><blockquote><p>Exploratory data analysis (EDA) is the process of analyzing data sets to create summaries and visualizations of the data. These summaries and visualizations are then used to guide the use of the data for solving business challenges.</p></blockquote><p><strong>HINT</strong>: This is the stage where we perform the initial EDA</p><p>Sometimes we need to perform a little EDA in order to determine how to best clean the data so these two steps are not necessarily distinct from each other. Visualization, basic hypothesis testing and simple feature engineering are among the most important tasks for EDA at this stage. An minimal example of a EDA plot is one where we look at the average number of words per sentence for the name mentions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## read in the book        </span></span><br><span class="line">text = open(<span class="string">'sherlock-holmes.txt'</span>, <span class="string">'r'</span>).read()</span><br><span class="line"></span><br><span class="line"><span class="comment">## do some basic parsing and cleaning of sentences</span></span><br><span class="line">stop_pattern = <span class="string">'\.|\?|\!'</span></span><br><span class="line">sentences = re.split(stop_pattern, text)</span><br><span class="line">sentences = [re.sub(<span class="string">"\r|\n"</span>,<span class="string">" "</span>,s.lower()) <span class="keyword">for</span> s <span class="keyword">in</span> sentences][<span class="number">3</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">## extract a few features and create a pandas df</span></span><br><span class="line">has_sherlock =  [<span class="keyword">True</span> <span class="keyword">if</span> re.search(<span class="string">"sherlock|holmes"</span>,s) <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">has_watson = [<span class="keyword">True</span> <span class="keyword">if</span> re.search(<span class="string">"john|watson"</span>,s) <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'text'</span>:sentences,<span class="string">'has_sherlock'</span>:has_sherlock,<span class="string">'has_watson'</span>:has_watson&#125;)</span><br><span class="line">df[<span class="string">'num_words'</span>] = df[<span class="string">'text'</span>].apply(<span class="keyword">lambda</span> x: len(x.split(<span class="string">" "</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">## make eda plot</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line"></span><br><span class="line">data1 = df[df[<span class="string">'has_sherlock'</span>]==<span class="keyword">True</span>]</span><br><span class="line">data2 = df[df[<span class="string">'has_watson'</span>]==<span class="keyword">True</span>]</span><br><span class="line"></span><br><span class="line">data = [df[df[col]==<span class="keyword">True</span>][<span class="string">'num_words'</span>].values <span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">'has_sherlock'</span>,<span class="string">'has_watson'</span>]]</span><br><span class="line"></span><br><span class="line">pos = [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">ax1.violinplot(data, pos, points=<span class="number">40</span>, widths=<span class="number">0.5</span>,showextrema=<span class="keyword">True</span>, showmedians=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">'Sherlock'</span>, <span class="string">'Watson'</span>]</span><br><span class="line">ax1.set_xticks(np.arange(<span class="number">1</span>, len(labels) + <span class="number">1</span>))</span><br><span class="line">ax1.set_xticklabels(labels)</span><br><span class="line">ax1.set_xlim(<span class="number">0.25</span>, len(labels) + <span class="number">0.75</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">'Feature'</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">'# Words'</span>)</span><br><span class="line">ax1.set_title(<span class="string">"Words per sentence"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="Ideate"><a href="#Ideate" class="headerlink" title="Ideate"></a>Ideate</h3><p><strong>This is the stage where we modify our data and our features</strong></p><p>Now that you have clean data the data processing must continue until you are ready to input your data into a model. This stage contains all of the possible data manipulations you might perform before modeling. Perhaps the data need to be log transformed, standardized, reduced in dimensionality, kernel transformed, engineered to contain more features or transformed in some other way.</p><p>For our text data we would likely want to dig into the sentences themselves to make sure they fit the desired use case. If we were building a chatbot to engage with in a very Holmes manner then we would likely want to remove any sentences that were not said by Mr. Holmes, but his name was mentioned. If we were building a predictive model to determine which story a phrase would most likely have been generated, we would need to create a new column in our data frame representing the books themselves.</p><p>When working with text data many models that we might consider prefer a numeric representation of the data. This may be occurrences, frequencies, or another transformation of the original data. It is in this stage that these types of transformations are readied or carried out. For example here we import the necessary transformers for usage in the next stage.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract the data to be used in the model from the df</span></span><br><span class="line">labels = np.zeros(df.shape[<span class="number">0</span>])</span><br><span class="line">labels[(df[<span class="string">'has_sherlock'</span>] == <span class="keyword">True</span>)] = <span class="number">1</span></span><br><span class="line">labels[(df[<span class="string">'has_watson'</span>] == <span class="keyword">True</span>)] = <span class="number">2</span></span><br><span class="line">df[<span class="string">'labels'</span>] = labels</span><br><span class="line">df = df[df[<span class="string">'labels'</span>]!=<span class="number">0</span>]</span><br><span class="line">X = df[<span class="string">'text'</span>].values</span><br><span class="line">y = df[<span class="string">'labels'</span>].values</span><br></pre></td></tr></table></figure><p>There are a lot of ways to prepare data for different models. In some case you will not know the best transformation or series of transformations until you have run the different models and made a comparison. The concept of pipelines is extremely useful for iterating over different permutations of transformers and models. The following topics will be covered in detail during Module 3.</p><ul><li>Unsupervised learning</li><li>Feature engineering</li><li>Dimension Reduction</li><li>Simulation</li><li>Missing value imputation</li><li>Outlier detection</li></ul><p><strong>HINT</strong>: This is the stage where we enumerate the advantages and disadvantages of the possible modeling solutions</p><p>Once the transformations are carried or staged as part of some pipeline it is a valuable exercise to document what you know about the process so far. The form that this most commonly takes is a table of possible modeling strategies complete with the advantages and disadvantages of each.</p><h3 id="Prototype"><a href="#Prototype" class="headerlink" title="Prototype"></a>Prototype</h3><p><strong>This is the modeling stage</strong></p><p>The data have been cleaned, processed and staged (ideally in a pipeline) for modeling. The modeling (classic statistics and machine learning) is the bread and butter of data science. This is the stage where most data scientists want to spend the majority of their time. It is where you will interface with the most intriguing aspects of this discipline.</p><p>To illustrate the process to the end shown below is a Support Vector Machine with Stochastic gradient decent as a model. The process involves the use of a train-test split and a pipeline because we want you to be exposed from the very beginning of this course with best practices. Given this example we also see that there can be considerable overlap between the ideate and prototype stages. The overlap exists because transformations of data are generally specific to models–as you will explore which model fits the situation best you will be modifying the transformations of your data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment">## carry out the train test split</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line">text_clf = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, CountVectorizer()),</span><br><span class="line">    (<span class="string">'tfidf'</span>, TfidfTransformer()),</span><br><span class="line">    (<span class="string">'clf'</span>, SGDClassifier(loss=<span class="string">'hinge'</span>, penalty=<span class="string">'l2'</span>,</span><br><span class="line">                        alpha=<span class="number">1e-3</span>, random_state=<span class="number">42</span>,</span><br><span class="line">                        max_iter=<span class="number">5</span>, tol=<span class="keyword">None</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">## train a model</span></span><br><span class="line">text_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p><strong>This is the production, testing and feedback loop stage</strong></p><p>The model works and there are evaluation metrics to provide insight into how well it works. However, the process does not end here. Perhaps the model runs, but it is not yet in production or maybe you want to try different models and/or transformers. Once in production you might want to run some tests to determine if it will handle load or if it will scale well as the data grows. A working model with an impressive f-score does not mean it will be effective in practice. This stage is dedicated to all of the considerations that come after the initial modeling is carried out.</p><p>It is also the stage where you will determine how best to iterate. Design thinking like data science is an iterative process. Our model performed very well (see below), possibly because Dr. Holmes and Dr. Watson are described in very different ways in the stories, but it could be something else.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment">## evaluate the model performance</span></span><br><span class="line">predicted = text_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(metrics.classification_report(y_test, predicted,</span><br><span class="line">      target_names=[<span class="string">'sherlock'</span>,<span class="string">'watson'</span>]))</span><br></pre></td></tr></table></figure><p>As a scientist you always want to remain skeptical about your findings until you have multiple ways to corroborate them. You will also want to always be aware of the overall goal of why you are doing the work you are doing. This example is an interesting metaphor for what can happen as a data scientist. It is possible to go down a path that may only marginally be related to the central business question. Developing a game here is not unlike using a new model for deep-learning or incorporating a new technology into your workflow—it may be fun and it may to some degree help the business case, but you need to always ask yourself is this the best way for me or my team to address the business problem? The questions your ask here are going to guide how best to iterate on the entire workflow.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Between design thinking and the above mentioned process models there are several 1:1 relationships between stages. The other relationship
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Process Model" scheme="https://zhangruochi.com/categories/AI-Workflow/Process-Model/"/>
    
    
  </entry>
  
  <entry>
    <title>Develop a blockchain application from scratch in Python</title>
    <link href="https://zhangruochi.com/Develop-a-blockchain-application-from-scratch-in-Python/2020/04/21/"/>
    <id>https://zhangruochi.com/Develop-a-blockchain-application-from-scratch-in-Python/2020/04/21/</id>
    <published>2020-04-20T16:37:03.000Z</published>
    <updated>2020-04-21T06:02:15.358Z</updated>
    
    <content type="html"><![CDATA[<p>Blockchain is a way of storing digital data. The data can literally be anything. For Bitcoin, it’s the transactions (logs of transfers of Bitcoin from one account to another), but it can even be files; it doesn’t matter. The data is stored in the form of blocks, which are linked (or chained) together using cryptographic hashes — hence the name “blockchain.”</p><p>All of the magic lies in the way this data is stored and added to the blockchain. A blockchain is essentially a linked list that contains ordered data, with a few constraints such as:</p><ul><li>Blocks can’t be modified once added; in other words, it is append only.</li><li>There are specific rules for appending data to it.</li><li>Its architecture is distributed.</li></ul><p>Enforcing these constraints yields the following benefits:</p><ul><li>Immutability and durability of data</li><li>No single point of control or failure</li><li>A verifiable audit trail of the order in which data was added</li></ul><h3 id="Store-transactions-into-blocks"><a href="#Store-transactions-into-blocks" class="headerlink" title="Store transactions into blocks"></a>Store transactions into blocks</h3><p>We’ll be storing data in our blockchain in a format that’s widely used: JSON. Here’s what a post stored in blockchain will look like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  <span class="string">"author"</span>: <span class="string">"some_author_name"</span>, </span><br><span class="line">  <span class="string">"content"</span>: <span class="string">"Some thoughts that author wants to share"</span>, </span><br><span class="line">  <span class="string">"timestamp"</span>: <span class="string">"The time at which the content was created"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The generic term “data” is often replaced on the internet by the term “transactions.” So, just to avoid confusion and maintain consistency, we’ll be using the term “transaction” to refer to data in our example application.</p><p>The transactions are packed into blocks. A block can contain one or many transactions. The blocks containing the transactions are generated frequently and added to the blockchain. Because there can be multiple blocks, each block should have a unique ID.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, index, transactions, timestamp)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Block` class.</span></span><br><span class="line"><span class="string">        :param index: Unique ID of the block.</span></span><br><span class="line"><span class="string">        :param transactions: List of transactions.</span></span><br><span class="line"><span class="string">        :param timestamp: Time of generation of the block.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.index = index </span><br><span class="line">        self.transactions = transactions </span><br><span class="line">        self.timestamp = timestamp</span><br></pre></td></tr></table></figure><h3 id="Add-digital-fingerprints-to-the-blocks"><a href="#Add-digital-fingerprints-to-the-blocks" class="headerlink" title="Add digital fingerprints to the blocks"></a>Add digital fingerprints to the blocks</h3><p>We’d like to prevent any kind of tampering in the data stored inside the block, and detection is the first step to that. To detect if the data in the block has been tampered with, you can use cryptographic hash functions.</p><p>A hash function is a function that takes data of any size and produces data of a fixed size from it (a hash), which is generally used to identify the input. The characteristics of an ideal hash function are:</p><ul><li>It should be easy to compute.</li><li>It should be deterministic, meaning the same data will always result in the same hash.</li><li>It should be uniformly random, meaning even a single bit change in the data should change the hash significantly.</li></ul><p>The consequence of this is:</p><ul><li>It is virtually impossible to guess the input data given the hash. (The only way is to try all possible input combinations.)</li><li>If you know both the input and the hash, you can simply pass the input through the hash function to verify the provided hash.</li></ul><p>This asymmetry of efforts that’s required to figure out the hash from an input (easy) vs. figuring out the input from a hash (almost impossible) is what blockchain leverages to obtain the desired characteristics.</p><p>We’ll store the hash of the block in a field inside our Block object, and it will act like a digital fingerprint (or signature) of data contained in it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns the hash of the block instance by first converting it</span></span><br><span class="line"><span class="string">    into JSON string.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br></pre></td></tr></table></figure><p><strong>Note</strong>: In most cryptocurrencies, even the individual transactions in the block are hashed and then stored to form a hash tree (also known as a merkle tree). The root of the tree usually represents the hash of the block. It’s not a necessary requirement for the functioning of the blockchain, so we’re omitting it to keep things simple.</p><h3 id="Chain-the-blocks"><a href="#Chain-the-blocks" class="headerlink" title="Chain the blocks"></a>Chain the blocks</h3><p>Okay, we’ve now set up the blocks. The blockchain is supposed to be a collection of blocks. We can store all the blocks in the Python list (the equivalent of an array). But this is not sufficient, because what if someone intentionally replaces an old block with a new block in the collection? Creating a new block with altered transactions, computing the hash, and replacing it with any older block is no big deal in our current implementation.</p><p>We need a way to make sure that any change in the previous blocks invalidates the entire chain. The Bitcoin way to do this is to create dependency among consecutive blocks by chaining them with the hash of the block immediately previous to them. By chaining here, we mean to include the hash of the previous block in the current block in a new field called previous_hash.</p><p>Okay, if every block is linked to the previous block through the previous_hash field, what about the very first block? That block is called the genesis block and it can be generated either manually or through some unique logic. Let’s add the previous_hash field to the Block class and implement the initial structure of our Blockchain class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    def__init__(self, index, transactions, timestamp, previous_hash):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Block` class.</span></span><br><span class="line"><span class="string">        :param index:         Unique ID of the block.</span></span><br><span class="line"><span class="string">        :param transactions:  List of transactions.</span></span><br><span class="line"><span class="string">        :param timestamp:     Time of generation of the block.</span></span><br><span class="line"><span class="string">        :param previous_hash: Hash of the previous block in the chain which this block is part of.                                        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.index = index</span><br><span class="line">        self.transactions = transactions</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line">        self.previous_hash = previous_hash <span class="comment"># Adding the previous hash field</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Returns the hash of the block instance by first converting it</span></span><br><span class="line"><span class="string">        into JSON string.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>) <span class="comment"># The string equivalent also considers the previous_hash field now</span></span><br><span class="line">        <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Blockchain` class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_genesis_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function to generate genesis block and appends it to</span></span><br><span class="line"><span class="string">        the chain. The block has index 0, previous_hash as 0, and</span></span><br><span class="line"><span class="string">        a valid hash.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        genesis_block = Block(<span class="number">0</span>, [], time.time(), <span class="string">"0"</span>)</span><br><span class="line">        genesis_block.hash = genesis_block.compute_hash()</span><br><span class="line">        self.chain.append(genesis_block)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">last_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A quick pythonic way to retrieve the most recent block in the chain. Note that</span></span><br><span class="line"><span class="string">        the chain will always consist of at least one block (i.e., genesis block)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.chain[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><p>Now, if the content of any of the previous blocks changes:</p><ul><li>The hash of that previous block would change.</li><li>This will lead to a mismatch with the previous_hash field in the next block.</li><li>Since the input data to compute the hash of any block also consists of the previous_hash field, the hash of the next block will also change.</li></ul><p>Ultimately, the entire chain following the replaced block is invalidated, and the only way to fix it is to recompute the entire chain.</p><h3 id="Implement-a-proof-of-work-algorithm"><a href="#Implement-a-proof-of-work-algorithm" class="headerlink" title="Implement a proof of work algorithm"></a>Implement a proof of work algorithm</h3><p>There is one problem, though. If we change the previous block, the hashes of all the blocks that follow can be re-computed quite easily to create a different valid blockchain. To prevent this, we can exploit the asymmetry in efforts of hash functions that we discussed earlier to make the task of calculating the hash difficult and random. Here’s how we do this: Instead of accepting any hash for the block, we add some constraint to it. Let’s add a constraint that our hash should start with “n leading zeroes” where n can be any positive integer.</p><p>We know that unless we change the data of the block, the hash is not going to change, and of course we don’t want to change existing data. So what do we do? Simple! We’ll add some dummy data that we can change. Let’s introduce a new field in our block called nonce. A nonce is a number that we can keep on changing until we get a hash that satisfies our constraint. The nonce satisfying the constraint serves as proof that some computation has been performed. This technique is a simplified version of the Hashcash algorithm used in Bitcoin. The number of zeroes specified in the constraint determines the difficulty of our proof of work algorithm (the greater the number of zeroes, the harder it is to figure out the nonce).</p><p>Also, due to the asymmetry, proof of work is difficult to compute but very easy to verify once you figure out the nonce (you just have to run the hash function again):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="comment"># difficulty of PoW algorithm</span></span><br><span class="line">    difficulty = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd..</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">proof_of_work</span><span class="params">(self, block)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Function that tries different values of the nonce to get a hash</span></span><br><span class="line"><span class="string">        that satisfies our difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        computed_hash = block.compute_hash()</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> computed_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty):</span><br><span class="line">            block.nonce += <span class="number">1</span></span><br><span class="line">            computed_hash = block.compute_hash()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> computed_hash</span><br></pre></td></tr></table></figure><p>Notice that there is no specific logic to figuring out the nonce quickly; it’s just brute force. The only definite improvement that you can make is to use hardware chips that are specially designed to compute the hash function in a smaller number of CPU instructions.</p><h3 id="Add-blocks-to-the-chain"><a href="#Add-blocks-to-the-chain" class="headerlink" title="Add blocks to the chain"></a>Add blocks to the chain</h3><p>To add a block to the chain, we’ll first have to verify that:</p><ul><li>The data has not been tampered with (the proof of work provided is correct).</li><li>The order of transactions is preserved (the previous_hash field of the block to be added points to the hash of the latest block in our chain).</li></ul><p>Let’s see the code for adding blocks into the chain:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd..</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_block</span><span class="params">(self, block, proof)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that adds the block to the chain after verification.</span></span><br><span class="line"><span class="string">        Verification includes:</span></span><br><span class="line"><span class="string">        * Checking if the proof is valid.</span></span><br><span class="line"><span class="string">        * The previous_hash referred in the block and the hash of a latest block</span></span><br><span class="line"><span class="string">          in the chain match.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        previous_hash = self.last_block.hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> previous_hash != block.previous_hash:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Blockchain.is_valid_proof(block, proof):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        block.hash = proof</span><br><span class="line">        self.chain.append(block)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_valid_proof</span><span class="params">(self, block, block_hash)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Check if block_hash is valid hash of block and satisfies</span></span><br><span class="line"><span class="string">        the difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (block_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty) <span class="keyword">and</span></span><br><span class="line">                block_hash == block.compute_hash())</span><br></pre></td></tr></table></figure><h3 id="Mining"><a href="#Mining" class="headerlink" title="Mining"></a>Mining</h3><p>The transactions will be initially stored as a pool of unconfirmed transactions. The process of putting the unconfirmed transactions in a block and computing proof of work is known as the mining of blocks. Once the nonce satisfying our constraints is figured out, we can say that a block has been mined and it can be put into the blockchain.</p><p>In most of the cryptocurrencies (including Bitcoin), miners may be awarded some cryptocurrency as a reward for spending their computing power to compute a proof of work. Here’s what our mining function looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions = [] <span class="comment"># data yet to get into blockchain</span></span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_new_transaction</span><span class="params">(self, transaction)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions.append(transaction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mine</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        This function serves as an interface to add the pending</span></span><br><span class="line"><span class="string">        transactions to the blockchain by adding them to the block</span></span><br><span class="line"><span class="string">        and figuring out proof of work.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.unconfirmed_transactions:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        last_block = self.last_block</span><br><span class="line"></span><br><span class="line">        new_block = Block(index=last_block.index + <span class="number">1</span>,</span><br><span class="line">                          transactions=self.unconfirmed_transactions,</span><br><span class="line">                          timestamp=time.time(),</span><br><span class="line">                          previous_hash=last_block.hash)</span><br><span class="line"></span><br><span class="line">        proof = self.proof_of_work(new_block)</span><br><span class="line">        self.add_block(new_block, proof)</span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        <span class="keyword">return</span> new_block.index</span><br></pre></td></tr></table></figure><h3 id="Combined-Code"><a href="#Combined-Code" class="headerlink" title="Combined Code"></a>Combined Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, index, transactions, timestamp, previous_hash)</span>:</span></span><br><span class="line">        self.index = index</span><br><span class="line">        self.transactions = transactions</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line">        self.previous_hash = previous_hash</span><br><span class="line">        self.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that return the hash of the block contents.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="comment"># difficulty of our PoW algorithm</span></span><br><span class="line">    difficulty = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_genesis_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function to generate genesis block and appends it to</span></span><br><span class="line"><span class="string">        the chain. The block has index 0, previous_hash as 0, and</span></span><br><span class="line"><span class="string">        a valid hash.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        genesis_block = Block(<span class="number">0</span>, [], time.time(), <span class="string">"0"</span>)</span><br><span class="line">        genesis_block.hash = genesis_block.compute_hash()</span><br><span class="line">        self.chain.append(genesis_block)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">last_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.chain[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_block</span><span class="params">(self, block, proof)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that adds the block to the chain after verification.</span></span><br><span class="line"><span class="string">        Verification includes:</span></span><br><span class="line"><span class="string">        * Checking if the proof is valid.</span></span><br><span class="line"><span class="string">        * The previous_hash referred in the block and the hash of latest block</span></span><br><span class="line"><span class="string">          in the chain match.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        previous_hash = self.last_block.hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> previous_hash != block.previous_hash:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.is_valid_proof(block, proof):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        block.hash = proof</span><br><span class="line">        self.chain.append(block)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_valid_proof</span><span class="params">(self, block, block_hash)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Check if block_hash is valid hash of block and satisfies</span></span><br><span class="line"><span class="string">        the difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (block_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty) <span class="keyword">and</span></span><br><span class="line">                block_hash == block.compute_hash())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">proof_of_work</span><span class="params">(self, block)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Function that tries different values of nonce to get a hash</span></span><br><span class="line"><span class="string">        that satisfies our difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        computed_hash = block.compute_hash()</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> computed_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty):</span><br><span class="line">            block.nonce += <span class="number">1</span></span><br><span class="line">            computed_hash = block.compute_hash()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> computed_hash</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_new_transaction</span><span class="params">(self, transaction)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions.append(transaction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mine</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        This function serves as an interface to add the pending</span></span><br><span class="line"><span class="string">        transactions to the blockchain by adding them to the block</span></span><br><span class="line"><span class="string">        and figuring out Proof Of Work.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.unconfirmed_transactions:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        new_block = Block(index=last_block.index + <span class="number">1</span>,</span><br><span class="line">                          transactions=self.unconfirmed_transactions,</span><br><span class="line">                          timestamp=time.time(),</span><br><span class="line">                          previous_hash=self.last_block.hash)</span><br><span class="line"></span><br><span class="line">        proof = self.proof_of_work(new_block)</span><br><span class="line">        self.add_block(new_block, proof)</span><br><span class="line"></span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        <span class="keyword">return</span> new_block.index</span><br></pre></td></tr></table></figure><h3 id="Create-interfaces"><a href="#Create-interfaces" class="headerlink" title="Create interfaces"></a>Create interfaces</h3><p>Okay, now it’s time to create interfaces for our blockchain node to interact with the application we’re going to build. We’ll be using a popular Python microframework called Flask to create a REST API that interacts with and invokes various operations in our blockchain node. If you’ve worked with any web framework before, the code below shouldn’t be difficult to follow along.</p><p>These REST endpoints can be used to play around with our blockchain by creating some transactions and then mining them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize flask application</span></span><br><span class="line">app =  Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a blockchain object.</span></span><br><span class="line">blockchain = Blockchain()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### We need an endpoint for our application to submit a new transaction. This will be used by our application to add new data (posts) to the blockchain:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Flask's way of declaring end-points</span></span><br><span class="line"><span class="meta">@app.route('/new_transaction', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_transaction</span><span class="params">()</span>:</span></span><br><span class="line">    tx_data = request.get_json()</span><br><span class="line">    required_fields = [<span class="string">"author"</span>, <span class="string">"content"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> field <span class="keyword">in</span> required_fields:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> tx_data.get(field):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"Invalid transaction data"</span>, <span class="number">404</span></span><br><span class="line"></span><br><span class="line">    tx_data[<span class="string">"timestamp"</span>] = time.time()</span><br><span class="line"></span><br><span class="line">    blockchain.add_new_transaction(tx_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Success"</span>, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### Here’s an endpoint to return the node’s copy of the chain. Our application will be using this endpoint to query all of the data to display:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/chain', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_chain</span><span class="params">()</span>:</span></span><br><span class="line">    chain_data = []</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blockchain.chain:</span><br><span class="line">        chain_data.append(block.__dict__)</span><br><span class="line">    <span class="keyword">return</span> json.dumps(&#123;<span class="string">"length"</span>: len(chain_data),</span><br><span class="line">                       <span class="string">"chain"</span>: chain_data&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here’s an endpoint to request the node to mine the unconfirmed transactions (if any). We’ll be using it to initiate a command to mine from our application itself:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/mine', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mine_unconfirmed_transactions</span><span class="params">()</span>:</span></span><br><span class="line">    result = blockchain.mine()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> result:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"No transactions to mine"</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Block #&#123;&#125; is mined."</span>.format(result)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/pending_tx')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pending_tx</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(blockchain.unconfirmed_transactions)</span><br></pre></td></tr></table></figure><h3 id="Establish-consensus-and-decentralization"><a href="#Establish-consensus-and-decentralization" class="headerlink" title="Establish consensus and decentralization"></a>Establish consensus and decentralization</h3><p>Up to this point, the blockchain that we’ve implemented is meant to run on a single computer. Even though we’re linking block with hashes and applying the proof of work constraint, we still can’t trust a single entity (in our case, a single machine). We need the data to be distributed, we need multiple nodes maintaining the blockchain. So, to transition from a single node to a peer-to-peer network, let’s first create a mechanism to let a new node become aware of other peers in the network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Contains the host addresses of other participating members of the network</span></span><br><span class="line">peers = set()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Endpoint to add new peers to the network</span></span><br><span class="line"><span class="meta">@app.route('/register_node', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_new_peers</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># The host address to the peer node </span></span><br><span class="line">    node_address = request.get_json()[<span class="string">"node_address"</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node_address:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Invalid data"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add the node to the peer list</span></span><br><span class="line">    peers.add(node_address)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the blockchain to the newly registered node so that it can sync</span></span><br><span class="line">    <span class="keyword">return</span> get_chain()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/register_with', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_with_existing_node</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Internally calls the `register_node` endpoint to</span></span><br><span class="line"><span class="string">    register current node with the remote node specified in the</span></span><br><span class="line"><span class="string">    request, and sync the blockchain as well with the remote node.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    node_address = request.get_json()[<span class="string">"node_address"</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node_address:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Invalid data"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    data = &#123;<span class="string">"node_address"</span>: request.host_url&#125;</span><br><span class="line">    headers = &#123;<span class="string">'Content-Type'</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make a request to register with remote node and obtain information</span></span><br><span class="line">    response = requests.post(node_address + <span class="string">"/register_node"</span>,</span><br><span class="line">                             data=json.dumps(data), headers=headers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">global</span> blockchain</span><br><span class="line">        <span class="keyword">global</span> peers</span><br><span class="line">        <span class="comment"># update chain and the peers</span></span><br><span class="line">        chain_dump = response.json()[<span class="string">'chain'</span>]</span><br><span class="line">        blockchain = create_chain_from_dump(chain_dump)</span><br><span class="line">        peers.update(response.json()[<span class="string">'peers'</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Registration successful"</span>, <span class="number">200</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># if something goes wrong, pass it on to the API response</span></span><br><span class="line">        <span class="keyword">return</span> response.content, response.status_code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_chain_from_dump</span><span class="params">(chain_dump)</span>:</span></span><br><span class="line">    blockchain = Blockchain()</span><br><span class="line">    <span class="keyword">for</span> idx, block_data <span class="keyword">in</span> enumerate(chain_dump):</span><br><span class="line">        block = Block(block_data[<span class="string">"index"</span>],</span><br><span class="line">                      block_data[<span class="string">"transactions"</span>],</span><br><span class="line">                      block_data[<span class="string">"timestamp"</span>],</span><br><span class="line">                      block_data[<span class="string">"previous_hash"</span>])</span><br><span class="line">        proof = block_data[<span class="string">'hash'</span>]</span><br><span class="line">        <span class="keyword">if</span> idx &gt; <span class="number">0</span>:</span><br><span class="line">            added = blockchain.add_block(block, proof)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> added:</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">"The chain dump is tampered!!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># the block is a genesis block, no verification needed</span></span><br><span class="line">            blockchain.chain.append(block)</span><br><span class="line">    <span class="keyword">return</span> blockchain</span><br></pre></td></tr></table></figure><p>A new node participating in the network can invoke the register_with_existing_node method (via the /register_with endpoint) to register with existing nodes in the network. This will help with the following:</p><ul><li>Asking the remote node to add a new peer to its list of known peers.</li><li>Initializing the blockchain of the new node with that of the remote node.</li><li>Resyncing the blockchain with the network if the node goes off-grid.</li></ul><p>However, there’s a problem with multiple nodes. Due to intentional manipulation or unintentional reasons (like network latency), the copy of chains of a few nodes can differ. In that case, the nodes need to agree upon some version of the chain to maintain the integrity of the entire system. In other words, we need to achieve consensus.</p><p>A simple consensus algorithm could be to agree upon the longest valid chain when the chains of different participating nodes in the network appear to diverge. The rationale behind this approach is that the longest chain is a good estimate of the most amount of work done (remember proof of work is difficult to compute):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span></span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class">    <span class="title">previous</span> <span class="title">code</span> <span class="title">continued</span>...</span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class">    <span class="title">def</span> <span class="title">check_chain_validity</span><span class="params">(cls, chain)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A helper method to check if the entire blockchain is valid.            </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = <span class="keyword">True</span></span><br><span class="line">        previous_hash = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate through every block</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> chain:</span><br><span class="line">            block_hash = block.hash</span><br><span class="line">            <span class="comment"># remove the hash field to recompute the hash again</span></span><br><span class="line">            <span class="comment"># using `compute_hash` method.</span></span><br><span class="line">            delattr(block, <span class="string">"hash"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cls.is_valid_proof(block, block.hash) <span class="keyword">or</span> \</span><br><span class="line">                    previous_hash != block.previous_hash:</span><br><span class="line">                result = <span class="keyword">False</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            block.hash, previous_hash = block_hash, block_hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consensus</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Our simple consensus algorithm. If a longer valid chain is</span></span><br><span class="line"><span class="string">    found, our chain is replaced with it.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">global</span> blockchain</span><br><span class="line"></span><br><span class="line">    longest_chain = <span class="keyword">None</span></span><br><span class="line">    current_len = len(blockchain.chain)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> peers:</span><br><span class="line">        response = requests.get(<span class="string">'&#123;&#125;/chain'</span>.format(node))</span><br><span class="line">        length = response.json()[<span class="string">'length'</span>]</span><br><span class="line">        chain = response.json()[<span class="string">'chain'</span>]</span><br><span class="line">        <span class="keyword">if</span> length &gt; current_len <span class="keyword">and</span> blockchain.check_chain_validity(chain):</span><br><span class="line">              <span class="comment"># Longer valid chain found!</span></span><br><span class="line">            current_len = length</span><br><span class="line">            longest_chain = chain</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> longest_chain:</span><br><span class="line">        blockchain = longest_chain</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>Next, we need to develop a way for any node to announce to the network that it has mined a block so that everyone can update their blockchain and move on to mine other transactions. Other nodes can simply verify the proof of work and add the mined block to their respective chains (remember that verification is easy once the nonce is known):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># endpoint to add a block mined by someone else to</span></span><br><span class="line"><span class="comment"># the node's chain. The node first verifies the block</span></span><br><span class="line"><span class="comment"># and then adds it to the chain.</span></span><br><span class="line"><span class="meta">@app.route('/add_block', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verify_and_add_block</span><span class="params">()</span>:</span></span><br><span class="line">    block_data = request.get_json()</span><br><span class="line">    block = Block(block_data[<span class="string">"index"</span>],</span><br><span class="line">                  block_data[<span class="string">"transactions"</span>],</span><br><span class="line">                  block_data[<span class="string">"timestamp"</span>],</span><br><span class="line">                  block_data[<span class="string">"previous_hash"</span>])</span><br><span class="line"></span><br><span class="line">    proof = block_data[<span class="string">'hash'</span>]</span><br><span class="line">    added = blockchain.add_block(block, proof)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> added:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"The block was discarded by the node"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Block added to the chain"</span>, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">announce_new_block</span><span class="params">(block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A function to announce to the network once a block has been mined.</span></span><br><span class="line"><span class="string">    Other blocks can simply verify the proof of work and add it to their</span></span><br><span class="line"><span class="string">    respective chains.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> peer <span class="keyword">in</span> peers:</span><br><span class="line">        url = <span class="string">"&#123;&#125;add_block"</span>.format(peer)</span><br><span class="line">        requests.post(url, data=json.dumps(block.__dict__, sort_keys=<span class="keyword">True</span>))</span><br></pre></td></tr></table></figure><p>The announce_new_block method should be called after every block is mined by the node so that peers can add it to their chains.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/mine', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mine_unconfirmed_transactions</span><span class="params">()</span>:</span></span><br><span class="line">    result = blockchain.mine()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> result:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"No transactions to mine"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Making sure we have the longest chain before announcing to the network</span></span><br><span class="line">        chain_length = len(blockchain.chain)</span><br><span class="line">        consensus()</span><br><span class="line">        <span class="keyword">if</span> chain_length == len(blockchain.chain):</span><br><span class="line">            <span class="comment"># announce the recently mined block to the network</span></span><br><span class="line">            announce_new_block(blockchain.last_block)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Block #&#123;&#125; is mined."</span>.format(blockchain.last_block.index)</span><br></pre></td></tr></table></figure><h3 id="Build-the-application"><a href="#Build-the-application" class="headerlink" title="Build the application"></a>Build the application</h3><p>Now, it’s time to start working on the interface of our application. We’ve used Jinja2 templating to render the web pages and some CSS to make things look nice.</p><p>Our application needs to connect to a node in the blockchain network to fetch the data and also to submit new data. There can also be multiple nodes, as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> render_template, redirect, request</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> app <span class="keyword">import</span> app</span><br><span class="line"></span><br><span class="line"><span class="comment"># Node in the blockchain network that our application will communicate with</span></span><br><span class="line"><span class="comment"># to fetch and add data.</span></span><br><span class="line">CONNECTED_NODE_ADDRESS = <span class="string">"http://127.0.0.1:8000"</span></span><br><span class="line"></span><br><span class="line">posts = []</span><br></pre></td></tr></table></figure><p>The fetch_posts function gets the data from the node’s /chain endpoint, parses the data, and stores it locally.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_posts</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function to fetch the chain from a blockchain node, parse the</span></span><br><span class="line"><span class="string">    data, and store it locally.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    get_chain_address = <span class="string">"&#123;&#125;/chain"</span>.format(CONNECTED_NODE_ADDRESS)</span><br><span class="line">    response = requests.get(get_chain_address)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        content = []</span><br><span class="line">        chain = json.loads(response.content)</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> chain[<span class="string">"chain"</span>]:</span><br><span class="line">            <span class="keyword">for</span> tx <span class="keyword">in</span> block[<span class="string">"transactions"</span>]:</span><br><span class="line">                tx[<span class="string">"index"</span>] = block[<span class="string">"index"</span>]</span><br><span class="line">                tx[<span class="string">"hash"</span>] = block[<span class="string">"previous_hash"</span>]</span><br><span class="line">                content.append(tx)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">global</span> posts</span><br><span class="line">        posts = sorted(content,</span><br><span class="line">                       key=<span class="keyword">lambda</span> k: k[<span class="string">'timestamp'</span>],</span><br><span class="line">                       reverse=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>The application has an HTML form to take user input and then makes a POST request to a connected node to add the transaction into the unconfirmed transactions pool. The transaction is then mined by the network, and then finally fetched once we refresh our web page:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/submit', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submit_textarea</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Endpoint to create a new transaction via our application</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    post_content = request.form[<span class="string">"content"</span>]</span><br><span class="line">    author = request.form[<span class="string">"author"</span>]</span><br><span class="line"></span><br><span class="line">    post_object = &#123;</span><br><span class="line">        <span class="string">'author'</span>: author,</span><br><span class="line">        <span class="string">'content'</span>: post_content,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Submit a transaction</span></span><br><span class="line">    new_tx_address = <span class="string">"&#123;&#125;/new_transaction"</span>.format(CONNECTED_NODE_ADDRESS)</span><br><span class="line"></span><br><span class="line">    requests.post(new_tx_address,</span><br><span class="line">                  json=post_object,</span><br><span class="line">                  headers=&#123;<span class="string">'Content-type'</span>: <span class="string">'application/json'</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return to the homepage</span></span><br><span class="line">    <span class="keyword">return</span> redirect(<span class="string">'/'</span>)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://developer.ibm.com/tutorials/develop-a-blockchain-application-from-scratch-in-python/" target="_blank" rel="noopener">https://developer.ibm.com/tutorials/develop-a-blockchain-application-from-scratch-in-python/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Blockchain is a way of storing digital data. The data can literally be anything. For Bitcoin, it’s the transactions (logs of transfers of
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Blockchain" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Blockchain/"/>
    
    
  </entry>
  
  <entry>
    <title>Cox Proportional Hazards and Random Survival Forests</title>
    <link href="https://zhangruochi.com/Cox-Proportional-Hazards-and-Random-Survival-Forests/2020/04/19/"/>
    <id>https://zhangruochi.com/Cox-Proportional-Hazards-and-Random-Survival-Forests/2020/04/19/</id>
    <published>2020-04-19T05:54:32.000Z</published>
    <updated>2020-04-19T17:54:52.554Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Cox-Proportional-Hazards-and-Random-Survival-Forests"><a href="#Cox-Proportional-Hazards-and-Random-Survival-Forests" class="headerlink" title="Cox Proportional Hazards and Random Survival Forests"></a>Cox Proportional Hazards and Random Survival Forests</h1><p>Welcome to the final assignment in Course 2! In this assignment you’ll develop risk models using survival data and a combination of linear and non-linear techniques. We’ll be using a dataset with survival data of patients with Primary Biliary Cirrhosis (pbc). PBC is a progressive disease of the liver caused by a buildup of bile within the liver (cholestasis) that results in damage to the small bile ducts that drain bile from the liver. Our goal will be to understand the effects of different factors on the survival times of the patients. Along the way you’ll learn about the following topics: </p><ul><li>Cox Proportional Hazards<ul><li>Data Preprocessing for Cox Models.</li></ul></li><li>Random Survival Forests<ul><li>Permutation Methods for Interpretation.</li></ul></li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1. Import Packages</a></li><li><a href="#2">2. Load the Dataset</a></li><li><a href="#3">3. Explore the Dataset</a></li><li><a href="#4">4. Cox Proportional Hazards</a><ul><li><a href="#Ex-1">Exercise 1</a></li></ul></li><li><a href="#5">5. Fitting and Interpreting a Cox Model</a></li><li><a href="#3">6. Hazard ratio</a><ul><li><a href="#Ex-2">Exercise 2</a></li></ul></li><li><a href="#7">7. Harrell’s C-Index</a><ul><li><a href="#Ex-3">Exercise 3</a></li></ul></li><li><a href="#8">8. Random Survival Forests</a></li><li><a href="#9">9. Permutation Method for Interpretation</a></li></ul><p><a name="1"></a></p><h2 id="1-Import-Packages"><a href="#1-Import-Packages" class="headerlink" title="1. Import Packages"></a>1. Import Packages</h2><p>We’ll first import all the packages that we need for this assignment. </p><ul><li><code>sklearn</code> is one of the most popular machine learning libraries.</li><li><code>numpy</code> is the fundamental package for scientific computing in python.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li><li><code>matplotlib</code> is a plotting library.</li><li><code>lifelines</code> is an open-source survival analysis library.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lifelines <span class="keyword">import</span> CoxPHFitter</span><br><span class="line"><span class="keyword">from</span> lifelines.utils <span class="keyword">import</span> concordance_index <span class="keyword">as</span> cindex</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> load_data</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-the-Dataset"><a href="#2-Load-the-Dataset" class="headerlink" title="2. Load the Dataset"></a>2. Load the Dataset</h2><p>Run the next cell to load the data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = load_data()</span><br></pre></td></tr></table></figure><p><a name="3"></a></p><h2 id="3-Explore-the-Dataset"><a href="#3-Explore-the-Dataset" class="headerlink" title="3. Explore the Dataset"></a>3. Explore the Dataset</h2><p>In the lecture videos <code>time</code> was in months, however in this assignment, <code>time</code> will be converted into years. Also notice that we have assigned a numeric value to <code>sex</code>, where <code>female = 0</code> and <code>male = 1</code>.</p><p>Next, familiarize yourself with the data and the shape of it. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(df.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># df.head() only outputs the top few rows</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><pre><code>(258, 19)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>status</th>      <th>trt</th>      <th>age</th>      <th>sex</th>      <th>ascites</th>      <th>hepato</th>      <th>spiders</th>      <th>edema</th>      <th>bili</th>      <th>chol</th>      <th>albumin</th>      <th>copper</th>      <th>alk.phos</th>      <th>ast</th>      <th>trig</th>      <th>platelet</th>      <th>protime</th>      <th>stage</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.095890</td>      <td>1.0</td>      <td>0.0</td>      <td>58.765229</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>14.5</td>      <td>261.0</td>      <td>2.60</td>      <td>156.0</td>      <td>1718.0</td>      <td>137.95</td>      <td>172.0</td>      <td>190.0</td>      <td>12.2</td>      <td>4.0</td>    </tr>    <tr>      <th>1</th>      <td>12.328767</td>      <td>0.0</td>      <td>0.0</td>      <td>56.446270</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.0</td>      <td>1.1</td>      <td>302.0</td>      <td>4.14</td>      <td>54.0</td>      <td>7394.8</td>      <td>113.52</td>      <td>88.0</td>      <td>221.0</td>      <td>10.6</td>      <td>3.0</td>    </tr>    <tr>      <th>2</th>      <td>2.772603</td>      <td>1.0</td>      <td>0.0</td>      <td>70.072553</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.5</td>      <td>1.4</td>      <td>176.0</td>      <td>3.48</td>      <td>210.0</td>      <td>516.0</td>      <td>96.10</td>      <td>55.0</td>      <td>151.0</td>      <td>12.0</td>      <td>4.0</td>    </tr>    <tr>      <th>3</th>      <td>5.273973</td>      <td>1.0</td>      <td>0.0</td>      <td>54.740589</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.5</td>      <td>1.8</td>      <td>244.0</td>      <td>2.54</td>      <td>64.0</td>      <td>6121.8</td>      <td>60.63</td>      <td>92.0</td>      <td>183.0</td>      <td>10.3</td>      <td>4.0</td>    </tr>    <tr>      <th>6</th>      <td>5.019178</td>      <td>0.0</td>      <td>1.0</td>      <td>55.534565</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>322.0</td>      <td>4.09</td>      <td>52.0</td>      <td>824.0</td>      <td>60.45</td>      <td>213.0</td>      <td>204.0</td>      <td>9.7</td>      <td>3.0</td>    </tr>  </tbody></table></div><p>Take a minute to examine particular cases.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">20</span></span><br><span class="line">df.iloc[i, :]</span><br></pre></td></tr></table></figure><pre><code>time          11.175342status         1.000000trt            0.000000age           44.520192sex            1.000000ascites        0.000000hepato         1.000000spiders        0.000000edema          0.000000bili           2.100000chol         456.000000albumin        4.000000copper       124.000000alk.phos    5719.000000ast          221.880000trig         230.000000platelet      70.000000protime        9.900000stage          2.000000Name: 23, dtype: float64</code></pre><p>Now, split your dataset into train, validation and test set using 60/20/20 split. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">df_dev, df_test = train_test_split(df, test_size = <span class="number">0.2</span>)</span><br><span class="line">df_train, df_val = train_test_split(df_dev, test_size = <span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total number of patients:"</span>, df.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"Total number of patients in training set:"</span>, df_train.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"Total number of patients in validation set:"</span>, df_val.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"Total number of patients in test set:"</span>, df_test.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>Total number of patients: 258Total number of patients in training set: 154Total number of patients in validation set: 52Total number of patients in test set: 52</code></pre><p>Before proceeding to modeling, let’s normalize the continuous covariates to make sure they’re on the same scale. Again, we should normalize the test data using statistics from the train data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">continuous_columns = [<span class="string">'age'</span>, <span class="string">'bili'</span>, <span class="string">'chol'</span>, <span class="string">'albumin'</span>, <span class="string">'copper'</span>, <span class="string">'alk.phos'</span>, <span class="string">'ast'</span>, <span class="string">'trig'</span>, <span class="string">'platelet'</span>, <span class="string">'protime'</span>]</span><br><span class="line">mean = df_train.loc[:, continuous_columns].mean()</span><br><span class="line">std = df_train.loc[:, continuous_columns].std()</span><br><span class="line">df_train.loc[:, continuous_columns] = (df_train.loc[:, continuous_columns] - mean) / std</span><br><span class="line">df_val.loc[:, continuous_columns] = (df_val.loc[:, continuous_columns] - mean) / std</span><br><span class="line">df_test.loc[:, continuous_columns] = (df_test.loc[:, continuous_columns] - mean) / std</span><br></pre></td></tr></table></figure><p>Let’s check the summary statistics on our training dataset to make sure it’s standardized.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_train.loc[:, continuous_columns].describe()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>age</th>      <th>bili</th>      <th>chol</th>      <th>albumin</th>      <th>copper</th>      <th>alk.phos</th>      <th>ast</th>      <th>trig</th>      <th>platelet</th>      <th>protime</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>    </tr>    <tr>      <th>mean</th>      <td>9.833404e-16</td>      <td>-3.258577e-16</td>      <td>1.153478e-16</td>      <td>1.153478e-16</td>      <td>5.767392e-18</td>      <td>1.326500e-16</td>      <td>-1.263059e-15</td>      <td>8.074349e-17</td>      <td>2.018587e-17</td>      <td>1.291896e-14</td>    </tr>    <tr>      <th>std</th>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>    </tr>    <tr>      <th>min</th>      <td>-2.304107e+00</td>      <td>-5.735172e-01</td>      <td>-1.115330e+00</td>      <td>-3.738104e+00</td>      <td>-9.856552e-01</td>      <td>-7.882167e-01</td>      <td>-1.489281e+00</td>      <td>-1.226674e+00</td>      <td>-2.058899e+00</td>      <td>-1.735556e+00</td>    </tr>    <tr>      <th>25%</th>      <td>-6.535035e-01</td>      <td>-4.895812e-01</td>      <td>-5.186963e-01</td>      <td>-5.697976e-01</td>      <td>-6.470611e-01</td>      <td>-5.186471e-01</td>      <td>-8.353982e-01</td>      <td>-6.884514e-01</td>      <td>-6.399831e-01</td>      <td>-7.382590e-01</td>    </tr>    <tr>      <th>50%</th>      <td>-6.443852e-03</td>      <td>-3.846612e-01</td>      <td>-2.576693e-01</td>      <td>5.663556e-02</td>      <td>-3.140636e-01</td>      <td>-3.416086e-01</td>      <td>-2.260984e-01</td>      <td>-2.495932e-01</td>      <td>-4.100373e-02</td>      <td>-1.398807e-01</td>    </tr>    <tr>      <th>75%</th>      <td>5.724289e-01</td>      <td>2.977275e-02</td>      <td>1.798617e-01</td>      <td>6.890921e-01</td>      <td>3.435366e-01</td>      <td>-4.620597e-03</td>      <td>6.061159e-01</td>      <td>3.755727e-01</td>      <td>6.617988e-01</td>      <td>3.587680e-01</td>    </tr>    <tr>      <th>max</th>      <td>2.654276e+00</td>      <td>5.239050e+00</td>      <td>6.243146e+00</td>      <td>2.140730e+00</td>      <td>5.495204e+00</td>      <td>4.869263e+00</td>      <td>3.058176e+00</td>      <td>5.165751e+00</td>      <td>3.190823e+00</td>      <td>4.447687e+00</td>    </tr>  </tbody></table></div><p><a name="4"></a></p><h2 id="4-Cox-Proportional-Hazards"><a href="#4-Cox-Proportional-Hazards" class="headerlink" title="4. Cox Proportional Hazards"></a>4. Cox Proportional Hazards</h2><p>Our goal is to build a risk score using the survival data that we have. We’ll begin by fitting a Cox Proportional Hazards model to your data.</p><p>Recall that the Cox Proportional Hazards model describes the hazard for an individual $i$ at time $t$ as </p><script type="math/tex; mode=display">\lambda(t, x) = \lambda_0(t)e^{\theta^T X_i}</script><p>The $\lambda_0$ term is a baseline hazard and incorporates the risk over time, and the other term incorporates the risk due to the individual’s covariates. After fitting the model, we can rank individuals using the person-dependent risk term $e^{\theta^T X_i}$. </p><p>Categorical variables cannot be used in a regression model as they are. In order to use them, conversion to a series of variables is required.</p><p>Since our data has a mix of categorical (<code>stage</code>) and continuous (<code>wblc</code>) variables, before we proceed further we need to do some data engineering. To tackle the issue at hand we’ll be using the <code>Dummy Coding</code> technique. In order to use Cox Proportional Hazards, we will have to turn the categorical data into one hot features so that we can fit our Cox model. Luckily, Pandas has a built-in function called <code>get_dummies</code> that will make it easier for us to implement our function. It turns categorical features into multiple binary features.</p><p><img src="1-hot-encode.png" style="padding-top: 5px;width: 60%;left: 0px;margin-left: 150px;margin-right: 0px;"></p><p><a name="Ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><p>In the cell below, implement the <code>to_one_hot(...)</code> function.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember to drop the first dummy for each each category to avoid convergence issues when fitting the proportional hazards model.</li>    <li> Check out the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html" target="_blank" rel="noopener"> get_dummies() </a>  documentation. </li>    <li>Use <code>dtype=np.float64</code>.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_one_hot</span><span class="params">(dataframe, columns)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Convert columns in dataframe to one-hot encoding.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataframe (dataframe): pandas dataframe containing covariates</span></span><br><span class="line"><span class="string">        columns (list of strings): list categorical column names to one hot encode</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        one_hot_df (dataframe): dataframe with categorical columns encoded</span></span><br><span class="line"><span class="string">                            as binary variables</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    one_hot_df = pd.get_dummies(dataframe,columns=columns, drop_first = <span class="keyword">True</span>, dtype=np.float64)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot_df</span><br></pre></td></tr></table></figure><p>Now we’ll use the function you coded to transform the training, validation, and test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List of categorical columns</span></span><br><span class="line">to_encode = [<span class="string">'edema'</span>, <span class="string">'stage'</span>]</span><br><span class="line"></span><br><span class="line">one_hot_train = to_one_hot(df_train, to_encode)</span><br><span class="line">one_hot_val = to_one_hot(df_val, to_encode)</span><br><span class="line">one_hot_test = to_one_hot(df_test, to_encode)</span><br><span class="line"></span><br><span class="line">print(one_hot_val.columns.tolist())</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;len(one_hot_val.columns)&#125;</span> columns"</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#39;time&#39;, &#39;status&#39;, &#39;trt&#39;, &#39;age&#39;, &#39;sex&#39;, &#39;ascites&#39;, &#39;hepato&#39;, &#39;spiders&#39;, &#39;bili&#39;, &#39;chol&#39;, &#39;albumin&#39;, &#39;copper&#39;, &#39;alk.phos&#39;, &#39;ast&#39;, &#39;trig&#39;, &#39;platelet&#39;, &#39;protime&#39;, &#39;edema_0.5&#39;, &#39;edema_1.0&#39;, &#39;stage_2.0&#39;, &#39;stage_3.0&#39;, &#39;stage_4.0&#39;]There are 22 columns</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'time'</span>, <span class="string">'status'</span>, <span class="string">'trt'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>, <span class="string">'ascites'</span>, <span class="string">'hepato'</span>, <span class="string">'spiders'</span>, <span class="string">'bili'</span>, <span class="string">'chol'</span>, <span class="string">'albumin'</span>, <span class="string">'copper'</span>, <span class="string">'alk.phos'</span>, <span class="string">'ast'</span>, <span class="string">'trig'</span>, <span class="string">'platelet'</span>, <span class="string">'protime'</span>, <span class="string">'edema_0.5'</span>, <span class="string">'edema_1.0'</span>, <span class="string">'stage_2.0'</span>, <span class="string">'stage_3.0'</span>, <span class="string">'stage_4.0'</span>]</span><br><span class="line">There are <span class="number">22</span> columns</span><br></pre></td></tr></table></figure><h3 id="Look-for-new-features"><a href="#Look-for-new-features" class="headerlink" title="Look for new features"></a>Look for new features</h3><p>Now, let’s take a peek at one of the transformed data sets. Do you notice any new features?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(one_hot_train.shape)</span><br><span class="line">one_hot_train.head()</span><br></pre></td></tr></table></figure><pre><code>(154, 22)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>status</th>      <th>trt</th>      <th>age</th>      <th>sex</th>      <th>ascites</th>      <th>hepato</th>      <th>spiders</th>      <th>bili</th>      <th>chol</th>      <th>...</th>      <th>alk.phos</th>      <th>ast</th>      <th>trig</th>      <th>platelet</th>      <th>protime</th>      <th>edema_0.5</th>      <th>edema_1.0</th>      <th>stage_2.0</th>      <th>stage_3.0</th>      <th>stage_4.0</th>    </tr>  </thead>  <tbody>    <tr>      <th>279</th>      <td>3.868493</td>      <td>0.0</td>      <td>0.0</td>      <td>-0.414654</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>-0.300725</td>      <td>-0.096081</td>      <td>...</td>      <td>0.167937</td>      <td>0.401418</td>      <td>0.330031</td>      <td>0.219885</td>      <td>-1.137178</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>    </tr>    <tr>      <th>137</th>      <td>3.553425</td>      <td>1.0</td>      <td>0.0</td>      <td>0.069681</td>      <td>1.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.895363</td>      <td>0.406085</td>      <td>...</td>      <td>0.101665</td>      <td>0.472367</td>      <td>1.621764</td>      <td>-0.120868</td>      <td>-0.239610</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>    </tr>    <tr>      <th>249</th>      <td>4.846575</td>      <td>0.0</td>      <td>1.0</td>      <td>-0.924494</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>-0.510565</td>      <td>-0.225352</td>      <td>...</td>      <td>0.245463</td>      <td>1.899020</td>      <td>-0.580807</td>      <td>0.422207</td>      <td>0.159309</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>    </tr>    <tr>      <th>266</th>      <td>0.490411</td>      <td>1.0</td>      <td>0.0</td>      <td>1.938314</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.748475</td>      <td>-0.608191</td>      <td>...</td>      <td>-0.650254</td>      <td>-0.288898</td>      <td>-0.481443</td>      <td>-0.727833</td>      <td>1.356065</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>    </tr>    <tr>      <th>1</th>      <td>12.328767</td>      <td>0.0</td>      <td>0.0</td>      <td>0.563645</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>-0.405645</td>      <td>-0.210436</td>      <td>...</td>      <td>2.173526</td>      <td>-0.144699</td>      <td>-0.531125</td>      <td>-0.450972</td>      <td>-0.139881</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>    </tr>  </tbody></table><p>5 rows × 22 columns</p></div><p><a name="5"></a></p><h2 id="5-Fitting-and-Interpreting-a-Cox-Model"><a href="#5-Fitting-and-Interpreting-a-Cox-Model" class="headerlink" title="5. Fitting and Interpreting a Cox Model"></a>5. Fitting and Interpreting a Cox Model</h2><p>Run the following cell to fit your Cox Proportional Hazards model using the <code>lifelines</code> package.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cph = CoxPHFitter()</span><br><span class="line">cph.fit(one_hot_train, duration_col = <span class="string">'time'</span>, event_col = <span class="string">'status'</span>, step_size=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;lifelines.CoxPHFitter: fitted with 154 total observations, 90 right-censored observations&gt;</code></pre><p>You can use <code>cph.print_summary()</code> to view the coefficients associated with each covariate as well as confidence intervals. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cph.print_summary()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <tbody>    <tr>      <th>model</th>      <td>lifelines.CoxPHFitter</td>    </tr>    <tr>      <th>duration col</th>      <td>'time'</td>    </tr>    <tr>      <th>event col</th>      <td>'status'</td>    </tr>    <tr>      <th>number of observations</th>      <td>154</td>    </tr>    <tr>      <th>number of events observed</th>      <td>64</td>    </tr>    <tr>      <th>partial log-likelihood</th>      <td>-230.82</td>    </tr>    <tr>      <th>time fit was run</th>      <td>2020-04-19 16:30:56 UTC</td>    </tr>  </tbody></table></div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>coef</th>      <th>exp(coef)</th>      <th>se(coef)</th>      <th>coef lower 95%</th>      <th>coef upper 95%</th>      <th>exp(coef) lower 95%</th>      <th>exp(coef) upper 95%</th>      <th>z</th>      <th>p</th>      <th>-log2(p)</th>    </tr>  </thead>  <tbody>    <tr>      <th>trt</th>      <td>-0.22</td>      <td>0.80</td>      <td>0.30</td>      <td>-0.82</td>      <td>0.37</td>      <td>0.44</td>      <td>1.45</td>      <td>-0.73</td>      <td>0.46</td>      <td>1.11</td>    </tr>    <tr>      <th>age</th>      <td>0.23</td>      <td>1.26</td>      <td>0.19</td>      <td>-0.13</td>      <td>0.60</td>      <td>0.88</td>      <td>1.82</td>      <td>1.26</td>      <td>0.21</td>      <td>2.27</td>    </tr>    <tr>      <th>sex</th>      <td>0.34</td>      <td>1.41</td>      <td>0.40</td>      <td>-0.45</td>      <td>1.14</td>      <td>0.64</td>      <td>3.11</td>      <td>0.84</td>      <td>0.40</td>      <td>1.33</td>    </tr>    <tr>      <th>ascites</th>      <td>-0.10</td>      <td>0.91</td>      <td>0.56</td>      <td>-1.20</td>      <td>1.01</td>      <td>0.30</td>      <td>2.75</td>      <td>-0.17</td>      <td>0.86</td>      <td>0.21</td>    </tr>    <tr>      <th>hepato</th>      <td>0.31</td>      <td>1.36</td>      <td>0.38</td>      <td>-0.44</td>      <td>1.06</td>      <td>0.64</td>      <td>2.89</td>      <td>0.81</td>      <td>0.42</td>      <td>1.26</td>    </tr>    <tr>      <th>spiders</th>      <td>-0.18</td>      <td>0.83</td>      <td>0.38</td>      <td>-0.94</td>      <td>0.57</td>      <td>0.39</td>      <td>1.77</td>      <td>-0.47</td>      <td>0.64</td>      <td>0.66</td>    </tr>    <tr>      <th>bili</th>      <td>0.05</td>      <td>1.05</td>      <td>0.18</td>      <td>-0.29</td>      <td>0.39</td>      <td>0.75</td>      <td>1.48</td>      <td>0.29</td>      <td>0.77</td>      <td>0.37</td>    </tr>    <tr>      <th>chol</th>      <td>0.19</td>      <td>1.20</td>      <td>0.15</td>      <td>-0.10</td>      <td>0.47</td>      <td>0.91</td>      <td>1.60</td>      <td>1.28</td>      <td>0.20</td>      <td>2.33</td>    </tr>    <tr>      <th>albumin</th>      <td>-0.40</td>      <td>0.67</td>      <td>0.18</td>      <td>-0.75</td>      <td>-0.06</td>      <td>0.47</td>      <td>0.94</td>      <td>-2.28</td>      <td>0.02</td>      <td>5.46</td>    </tr>    <tr>      <th>copper</th>      <td>0.30</td>      <td>1.35</td>      <td>0.16</td>      <td>-0.01</td>      <td>0.61</td>      <td>0.99</td>      <td>1.84</td>      <td>1.91</td>      <td>0.06</td>      <td>4.14</td>    </tr>    <tr>      <th>alk.phos</th>      <td>-0.22</td>      <td>0.80</td>      <td>0.14</td>      <td>-0.49</td>      <td>0.05</td>      <td>0.61</td>      <td>1.05</td>      <td>-1.62</td>      <td>0.11</td>      <td>3.24</td>    </tr>    <tr>      <th>ast</th>      <td>0.21</td>      <td>1.24</td>      <td>0.16</td>      <td>-0.10</td>      <td>0.53</td>      <td>0.91</td>      <td>1.69</td>      <td>1.34</td>      <td>0.18</td>      <td>2.48</td>    </tr>    <tr>      <th>trig</th>      <td>0.20</td>      <td>1.23</td>      <td>0.16</td>      <td>-0.11</td>      <td>0.52</td>      <td>0.89</td>      <td>1.68</td>      <td>1.27</td>      <td>0.21</td>      <td>2.28</td>    </tr>    <tr>      <th>platelet</th>      <td>0.14</td>      <td>1.15</td>      <td>0.15</td>      <td>-0.16</td>      <td>0.43</td>      <td>0.86</td>      <td>1.54</td>      <td>0.92</td>      <td>0.36</td>      <td>1.48</td>    </tr>    <tr>      <th>protime</th>      <td>0.36</td>      <td>1.43</td>      <td>0.17</td>      <td>0.03</td>      <td>0.69</td>      <td>1.03</td>      <td>1.99</td>      <td>2.15</td>      <td>0.03</td>      <td>4.97</td>    </tr>    <tr>      <th>edema_0.5</th>      <td>1.24</td>      <td>3.47</td>      <td>0.46</td>      <td>0.35</td>      <td>2.14</td>      <td>1.42</td>      <td>8.50</td>      <td>2.72</td>      <td>0.01</td>      <td>7.28</td>    </tr>    <tr>      <th>edema_1.0</th>      <td>2.02</td>      <td>7.51</td>      <td>0.60</td>      <td>0.84</td>      <td>3.20</td>      <td>2.31</td>      <td>24.43</td>      <td>3.35</td>      <td>&lt;0.005</td>      <td>10.28</td>    </tr>    <tr>      <th>stage_2.0</th>      <td>1.21</td>      <td>3.35</td>      <td>1.08</td>      <td>-0.92</td>      <td>3.33</td>      <td>0.40</td>      <td>28.06</td>      <td>1.11</td>      <td>0.27</td>      <td>1.91</td>    </tr>    <tr>      <th>stage_3.0</th>      <td>1.18</td>      <td>3.27</td>      <td>1.09</td>      <td>-0.96</td>      <td>3.33</td>      <td>0.38</td>      <td>27.86</td>      <td>1.08</td>      <td>0.28</td>      <td>1.84</td>    </tr>    <tr>      <th>stage_4.0</th>      <td>1.41</td>      <td>4.10</td>      <td>1.15</td>      <td>-0.85</td>      <td>3.67</td>      <td>0.43</td>      <td>39.43</td>      <td>1.22</td>      <td>0.22</td>      <td>2.18</td>    </tr>  </tbody></table><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <tbody>    <tr>      <th>Concordance</th>      <td>0.83</td>    </tr>    <tr>      <th>Log-likelihood ratio test</th>      <td>97.63 on 20 df, -log2(p)=38.13</td>    </tr>  </tbody></table></div><p><strong>Question:</strong></p><ul><li>According to the model, was treatment <code>trt</code> beneficial? </li><li>What was its associated hazard ratio? <ul><li>Note that the hazard ratio is how much an incremental increase in the feature variable changes the hazard.</li></ul></li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Check your answer!</b></font></summary></p><p><ul><ul>    <li>You should see that the treatment (trt) was beneficial because it has a negative impact on the hazard (the coefficient is negative, and exp(coef) is less than 1).</li>    <li>The associated hazard ratio is ~0.8, because this is the exp(coef) of treatment.</li></ul></ul></p><p>We can compare the predicted survival curves for treatment variables. Run the next cell to plot survival curves using the <code>plot_covariate_groups()</code> function. </p><ul><li>The y-axis is th survival rate</li><li>The x-axis is time</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cph.plot_covariate_groups(<span class="string">'trt'</span>, values=[<span class="number">0</span>, <span class="number">1</span>]);</span><br></pre></td></tr></table></figure><p>Notice how the group without treatment has a lower survival rate at all times (the x-axis is time) compared to the treatment group.</p><p><a name="6"></a></p><h2 id="6-Hazard-Ratio"><a href="#6-Hazard-Ratio" class="headerlink" title="6. Hazard Ratio"></a>6. Hazard Ratio</h2><p>Recall from the lecture videos that the Hazard Ratio between two patients was the likelihood of one patient (e.g smoker) being more at risk than the other (e.g non-smoker).</p><script type="math/tex; mode=display">\frac{\lambda_{smoker}(t)}{\lambda_{nonsmoker}(t)} = e^{\theta (X_{smoker} - X_{nonsmoker})^T}</script><p>Where</p><script type="math/tex; mode=display">\lambda_{smoker}(t) = \lambda_0(t)e^{\theta X_{smoker}^T}</script><p>and</p><script type="math/tex; mode=display">\lambda_{nonsmoker}(t) = \lambda_0(t)e^{\theta X_{nonsmoker}^T} \\</script><p><a name="Ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><p>In the cell below, write a function to compute the hazard ratio between two individuals given the model’s coefficients.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>use numpy.dot</li>    <li>use nump.exp</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hazard_ratio</span><span class="params">(case_1, case_2, cox_params)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the hazard ratio of case_1 : case_2 using</span></span><br><span class="line"><span class="string">    the coefficients of the cox model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        case_1 (np.array): (1 x d) array of covariates</span></span><br><span class="line"><span class="string">        case_2 (np.array): (1 x d) array of covariates</span></span><br><span class="line"><span class="string">        model (np.array): (1 x d) array of cox model coefficients</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        hazard_ratio (float): hazard ratio of case_1 : case_2</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    hr = np.exp(np.dot(cox_params,(case_1 - case_2).T))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> hr</span><br></pre></td></tr></table></figure><p>Now, evaluate it on the following pair of indivduals: <code>i = 1</code> and <code>j = 5</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">1</span></span><br><span class="line">case_1 = one_hot_train.iloc[i, :].drop([<span class="string">'time'</span>, <span class="string">'status'</span>])</span><br><span class="line"></span><br><span class="line">j = <span class="number">5</span></span><br><span class="line">case_2 = one_hot_train.iloc[j, :].drop([<span class="string">'time'</span>, <span class="string">'status'</span>])</span><br><span class="line"></span><br><span class="line">print(hazard_ratio(case_1.values, case_2.values, cph.params_.values))</span><br></pre></td></tr></table></figure><pre><code>15.029017732492221</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15.029017732492221</span></span><br></pre></td></tr></table></figure><p><strong>Question:</strong> </p><p>Is <code>case_1</code> or <code>case_2</code> at greater risk? </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Check your answer!</b></font></summary></p><p><ul><ul>    <li>You should see that `case_1` is at higher risk.</li>    <li>The hazard ratio of case 1 / case 2 is greater than 1, so case 1 had a higher hazard relative to case 2</li></ul></ul></p><p>Inspect different pairs, and see if you can figure out which patient is more at risk.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">4</span></span><br><span class="line">case_a = one_hot_train.iloc[i, :].drop([<span class="string">'time'</span>, <span class="string">'status'</span>])</span><br><span class="line"></span><br><span class="line">j = <span class="number">7</span></span><br><span class="line">case_b = one_hot_train.iloc[j, :].drop([<span class="string">'time'</span>, <span class="string">'status'</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Case A\n\n"</span>, case_a, <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"Case B\n\n"</span>, case_b, <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"Hazard Ratio:"</span>, hazard_ratio(case_a.values, case_b.values, cph.params_.values))</span><br></pre></td></tr></table></figure><pre><code>Case A trt          0.000000age          0.563645sex          0.000000ascites      0.000000hepato       1.000000spiders      1.000000bili        -0.405645chol        -0.210436albumin      1.514297copper      -0.481961alk.phos     2.173526ast         -0.144699trig        -0.531125platelet    -0.450972protime     -0.139881edema_0.5    0.000000edema_1.0    0.000000stage_2.0    0.000000stage_3.0    1.000000stage_4.0    0.000000Name: 1, dtype: float64 Case B trt          0.000000age          0.463447sex          0.000000ascites      0.000000hepato       1.000000spiders      0.000000bili        -0.489581chol        -0.309875albumin     -1.232371copper      -0.504348alk.phos     2.870427ast         -0.936261trig        -0.150229platelet     3.190823protime     -0.139881edema_0.5    0.000000edema_1.0    0.000000stage_2.0    0.000000stage_3.0    0.000000stage_4.0    1.000000Name: 38, dtype: float64 Hazard Ratio: 0.1780450006997129</code></pre><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Check your answer!</b></font></summary></p><p><ul><ul>    <li>You should see that `case_2` is at higher risk.</li>    <li>The hazard ratio of case 1 / case 2 is less than 1, so case 2 had a higher hazard relative to case 1</li></ul></ul></p><p><a name="7"></a></p><h2 id="7-Harrell’s-C-index"><a href="#7-Harrell’s-C-index" class="headerlink" title="7. Harrell’s C-index"></a>7. Harrell’s C-index</h2><p>To evaluate how good our model is performing, we will write our own version of the C-index. Similar to the week 1 case, C-index in the survival context is the probability that, given a randomly selected pair of individuals, the one who died sooner has a higher risk score. </p><p>However, we need to take into account censoring. Imagine a pair of patients, $A$ and $B$. </p><h4 id="Scenario-1"><a href="#Scenario-1" class="headerlink" title="Scenario 1"></a>Scenario 1</h4><ul><li>A was censored at time $t_A$ </li><li>B died at $t_B$</li><li>$t_A &lt; t_B$. </li></ul><p>Because of censoring, we can’t say whether $A$ or $B$ should have a higher risk score. </p><h4 id="Scenario-2"><a href="#Scenario-2" class="headerlink" title="Scenario 2"></a>Scenario 2</h4><p>Now imagine that $t_A &gt; t_B$.</p><ul><li>A was censored at time $t_A$ </li><li>B died at $t_B$</li><li>$t_A &gt; t_B$</li></ul><p>Now we can definitively say that $B$ should have a higher risk score than $A$, since we know for a fact that $A$ lived longer. </p><p>Therefore, when we compute our C-index</p><ul><li>We should only consider pairs where at most one person is censored</li><li>If they are censored, then their censored time should occur <em>after</em> the other person’s time of death. </li></ul><p>The metric we get if we use this rule is called <strong>Harrel’s C-index</strong>.</p><p>Note that in this case, being censored at time $t$ means that the true death time was some time AFTER time $t$ and not at $t$. </p><ul><li>Therefore if $t_A = t_B$ and A was censored:<ul><li>Then $A$ actually lived longer than $B$. </li><li>This will effect how you deal with ties in the exercise below!</li></ul></li></ul><p><a name="Ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><p>Fill in the function below to compute Harrel’s C-index.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>If you get a division by zero error, consider checking how you count when a pair is permissible (in the case where one patient is censored and the other is not censored).</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">harrell_c</span><span class="params">(y_true, scores, event)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Compute Harrel C-index given true event/censoring times,</span></span><br><span class="line"><span class="string">    model output, and event indicators.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (array): array of true event times</span></span><br><span class="line"><span class="string">        scores (array): model risk scores</span></span><br><span class="line"><span class="string">        event (array): indicator, 1 if event occurred at that index, 0 for censorship</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        result (float): C-index metric</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    n = len(y_true)</span><br><span class="line">    <span class="keyword">assert</span> (len(scores) == n <span class="keyword">and</span> len(event) == n)</span><br><span class="line">    </span><br><span class="line">    concordant = <span class="number">0.0</span></span><br><span class="line">    permissible = <span class="number">0.0</span></span><br><span class="line">    ties = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    result = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' and 'pass' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use double for loop to go through cases</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># set lower bound on j to avoid double counting</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># check if at most one is censored</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (event[i] == <span class="number">0</span> <span class="keyword">and</span> event[j] == <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if neither are censored</span></span><br><span class="line">                <span class="keyword">if</span> event[i] == <span class="number">1</span> <span class="keyword">and</span> event[j] == <span class="number">1</span>:</span><br><span class="line">                    permissible += <span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># check if scores are tied</span></span><br><span class="line">                    <span class="keyword">if</span> y_true[i] == y_true[j]:</span><br><span class="line">                        ties += <span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># check for concordant</span></span><br><span class="line">                    <span class="keyword">elif</span> y_true[i] &gt; y_true[j] <span class="keyword">and</span> scores[i] &lt; scores[j]:</span><br><span class="line">                        concordant += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">elif</span> y_true[i] &lt; y_true[j] <span class="keyword">and</span> scores[i] &gt; scores[j]:</span><br><span class="line">                        concordant += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># check if one is censored</span></span><br><span class="line">                <span class="keyword">elif</span> event[i] == <span class="number">0</span> <span class="keyword">or</span> event[j] == <span class="number">0</span>:</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># get censored index</span></span><br><span class="line">                    censored = j</span><br><span class="line">                    uncensored = i</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> event[i] == <span class="number">0</span>:</span><br><span class="line">                        censored = i</span><br><span class="line">                        uncensored = j</span><br><span class="line">                        </span><br><span class="line">                    <span class="comment"># check if permissible</span></span><br><span class="line">                    <span class="comment"># Note: in this case, we are assuming that censored at a time</span></span><br><span class="line">                    <span class="comment"># means that you did NOT die at that time. That is, if you</span></span><br><span class="line">                    <span class="comment"># live until time 30 and have event = 0, then you lived THROUGH</span></span><br><span class="line">                    <span class="comment"># time 30.</span></span><br><span class="line">                    <span class="keyword">if</span> y_true[censored] &gt;= y_true[uncensored]:</span><br><span class="line">                        permissible += <span class="number">1</span></span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># check if scores are tied</span></span><br><span class="line">                        <span class="keyword">if</span> scores[i] == scores[j]:</span><br><span class="line">                            <span class="comment"># update ties </span></span><br><span class="line">                            ties += <span class="number">1</span></span><br><span class="line">                            </span><br><span class="line">                        <span class="comment"># check if scores are concordant </span></span><br><span class="line">                        <span class="keyword">if</span> y_true[censored] &gt;= y_true[uncensored] <span class="keyword">and</span> scores[censored] &lt; scores[uncensored]:</span><br><span class="line">                            concordant += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set result to c-index computed from number of concordant pairs,</span></span><br><span class="line">    <span class="comment"># number of ties, and number of permissible pairs (REPLACE 0 with your code) </span></span><br><span class="line">    result = (concordant + <span class="number">0.5</span> * ties) / permissible</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>You can test your function on the following test cases:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">y_true = [<span class="number">30</span>, <span class="number">12</span>, <span class="number">84</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 1</span></span><br><span class="line">event = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">scores = [<span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">1.0</span>]</span><br><span class="line">print(<span class="string">"Case 1"</span>)</span><br><span class="line">print(<span class="string">"Expected: 1.0, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 2</span></span><br><span class="line">scores = [<span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0.1</span>]</span><br><span class="line">print(<span class="string">"\nCase 2"</span>)</span><br><span class="line">print(<span class="string">"Expected: 0.0, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 3</span></span><br><span class="line">event = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">scores = [<span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">1.0</span>]</span><br><span class="line">print(<span class="string">"\nCase 3"</span>)</span><br><span class="line">print(<span class="string">"Expected: 1.0, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 4</span></span><br><span class="line">y_true = [<span class="number">30</span>, <span class="number">30</span>, <span class="number">20</span>, <span class="number">20</span>]</span><br><span class="line">event = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">scores = [<span class="number">10</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">20</span>]</span><br><span class="line">print(<span class="string">"\nCase 4"</span>)</span><br><span class="line">print(<span class="string">"Expected: 0.75, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 5</span></span><br><span class="line">y_true = list(reversed([<span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">20</span>, <span class="number">20</span>]))</span><br><span class="line">event = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">scores = list(reversed([<span class="number">15</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">20</span>]))</span><br><span class="line">print(<span class="string">"\nCase 5"</span>)</span><br><span class="line">print(<span class="string">"Expected: 0.583, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 6</span></span><br><span class="line">y_true = [<span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">event = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">scores = [<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">print(<span class="string">"\nCase 6"</span>)</span><br><span class="line">print(<span class="string">f"Expected: 1.0 , Output:<span class="subst">&#123;harrell_c(y_true, scores, event):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Case 1Expected: 1.0, Output: 1.0Case 2Expected: 0.0, Output: 0.0Case 3Expected: 1.0, Output: 1.0Case 4Expected: 0.75, Output: 0.75Case 5Expected: 0.583, Output: 0.5833333333333334Case 6Expected: 1.0 , Output:1.0000</code></pre><p>Now use the Harrell’s C-index function to evaluate the cox model on our data sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train</span></span><br><span class="line">scores = cph.predict_partial_hazard(one_hot_train)</span><br><span class="line">cox_train_scores = harrell_c(one_hot_train[<span class="string">'time'</span>].values, scores.values, one_hot_train[<span class="string">'status'</span>].values)</span><br><span class="line"><span class="comment"># Validation</span></span><br><span class="line">scores = cph.predict_partial_hazard(one_hot_val)</span><br><span class="line">cox_val_scores = harrell_c(one_hot_val[<span class="string">'time'</span>].values, scores.values, one_hot_val[<span class="string">'status'</span>].values)</span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">scores = cph.predict_partial_hazard(one_hot_test)</span><br><span class="line">cox_test_scores = harrell_c(one_hot_test[<span class="string">'time'</span>].values, scores.values, one_hot_test[<span class="string">'status'</span>].values)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Train:"</span>, cox_train_scores)</span><br><span class="line">print(<span class="string">"Val:"</span>, cox_val_scores)</span><br><span class="line">print(<span class="string">"Test:"</span>, cox_test_scores)</span><br></pre></td></tr></table></figure><pre><code>Train: 0.8265139116202946Val: 0.8544776119402985Test: 0.8478543563068921</code></pre><p>What do these values tell us ?</p><p><a name="8"></a></p><h2 id="8-Random-Survival-Forests"><a href="#8-Random-Survival-Forests" class="headerlink" title="8. Random Survival Forests"></a>8. Random Survival Forests</h2><p>This performed well, but you have a hunch you can squeeze out better performance by using a machine learning approach. You decide to use a Random Survival Forest. To do this, you can use the <code>RandomForestSRC</code> package in R. To call R function from Python, we’ll use the <code>r2py</code> package. Run the following cell to import the necessary requirements. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%load_ext rpy2.ipython</span><br><span class="line">%R require(ggplot2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rpy2.robjects.packages <span class="keyword">import</span> importr</span><br><span class="line"><span class="comment"># import R's "base" package</span></span><br><span class="line">base = importr(<span class="string">'base'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import R's "utils" package</span></span><br><span class="line">utils = importr(<span class="string">'utils'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import rpy2's package module</span></span><br><span class="line"><span class="keyword">import</span> rpy2.robjects.packages <span class="keyword">as</span> rpackages</span><br><span class="line"></span><br><span class="line">forest = rpackages.importr(<span class="string">'randomForestSRC'</span>, lib_loc=<span class="string">'R'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rpy2 <span class="keyword">import</span> robjects <span class="keyword">as</span> ro</span><br><span class="line">R = ro.r</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rpy2.robjects <span class="keyword">import</span> pandas2ri</span><br><span class="line">pandas2ri.activate()</span><br></pre></td></tr></table></figure><pre><code>R[write to console]: Loading required package: ggplot2</code></pre><p>Instead of encoding our categories as binary features, we can use the original dataframe since trees deal well with raw categorical data (can you think why this might be?).</p><p>Run the code cell below to build your forest.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = forest.rfsrc(ro.Formula(<span class="string">'Surv(time, status) ~ .'</span>), data=df_train, ntree=<span class="number">300</span>, nodedepth=<span class="number">5</span>, seed=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(model)</span><br></pre></td></tr></table></figure><pre><code>                         Sample size: 154                    Number of deaths: 64                     Number of trees: 300           Forest terminal node size: 15       Average no. of terminal nodes: 6.54No. of variables tried at each split: 5              Total no. of variables: 17       Resampling used to grow trees: swor    Resample size used to grow trees: 97                            Analysis: RSF                              Family: surv                      Splitting rule: logrank *random*       Number of random split points: 10                          Error rate: 19.07%</code></pre><p>Finally, let’s evaluate on our validation and test sets, and compare it with our Cox model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = R.predict(model, newdata=df_val)</span><br><span class="line">scores = np.array(result.rx(<span class="string">'predicted'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Cox Model Validation Score:"</span>, cox_val_scores)</span><br><span class="line">print(<span class="string">"Survival Forest Validation Score:"</span>, harrell_c(df_val[<span class="string">'time'</span>].values, scores, df_val[<span class="string">'status'</span>].values))</span><br></pre></td></tr></table></figure><pre><code>Cox Model Validation Score: 0.8544776119402985Survival Forest Validation Score: 0.8296019900497512</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = R.predict(model, newdata=df_test)</span><br><span class="line">scores = np.array(result.rx(<span class="string">'predicted'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Cox Model Test Score:"</span>, cox_test_scores)</span><br><span class="line">print(<span class="string">"Survival Forest Validation Score:"</span>, harrell_c(df_test[<span class="string">'time'</span>].values, scores, df_test[<span class="string">'status'</span>].values))</span><br></pre></td></tr></table></figure><pre><code>Cox Model Test Score: 0.8478543563068921Survival Forest Validation Score: 0.8621586475942783</code></pre><p>Your random forest model should be outperforming the Cox model slightly. Let’s dig deeper to see how they differ.</p><p><a name="9"></a></p><h2 id="9-Permutation-Method-for-Interpretation"><a href="#9-Permutation-Method-for-Interpretation" class="headerlink" title="9. Permutation Method for Interpretation"></a>9. Permutation Method for Interpretation</h2><p>We’ll dig a bit deeper into interpretation methods for forests a bit later, but for now just know that random surival forests come with their own built in variable importance feature. The method is referred to as VIMP, and for the purpose of this section you should just know that higher absolute value of the VIMP means that the variable generally has a larger effect on the model outcome.</p><p>Run the next cell to compute and plot VIMP for the random survival forest.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vimps = np.array(forest.vimp(model).rx(<span class="string">'importance'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">y = np.arange(len(vimps))</span><br><span class="line">plt.barh(y, np.abs(vimps))</span><br><span class="line">plt.yticks(y, df_train.drop([<span class="string">'time'</span>, <span class="string">'status'</span>], axis=<span class="number">1</span>).columns)</span><br><span class="line">plt.title(<span class="string">"VIMP (absolute value)"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_67_0.png" alt="png"></p><h3 id="Question"><a href="#Question" class="headerlink" title="Question:"></a>Question:</h3><p>How does the variable importance compare to that of the Cox model? Which variable is important in both models? Which variable is important in the random survival forest but not in the Cox model? You should see that <code>edema</code> is important in both the random survival forest and the Cox model. You should also see that <code>bili</code> is important in the random survival forest but not the Cox model .</p><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You’ve finished the last assignment in course 2! Take a minute to look back at the analysis you’ve done over the last four assignments. You’ve done a great job!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Cox-Proportional-Hazards-and-Random-Survival-Forests&quot;&gt;&lt;a href=&quot;#Cox-Proportional-Hazards-and-Random-Survival-Forests&quot; class=&quot;headerl
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Survival Estimates that Vary with Time</title>
    <link href="https://zhangruochi.com/Survival-Estimates-that-Vary-with-Time/2020/04/18/"/>
    <id>https://zhangruochi.com/Survival-Estimates-that-Vary-with-Time/2020/04/18/</id>
    <published>2020-04-18T15:54:54.000Z</published>
    <updated>2020-04-19T15:09:46.620Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Survival-Estimates-that-Vary-with-Time"><a href="#Survival-Estimates-that-Vary-with-Time" class="headerlink" title="Survival Estimates that Vary with Time"></a>Survival Estimates that Vary with Time</h1><p>Welcome to the third assignment of Course 2. In this assignment, we’ll use Python to build some of the statistical models we learned this past week to analyze surivival estimates for a dataset of lymphoma patients. We’ll also evaluate these models and interpret their outputs. Along the way, you will be learning about the following: </p><ul><li>Censored Data</li><li>Kaplan-Meier Estimates</li><li>Subgroup Analysis</li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1. Import Packages</a></li><li><a href="#2">2. Load the Dataset</a></li><li><a href="#">3. Censored Data</a><ul><li><a href="#Ex-1">Exercise 1</a></li></ul></li><li><a href="#4">4. Survival Estimates</a><ul><li><a href="#Ex-2">Exercise 2</a></li><li><a href="#Ex-3">Exercise 3</a></li></ul></li><li><a href="#5">5. Subgroup Analysis</a><ul><li><a href="#5-1">5.1 Bonus: Log Rank Test</a></li></ul></li></ul><p><a name="1"></a></p><h2 id="1-Import-Packages"><a href="#1-Import-Packages" class="headerlink" title="1. Import Packages"></a>1. Import Packages</h2><p>We’ll first import all the packages that we need for this assignment. </p><ul><li><code>lifelines</code> is an open-source library for data analysis.</li><li><code>numpy</code> is the fundamental package for scientific computing in python.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li><li><code>matplotlib</code> is a plotting library.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lifelines</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> load_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lifelines <span class="keyword">import</span> KaplanMeierFitter <span class="keyword">as</span> KM</span><br><span class="line"><span class="keyword">from</span> lifelines.statistics <span class="keyword">import</span> logrank_test</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-the-Dataset"><a href="#2-Load-the-Dataset" class="headerlink" title="2. Load the Dataset"></a>2. Load the Dataset</h2><p>Run the next cell to load the lymphoma data set. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = load_data()</span><br></pre></td></tr></table></figure><p>As always, you first look over your data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"data shape: &#123;&#125;"</span>.format(data.shape))</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><pre><code>data shape: (80, 3)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Stage_group</th>      <th>Time</th>      <th>Event</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>6</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>19</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>32</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>42</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>1</td>      <td>42</td>      <td>1</td>    </tr>  </tbody></table></div><p>The column <code>Time</code> states how long the patient lived before they died or were censored.</p><p>The column <code>Event</code> says whether a death was observed or not. <code>Event</code> is 1 if the event is observed (i.e. the patient died) and 0 if data was censored.</p><p>Censorship here means that the observation has ended without any observed event.<br>For example, let a patient be in a hospital for 100 days at most. If a patient dies after only 44 days, their event will be recorded as <code>Time = 44</code> and <code>Event = 1</code>. If a patient walks out after 100 days and dies 3 days later (103 days total), this event is not observed in our process and the corresponding row has <code>Time = 100</code> and <code>Event = 0</code>. If a patient survives for 25 years after being admitted, their data for are still <code>Time = 100</code> and <code>Event = 0</code>.</p><p><a name="3"></a></p><h2 id="3-Censored-Data"><a href="#3-Censored-Data" class="headerlink" title="3. Censored Data"></a>3. Censored Data</h2><p>We can plot a histogram of the survival times to see in general how long cases survived before censorship or events.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data.Time.hist();</span><br><span class="line">plt.xlabel(<span class="string">"Observation time before death or censorship (days)"</span>);</span><br><span class="line">plt.ylabel(<span class="string">"Frequency (number of patients)"</span>);</span><br><span class="line"><span class="comment"># Note that the semicolon at the end of the plotting line</span></span><br><span class="line"><span class="comment"># silences unnecessary textual output - try removing it</span></span><br><span class="line"><span class="comment"># to observe its effect</span></span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">"Event"</span>].unique()</span><br></pre></td></tr></table></figure><pre><code>array([1, 0])</code></pre><p><a name="Ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><p>In the next cell, write a function to compute the fraction ($\in [0, 1]$) of observations which were censored. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Summing up the <code>'Event'</code> column will give you the number of observations where censorship has NOT occurred.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frac_censored</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return percent of observations which were censored.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        df (dataframe): dataframe which contains column 'Event' which is </span></span><br><span class="line"><span class="string">                        1 if an event occurred (death)</span></span><br><span class="line"><span class="string">                        0 if the event did not occur (censored)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        frac_censored (float): fraction of cases which were censored. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    result = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    result = <span class="number">1</span>- df[<span class="string">'Event'</span>].sum(axis = <span class="number">0</span>) / df.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(frac_censored(data))</span><br></pre></td></tr></table></figure><pre><code>0.32499999999999996</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.325</span></span><br></pre></td></tr></table></figure><p>Run the next cell to see the distributions of survival times for censored and uncensored examples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df_censored = data[data.Event == <span class="number">0</span>]</span><br><span class="line">df_uncensored = data[data.Event == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">df_censored.Time.hist()</span><br><span class="line">plt.title(<span class="string">"Censored"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time (days)"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Frequency"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">df_uncensored.Time.hist()</span><br><span class="line">plt.title(<span class="string">"Uncensored"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time (days)"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Frequency"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_19_0.png" alt="png"></p><p><img src="output_19_1.png" alt="png"></p><p><a name="4"></a></p><h2 id="4-Survival-Estimates"><a href="#4-Survival-Estimates" class="headerlink" title="4. Survival Estimates"></a>4. Survival Estimates</h2><p>We’ll now try to estimate the survival function:</p><script type="math/tex; mode=display">S(t) = P(T > t)</script><p>To illustrate the strengths of Kaplan Meier, we’ll start with a naive estimator of the above survival function. To estimate this quantity, we’ll divide the number of people who we know lived past time $t$ by the number of people who were not censored before $t$.</p><p>Formally, let $i$ = 1, …, $n$ be the cases, and let $t_i$ be the time when $i$ was censored or an event happened. Let $e_i= 1$ if an event was observed for $i$ and 0 otherwise. Then let $X_t = \{i : T_i &gt; t\}$, and let $M_t = \{i : e_i = 1 \text{ or } T_i &gt; t\}$. The estimator you will compute will be:</p><script type="math/tex; mode=display">\hat{S}(t) = \frac{|X_t|}{|M_t|}</script><p><a name="Ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><p>Write a function to compute this estimate for arbitrary $t$ in the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_estimator</span><span class="params">(t, df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return naive estimate for S(t), the probability</span></span><br><span class="line"><span class="string">    of surviving past time t. Given by number</span></span><br><span class="line"><span class="string">    of cases who survived past time t divided by the</span></span><br><span class="line"><span class="string">    number of cases who weren't censored before time t.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        t (int): query time</span></span><br><span class="line"><span class="string">        df (dataframe): survival data. Has a Time column,</span></span><br><span class="line"><span class="string">                        which says how long until that case</span></span><br><span class="line"><span class="string">                        experienced an event or was censored,</span></span><br><span class="line"><span class="string">                        and an Event column, which is 1 if an event</span></span><br><span class="line"><span class="string">                        was observed and 0 otherwise.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        S_t (float): estimator for survival function evaluated at t.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    S_t = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    S_t = df[df[<span class="string">'Time'</span>] &gt; t].shape[<span class="number">0</span>] / df[ (df[<span class="string">'Event'</span>] == <span class="number">1</span>) | (df[<span class="string">'Time'</span>] &gt; t) ].shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> S_t</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Test Cases"</span>)</span><br><span class="line"></span><br><span class="line">sample_df = pd.DataFrame(columns = [<span class="string">"Time"</span>, <span class="string">"Event"</span>])</span><br><span class="line">sample_df.Time = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">sample_df.Event = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">print(<span class="string">"Sample dataframe for testing code:"</span>)</span><br><span class="line">print(sample_df)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case 1: S(3)"</span>)</span><br><span class="line">print(<span class="string">"Output: &#123;&#125;, Expected: &#123;&#125;\n"</span>.format(naive_estimator(<span class="number">3</span>, sample_df), <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case 2: S(12)"</span>)</span><br><span class="line">print(<span class="string">"Output: &#123;&#125;, Expected: &#123;&#125;\n"</span>.format(naive_estimator(<span class="number">12</span>, sample_df), <span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case 3: S(20)"</span>)</span><br><span class="line">print(<span class="string">"Output: &#123;&#125;, Expected: &#123;&#125;\n"</span>.format(naive_estimator(<span class="number">20</span>, sample_df), <span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test case 4</span></span><br><span class="line">sample_df = pd.DataFrame(&#123;<span class="string">'Time'</span>: [<span class="number">5</span>,<span class="number">5</span>,<span class="number">10</span>],</span><br><span class="line">                          <span class="string">'Event'</span>: [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">                         &#125;)</span><br><span class="line">print(<span class="string">"Test case 4: S(5)"</span>)</span><br><span class="line">print(<span class="string">f"Output: <span class="subst">&#123;naive_estimator(<span class="number">5</span>, sample_df)&#125;</span>, Expected: 0.5"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test CasesSample dataframe for testing code:   Time  Event0     5      01    10      12    15      0Test Case 1: S(3)Output: 1.0, Expected: 1.0Test Case 2: S(12)Output: 0.5, Expected: 0.5Test Case 3: S(20)Output: 0.0, Expected: 0.0Test case 4: S(5)Output: 0.5, Expected: 0.5</code></pre><p>In the next cell, we will plot the naive estimator using the real data up to the maximum time in the dataset. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">max_time = data.Time.max()</span><br><span class="line">x = range(<span class="number">0</span>, max_time+<span class="number">1</span>)</span><br><span class="line">y = np.zeros(len(x))</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(x):</span><br><span class="line">    y[i] = naive_estimator(t, data)</span><br><span class="line">    </span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">"Naive Survival Estimate"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Estimated cumulative survival rate"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_25_0.png" alt="png"></p><p><a name="Ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><p>Next let’s compare this with the Kaplan Meier estimate. In the cell below, write a function that computes the Kaplan Meier estimate of $S(t)$ at every distinct time in the dataset. </p><p>Recall the Kaplan-Meier estimate:</p><script type="math/tex; mode=display">S(t) = \prod_{t_i \leq t} (1 - \frac{d_i}{n_i})</script><p>where $t_i$ are the events observed in the dataset and $d_i$ is the number of deaths at time $t_i$ and $n_i$ is the number of people who we know have survived up to time $t_i$.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Try sorting by Time.</li>    <li>Use <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html" target="_blank" rel="noopener">pandas.Series.unique<a> </a></a></li>    <li>If you get a division by zero error, please double-check how you calculated `n_t`</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HomemadeKM</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return KM estimate evaluated at every distinct</span></span><br><span class="line"><span class="string">    time (event or censored) recorded in the dataset.</span></span><br><span class="line"><span class="string">    Event times and probabilities should begin with</span></span><br><span class="line"><span class="string">    time 0 and probability 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    input: </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">         Time  Censor</span></span><br><span class="line"><span class="string">    0     5       0</span></span><br><span class="line"><span class="string">    1    10       1</span></span><br><span class="line"><span class="string">    2    15       0</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    correct output: </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    event_times: [0, 5, 10, 15]</span></span><br><span class="line"><span class="string">    S: [1.0, 1.0, 0.5, 0.5]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        df (dataframe): dataframe which has columns for Time</span></span><br><span class="line"><span class="string">                          and Event, defined as usual.</span></span><br><span class="line"><span class="string">                          </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        event_times (list of ints): array of unique event times</span></span><br><span class="line"><span class="string">                                      (begins with 0).</span></span><br><span class="line"><span class="string">        S (list of floats): array of survival probabilites, so that</span></span><br><span class="line"><span class="string">                            S[i] = P(T &gt; event_times[i]). This </span></span><br><span class="line"><span class="string">                            begins with 1.0 (since no one dies at time</span></span><br><span class="line"><span class="string">                            0).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># individuals are considered to have survival probability 1</span></span><br><span class="line">    <span class="comment"># at time 0</span></span><br><span class="line">    event_times = [<span class="number">0</span>]</span><br><span class="line">    p = <span class="number">1.0</span></span><br><span class="line">    S = [p]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get collection of unique observed event times</span></span><br><span class="line">    observed_event_times = df[<span class="string">'Time'</span>].unique().tolist()</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># sort event times</span></span><br><span class="line">    observed_event_times = sorted(observed_event_times)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># iterate through event times</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> observed_event_times:</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># compute n_t, number of people who survive to time t</span></span><br><span class="line">        n_t = df[df[<span class="string">'Time'</span>] &gt;= t].shape[<span class="number">0</span>]</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># compute d_t, number of people who die at time t</span></span><br><span class="line">        d_t = df[(df[<span class="string">'Time'</span>] == t) &amp; (df[<span class="string">'Event'</span>] == <span class="number">1</span>)].shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># update p</span></span><br><span class="line">        p = p * ( <span class="number">1</span> - d_t / n_t)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># update S and event_times (ADD code below)</span></span><br><span class="line">        <span class="comment"># hint: use append</span></span><br><span class="line">        S.append(p)</span><br><span class="line">        event_times.append(t)</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> event_times, S</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"TEST CASES:\n"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case 1\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test DataFrame:"</span>)</span><br><span class="line">sample_df = pd.DataFrame(columns = [<span class="string">"Time"</span>, <span class="string">"Event"</span>])</span><br><span class="line">sample_df.Time = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">sample_df.Event = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">print(sample_df.head())</span><br><span class="line">print(<span class="string">"\nOutput:"</span>)</span><br><span class="line">x, y = HomemadeKM(sample_df)</span><br><span class="line">print(<span class="string">"Event times: &#123;&#125;, Survival Probabilities: &#123;&#125;"</span>.format(x, y))</span><br><span class="line">print(<span class="string">"\nExpected:"</span>)</span><br><span class="line">print(<span class="string">"Event times: [0, 5, 10, 15], Survival Probabilities: [1.0, 1.0, 0.5, 0.5]"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nTest Case 2\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test DataFrame:"</span>)</span><br><span class="line"></span><br><span class="line">sample_df = pd.DataFrame(columns = [<span class="string">"Time"</span>, <span class="string">"Event"</span>])</span><br><span class="line">sample_df.loc[:, <span class="string">"Time"</span>] = [<span class="number">2</span>, <span class="number">15</span>, <span class="number">12</span>, <span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line">sample_df.loc[:, <span class="string">"Event"</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">print(sample_df.head())</span><br><span class="line">print(<span class="string">"\nOutput:"</span>)</span><br><span class="line">x, y = HomemadeKM(sample_df)</span><br><span class="line">print(<span class="string">"Event times: &#123;&#125;, Survival Probabilities: &#123;&#125;"</span>.format(x, y))</span><br><span class="line">print(<span class="string">"\nExpected:"</span>)</span><br><span class="line">print(<span class="string">"Event times: [0, 2, 10, 12, 15, 20], Survival Probabilities: [1.0, 1.0, 0.75, 0.5, 0.5, 0.0]"</span>)</span><br></pre></td></tr></table></figure><pre><code>TEST CASES:Test Case 1Test DataFrame:   Time  Event0     5      01    10      12    15      0Output:Event times: [0, 5, 10, 15], Survival Probabilities: [1.0, 1.0, 0.5, 0.5]Expected:Event times: [0, 5, 10, 15], Survival Probabilities: [1.0, 1.0, 0.5, 0.5]Test Case 2Test DataFrame:   Time  Event0     2      01    15      02    12      13    10      14    20      1Output:Event times: [0, 2, 10, 12, 15, 20], Survival Probabilities: [1.0, 1.0, 0.75, 0.5, 0.5, 0.0]Expected:Event times: [0, 2, 10, 12, 15, 20], Survival Probabilities: [1.0, 1.0, 0.75, 0.5, 0.5, 0.0]</code></pre><p>Now let’s plot the two against each other on the data to see the difference.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">max_time = data.Time.max()</span><br><span class="line">x = range(<span class="number">0</span>, max_time+<span class="number">1</span>)</span><br><span class="line">y = np.zeros(len(x))</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(x):</span><br><span class="line">    y[i] = naive_estimator(t, data)</span><br><span class="line">    </span><br><span class="line">plt.plot(x, y, label=<span class="string">"Naive"</span>)</span><br><span class="line"></span><br><span class="line">x, y = HomemadeKM(data)</span><br><span class="line">plt.step(x, y, label=<span class="string">"Kaplan-Meier"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Survival probability estimate"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_31_0.png" alt="png"></p><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>What differences do you observe between the naive estimator and Kaplan-Meier estimator? Do any of our earlier explorations of the dataset help to explain these differences?</p><p><a name="5"></a></p><h2 id="5-Subgroup-Analysis"><a href="#5-Subgroup-Analysis" class="headerlink" title="5. Subgroup Analysis"></a>5. Subgroup Analysis</h2><p>We see that along with Time and Censor, we have a column called <code>Stage_group</code>. </p><ul><li>A value of 1 in this column denotes a patient with stage III cancer</li><li>A value of 2 denotes stage IV. </li></ul><p>We want to compare the survival functions of these two groups.</p><p>This time we’ll use the <code>KaplanMeierFitter</code> class from <code>lifelines</code>. Run the next cell to fit and plot the Kaplan Meier curves for each group. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">S1 = data[data.Stage_group == <span class="number">1</span>]</span><br><span class="line">km1 = KM()</span><br><span class="line">km1.fit(S1.loc[:, <span class="string">'Time'</span>], event_observed = S1.loc[:, <span class="string">'Event'</span>], label = <span class="string">'Stage III'</span>)</span><br><span class="line"></span><br><span class="line">S2 = data[data.Stage_group == <span class="number">2</span>]</span><br><span class="line">km2 = KM()</span><br><span class="line">km2.fit(S2.loc[:, <span class="string">"Time"</span>], event_observed = S2.loc[:, <span class="string">'Event'</span>], label = <span class="string">'Stage IV'</span>)</span><br><span class="line"></span><br><span class="line">ax = km1.plot(ci_show=<span class="keyword">False</span>)</span><br><span class="line">km2.plot(ax = ax, ci_show=<span class="keyword">False</span>)</span><br><span class="line">plt.xlabel(<span class="string">'time'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Survival probability estimate'</span>)</span><br><span class="line">plt.savefig(<span class="string">'two_km_curves'</span>, dpi=<span class="number">300</span>)</span><br></pre></td></tr></table></figure><p><img src="output_34_0.png" alt="png"></p><p>Let’s compare the survival functions at 90, 180, 270, and 360 days</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">survivals = pd.DataFrame([<span class="number">90</span>, <span class="number">180</span>, <span class="number">270</span>, <span class="number">360</span>], columns = [<span class="string">'time'</span>])</span><br><span class="line">survivals.loc[:, <span class="string">'Group 1'</span>] = km1.survival_function_at_times(survivals[<span class="string">'time'</span>]).values</span><br><span class="line">survivals.loc[:, <span class="string">'Group 2'</span>] = km2.survival_function_at_times(survivals[<span class="string">'time'</span>]).values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">survivals</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>Group 1</th>      <th>Group 2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>90</td>      <td>0.736842</td>      <td>0.424529</td>    </tr>    <tr>      <th>1</th>      <td>180</td>      <td>0.680162</td>      <td>0.254066</td>    </tr>    <tr>      <th>2</th>      <td>270</td>      <td>0.524696</td>      <td>0.195436</td>    </tr>    <tr>      <th>3</th>      <td>360</td>      <td>0.524696</td>      <td>0.195436</td>    </tr>  </tbody></table></div><p>This makes clear the difference in survival between the Stage III and IV cancer groups in the dataset. </p><p><a name="5-1"></a></p><h2 id="5-1-Bonus-Log-Rank-Test"><a href="#5-1-Bonus-Log-Rank-Test" class="headerlink" title="5.1 Bonus: Log-Rank Test"></a>5.1 Bonus: Log-Rank Test</h2><p>To say whether there is a statistical difference between the survival curves we can run the log-rank test. This test tells us the probability that we could observe this data if the two curves were the same. The derivation of the log-rank test is somewhat complicated, but luckily <code>lifelines</code> has a simple function to compute it. </p><p>Run the next cell to compute a p-value using <code>lifelines.statistics.logrank_test</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logrank_p_value</span><span class="params">(group_1_data, group_2_data)</span>:</span></span><br><span class="line">    result = logrank_test(group_1_data.Time, group_2_data.Time,</span><br><span class="line">                          group_1_data.Event, group_2_data.Event)</span><br><span class="line">    <span class="keyword">return</span> result.p_value</span><br><span class="line"></span><br><span class="line">logrank_p_value(S1, S2)</span><br></pre></td></tr></table></figure><pre><code>0.009588929834755544</code></pre><p>If everything is correct, you should see a p value of less than <code>0.05</code>, which indicates that the difference in the curves is indeed statistically significant.</p><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You’ve completed the third assignment of Course 2. You’ve learned about the Kaplan Meier estimator, a fundamental non-parametric estimator in survival analysis. Next week we’ll learn how to take into account patient covariates in our survival estimates!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Survival-Estimates-that-Vary-with-Time&quot;&gt;&lt;a href=&quot;#Survival-Estimates-that-Vary-with-Time&quot; class=&quot;headerlink&quot; title=&quot;Survival Estimat
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
</feed>
