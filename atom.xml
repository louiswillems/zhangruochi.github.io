<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-07-19T09:46:20.535Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Auto-Complete</title>
    <link href="https://zhangruochi.com/Auto-Complete/2020/07/19/"/>
    <id>https://zhangruochi.com/Auto-Complete/2020/07/19/</id>
    <published>2020-07-19T09:45:54.000Z</published>
    <updated>2020-07-19T09:46:20.535Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Language-Models-Auto-Complete"><a href="#Language-Models-Auto-Complete" class="headerlink" title="Language Models: Auto-Complete"></a>Language Models: Auto-Complete</h1><p>In this assignment, you will build an auto-complete system.  Auto-complete system is something you may see every day</p><ul><li>When you google something, you often have suggestions to help you complete your search. </li><li>When you are writing an email, you get suggestions telling you possible endings to your sentence.  </li></ul><p>By the end of this assignment, you will develop a prototype of such a system.</p><p><img src="stanford.png" style="width:700px;height:300px;"></p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1 Load and Preprocess Data</a></li><li><a href="#1.1">1.1: Load the data</a></li><li><a href="#1.2">1.2 Pre-process the data</a><ul><li><a href="#ex-01">Exercise 01</a></li><li><a href="#ex-02">Exercise 02</a></li><li><a href="#ex-03">Exercise 03</a></li><li><a href="#ex-04">Exercise 04</a></li><li><a href="#ex-05">Exercise 05</a></li><li><a href="#ex-06">Exercise 06</a></li><li><a href="#ex-07">Exercise 07</a></li></ul></li><li><a href="#2">2 Develop n-gram based language models</a><ul><li><a href="#ex-08">Exercise 08</a></li><li><a href="#ex-09">Exercise 09</a>    </li></ul></li><li><a href="#3">3 Perplexity</a><ul><li><a href="#ex-10">Exercise 10</a></li></ul></li><li><a href="#4">4 Build an auto-complete system</a><ul><li><a href="#ex-11">Exercise 11</a></li></ul></li></ul><p>A key building block for an auto-complete system is a language model.<br>A language model assigns the probability to a sequence of words, in a way that more “likely” sequences receive higher scores.  For example, </p><blockquote><p>“I have a pen”<br>is expected to have a higher probability than<br>“I am a pen”<br>since the first one seems to be a more natural sentence in the real world.</p></blockquote><p>You can take advantage of this probability calculation to develop an auto-complete system.<br>Suppose the user typed </p><blockquote><p>“I eat scrambled”<br>Then you can find a word <code>x</code>  such that “I eat scrambled x” receives the highest probability.  If x = “eggs”, the sentence would be<br>“I eat scrambled eggs”</p></blockquote><p>While a variety of language models have been developed, this assignment uses <strong>N-grams</strong>, a simple but powerful method for language modeling.</p><ul><li>N-grams are also used in machine translation and speech recognition. </li></ul><p>Here are the steps of this assignment:</p><ol><li>Load and preprocess data<ul><li>Load and tokenize data.</li><li>Split the sentences into train and test sets.</li><li>Replace words with a low frequency by an unknown marker <code>&lt;unk&gt;</code>.</li></ul></li><li>Develop N-gram based language models<ul><li>Compute the count of n-grams from a given data set.</li><li>Estimate the conditional probability of a next word with k-smoothing.</li></ul></li><li>Evaluate the N-gram models by computing the perplexity score.</li><li>Use your own model to suggest an upcoming word given your sentence. </li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.data.path.append(<span class="string">'.'</span>)</span><br></pre></td></tr></table></figure><p><a name="1"></a></p><h2 id="Part-1-Load-and-Preprocess-Data"><a href="#Part-1-Load-and-Preprocess-Data" class="headerlink" title="Part 1: Load and Preprocess Data"></a>Part 1: Load and Preprocess Data</h2><p><a name="1.1"></a></p><h3 id="Part-1-1-Load-the-data"><a href="#Part-1-1-Load-the-data" class="headerlink" title="Part 1.1: Load the data"></a>Part 1.1: Load the data</h3><p>You will use twitter data.<br>Load the data and view the first few sentences by running the next cell.</p><p>Notice that data is a long string that contains many many tweets.<br>Observe that there is a line break “\n” between tweets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"en_US.twitter.txt"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line">print(<span class="string">"Data type:"</span>, type(data))</span><br><span class="line">print(<span class="string">"Number of letters:"</span>, len(data))</span><br><span class="line">print(<span class="string">"First 300 letters of the data"</span>)</span><br><span class="line">print(<span class="string">"-------"</span>)</span><br><span class="line">display(data[<span class="number">0</span>:<span class="number">300</span>])</span><br><span class="line">print(<span class="string">"-------"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Last 300 letters of the data"</span>)</span><br><span class="line">print(<span class="string">"-------"</span>)</span><br><span class="line">display(data[<span class="number">-300</span>:])</span><br><span class="line">print(<span class="string">"-------"</span>)</span><br></pre></td></tr></table></figure><pre><code>Data type: &lt;class &#39;str&#39;&gt;Number of letters: 3335477First 300 letters of the data-------&quot;How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\nWhen you meet someone special... you&#39;ll know. Your heart will beat more rapidly and you&#39;ll smile for no reason.\nthey&#39;ve decided its more fun if I don&#39;t.\nSo Tired D; Played Lazer Tag &amp; Ran A &quot;-------Last 300 letters of the data-------&quot;ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\nColombia is with an &#39;o&#39;...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\n#GutsiestMovesYouCanMake Giving a cat a bath.\nCoffee after 5 was a TERRIBLE idea.\n&quot;-------</code></pre><p><a name="1.2"></a></p><h3 id="Part-1-2-Pre-process-the-data"><a href="#Part-1-2-Pre-process-the-data" class="headerlink" title="Part 1.2 Pre-process the data"></a>Part 1.2 Pre-process the data</h3><p>Preprocess this data with the following steps:</p><ol><li>Split data into sentences using “\n” as the delimiter.</li><li>Split each sentence into tokens. Note that in this assignment we use “token” and “words” interchangeably.</li><li>Assign sentences into train or test sets.</li><li>Find tokens that appear at least N times in the training data.</li><li>Replace tokens that appear less than N times by <code>&lt;unk&gt;</code></li></ol><p>Note: we omit validation data in this exercise.</p><ul><li>In real applications, we should hold a part of data as a validation set and use it to tune our training.</li><li>We skip this process for simplicity.</li></ul><p><a name="ex-01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p>Split data into sentences.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Use <a href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split" target="_blank" rel="noopener">str.split</a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: split_to_sentences ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_to_sentences</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Split data by linebreak "\n"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: str</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A list of sentences</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    sentences = data.split(<span class="string">"\n"</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Additional clearning (This part is already implemented)</span></span><br><span class="line">    <span class="comment"># - Remove leading and trailing spaces from each sentence</span></span><br><span class="line">    <span class="comment"># - Drop sentences if they are empty strings.</span></span><br><span class="line">    sentences = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">    sentences = [s <span class="keyword">for</span> s <span class="keyword">in</span> sentences <span class="keyword">if</span> len(s) &gt; <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sentences</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">x = <span class="string">"""</span></span><br><span class="line"><span class="string">I have a pen.\nI have an apple. \nAh\nApple pen.\n</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">split_to_sentences(x)</span><br></pre></td></tr></table></figure><pre><code>I have a pen.I have an apple. AhApple pen.[&#39;I have a pen.&#39;, &#39;I have an apple.&#39;, &#39;Ah&#39;, &#39;Apple pen.&#39;]</code></pre><p>Expected answer:<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']</span><br></pre></td></tr></table></figure></p><p><a name="ex-02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p>The next step is to tokenize sentences (split a sentence into a list of words). </p><ul><li>Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.</li><li>Append each tokenized list of words into a list of tokenized sentences.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Use <a href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.lower" target="_blank" rel="noopener">str.lower</a> to convert strings to lowercase. </li>    <li>Please use <a href="https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize" target="_blank" rel="noopener">nltk.word_tokenize</a> to split sentences into tokens.</li>    <li>If you used str.split insteaad of nltk.word_tokenize, there are additional edge cases to handle, such as the punctuation (comma, period) that follows a word.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: tokenize_sentences ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_sentences</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Tokenize sentences into tokens (words)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sentences: List of strings</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of tokens</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the list of lists of tokenized sentences</span></span><br><span class="line">    tokenized_sentences = []</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert to lowercase letters</span></span><br><span class="line">        sentence = sentence.lower()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert into a list of words</span></span><br><span class="line">        tokenized = nltk.word_tokenize(sentence)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the list of words to the list of lists</span></span><br><span class="line">        tokenized_sentences.append(tokenized)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenized_sentences</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [<span class="string">"Sky is blue."</span>, <span class="string">"Leaves are green."</span>, <span class="string">"Roses are red."</span>]</span><br><span class="line">tokenize_sentences(sentences)</span><br></pre></td></tr></table></figure><pre><code>[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;], [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;, &#39;.&#39;], [&#39;roses&#39;, &#39;are&#39;, &#39;red&#39;, &#39;.&#39;]]</code></pre><h3 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[['sky', 'is', 'blue', '.'],</span><br><span class="line"> ['leaves', 'are', 'green', '.'],</span><br><span class="line"> ['roses', 'are', 'red', '.']]</span><br></pre></td></tr></table></figure><p><a name="ex-03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p>Use the two functions that you have just implemented to get the tokenized data.</p><ul><li>split the data into sentences</li><li>tokenize those sentences</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: get_tokenized_data ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tokenized_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Make a list of tokenized sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: String</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of tokens</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the sentences by splitting up the data</span></span><br><span class="line">    sentences = split_to_sentences(data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the list of lists of tokens by tokenizing the sentences</span></span><br><span class="line">    tokenized_sentences = tokenize_sentences(sentences)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenized_sentences</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your function</span></span><br><span class="line">x = <span class="string">"Sky is blue.\nLeaves are green\nRoses are red."</span></span><br><span class="line">get_tokenized_data(x)</span><br></pre></td></tr></table></figure><pre><code>[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;], [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;], [&#39;roses&#39;, &#39;are&#39;, &#39;red&#39;, &#39;.&#39;]]</code></pre><h5 id="Expected-outcome"><a href="#Expected-outcome" class="headerlink" title="Expected outcome"></a>Expected outcome</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[['sky', 'is', 'blue', '.'],</span><br><span class="line"> ['leaves', 'are', 'green'],</span><br><span class="line"> ['roses', 'are', 'red', '.']]</span><br></pre></td></tr></table></figure><h3 id="Split-into-train-and-test-sets"><a href="#Split-into-train-and-test-sets" class="headerlink" title="Split into train and test sets"></a>Split into train and test sets</h3><p>Now run the cell below to split data into training and test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenized_data = get_tokenized_data(data)</span><br><span class="line">random.seed(<span class="number">87</span>)</span><br><span class="line">random.shuffle(tokenized_data)</span><br><span class="line"></span><br><span class="line">train_size = int(len(tokenized_data) * <span class="number">0.8</span>)</span><br><span class="line">train_data = tokenized_data[<span class="number">0</span>:train_size]</span><br><span class="line">test_data = tokenized_data[train_size:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"&#123;&#125; data are split into &#123;&#125; train and &#123;&#125; test set"</span>.format(</span><br><span class="line">    len(tokenized_data), len(train_data), len(test_data)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"First training sample:"</span>)</span><br><span class="line">print(train_data[<span class="number">0</span>])</span><br><span class="line">      </span><br><span class="line">print(<span class="string">"First test sample"</span>)</span><br><span class="line">print(test_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>47961 data are split into 38368 train and 9593 test setFirst training sample:[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;, &#39;team&#39;, &#39;local&#39;, &#39;company&#39;, &#39;and&#39;, &#39;quality&#39;, &#39;production&#39;]First test sample[&#39;that&#39;, &#39;picture&#39;, &#39;i&#39;, &#39;just&#39;, &#39;seen&#39;, &#39;whoa&#39;, &#39;dere&#39;, &#39;!&#39;, &#39;!&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;]</code></pre><h5 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">47961</span> data are split into <span class="number">38368</span> train <span class="keyword">and</span> <span class="number">9593</span> test <span class="built_in">set</span></span><br><span class="line">First training sample:</span><br><span class="line">['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']</span><br><span class="line">First test sample</span><br><span class="line">['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;']</span><br></pre></td></tr></table></figure><p><a name="ex-04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p>You won’t use all the tokens (words) appearing in the data for training.  Instead, you will use the more frequently used words.  </p><ul><li>You will focus on the words that appear at least N times in the data.</li><li>First count how many times each word appears in the data.</li></ul><p>You will need a double for-loop, one for sentences and the other for tokens within a sentence.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>If you decide to import and use defaultdict, remember to cast the dictionary back to a regular 'dict' before returning it. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: count_words ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_words</span><span class="params">(tokenized_sentences)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count the number of word appearence in the tokenized sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of strings</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict that maps word (str) to the frequency (int)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    word_counts = &#123;&#125;</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokenized_sentences: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each token in the sentence</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the token is not in the dictionary yet, set the count to 1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> token <span class="keyword">in</span> word_counts: <span class="comment"># complete this line</span></span><br><span class="line">                word_counts[token] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the token is already in the dictionary, increment the count by 1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                word_counts[token] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> word_counts</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tokenized_sentences = [[<span class="string">'sky'</span>, <span class="string">'is'</span>, <span class="string">'blue'</span>, <span class="string">'.'</span>],</span><br><span class="line">                       [<span class="string">'leaves'</span>, <span class="string">'are'</span>, <span class="string">'green'</span>, <span class="string">'.'</span>],</span><br><span class="line">                       [<span class="string">'roses'</span>, <span class="string">'are'</span>, <span class="string">'red'</span>, <span class="string">'.'</span>]]</span><br><span class="line">count_words(tokenized_sentences)</span><br></pre></td></tr></table></figure><pre><code>{&#39;sky&#39;: 1, &#39;is&#39;: 1, &#39;blue&#39;: 1, &#39;.&#39;: 3, &#39;leaves&#39;: 1, &#39;are&#39;: 2, &#39;green&#39;: 1, &#39;roses&#39;: 1, &#39;red&#39;: 1}</code></pre><h5 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output"></a>Expected output</h5><p>Note that the order may differ.</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;'sky': 1,</span><br><span class="line"> 'is': 1,</span><br><span class="line"> 'blue': 1,</span><br><span class="line"> <span class="string">'.'</span>: <span class="number">3</span>,</span><br><span class="line"> 'leaves': 1,</span><br><span class="line"> 'are': 2,</span><br><span class="line"> 'green': 1,</span><br><span class="line"> 'roses': 1,</span><br><span class="line"> 'red': 1&#125;</span><br></pre></td></tr></table></figure><h3 id="Handling-‘Out-of-Vocabulary’-words"><a href="#Handling-‘Out-of-Vocabulary’-words" class="headerlink" title="Handling ‘Out of Vocabulary’ words"></a>Handling ‘Out of Vocabulary’ words</h3><p>If your model is performing autocomplete, but encounters a word that it never saw during training, it won’t have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. </p><ul><li>This ‘new’ word is called an ‘unknown word’, or <b>out of vocabulary (OOV)</b> words.</li><li>The percentage of unknown words in the test set is called the <b> OOV </b> rate. </li></ul><p>To handle unknown words during prediction, use a special token to represent all unknown words ‘unk’. </p><ul><li>Modify the training data so that it has some ‘unknown’ words to train on.</li><li>Words to convert into “unknown” words are those that do not occur very frequently in the training set.</li><li>Create a list of the most frequent words in the training set, called the <b> closed vocabulary </b>. </li><li>Convert all the other words that are not part of the closed vocabulary to the token ‘unk’. </li></ul><p><a name="ex-05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>You will now create a function that takes in a text document and a threshold ‘count_threshold’.</p><ul><li>Any word whose count is greater than or equal to the threshold ‘count_threshold’ is kept in the closed vocabulary.</li><li>used that you want to keep, returns the document containing only the word closed vocabulary and the word unk. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: get_words_with_nplus_frequency ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_words_with_nplus_frequency</span><span class="params">(tokenized_sentences, count_threshold)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Find the words that appear N times or more</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of sentences</span></span><br><span class="line"><span class="string">        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of words that appear N times or more</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize an empty list to contain the words that</span></span><br><span class="line">    <span class="comment"># appear at least 'minimum_freq' times.</span></span><br><span class="line">    closed_vocab = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the word couts of the tokenized sentences</span></span><br><span class="line">    <span class="comment"># Use the function that you defined earlier to count the words</span></span><br><span class="line">    word_counts = count_words(tokenized_sentences)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># for each word and its count</span></span><br><span class="line">    <span class="keyword">for</span> word, cnt <span class="keyword">in</span> word_counts.items(): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check that the word's count</span></span><br><span class="line">        <span class="comment"># is at least as great as the minimum count</span></span><br><span class="line">        <span class="keyword">if</span> cnt &gt;= count_threshold:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the word to the list</span></span><br><span class="line">            closed_vocab.append(word)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> closed_vocab</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tokenized_sentences = [[<span class="string">'sky'</span>, <span class="string">'is'</span>, <span class="string">'blue'</span>, <span class="string">'.'</span>],</span><br><span class="line">                       [<span class="string">'leaves'</span>, <span class="string">'are'</span>, <span class="string">'green'</span>, <span class="string">'.'</span>],</span><br><span class="line">                       [<span class="string">'roses'</span>, <span class="string">'are'</span>, <span class="string">'red'</span>, <span class="string">'.'</span>]]</span><br><span class="line">tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">f"Closed vocabulary:"</span>)</span><br><span class="line">print(tmp_closed_vocab)</span><br></pre></td></tr></table></figure><pre><code>Closed vocabulary:[&#39;.&#39;, &#39;are&#39;]</code></pre><h5 id="Expected-output-3"><a href="#Expected-output-3" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Closed vocabulary:</span><br><span class="line">['.', 'are']</span><br></pre></td></tr></table></figure><p><a name="ex-06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p>The words that appear ‘count_threshold’ times or more are in the ‘closed vocabulary. </p><ul><li>All other words are regarded as ‘unknown’.</li><li>Replace words not in the closed vocabulary with the token “<unk\>“.</unk\></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: replace_oov_words_by_unk ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_oov_words_by_unk</span><span class="params">(tokenized_sentences, vocabulary, unknown_token=<span class="string">"&lt;unk&gt;"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Replace words not in the given vocabulary with '&lt;unk&gt;' token.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of strings</span></span><br><span class="line"><span class="string">        vocabulary: List of strings that we will use</span></span><br><span class="line"><span class="string">        unknown_token: A string representing unknown (out-of-vocabulary) words</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of strings, with words not in the vocabulary replaced</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Place vocabulary into a set for faster search</span></span><br><span class="line">    vocabulary = set(vocabulary)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a list that will hold the sentences</span></span><br><span class="line">    <span class="comment"># after less frequent words are replaced by the unknown token</span></span><br><span class="line">    replaced_tokenized_sentences = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokenized_sentences:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize the list that will contain</span></span><br><span class="line">        <span class="comment"># a single sentence with "unknown_token" replacements</span></span><br><span class="line">        replaced_sentence = []</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># for each token in the sentence</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the token is in the closed vocabulary</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> vocabulary: <span class="comment"># complete this line</span></span><br><span class="line">                <span class="comment"># If so, append the word to the replaced_sentence</span></span><br><span class="line">                replaced_sentence.append(token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># otherwise, append the unknown token instead</span></span><br><span class="line">                replaced_sentence.append(unknown_token)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Append the list of tokens to the list of lists</span></span><br><span class="line">        replaced_tokenized_sentences.append(replaced_sentence)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> replaced_tokenized_sentences</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenized_sentences = [[<span class="string">"dogs"</span>, <span class="string">"run"</span>], [<span class="string">"cats"</span>, <span class="string">"sleep"</span>]]</span><br><span class="line">vocabulary = [<span class="string">"dogs"</span>, <span class="string">"sleep"</span>]</span><br><span class="line">tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)</span><br><span class="line">print(<span class="string">f"Original sentence:"</span>)</span><br><span class="line">print(tokenized_sentences)</span><br><span class="line">print(<span class="string">f"tokenized_sentences with less frequent words converted to '&lt;unk&gt;':"</span>)</span><br><span class="line">print(tmp_replaced_tokenized_sentences)</span><br></pre></td></tr></table></figure><pre><code>Original sentence:[[&#39;dogs&#39;, &#39;run&#39;], [&#39;cats&#39;, &#39;sleep&#39;]]tokenized_sentences with less frequent words converted to &#39;&lt;unk&gt;&#39;:[[&#39;dogs&#39;, &#39;&lt;unk&gt;&#39;], [&#39;&lt;unk&gt;&#39;, &#39;sleep&#39;]]</code></pre><h3 id="Expected-answer"><a href="#Expected-answer" class="headerlink" title="Expected answer"></a>Expected answer</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original sentence:</span><br><span class="line">[['dogs', 'run'], ['cats', 'sleep']]</span><br><span class="line">tokenized_sentences with less frequent words converted to '&lt;unk&gt;':</span><br><span class="line">[['dogs', '&lt;unk&gt;'], ['&lt;unk&gt;', 'sleep']]</span><br></pre></td></tr></table></figure><p><a name="ex-07"></a></p><h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Now we are ready to process our data by combining the functions that you just implemented.</p><ol><li>Find tokens that appear at least count_threshold times in the training data.</li><li>Replace tokens that appear less than count_threshold times by “<unk\>“ both for training and test data.</unk\></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: preprocess_data ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span><span class="params">(train_data, test_data, count_threshold)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Preprocess data, i.e.,</span></span><br><span class="line"><span class="string">        - Find tokens that appear at least N times in the training data.</span></span><br><span class="line"><span class="string">        - Replace tokens that appear less than N times by "&lt;unk&gt;" both for training and test data.        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        train_data, test_data: List of lists of strings.</span></span><br><span class="line"><span class="string">        count_threshold: Words whose count is less than this are </span></span><br><span class="line"><span class="string">                      treated as unknown.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tuple of</span></span><br><span class="line"><span class="string">        - training data with low frequent words replaced by "&lt;unk&gt;"</span></span><br><span class="line"><span class="string">        - test data with low frequent words replaced by "&lt;unk&gt;"</span></span><br><span class="line"><span class="string">        - vocabulary of words that appear n times or more in the training data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the closed vocabulary using the train data</span></span><br><span class="line">    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For the train data, replace less common words with "&lt;unk&gt;"</span></span><br><span class="line">    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token=<span class="string">"&lt;unk&gt;"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For the test data, replace less common words with "&lt;unk&gt;"</span></span><br><span class="line">    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token=<span class="string">"&lt;unk&gt;"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> train_data_replaced, test_data_replaced, vocabulary</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tmp_train = [[<span class="string">'sky'</span>, <span class="string">'is'</span>, <span class="string">'blue'</span>, <span class="string">'.'</span>],</span><br><span class="line">     [<span class="string">'leaves'</span>, <span class="string">'are'</span>, <span class="string">'green'</span>]]</span><br><span class="line">tmp_test = [[<span class="string">'roses'</span>, <span class="string">'are'</span>, <span class="string">'red'</span>, <span class="string">'.'</span>]]</span><br><span class="line"></span><br><span class="line">tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, </span><br><span class="line">                                                           tmp_test, </span><br><span class="line">                                                           count_threshold = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"tmp_train_repl"</span>)</span><br><span class="line">print(tmp_train_repl)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"tmp_test_repl"</span>)</span><br><span class="line">print(tmp_test_repl)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"tmp_vocab"</span>)</span><br><span class="line">print(tmp_vocab)</span><br></pre></td></tr></table></figure><pre><code>tmp_train_repl[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;], [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;]]tmp_test_repl[[&#39;&lt;unk&gt;&#39;, &#39;are&#39;, &#39;&lt;unk&gt;&#39;, &#39;.&#39;]]tmp_vocab[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;, &#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;]</code></pre><h5 id="Expected-outcome-1"><a href="#Expected-outcome-1" class="headerlink" title="Expected outcome"></a>Expected outcome</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_train_repl</span><br><span class="line">[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]</span><br><span class="line"></span><br><span class="line">tmp_test_repl</span><br><span class="line">[['&lt;unk&gt;', 'are', '&lt;unk&gt;', '.']]</span><br><span class="line"></span><br><span class="line">tmp_vocab</span><br><span class="line">['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']</span><br></pre></td></tr></table></figure><h3 id="Preprocess-the-train-and-test-data"><a href="#Preprocess-the-train-and-test-data" class="headerlink" title="Preprocess the train and test data"></a>Preprocess the train and test data</h3><p>Run the cell below to complete the preprocessing both for training and test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">minimum_freq = <span class="number">2</span></span><br><span class="line">train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, </span><br><span class="line">                                                                        test_data, </span><br><span class="line">                                                                        minimum_freq)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"First preprocessed training sample:"</span>)</span><br><span class="line">print(train_data_processed[<span class="number">0</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"First preprocessed test sample:"</span>)</span><br><span class="line">print(test_data_processed[<span class="number">0</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"First 10 vocabulary:"</span>)</span><br><span class="line">print(vocabulary[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Size of vocabulary:"</span>, len(vocabulary))</span><br></pre></td></tr></table></figure><pre><code>First preprocessed training sample:[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;, &#39;team&#39;, &#39;local&#39;, &#39;company&#39;, &#39;and&#39;, &#39;quality&#39;, &#39;production&#39;]First preprocessed test sample:[&#39;that&#39;, &#39;picture&#39;, &#39;i&#39;, &#39;just&#39;, &#39;seen&#39;, &#39;whoa&#39;, &#39;dere&#39;, &#39;!&#39;, &#39;!&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;]First 10 vocabulary:[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;]Size of vocabulary: 14821</code></pre><h5 id="Expected-output-4"><a href="#Expected-output-4" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">First preprocessed training sample:</span><br><span class="line">['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']</span><br><span class="line"></span><br><span class="line">First preprocessed test sample:</span><br><span class="line">['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;']</span><br><span class="line"></span><br><span class="line">First <span class="number">10</span> vocabulary:</span><br><span class="line">['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']</span><br><span class="line"></span><br><span class="line">Size of vocabulary: <span class="number">14821</span></span><br></pre></td></tr></table></figure><p>You are done with the preprocessing section of the assignment.<br>Objects <code>train_data_processed</code>, <code>test_data_processed</code>, and <code>vocabulary</code> will be used in the rest of the exercises.</p><p><a name="2"></a></p><h2 id="Part-2-Develop-n-gram-based-language-models"><a href="#Part-2-Develop-n-gram-based-language-models" class="headerlink" title="Part 2: Develop n-gram based language models"></a>Part 2: Develop n-gram based language models</h2><p>In this section, you will develop the n-grams language model.</p><ul><li>Assume the probability of the next word depends only on the previous n-gram.</li><li>The previous n-gram is the series of the previous ‘n’ words.</li></ul><p>The conditional probability for the word at position ‘t’ in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \cdots w_{t-n}$ is:</p><script type="math/tex; mode=display">P(w_t | w_{t-1}\dots w_{t-n}) \tag{1}</script><p>You can estimate this probability  by counting the occurrences of these series of words in the training data.</p><ul><li>The probability can be estimated as a ratio, where</li><li>The numerator is the number of times word ‘t’ appears after words t-1 through t-n appear in the training data.</li><li>The denominator is the number of times word t-1 through t-n appears in the training data.</li></ul><script type="math/tex; mode=display">\hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2}</script><ul><li>The function $C(\cdots)$ denotes the number of occurence of the given sequence. </li><li>$\hat{P}$ means the estimation of $P$. </li><li>Notice that denominator of the equation (2) is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$.</li></ul><p>Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.</p><p>The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).</p><p><a name="ex-08"></a></p><h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p>Next, you will implement a function that computes the counts of n-grams for an arbitrary number $n$.</p><p>When computing the counts for n-grams, prepare the sentence beforehand by prepending $n-1$ starting markers “<s\>“ to indicate the beginning of the sentence.  </s\></p><ul><li>For example, in the bi-gram model (N=2), a sequence with two start tokens “<s\><s\>“ should predict the first word of a sentence.</s\></s\></li><li>So, if the sentence is “I like food”, modify it to be “<s\><s\> I like food”.</s\></s\></li><li>Also prepare the sentence for counting by appending an end token “<e\>“ so that the model can predict when to finish a sentence.</e\></li></ul><p>Technical note: In this implementation, you will store the counts as a dictionary.</p><ul><li>The key of each key-value pair in the dictionary is a <strong>tuple</strong> of n words (and not a list)</li><li>The value in the key-value pair is the number of occurrences.  </li><li>The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created).  A tuple is “immutable”, so it cannot be altered after it is first created.  This makes a tuple suitable as a data type for the key in a dictionary.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> To prepend or append, you can create lists and concatenate them using the + operator </li>    <li> To create a list of a repeated value, you can follow this syntax: <code>['a'] * 3</code> to get <code>['a','a','a']</code> </li>    <li>To set the range for index 'i', think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token).  So the index positions are <code>[0,1,2,3,4]</code>.  The largest index 'i' where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram. </li>    <li>Remember that the <code>range()</code> function excludes the value that is used for the maximum of the range.  <code> range(3) </code> produces (0,1,2) but excludes 3. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED FUNCTION: count_n_grams ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_n_grams</span><span class="params">(data, n, start_token=<span class="string">'&lt;s&gt;'</span>, end_token = <span class="string">'&lt;e&gt;'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count all n-grams in the data</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: List of lists of words</span></span><br><span class="line"><span class="string">        n: number of words in a sequence</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A dictionary that maps a tuple of n-words to its frequency</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dictionary of n-grams and their counts</span></span><br><span class="line">    n_grams = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence in the data</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> data: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># prepend start token n times, and  append &lt;e&gt; one time</span></span><br><span class="line">        sentence = [start_token] * n + sentence + [end_token]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert list to tuple</span></span><br><span class="line">        <span class="comment"># So that the sequence of words can be used as</span></span><br><span class="line">        <span class="comment"># a key in the dictionary</span></span><br><span class="line">        sentence = tuple(sentence)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use 'i' to indicate the start of the n-gram</span></span><br><span class="line">        <span class="comment"># from index 0</span></span><br><span class="line">        <span class="comment"># to the last index where the end of the n-gram</span></span><br><span class="line">        <span class="comment"># is within the sentence.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sentence)-n+<span class="number">1</span>): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Get the n-gram from i to i+n</span></span><br><span class="line">            n_gram = sentence[i:i+n]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the n-gram is in the dictionary</span></span><br><span class="line">            <span class="keyword">if</span> n_gram <span class="keyword">in</span> n_grams: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Increment the count for this n-gram</span></span><br><span class="line">                n_grams[n_gram] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Initialize this n-gram count to 1</span></span><br><span class="line">                n_grams[n_gram] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> n_grams</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line"><span class="comment"># CODE REVIEW COMMENT: Outcome does not match expected outcome</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">print(<span class="string">"Uni-gram:"</span>)</span><br><span class="line">print(count_n_grams(sentences, <span class="number">1</span>))</span><br><span class="line">print(<span class="string">"Bi-gram:"</span>)</span><br><span class="line">print(count_n_grams(sentences, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><pre><code>Uni-gram:{(&#39;&lt;s&gt;&#39;,): 2, (&#39;i&#39;,): 1, (&#39;like&#39;,): 2, (&#39;a&#39;,): 2, (&#39;cat&#39;,): 2, (&#39;&lt;e&gt;&#39;,): 2, (&#39;this&#39;,): 1, (&#39;dog&#39;,): 1, (&#39;is&#39;,): 1}Bi-gram:{(&#39;&lt;s&gt;&#39;, &#39;&lt;s&gt;&#39;): 2, (&#39;&lt;s&gt;&#39;, &#39;i&#39;): 1, (&#39;i&#39;, &#39;like&#39;): 1, (&#39;like&#39;, &#39;a&#39;): 2, (&#39;a&#39;, &#39;cat&#39;): 2, (&#39;cat&#39;, &#39;&lt;e&gt;&#39;): 2, (&#39;&lt;s&gt;&#39;, &#39;this&#39;): 1, (&#39;this&#39;, &#39;dog&#39;): 1, (&#39;dog&#39;, &#39;is&#39;): 1, (&#39;is&#39;, &#39;like&#39;): 1}</code></pre><p>Expected outcome:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Uni-gram:</span><br><span class="line">&#123;('&lt;s&gt;',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('&lt;e&gt;',): 2, ('this',): 1, ('dog',): 1, ('is',): 1&#125;</span><br><span class="line">Bi-gram:</span><br><span class="line">&#123;('&lt;s&gt;', '&lt;s&gt;'): 2, ('&lt;s&gt;', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '&lt;e&gt;'): 2, ('&lt;s&gt;', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1&#125;</span><br></pre></td></tr></table></figure><p><a name="ex-09"></a></p><h3 id="Exercise-09"><a href="#Exercise-09" class="headerlink" title="Exercise 09"></a>Exercise 09</h3><p>Next, estimate the probability of a word given the prior ‘n’ words using the n-gram counts.</p><script type="math/tex; mode=display">\hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2}</script><p>This formula doesn’t work when a count of an n-gram is zero..</p><ul><li>Suppose we encounter an n-gram that did not occur in the training data.  </li><li>Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).</li></ul><p>A way to handle zero counts is to add k-smoothing.  </p><ul><li>K-smoothing adds a positive constant $k$ to each numerator and $k \times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.</li></ul><script type="math/tex; mode=display">\hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n) + k}{C(w_{t-1}\dots w_{t-n}) + k|V|} \tag{3}</script><p>For n-grams that have a zero count, the equation (3) becomes $\frac{1}{|V|}$.</p><ul><li>This means that any n-gram with zero count has the same probability of $\frac{1}{|V|}$.</li></ul><p>Define a function that computes the probability estimate (3) from n-gram counts and a constant $k$.</p><ul><li>The function takes in a dictionary ‘n_gram_counts’, where the key is the n-gram and the value is the count of that n-gram.</li><li>The function also takes another dictionary n_plus1_gram_counts, which you’ll use to find the count for the previous n-gram plus the current word.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>To define a tuple containing a single value, add a comma after that value.  For example: <code>('apple',)</code> is a tuple containing a single string 'apple' </li>    <li>To concatenate two tuples, use the '+' operator</li>    <li><a href> words </a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED FUNCTION: estimate_probabilityy ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_probability</span><span class="params">(word, previous_n_gram, </span></span></span><br><span class="line"><span class="function"><span class="params">                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        word: next word</span></span><br><span class="line"><span class="string">        previous_n_gram: A sequence of words of length n</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of n-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary_size: number of words in the vocabulary</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A probability</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># convert list to tuple to use it as a dictionary key</span></span><br><span class="line">    previous_n_gram = tuple(previous_n_gram)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the denominator</span></span><br><span class="line">    <span class="comment"># If the previous n-gram exists in the dictionary of n-gram counts,</span></span><br><span class="line">    <span class="comment"># Get its count.  Otherwise set the count to zero</span></span><br><span class="line">    <span class="comment"># Use the dictionary that has counts for n-grams</span></span><br><span class="line">    previous_n_gram_count = n_gram_counts.get(previous_n_gram, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Calculate the denominator using the count of the previous n gram</span></span><br><span class="line">    <span class="comment"># and apply k-smoothing</span></span><br><span class="line">    denominator = previous_n_gram_count + k * vocabulary_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define n plus 1 gram as the previous n-gram plus the current word as a tuple</span></span><br><span class="line">    n_plus1_gram = n_gram_counts.get(previous_n_gram, <span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Set the count to the count in the dictionary,</span></span><br><span class="line">    <span class="comment"># otherwise 0 if not in the dictionary</span></span><br><span class="line">    <span class="comment"># use the dictionary that has counts for the n-gram plus current word</span></span><br><span class="line">    n_plus1_gram_count = n_plus1_gram_counts.get(previous_n_gram + (word, ) ,<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Define the numerator use the count of the n-gram plus current word,</span></span><br><span class="line">    <span class="comment"># and apply smoothing</span></span><br><span class="line">    numerator = n_plus1_gram_count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the probability as the numerator divided by denominator</span></span><br><span class="line">    probability = numerator / denominator</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> probability</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">tmp_prob = estimate_probability(<span class="string">"cat"</span>, <span class="string">"a"</span>, unigram_counts, bigram_counts, len(unique_words), k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The estimated probability of word 'cat' given the previous n-gram 'a' is: <span class="subst">&#123;tmp_prob:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>The estimated probability of word &#39;cat&#39; given the previous n-gram &#39;a&#39; is: 0.3333</code></pre><h5 id="Expected-output-5"><a href="#Expected-output-5" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333</span><br></pre></td></tr></table></figure><h3 id="Estimate-probabilities-for-all-words"><a href="#Estimate-probabilities-for-all-words" class="headerlink" title="Estimate probabilities for all words"></a>Estimate probabilities for all words</h3><p>The function defined below loops over all words in vocabulary to calculate probabilities for all possible words.</p><ul><li>This function is provided for you.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_probabilities</span><span class="params">(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Estimate the probabilities of next words using the n-gram counts with k-smoothing</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        previous_n_gram: A sequence of words of length n</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary: List of words</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A dictionary mapping from next words to the probability.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert list to tuple to use it as a dictionary key</span></span><br><span class="line">    previous_n_gram = tuple(previous_n_gram)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span></span><br><span class="line">    <span class="comment"># &lt;s&gt; is not needed since it should not appear as the next word</span></span><br><span class="line">    vocabulary = vocabulary + [<span class="string">"&lt;e&gt;"</span>, <span class="string">"&lt;unk&gt;"</span>]</span><br><span class="line">    vocabulary_size = len(vocabulary)</span><br><span class="line">    </span><br><span class="line">    probabilities = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocabulary:</span><br><span class="line">        probability = estimate_probability(word, previous_n_gram, </span><br><span class="line">                                           n_gram_counts, n_plus1_gram_counts, </span><br><span class="line">                                           vocabulary_size, k=k)</span><br><span class="line">        probabilities[word] = probability</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> probabilities</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">estimate_probabilities(<span class="string">"a"</span>, unigram_counts, bigram_counts, unique_words, k=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>{&#39;dog&#39;: 0.09090909090909091, &#39;like&#39;: 0.09090909090909091, &#39;cat&#39;: 0.2727272727272727, &#39;i&#39;: 0.09090909090909091, &#39;is&#39;: 0.09090909090909091, &#39;this&#39;: 0.09090909090909091, &#39;a&#39;: 0.09090909090909091, &#39;&lt;e&gt;&#39;: 0.09090909090909091, &#39;&lt;unk&gt;&#39;: 0.09090909090909091}</code></pre><h5 id="Expected-output-6"><a href="#Expected-output-6" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;'cat': 0.2727272727272727,</span><br><span class="line"> <span class="string">'i'</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> 'this': 0.09090909090909091,</span><br><span class="line"> <span class="string">'a'</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> 'is': 0.09090909090909091,</span><br><span class="line"> 'like': 0.09090909090909091,</span><br><span class="line"> 'dog': 0.09090909090909091,</span><br><span class="line"> '&lt;e&gt;': 0.09090909090909091,</span><br><span class="line"> '&lt;unk&gt;': 0.09090909090909091&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Additional test</span></span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">estimate_probabilities([<span class="string">"&lt;s&gt;"</span>, <span class="string">"&lt;s&gt;"</span>], bigram_counts, trigram_counts, unique_words, k=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>{&#39;dog&#39;: 0.09090909090909091, &#39;like&#39;: 0.09090909090909091, &#39;cat&#39;: 0.09090909090909091, &#39;i&#39;: 0.18181818181818182, &#39;is&#39;: 0.09090909090909091, &#39;this&#39;: 0.18181818181818182, &#39;a&#39;: 0.09090909090909091, &#39;&lt;e&gt;&#39;: 0.09090909090909091, &#39;&lt;unk&gt;&#39;: 0.09090909090909091}</code></pre><h5 id="Expected-output-7"><a href="#Expected-output-7" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;'cat': 0.09090909090909091,</span><br><span class="line"> <span class="string">'i'</span>: <span class="number">0.18181818181818182</span>,</span><br><span class="line"> 'this': 0.18181818181818182,</span><br><span class="line"> <span class="string">'a'</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> 'is': 0.09090909090909091,</span><br><span class="line"> 'like': 0.09090909090909091,</span><br><span class="line"> 'dog': 0.09090909090909091,</span><br><span class="line"> '&lt;e&gt;': 0.09090909090909091,</span><br><span class="line"> '&lt;unk&gt;': 0.09090909090909091&#125;</span><br></pre></td></tr></table></figure><h3 id="Count-and-probability-matrices"><a href="#Count-and-probability-matrices" class="headerlink" title="Count and probability matrices"></a>Count and probability matrices</h3><p>As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.  </p><ul><li>It can be more intuitive to present them as count or probability matrices.</li><li>The functions defined in the next cells return count or probability matrices.</li><li>This function is provided for you.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_count_matrix</span><span class="params">(n_plus1_gram_counts, vocabulary)</span>:</span></span><br><span class="line">    <span class="comment"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span></span><br><span class="line">    <span class="comment"># &lt;s&gt; is omitted since it should not appear as the next word</span></span><br><span class="line">    vocabulary = vocabulary + [<span class="string">"&lt;e&gt;"</span>, <span class="string">"&lt;unk&gt;"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># obtain unique n-grams</span></span><br><span class="line">    n_grams = []</span><br><span class="line">    <span class="keyword">for</span> n_plus1_gram <span class="keyword">in</span> n_plus1_gram_counts.keys():</span><br><span class="line">        n_gram = n_plus1_gram[<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">        n_grams.append(n_gram)</span><br><span class="line">    n_grams = list(set(n_grams))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># mapping from n-gram to row</span></span><br><span class="line">    row_index = &#123;n_gram:i <span class="keyword">for</span> i, n_gram <span class="keyword">in</span> enumerate(n_grams)&#125;</span><br><span class="line">    <span class="comment"># mapping from next word to column</span></span><br><span class="line">    col_index = &#123;word:j <span class="keyword">for</span> j, word <span class="keyword">in</span> enumerate(vocabulary)&#125;</span><br><span class="line">    </span><br><span class="line">    nrow = len(n_grams)</span><br><span class="line">    ncol = len(vocabulary)</span><br><span class="line">    count_matrix = np.zeros((nrow, ncol))</span><br><span class="line">    <span class="keyword">for</span> n_plus1_gram, count <span class="keyword">in</span> n_plus1_gram_counts.items():</span><br><span class="line">        n_gram = n_plus1_gram[<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">        word = n_plus1_gram[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocabulary:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        i = row_index[n_gram]</span><br><span class="line">        j = col_index[word]</span><br><span class="line">        count_matrix[i, j] = count</span><br><span class="line">    </span><br><span class="line">    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)</span><br><span class="line">    <span class="keyword">return</span> count_matrix</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">                 [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'bigram counts'</span>)</span><br><span class="line">display(make_count_matrix(bigram_counts, unique_words))</span><br></pre></td></tr></table></figure><pre><code>bigram counts</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>dog</th>      <th>like</th>      <th>cat</th>      <th>i</th>      <th>is</th>      <th>this</th>      <th>a</th>      <th>&lt;e&gt;</th>      <th>&lt;unk&gt;</th>    </tr>  </thead>  <tbody>    <tr>      <th>(dog,)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(like,)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(&lt;s&gt;,)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(i,)</th>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(is,)</th>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(this,)</th>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(cat,)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(a,)</th>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>  </tbody></table></div><h5 id="Expected-output-8"><a href="#Expected-output-8" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bigram counts</span><br><span class="line">          cat    i   <span class="keyword">this</span>   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;</span><br><span class="line">(&lt;s&gt;,)    <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(a,)      <span class="number">2.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(<span class="keyword">this</span>,)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(like,)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">2.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(dog,)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(cat,)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">2.0</span>    <span class="number">0.0</span></span><br><span class="line">(is,)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(i,)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show trigram counts</span></span><br><span class="line">print(<span class="string">'\ntrigram counts'</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">display(make_count_matrix(trigram_counts, unique_words))</span><br></pre></td></tr></table></figure><pre><code>trigram counts</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>dog</th>      <th>like</th>      <th>cat</th>      <th>i</th>      <th>is</th>      <th>this</th>      <th>a</th>      <th>&lt;e&gt;</th>      <th>&lt;unk&gt;</th>    </tr>  </thead>  <tbody>    <tr>      <th>(&lt;s&gt;, &lt;s&gt;)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(&lt;s&gt;, i)</th>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(is, like)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(i, like)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(&lt;s&gt;, this)</th>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(a, cat)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(like, a)</th>      <td>0.0</td>      <td>0.0</td>      <td>2.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(dog, is)</th>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <th>(this, dog)</th>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>  </tbody></table></div><h5 id="Expected-output-9"><a href="#Expected-output-9" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trigram counts</span><br><span class="line">              cat    i   <span class="keyword">this</span>   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;</span><br><span class="line">(dog, is)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(<span class="keyword">this</span>, dog)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(a, cat)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">2.0</span>    <span class="number">0.0</span></span><br><span class="line">(like, a)     <span class="number">2.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(is, like)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, i)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(i, like)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, &lt;s&gt;)    <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, <span class="keyword">this</span>)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br></pre></td></tr></table></figure><p>The following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form.</p><ul><li>This function is provided for you.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_probability_matrix</span><span class="params">(n_plus1_gram_counts, vocabulary, k)</span>:</span></span><br><span class="line">    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)</span><br><span class="line">    count_matrix += k</span><br><span class="line">    prob_matrix = count_matrix.div(count_matrix.sum(axis=<span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> prob_matrix</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">                 [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">print(<span class="string">"bigram probabilities"</span>)</span><br><span class="line">display(make_probability_matrix(bigram_counts, unique_words, k=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>bigram probabilities</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>dog</th>      <th>like</th>      <th>cat</th>      <th>i</th>      <th>is</th>      <th>this</th>      <th>a</th>      <th>&lt;e&gt;</th>      <th>&lt;unk&gt;</th>    </tr>  </thead>  <tbody>    <tr>      <th>(dog,)</th>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(like,)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>    <tr>      <th>(&lt;s&gt;,)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.181818</td>      <td>0.090909</td>      <td>0.181818</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>    <tr>      <th>(i,)</th>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(is,)</th>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(this,)</th>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(cat,)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>    </tr>    <tr>      <th>(a,)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"trigram probabilities"</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">display(make_probability_matrix(trigram_counts, unique_words, k=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>trigram probabilities</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>dog</th>      <th>like</th>      <th>cat</th>      <th>i</th>      <th>is</th>      <th>this</th>      <th>a</th>      <th>&lt;e&gt;</th>      <th>&lt;unk&gt;</th>    </tr>  </thead>  <tbody>    <tr>      <th>(&lt;s&gt;, &lt;s&gt;)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.181818</td>      <td>0.090909</td>      <td>0.181818</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>    <tr>      <th>(&lt;s&gt;, i)</th>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(is, like)</th>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(i, like)</th>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(&lt;s&gt;, this)</th>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(a, cat)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>    </tr>    <tr>      <th>(like, a)</th>      <td>0.090909</td>      <td>0.090909</td>      <td>0.272727</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>      <td>0.090909</td>    </tr>    <tr>      <th>(dog, is)</th>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>    <tr>      <th>(this, dog)</th>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.200000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>      <td>0.100000</td>    </tr>  </tbody></table></div><p>Confirm that you obtain the same results as for the <code>estimate_probabilities</code> function that you implemented.</p><p><a name="3"></a></p><h2 id="Part-3-Perplexity"><a href="#Part-3-Perplexity" class="headerlink" title="Part 3: Perplexity"></a>Part 3: Perplexity</h2><p>In this section, you will generate the perplexity score to evaluate your model on the test set. </p><ul><li>You will also use back-off when needed. </li><li>Perplexity is used as an evaluation metric of your language model. </li><li>To calculate the  the perplexity score of the test set on an n-gram model, use: </li></ul><script type="math/tex; mode=display">PP(W) =\sqrt[N]{ \prod_{t=n+1}^N \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4}</script><ul><li>where $N$ is the length of the sentence.</li><li>$n$ is the number of words in the n-gram (e.g. 2 for a bigram).</li><li>In math, the numbering starts at one and not zero.</li></ul><p>In code, array indexing starts at zero, so the code will use ranges for $t$ according to this formula:</p><script type="math/tex; mode=display">PP(W) =\sqrt[N]{ \prod_{t=n}^{N-1} \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4.1}</script><p>The higher the probabilities are, the lower the perplexity will be. </p><ul><li>The more the n-grams tell us about the sentence, the lower the perplexity score will be. </li></ul><p><a name="ex-10"></a></p><h3 id="Exercise-10"><a href="#Exercise-10" class="headerlink" title="Exercise 10"></a>Exercise 10</h3><p>Compute the perplexity score given an N-gram count matrix and a sentence. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember that <code>range(2,4)</code> produces the integers [2, 3] (and excludes 4).</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: calculate_perplexity</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_perplexity</span><span class="params">(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate perplexity for a list of sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sentence: List of strings</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary_size: number of unique words in the vocabulary</span></span><br><span class="line"><span class="string">        k: Positive smoothing constant</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Perplexity score</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># length of previous words</span></span><br><span class="line">    n = len(list(n_gram_counts.keys())[<span class="number">0</span>]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># prepend &lt;s&gt; and append &lt;e&gt;</span></span><br><span class="line">    sentence = [<span class="string">"&lt;s&gt;"</span>] * n + sentence + [<span class="string">"&lt;e&gt;"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Cast the sentence from a list to a tuple</span></span><br><span class="line">    sentence = tuple(sentence)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># length of sentence (after adding &lt;s&gt; and &lt;e&gt; tokens)</span></span><br><span class="line">    N = len(sentence)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The variable p will hold the product</span></span><br><span class="line">    <span class="comment"># that is calculated inside the n-root</span></span><br><span class="line">    <span class="comment"># Update this in the code below</span></span><br><span class="line">    product_pi = <span class="number">1.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Index t ranges from n to N - 1, inclusive on both ends</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(n, N): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the n-gram preceding the word at position t</span></span><br><span class="line">        n_gram = sentence[t-n:t]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the word at position t</span></span><br><span class="line">        word = sentence[t]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Estimate the probability of the word given the n-gram</span></span><br><span class="line">        <span class="comment"># using the n-gram counts, n-plus1-gram counts,</span></span><br><span class="line">        <span class="comment"># vocabulary size, and smoothing constant</span></span><br><span class="line">        probability = estimate_probability(word, n_gram, </span><br><span class="line">                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the product of the probabilities</span></span><br><span class="line">        <span class="comment"># This 'product_pi' is a cumulative product </span></span><br><span class="line">        <span class="comment"># of the (1/P) factors that are calculated in the loop</span></span><br><span class="line">        product_pi *= (<span class="number">1</span>/probability)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take the Nth root of the product</span></span><br><span class="line">    perplexity = product_pi ** (<span class="number">1</span>/N)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> perplexity</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line"></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">                 [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perplexity_train1 = calculate_perplexity(sentences[<span class="number">0</span>],</span><br><span class="line">                                         unigram_counts, bigram_counts,</span><br><span class="line">                                         len(unique_words), k=<span class="number">1.0</span>)</span><br><span class="line">print(<span class="string">f"Perplexity for first train sample: <span class="subst">&#123;perplexity_train1:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">test_sentence = [<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'dog'</span>]</span><br><span class="line">perplexity_test = calculate_perplexity(test_sentence,</span><br><span class="line">                                       unigram_counts, bigram_counts,</span><br><span class="line">                                       len(unique_words), k=<span class="number">1.0</span>)</span><br><span class="line">print(<span class="string">f"Perplexity for test sample: <span class="subst">&#123;perplexity_test:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Perplexity for first train sample: 2.8040Perplexity for test sample: 3.9654</code></pre><h3 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Perplexity <span class="keyword">for</span> first train sample: <span class="number">2.8040</span></span><br><span class="line">Perplexity <span class="keyword">for</span> test sample: <span class="number">3.9654</span></span><br></pre></td></tr></table></figure><p><b> Note: </b> If your sentence is really long, there will be underflow when multiplying many fractions.</p><ul><li>To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.</li></ul><p><a name="4"></a></p><h2 id="Part-4-Build-an-auto-complete-system"><a href="#Part-4-Build-an-auto-complete-system" class="headerlink" title="Part 4: Build an auto-complete system"></a>Part 4: Build an auto-complete system</h2><p>In this section, you will combine the language models developed so far to implement an auto-complete system. </p><p><a name="ex-11"></a></p><h3 id="Exercise-11"><a href="#Exercise-11" class="headerlink" title="Exercise 11"></a>Exercise 11</h3><p>Compute probabilities for all possible next words and suggest the most likely one.</p><ul><li>This function also take an optional argument <code>start_with</code>, which specifies the first few letters of the next words.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li><code>estimate_probabilities</code> returns a dictionary where the key is a word and the value is the word's probability.</li>    <li> Use <code>str1.startswith(str2)</code> to determine if a string starts with the letters of another string.  For example, <code>'learning'.startswith('lea')</code> returns True, whereas <code>'learning'.startswith('ear')</code> returns False. There are two additional parameters in <code>str.startswith()</code>, but you can use the default values for those parameters in this case.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: suggest_a_word</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">suggest_a_word</span><span class="params">(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>, start_with=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Get suggestion for the next word</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        previous_tokens: The sentence you input where each token is a word. Must have length &gt; n </span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary: List of words</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">        start_with: If not None, specifies the first few letters of the next word</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A tuple of </span></span><br><span class="line"><span class="string">          - string of the most likely next word</span></span><br><span class="line"><span class="string">          - corresponding probability</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># length of previous words</span></span><br><span class="line">    n = len(list(n_gram_counts.keys())[<span class="number">0</span>]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># From the words that the user already typed</span></span><br><span class="line">    <span class="comment"># get the most recent 'n' words as the previous n-gram</span></span><br><span class="line">    previous_n_gram = previous_tokens[-n:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Estimate the probabilities that each word in the vocabulary</span></span><br><span class="line">    <span class="comment"># is the next word,</span></span><br><span class="line">    <span class="comment"># given the previous n-gram, the dictionary of n-gram counts,</span></span><br><span class="line">    <span class="comment"># the dictionary of n plus 1 gram counts, and the smoothing constant</span></span><br><span class="line">    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize suggested word to None</span></span><br><span class="line">    <span class="comment"># This will be set to the word with highest probability</span></span><br><span class="line">    suggestion = <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the highest word probability to 0</span></span><br><span class="line">    <span class="comment"># this will be set to the highest probability </span></span><br><span class="line">    <span class="comment"># of all words to be suggested</span></span><br><span class="line">    max_prob = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For each word and its probability in the probabilities dictionary:</span></span><br><span class="line">    <span class="keyword">for</span> word, prob <span class="keyword">in</span> probabilities.items(): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the optional start_with string is set</span></span><br><span class="line">        <span class="keyword">if</span> start_with: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the beginning of word does not match with the letters in 'start_with'</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> word.startswith(start_with): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># if they don't match, skip this word (move onto the next word)</span></span><br><span class="line">                 <span class="keyword">continue</span><span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if this word's probability</span></span><br><span class="line">        <span class="comment"># is greater than the current maximum probability</span></span><br><span class="line">        <span class="keyword">if</span> prob &gt; max_prob: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If so, save this word as the best suggestion (so far)</span></span><br><span class="line">            suggestion = word</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the new maximum probability</span></span><br><span class="line">            max_prob = prob</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> suggestion, max_prob</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">previous_tokens = [<span class="string">"i"</span>, <span class="string">"like"</span>]</span><br><span class="line">tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=<span class="number">1.0</span>)</span><br><span class="line">print(<span class="string">f"The previous words are 'i like',\n\tand the suggested word is `<span class="subst">&#123;tmp_suggest1[<span class="number">0</span>]&#125;</span>` with a probability of <span class="subst">&#123;tmp_suggest1[<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print()</span><br><span class="line"><span class="comment"># test your code when setting the starts_with</span></span><br><span class="line">tmp_starts_with = <span class="string">'c'</span></span><br><span class="line">tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=<span class="number">1.0</span>, start_with=tmp_starts_with)</span><br><span class="line">print(<span class="string">f"The previous words are 'i like', the suggestion must start with `<span class="subst">&#123;tmp_starts_with&#125;</span>`\n\tand the suggested word is `<span class="subst">&#123;tmp_suggest2[<span class="number">0</span>]&#125;</span>` with a probability of <span class="subst">&#123;tmp_suggest2[<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>The previous words are &#39;i like&#39;,    and the suggested word is `a` with a probability of 0.2727The previous words are &#39;i like&#39;, the suggestion must start with `c`    and the suggested word is `cat` with a probability of 0.0909</code></pre><h3 id="Expected-output-10"><a href="#Expected-output-10" class="headerlink" title="Expected output"></a>Expected output</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The previous words are 'i like',</span><br><span class="line">    <span class="keyword">and</span> the suggested word is `a` with a probability of <span class="number">0.2727</span></span><br><span class="line"></span><br><span class="line">The previous words are 'i like', the suggestion must start with `c`</span><br><span class="line">    <span class="keyword">and</span> the suggested word is `cat` with a probability of <span class="number">0.0909</span></span><br></pre></td></tr></table></figure><h3 id="Get-multiple-suggestions"><a href="#Get-multiple-suggestions" class="headerlink" title="Get multiple suggestions"></a>Get multiple suggestions</h3><p>The function defined below loop over varioud n-gram models to get multiple suggestions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_suggestions</span><span class="params">(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>, start_with=None)</span>:</span></span><br><span class="line">    model_counts = len(n_gram_counts_list)</span><br><span class="line">    suggestions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(model_counts<span class="number">-1</span>):</span><br><span class="line">        n_gram_counts = n_gram_counts_list[i]</span><br><span class="line">        n_plus1_gram_counts = n_gram_counts_list[i+<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        suggestion = suggest_a_word(previous_tokens, n_gram_counts,</span><br><span class="line">                                    n_plus1_gram_counts, vocabulary,</span><br><span class="line">                                    k=k, start_with=start_with)</span><br><span class="line">        suggestions.append(suggestion)</span><br><span class="line">    <span class="keyword">return</span> suggestions</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">'i'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>],</span><br><span class="line">             [<span class="string">'this'</span>, <span class="string">'dog'</span>, <span class="string">'is'</span>, <span class="string">'like'</span>, <span class="string">'a'</span>, <span class="string">'cat'</span>]]</span><br><span class="line">unique_words = list(set(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">quadgram_counts = count_n_grams(sentences, <span class="number">4</span>)</span><br><span class="line">qintgram_counts = count_n_grams(sentences, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]</span><br><span class="line">previous_tokens = [<span class="string">"i"</span>, <span class="string">"like"</span>]</span><br><span class="line">tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are 'i like', the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest3)</span><br></pre></td></tr></table></figure><pre><code>The previous words are &#39;i like&#39;, the suggestions are:[(&#39;a&#39;, 0.2727272727272727), (&#39;a&#39;, 0.2), (&#39;dog&#39;, 0.1111111111111111), (&#39;dog&#39;, 0.1111111111111111)]</code></pre><h3 id="Suggest-multiple-words-using-n-grams-of-varying-length"><a href="#Suggest-multiple-words-using-n-grams-of-varying-length" class="headerlink" title="Suggest multiple words using n-grams of varying length"></a>Suggest multiple words using n-grams of varying length</h3><p>Congratulations!  You have developed all building blocks for implementing your own auto-complete systems.</p><p>Let’s see this with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams…6-grams).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_gram_counts_list = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    print(<span class="string">"Computing n-gram counts with n ="</span>, n, <span class="string">"..."</span>)</span><br><span class="line">    n_model_counts = count_n_grams(train_data_processed, n)</span><br><span class="line">    n_gram_counts_list.append(n_model_counts)</span><br></pre></td></tr></table></figure><pre><code>Computing n-gram counts with n = 1 ...Computing n-gram counts with n = 2 ...Computing n-gram counts with n = 3 ...Computing n-gram counts with n = 4 ...Computing n-gram counts with n = 5 ...</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"i"</span>, <span class="string">"am"</span>, <span class="string">"to"</span>]</span><br><span class="line">tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest4)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;i&#39;, &#39;am&#39;, &#39;to&#39;], the suggestions are:[(&#39;be&#39;, 0.027665685098338604), (&#39;have&#39;, 0.00013487086115044844), (&#39;have&#39;, 0.00013490725126475548), (&#39;i&#39;, 6.746272684341901e-05)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"i"</span>, <span class="string">"want"</span>, <span class="string">"to"</span>, <span class="string">"go"</span>]</span><br><span class="line">tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest5)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;i&#39;, &#39;want&#39;, &#39;to&#39;, &#39;go&#39;], the suggestions are:[(&#39;to&#39;, 0.014051961029228078), (&#39;to&#39;, 0.004697942168993581), (&#39;to&#39;, 0.0009424436216762033), (&#39;to&#39;, 0.0004044489383215369)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"hey"</span>, <span class="string">"how"</span>, <span class="string">"are"</span>]</span><br><span class="line">tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest6)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;], the suggestions are:[(&#39;you&#39;, 0.023426812585499317), (&#39;you&#39;, 0.003559435862995299), (&#39;you&#39;, 0.00013491635186184566), (&#39;i&#39;, 6.746272684341901e-05)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"hey"</span>, <span class="string">"how"</span>, <span class="string">"are"</span>, <span class="string">"you"</span>]</span><br><span class="line">tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest7)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;], the suggestions are:[(&quot;&#39;re&quot;, 0.023973994311255586), (&#39;?&#39;, 0.002888465830762161), (&#39;?&#39;, 0.0016134453781512605), (&#39;&lt;e&gt;&#39;, 0.00013491635186184566)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">"hey"</span>, <span class="string">"how"</span>, <span class="string">"are"</span>, <span class="string">"you"</span>]</span><br><span class="line">tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>, start_with=<span class="string">"d"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:"</span>)</span><br><span class="line">display(tmp_suggest8)</span><br></pre></td></tr></table></figure><pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;], the suggestions are:[(&#39;do&#39;, 0.009020723283218204), (&#39;doing&#39;, 0.0016411737674785006), (&#39;doing&#39;, 0.00047058823529411766), (&#39;dvd&#39;, 6.745817593092283e-05)]</code></pre><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You’ve completed this assignment by building an autocomplete model using an n-gram language model!  </p><p>Please continue onto the fourth and final week of this course!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Language-Models-Auto-Complete&quot;&gt;&lt;a href=&quot;#Language-Models-Auto-Complete&quot; class=&quot;headerlink&quot; title=&quot;Language Models: Auto-Complete&quot;&gt;&lt;/
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Auto Correct</title>
    <link href="https://zhangruochi.com/Auto-Correct/2020/07/19/"/>
    <id>https://zhangruochi.com/Auto-Correct/2020/07/19/</id>
    <published>2020-07-19T08:27:04.000Z</published>
    <updated>2020-07-19T08:27:50.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-1-Auto-Correct"><a href="#Assignment-1-Auto-Correct" class="headerlink" title="Assignment 1: Auto Correct"></a>Assignment 1: Auto Correct</h1><p>Welcome to the first assignment of Course 2. This assignment will give you a chance to brush up on your python and probability skills. In doing so, you will implement an auto-correct system that is very effective and useful.</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">0. Overview</a><ul><li><a href="#0-1">0.1 Edit Distance</a></li></ul></li><li><a href="#1">1. Data Preprocessing</a><ul><li><a href="#ex-1">1.1 Exercise 1</a></li><li><a href="#ex-2">1.2 Exercise 2</a></li><li><a href="#ex-3">1.3 Exercise 3</a></li></ul></li><li><a href="#2">2. String Manipulation</a><ul><li><a href="#ex-4">2.1 Exercise 4</a></li><li><a href="#ex-5">2.2 Exercise 5</a></li><li><a href="#ex-6">2.3 Exercise 6</a></li><li><a href="#ex-7">2.4 Exercise 7</a></li></ul></li><li><a href="#3">3. Combining the edits</a><ul><li><a href="#ex-8">3.1 Exercise 8</a></li><li><a href="#ex-9">3.2 Exercise 9</a></li><li><a href="#ex-10">3.3 Exercise 10</a></li></ul></li><li><a href="#4">4. Minimum Edit Distance</a><ul><li><a href="#ex-11">4.1 Exercise 11</a></li></ul></li><li><a href="#5">5. Backtrace (Optional)</a></li></ul><p><a name="0"></a></p><h2 id="0-Overview"><a href="#0-Overview" class="headerlink" title="0. Overview"></a>0. Overview</h2><p>You use autocorrect every day on your cell phone and computer. In this assignment, you will explore what really goes on behind the scenes. Of course, the model you are about to implement is not identical to the one used in your phone, but it is still quite good. </p><p>By completing this assignment you will learn how to: </p><ul><li>Get a word count given a corpus</li><li>Get a word probability in the corpus </li><li>Manipulate strings </li><li>Filter strings </li><li>Implement Minimum edit distance to compare strings and to help find the optimal path for the edits. </li><li>Understand how dynamic programming works</li></ul><p>Similar systems are used everywhere. </p><ul><li>For example, if you type in the word <strong>“I am lerningg”</strong>, chances are very high that you meant to write <strong>“learning”</strong>, as shown in <strong>Figure 1</strong>. </li></ul><div style="width:image width px; font-size:100%; text-align:center;"><img src="auto-correct.png" alt="alternate text" width="width" height="height" style="width:300px;height:250px;"> Figure 1 </div><p><a name="0-1"></a></p><h4 id="0-1-Edit-Distance"><a href="#0-1-Edit-Distance" class="headerlink" title="0.1 Edit Distance"></a>0.1 Edit Distance</h4><p>In this assignment, you will implement models that correct words that are 1 and 2 edit distances away. </p><ul><li>We say two words are n edit distance away from each other when we need n edits to change one word into another. </li></ul><p>An edit could consist of one of the following options: </p><ul><li>Delete (remove a letter): ‘hat’ =&gt; ‘at, ha, ht’</li><li>Switch (swap 2 adjacent letters): ‘eta’ =&gt; ‘eat, tea,…’</li><li>Replace (change 1 letter to another): ‘jat’ =&gt; ‘hat, rat, cat, mat, …’</li><li>Insert (add a letter): ‘te’ =&gt; ‘the, ten, ate, …’</li></ul><p>You will be using the four methods above to implement an Auto-correct. </p><ul><li>To do so, you will need to compute probabilities that a certain word is correct given an input. </li></ul><p>This auto-correct you are about to implement was first created by <a href="https://en.wikipedia.org/wiki/Peter_Norvig" target="_blank" rel="noopener">Peter Norvig</a> in 2007. </p><ul><li>His <a href="https://norvig.com/spell-correct.html" target="_blank" rel="noopener">original article</a> may be a useful reference for this assignment.</li></ul><p>The goal of our spell check model is to compute the following probability:</p><script type="math/tex; mode=display">P(c|w) = \frac{P(w|c)\times P(c)}{P(w)} \tag{Eqn-1}</script><p>The equation above is <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="noopener">Bayes Rule</a>. </p><ul><li>Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word $w$, given that it is correct $P(w|c)$, multiplied by the probability of being correct in general $P(C)$ divided by the probability of that word $w$ appearing $P(w)$ in general.</li><li>To compute equation 1, you will first import a data set and then create all the probabilities that you need using that data set. </li></ul><p><a name="1"></a></p><h1 id="Part-1-Data-Preprocessing"><a href="#Part-1-Data-Preprocessing" class="headerlink" title="Part 1: Data Preprocessing"></a>Part 1: Data Preprocessing</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><p>As in any other machine learning task, the first thing you have to do is process your data set. </p><ul><li>Many courses load in pre-processed data for you. </li><li>However, in the real world, when you build these NLP systems, you load the datasets and process them.</li><li>So let’s get some real world practice in pre-processing the data!</li></ul><p>Your first task is to read in a file called <strong>‘shakespeare.txt’</strong> which is found in your file directory. To look at this file you can go to <code>File ==&gt; Open</code>. </p><p><a name="ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><p>Implement the function <code>process_data</code> which </p><p>1) Reads in a corpus (text file)</p><p>2) Changes everything to lowercase</p><p>3) Returns a list of words. </p><h4 id="Options-and-Hints"><a href="#Options-and-Hints" class="headerlink" title="Options and Hints"></a>Options and Hints</h4><ul><li>If you would like more of a real-life practice, don’t open the ‘Hints’ below (yet) and try searching the web to derive your answer.</li><li>If you want a little help, click on the green “General Hints” section by clicking on it with your mouse.</li><li>If you get stuck or are not getting the expected results, click on the green ‘Detailed Hints’ section to get hints for each step that you’ll take to complete this function.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>General Hints</b></font></summary></p><p>General Hints to get started<ul>    <li>Python <a href="https://docs.python.org/3/tutorial/inputoutput.html" target="_blank" rel="noopener">input and output<a></a></a></li>    <li>Python <a href="https://docs.python.org/3/library/re.html" target="_blank" rel="noopener">'re' documentation </a> </li></ul></p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Detailed Hints</b></font></summary></p><p>     Detailed hints if you're stuck<ul>    <li>Use 'with' syntax to read a file</li>    <li>Decide whether to use 'read()' or 'readline().  What's the difference?</li>    <li>Choose whether to use either str.lower() or str.lowercase().  What is the difference?</li>    <li>Use re.findall(pattern, string)</li>    <li>Look for the "Raw String Notation" section in the Python 're' documentation to understand the difference between r'\W', r'\W' and '\\W'. </li>    <li>For the pattern, decide between using '\s', '\w', '\s+' or '\w+'.  What do you think are the differences?</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: process_data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_data</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        A file_name which is found in your current directory. You just have to read it in. </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        words: a list containing all the words in the corpus (text file you read) in lower case. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words = [] <span class="comment"># return this variable correctly</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> open(file_name, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        content = f.read().lower()</span><br><span class="line">    </span><br><span class="line">    words = re.findall(<span class="string">'\w+'</span>,content)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> words</span><br></pre></td></tr></table></figure><p>Note, in the following cell, ‘words’ is converted to a python <code>set</code>. This eliminates any duplicate entries.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">word_l = process_data(<span class="string">'shakespeare.txt'</span>)</span><br><span class="line">vocab = set(word_l)  <span class="comment"># this will be your new vocabulary</span></span><br><span class="line">print(<span class="string">f"The first ten words in the text are: \n<span class="subst">&#123;word_l[<span class="number">0</span>:<span class="number">10</span>]&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;len(vocab)&#125;</span> unique words in the vocabulary."</span>)</span><br></pre></td></tr></table></figure><pre><code>The first ten words in the text are: [&#39;o&#39;, &#39;for&#39;, &#39;a&#39;, &#39;muse&#39;, &#39;of&#39;, &#39;fire&#39;, &#39;that&#39;, &#39;would&#39;, &#39;ascend&#39;, &#39;the&#39;]There are 6116 unique words in the vocabulary.</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The first ten words <span class="keyword">in</span> the text are: </span><br><span class="line">[<span class="string">'o'</span>, <span class="string">'for'</span>, <span class="string">'a'</span>, <span class="string">'muse'</span>, <span class="string">'of'</span>, <span class="string">'fire'</span>, <span class="string">'that'</span>, <span class="string">'would'</span>, <span class="string">'ascend'</span>, <span class="string">'the'</span>]</span><br><span class="line">There are <span class="number">6116</span> unique words <span class="keyword">in</span> the vocabulary.</span><br></pre></td></tr></table></figure><p><a name="ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><p>Implement a <code>get_count</code> function that returns a dictionary</p><ul><li>The dictionary’s keys are words</li><li>The value for each word is the number of times that word appears in the corpus. </li></ul><p>For example, given the following sentence: <strong>“I am happy because I am learning”</strong>, your dictionary should return the following: </p><table style="width:20%">  <tr>    <td> <b>Key </b>  </td>    <td> <b>Value </b> </td>   </tr>  <tr>    <td> I  </td>    <td> 2</td>   </tr>  <tr>    <td>am</td>    <td>2</td>   </tr>  <tr>    <td>happy</td>    <td>1</td>   </tr>   <tr>    <td>because</td>    <td>1</td>   </tr>   <tr>    <td>learning</td>    <td>1</td>   </tr></table><p><strong>Instructions</strong>:<br>Implement a <code>get_count</code> which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.  </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Try implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions</li>    <li>You can also use defaultdict instead of a regualr dictionary, along with the for loop</li>    <li>Otherwise, to skip using a for loop, you can use Python's <a href="https://docs.python.org/3.7/library/collections.html#collections.Counter" target="_blank" rel="noopener"> Counter class</a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_count</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_count</span><span class="params">(word_l)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word_l: a set of words representing the corpus. </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    word_count_dict = &#123;&#125;  <span class="comment"># fill this with word counts</span></span><br><span class="line">    <span class="comment">### START CODE HERE </span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_l:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_count_dict:</span><br><span class="line">            word_count_dict[word] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            word_count_dict[word] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> word_count_dict</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">word_count_dict = get_count(word_l)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;len(word_count_dict)&#125;</span> key values pairs"</span>)</span><br><span class="line">print(<span class="string">f"The count for the word 'thee' is <span class="subst">&#123;word_count_dict.get(<span class="string">'thee'</span>,<span class="number">0</span>)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>There are 6116 key values pairsThe count for the word &#39;thee&#39; is 240</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">There are <span class="number">6116</span> key values pairs</span><br><span class="line">The count <span class="keyword">for</span> the word <span class="string">'thee'</span> <span class="keyword">is</span> <span class="number">240</span></span><br></pre></td></tr></table></figure><p><a name="ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><p>Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.</p><script type="math/tex; mode=display">P(w_i) = \frac{C(w_i)}{M} \tag{Eqn-2}</script><p>where </p><p>$C(w_i)$ is the total number of times $w_i$ appears in the corpus.</p><p>$M$ is the total number of words in the corpus.</p><p>For example, the probability of the word ‘am’ in the sentence <strong>‘I am happy because I am learning’</strong> is:</p><script type="math/tex; mode=display">P(am) = \frac{C(w_i)}{M} = \frac {2}{7} \tag{Eqn-3}.</script><p><strong>Instructions:</strong> Implement <code>get_probs</code> function which gives you the probability<br>that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p>General advice<ul>    <li> Use dictionary.values() </li>    <li> Use sum() </li>    <li> The cardinality (number of words in the corpus should be equal to len(word_l).  You will calculate this same number, but using the word count dictionary.</li></ul>If you're using a for loop:<ul>    <li> Use dictionary.keys() </li></ul>If you're using a dictionary comprehension:<ul>    <li>Use dictionary.items() </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_probs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probs</span><span class="params">(word_count_dict)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        probs: A dictionary where keys are the words and the values are the probability that a word will occur. </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    probs = &#123;&#125;  <span class="comment"># return this variable correctly</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    M = np.sum(list(word_count_dict.values()))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word, C <span class="keyword">in</span> word_count_dict.items():</span><br><span class="line">        probs[word] =  float(C) / M</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> probs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">probs = get_probs(word_count_dict)</span><br><span class="line">print(<span class="string">f"Length of probs is <span class="subst">&#123;len(probs)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"P('thee') is <span class="subst">&#123;probs[<span class="string">'thee'</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Length of probs is 6116P(&#39;thee&#39;) is 0.0045</code></pre><h4 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Length of probs <span class="keyword">is</span> <span class="number">6116</span></span><br><span class="line">P(<span class="string">'thee'</span>) <span class="keyword">is</span> <span class="number">0.0045</span></span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h1 id="Part-2-String-Manipulations"><a href="#Part-2-String-Manipulations" class="headerlink" title="Part 2: String Manipulations"></a>Part 2: String Manipulations</h1><p>Now, that you have computed $P(w_i)$ for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, you will implement four functions: </p><ul><li><code>delete_letter</code>: given a word, it returns all the possible strings that have <strong>one character removed</strong>. </li><li><code>switch_letter</code>: given a word, it returns all the possible strings that have <strong>two adjacent letters switched</strong>.</li><li><code>replace_letter</code>: given a word, it returns all the possible strings that have <strong>one character replaced by another different letter</strong>.</li><li><code>insert_letter</code>: given a word, it returns all the possible strings that have an <strong>additional character inserted</strong>. </li></ul><h4 id="List-comprehensions"><a href="#List-comprehensions" class="headerlink" title="List comprehensions"></a>List comprehensions</h4><p>String and list manipulation in python will often make use of a python feature called  <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions" target="_blank" rel="noopener">list comprehensions</a>. The routines below will be described as using list comprehensions, but if you would rather implement them in another way, you are free to do so as long as the result is the same. Further, the following section will provide detailed instructions on how to use list comprehensions and how to implement the desired functions. If you are a python expert, feel free to skip the python hints and move to implementing the routines directly.</p><p>Python List Comprehensions embed a looping structure inside of a list declaration, collapsing many lines of code into a single line. If you are not familiar with them, they seem slightly out of order relative to for loops. </p><div style="width:image width px; font-size:100%; text-align:center;"><img src="GenericListComp3.PNG" alt="alternate text" width="width" height="height" style="width:800px;height:400px;"> Figure 2 </div><p>The diagram above shows that the components of a list comprehension are the same components you would find in a typical for loop that appends to a list, but in a different order. With that in mind, we’ll continue the specifics of this assignment. We will be very descriptive for the first function, <code>deletes()</code>, and less so in later functions as you become familiar with list comprehensions.</p><p><a name="ex-4"></a></p><h3 id="Exercise-4"><a href="#Exercise-4" class="headerlink" title="Exercise 4"></a>Exercise 4</h3><p><strong>Instructions for delete_letter():</strong> Implement a <code>delete_letter()</code> function that, given a word, returns a list of strings with one character deleted. </p><p>For example, given the word <strong>nice</strong>, it would return the set: {‘ice’, ‘nce’, ‘nic’, ‘nie’}. </p><p><strong>Step 1:</strong> Create a list of ‘splits’. This is all the ways you can split a word into Left and Right: For example,<br>‘nice is split into : <code>[(&#39;&#39;, &#39;nice&#39;), (&#39;n&#39;, &#39;ice&#39;), (&#39;ni&#39;, &#39;ce&#39;), (&#39;nic&#39;, &#39;e&#39;), (&#39;nice&#39;, &#39;&#39;)]</code><br>This is common to all four functions (delete, replace, switch, insert).</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="Splits1.PNG" alt="alternate text" width="width" height="height" style="width:650px;height:200px;"> Figure 3 </div><p><strong>Step 2:</strong> This is specific to <code>delete_letter</code>. Here, we are generating all words that result from deleting one character.<br>This can be done in a single line with a list comprehension. You can makes use of this type of syntax:<br><code>[f(a,b) for a, b in splits if condition]</code>  </p><p>For our ‘nice’ example you get:<br>[‘ice’, ‘nce’, ‘nie’, ‘nic’]</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="ListComp2.PNG" alt="alternate text" width="width" height="height" style="width:550px;height:300px;"> Figure 4 </div><h4 id="Levels-of-assistance"><a href="#Levels-of-assistance" class="headerlink" title="Levels of assistance"></a>Levels of assistance</h4><p>Try this exercise with these levels of assistance.  </p><ul><li>We hope that this will make it both a meaningful experience but also not a frustrating experience. </li><li><p>Start with level 1, then move onto level 2, and 3 as needed.</p><ul><li>Level 1. Try to think this through and implement this yourself.</li><li>Level 2. Click on the “Level 2 Hints” section for some hints to get started.</li><li>Level 3. If you would prefer more guidance, please click on the “Level 3 Hints” cell for step by step instructions.</li></ul></li><li><p>If you are still stuck, look at the images in the “list comprehensions” section above.</p></li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Level 2 Hints</b></font></summary></p><p><ul>    <li><a href> Use array slicing like my_string[0:2] </a> </li>    <li><a href> Use list comprehensions or for loops </a> </li></ul></p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Level 3 Hints</b></font></summary></p><p><ul>    <li>splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.</li>    <li>Do this in a loop or list comprehension, so that you have a list of tuples.    </li><li> For example, "cake" can get split into "ca" and "ke". They're stored in a tuple ("ca","ke"), and the tuple is appended to a list.  We'll refer to these as L and R, so the tuple is (L,R)</li>    <li>When choosing the range for your loop, if you input the word "cans" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) </li>    <li>deletes: Go through the list of tuples and combine the two strings together. You can use the + operator to combine two strings</li>    <li>When combining the tuples, make sure that you leave out a middle character.</li>    <li>Use array slicing to leave out the first character of the right substring.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: deletes</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete_letter</span><span class="params">(word, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the string/word for which you will generate all possible words </span></span><br><span class="line"><span class="string">                in the vocabulary which have 1 missing character</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        delete_l: a list of all possible strings obtained by deleting 1 character from word</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    delete_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    delete_l = [L + R[<span class="number">1</span>:] <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> R]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">f"input word <span class="subst">&#123;word&#125;</span>, \nsplit_l = <span class="subst">&#123;split_l&#125;</span>, \ndelete_l = <span class="subst">&#123;delete_l&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> delete_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">delete_word_l = delete_letter(word=<span class="string">"cans"</span>,</span><br><span class="line">                        verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>input word cans, split_l = [(&#39;&#39;, &#39;cans&#39;), (&#39;c&#39;, &#39;ans&#39;), (&#39;ca&#39;, &#39;ns&#39;), (&#39;can&#39;, &#39;s&#39;), (&#39;cans&#39;, &#39;&#39;)], delete_l = [&#39;ans&#39;, &#39;cns&#39;, &#39;cas&#39;, &#39;can&#39;]</code></pre><h4 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input word cans, </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'cans'</span>), (<span class="string">'c'</span>, <span class="string">'ans'</span>), (<span class="string">'ca'</span>, <span class="string">'ns'</span>), (<span class="string">'can'</span>, <span class="string">'s'</span>)], </span><br><span class="line">delete_l = [<span class="string">'ans'</span>, <span class="string">'cns'</span>, <span class="string">'cas'</span>, <span class="string">'can'</span>]</span><br></pre></td></tr></table></figure><h4 id="Note-1"><a href="#Note-1" class="headerlink" title="Note 1"></a>Note 1</h4><p>You might get a slightly different result with split_l.  </p><ul><li>Notice how it has the extra tuple <code>(&#39;cans&#39;, &#39;&#39;)</code>.</li><li>This will be fine as long as you have checked the size of the right-side substring in tuple (L,R).</li><li>Can you explain why this will give you the same result for the list of deletion strings (delete_l)?</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input word cans, </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'cans'</span>), (<span class="string">'c'</span>, <span class="string">'ans'</span>), (<span class="string">'ca'</span>, <span class="string">'ns'</span>), (<span class="string">'can'</span>, <span class="string">'s'</span>), (<span class="string">'cans'</span>, <span class="string">''</span>)], </span><br><span class="line">delete_l = [<span class="string">'ans'</span>, <span class="string">'cns'</span>, <span class="string">'cas'</span>, <span class="string">'can'</span>]</span><br></pre></td></tr></table></figure><h4 id="Note-2"><a href="#Note-2" class="headerlink" title="Note 2"></a>Note 2</h4><p>If you end up getting the same word as your input word, like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input word cans, </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'cans'</span>), (<span class="string">'c'</span>, <span class="string">'ans'</span>), (<span class="string">'ca'</span>, <span class="string">'ns'</span>), (<span class="string">'can'</span>, <span class="string">'s'</span>), (<span class="string">'cans'</span>, <span class="string">''</span>)], </span><br><span class="line">delete_l = [<span class="string">'ans'</span>, <span class="string">'cns'</span>, <span class="string">'cas'</span>, <span class="string">'can'</span>, <span class="string">'cans'</span>]</span><br></pre></td></tr></table></figure><ul><li>Check how you set the <code>range</code>.</li><li>See if you check the length of the string on the right-side of the split.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line">print(<span class="string">f"Number of outputs of delete_letter('at') is <span class="subst">&#123;len(delete_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of outputs of delete_letter(&#39;at&#39;) is 2</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of outputs of delete_letter('at') is 2</span><br></pre></td></tr></table></figure><p><a name="ex-5"></a></p><h3 id="Exercise-5"><a href="#Exercise-5" class="headerlink" title="Exercise 5"></a>Exercise 5</h3><p><strong>Instructions for switch_letter()</strong>: Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters <strong>that are adjacent to each other</strong>. </p><ul><li>For example, given the word ‘eta’, it returns {‘eat’, ‘tea’}, but does not return ‘ate’.</li></ul><p><strong>Step 1:</strong> is the same as in delete_letter()<br><strong>Step 2:</strong> A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:<br><code>[f(L,R) for L, R in splits if condition]</code>  where ‘condition’ will test the length of R in a given iteration. See below.</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="Switches1.PNG" alt="alternate text" width="width" height="height" style="width:600px;height:200px;"> Figure 5 </div>      <h4 id="Levels-of-difficulty"><a href="#Levels-of-difficulty" class="headerlink" title="Levels of difficulty"></a>Levels of difficulty</h4><p>Try this exercise with these levels of difficulty.  </p><ul><li>Level 1. Try to think this through and implement this yourself.</li><li>Level 2. Click on the “Level 2 Hints” section for some hints to get started.</li><li>Level 3. If you would prefer more guidance, please click on the “Level 3 Hints” cell for step by step instructions.</li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Level 2 Hints</b></font></summary></p><p><ul>    <li><a href> Use array slicing like my_string[0:2] </a> </li>    <li><a href> Use list comprehensions or for loops </a> </li>    <li>To do a switch, think of the whole word as divided into 4 distinct parts.  Write out 'cupcakes' on a piece of paper and see how you can split it into ('cupc', 'k', 'a', 'es')</li></ul></p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Level 3 Hints</b></font></summary></p><p><ul>    <li>splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.</li>    <li>Splitting is the same as for delete_letter</li>    <li>To perform the switch, go through the list of tuples and combine four strings together. You can use the + operator to combine strings</li>    <li>The four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.</li>    <li>Unlike delete_letter, you will want to check that your right substring is at least a minimum length.  To see why, review the previous hint bullet point (directly before this one).</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: switches</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">switch_letter</span><span class="params">(word, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: input string</span></span><br><span class="line"><span class="string">     Output:</span></span><br><span class="line"><span class="string">        switches: a list of all possible strings with one adjacent charater switched</span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    </span><br><span class="line">    switch_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(L,R)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> L[:<span class="number">-1</span>] + R[<span class="number">0</span>] + L[<span class="number">-1</span>] + R[<span class="number">1</span>:]</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    switch_l = [f(L,R) <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> L <span class="keyword">and</span> R]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">f"Input word = <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \nswitch_l = <span class="subst">&#123;switch_l&#125;</span>"</span>) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> switch_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">switch_word_l = switch_letter(word=<span class="string">"eta"</span>,</span><br><span class="line">                         verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Input word = eta split_l = [(&#39;&#39;, &#39;eta&#39;), (&#39;e&#39;, &#39;ta&#39;), (&#39;et&#39;, &#39;a&#39;), (&#39;eta&#39;, &#39;&#39;)] switch_l = [&#39;tea&#39;, &#39;eat&#39;]</code></pre><h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = eta </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'eta'</span>), (<span class="string">'e'</span>, <span class="string">'ta'</span>), (<span class="string">'et'</span>, <span class="string">'a'</span>)] </span><br><span class="line">switch_l = [<span class="string">'tea'</span>, <span class="string">'eat'</span>]</span><br></pre></td></tr></table></figure><h4 id="Note-1-1"><a href="#Note-1-1" class="headerlink" title="Note 1"></a>Note 1</h4><p>You may get this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = eta </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'eta'</span>), (<span class="string">'e'</span>, <span class="string">'ta'</span>), (<span class="string">'et'</span>, <span class="string">'a'</span>), (<span class="string">'eta'</span>, <span class="string">''</span>)] </span><br><span class="line">switch_l = [<span class="string">'tea'</span>, <span class="string">'eat'</span>]</span><br></pre></td></tr></table></figure></p><ul><li>Notice how it has the extra tuple <code>(&#39;eta&#39;, &#39;&#39;)</code>.</li><li>This is also correct.</li><li>Can you think of why this is the case?</li></ul><h4 id="Note-2-1"><a href="#Note-2-1" class="headerlink" title="Note 2"></a>Note 2</h4><p>If you get an error<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IndexError: string index out of range</span><br></pre></td></tr></table></figure></p><ul><li>Please see if you have checked the length of the strings when switching characters.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line">print(<span class="string">f"Number of outputs of switch_letter('at') is <span class="subst">&#123;len(switch_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of outputs of switch_letter(&#39;at&#39;) is 1</code></pre><h4 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of outputs of switch_letter('at') is 1</span><br></pre></td></tr></table></figure><p><a name="ex-6"></a></p><h3 id="Exercise-6"><a href="#Exercise-6" class="headerlink" title="Exercise 6"></a>Exercise 6</h3><p><strong>Instructions for replace_letter()</strong>: Now implement a function that takes in a word and returns a list of strings with one <strong>replaced letter</strong> from the original word. </p><p><strong>Step 1:</strong> is the same as in <code>delete_letter()</code></p><p><strong>Step 2:</strong> A list comprehension or for loop which form strings by replacing letters.  This can be of the form:<br><code>[f(a,b,c) for a, b in splits if condition for c in string]</code>   Note the use of the second for loop.<br>It is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of ‘ear’ with ‘e’ will return ‘ear’.</p><p><strong>Step 3:</strong> Remove the original input letter from the output.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>To remove a word from a list, first store its contents inside a set()</li>    <li>Use set.discard('the_word') to remove a word in a set (if the word does not exist in the set, then it will not throw a KeyError.  Using set.remove('the_word') throws a KeyError if the word does not exist in the set. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: replaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_letter</span><span class="params">(word, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        replaces: a list of all possible strings where we replaced one letter from the original word. </span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    </span><br><span class="line">    letters = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">    replace_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    replace_set = [L + C + R[<span class="number">1</span>:] <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> R <span class="keyword">for</span> C <span class="keyword">in</span> letters <span class="keyword">if</span> C <span class="keyword">is</span> <span class="keyword">not</span> R[<span class="number">0</span>]]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># turn the set back into a list and sort it, for easier viewing</span></span><br><span class="line">    replace_l = sorted(list(replace_set))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">f"Input word = <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \nreplace_l <span class="subst">&#123;replace_l&#125;</span>"</span>)   </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> replace_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">replace_l = replace_letter(word=<span class="string">'can'</span>,</span><br><span class="line">                              verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Input word = can split_l = [(&#39;&#39;, &#39;can&#39;), (&#39;c&#39;, &#39;an&#39;), (&#39;ca&#39;, &#39;n&#39;), (&#39;can&#39;, &#39;&#39;)] replace_l [&#39;aan&#39;, &#39;ban&#39;, &#39;caa&#39;, &#39;cab&#39;, &#39;cac&#39;, &#39;cad&#39;, &#39;cae&#39;, &#39;caf&#39;, &#39;cag&#39;, &#39;cah&#39;, &#39;cai&#39;, &#39;caj&#39;, &#39;cak&#39;, &#39;cal&#39;, &#39;cam&#39;, &#39;cao&#39;, &#39;cap&#39;, &#39;caq&#39;, &#39;car&#39;, &#39;cas&#39;, &#39;cat&#39;, &#39;cau&#39;, &#39;cav&#39;, &#39;caw&#39;, &#39;cax&#39;, &#39;cay&#39;, &#39;caz&#39;, &#39;cbn&#39;, &#39;ccn&#39;, &#39;cdn&#39;, &#39;cen&#39;, &#39;cfn&#39;, &#39;cgn&#39;, &#39;chn&#39;, &#39;cin&#39;, &#39;cjn&#39;, &#39;ckn&#39;, &#39;cln&#39;, &#39;cmn&#39;, &#39;cnn&#39;, &#39;con&#39;, &#39;cpn&#39;, &#39;cqn&#39;, &#39;crn&#39;, &#39;csn&#39;, &#39;ctn&#39;, &#39;cun&#39;, &#39;cvn&#39;, &#39;cwn&#39;, &#39;cxn&#39;, &#39;cyn&#39;, &#39;czn&#39;, &#39;dan&#39;, &#39;ean&#39;, &#39;fan&#39;, &#39;gan&#39;, &#39;han&#39;, &#39;ian&#39;, &#39;jan&#39;, &#39;kan&#39;, &#39;lan&#39;, &#39;man&#39;, &#39;nan&#39;, &#39;oan&#39;, &#39;pan&#39;, &#39;qan&#39;, &#39;ran&#39;, &#39;san&#39;, &#39;tan&#39;, &#39;uan&#39;, &#39;van&#39;, &#39;wan&#39;, &#39;xan&#39;, &#39;yan&#39;, &#39;zan&#39;]</code></pre><h4 id="Expected-Output-4"><a href="#Expected-Output-4" class="headerlink" title="Expected Output**:"></a>Expected Output**:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'can'</span>), (<span class="string">'c'</span>, <span class="string">'an'</span>), (<span class="string">'ca'</span>, <span class="string">'n'</span>)] </span><br><span class="line">replace_l [<span class="string">'aan'</span>, <span class="string">'ban'</span>, <span class="string">'caa'</span>, <span class="string">'cab'</span>, <span class="string">'cac'</span>, <span class="string">'cad'</span>, <span class="string">'cae'</span>, <span class="string">'caf'</span>, <span class="string">'cag'</span>, <span class="string">'cah'</span>, <span class="string">'cai'</span>, <span class="string">'caj'</span>, <span class="string">'cak'</span>, <span class="string">'cal'</span>, <span class="string">'cam'</span>, <span class="string">'cao'</span>, <span class="string">'cap'</span>, <span class="string">'caq'</span>, <span class="string">'car'</span>, <span class="string">'cas'</span>, <span class="string">'cat'</span>, <span class="string">'cau'</span>, <span class="string">'cav'</span>, <span class="string">'caw'</span>, <span class="string">'cax'</span>, <span class="string">'cay'</span>, <span class="string">'caz'</span>, <span class="string">'cbn'</span>, <span class="string">'ccn'</span>, <span class="string">'cdn'</span>, <span class="string">'cen'</span>, <span class="string">'cfn'</span>, <span class="string">'cgn'</span>, <span class="string">'chn'</span>, <span class="string">'cin'</span>, <span class="string">'cjn'</span>, <span class="string">'ckn'</span>, <span class="string">'cln'</span>, <span class="string">'cmn'</span>, <span class="string">'cnn'</span>, <span class="string">'con'</span>, <span class="string">'cpn'</span>, <span class="string">'cqn'</span>, <span class="string">'crn'</span>, <span class="string">'csn'</span>, <span class="string">'ctn'</span>, <span class="string">'cun'</span>, <span class="string">'cvn'</span>, <span class="string">'cwn'</span>, <span class="string">'cxn'</span>, <span class="string">'cyn'</span>, <span class="string">'czn'</span>, <span class="string">'dan'</span>, <span class="string">'ean'</span>, <span class="string">'fan'</span>, <span class="string">'gan'</span>, <span class="string">'han'</span>, <span class="string">'ian'</span>, <span class="string">'jan'</span>, <span class="string">'kan'</span>, <span class="string">'lan'</span>, <span class="string">'man'</span>, <span class="string">'nan'</span>, <span class="string">'oan'</span>, <span class="string">'pan'</span>, <span class="string">'qan'</span>, <span class="string">'ran'</span>, <span class="string">'san'</span>, <span class="string">'tan'</span>, <span class="string">'uan'</span>, <span class="string">'van'</span>, <span class="string">'wan'</span>, <span class="string">'xan'</span>, <span class="string">'yan'</span>, <span class="string">'zan'</span>]</span><br></pre></td></tr></table></figure><ul><li>Note how the input word ‘can’ should not be one of the output words.</li></ul><h4 id="Note-1-2"><a href="#Note-1-2" class="headerlink" title="Note 1"></a>Note 1</h4><p>If you get something like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'can'</span>), (<span class="string">'c'</span>, <span class="string">'an'</span>), (<span class="string">'ca'</span>, <span class="string">'n'</span>), (<span class="string">'can'</span>, <span class="string">''</span>)] </span><br><span class="line">replace_l [<span class="string">'aan'</span>, <span class="string">'ban'</span>, <span class="string">'caa'</span>, <span class="string">'cab'</span>, <span class="string">'cac'</span>, <span class="string">'cad'</span>, <span class="string">'cae'</span>, <span class="string">'caf'</span>, <span class="string">'cag'</span>, <span class="string">'cah'</span>, <span class="string">'cai'</span>, <span class="string">'caj'</span>, <span class="string">'cak'</span>, <span class="string">'cal'</span>, <span class="string">'cam'</span>, <span class="string">'cao'</span>, <span class="string">'cap'</span>, <span class="string">'caq'</span>, <span class="string">'car'</span>, <span class="string">'cas'</span>, <span class="string">'cat'</span>, <span class="string">'cau'</span>, <span class="string">'cav'</span>, <span class="string">'caw'</span>, <span class="string">'cax'</span>, <span class="string">'cay'</span>, <span class="string">'caz'</span>, <span class="string">'cbn'</span>, <span class="string">'ccn'</span>, <span class="string">'cdn'</span>, <span class="string">'cen'</span>, <span class="string">'cfn'</span>, <span class="string">'cgn'</span>, <span class="string">'chn'</span>, <span class="string">'cin'</span>, <span class="string">'cjn'</span>, <span class="string">'ckn'</span>, <span class="string">'cln'</span>, <span class="string">'cmn'</span>, <span class="string">'cnn'</span>, <span class="string">'con'</span>, <span class="string">'cpn'</span>, <span class="string">'cqn'</span>, <span class="string">'crn'</span>, <span class="string">'csn'</span>, <span class="string">'ctn'</span>, <span class="string">'cun'</span>, <span class="string">'cvn'</span>, <span class="string">'cwn'</span>, <span class="string">'cxn'</span>, <span class="string">'cyn'</span>, <span class="string">'czn'</span>, <span class="string">'dan'</span>, <span class="string">'ean'</span>, <span class="string">'fan'</span>, <span class="string">'gan'</span>, <span class="string">'han'</span>, <span class="string">'ian'</span>, <span class="string">'jan'</span>, <span class="string">'kan'</span>, <span class="string">'lan'</span>, <span class="string">'man'</span>, <span class="string">'nan'</span>, <span class="string">'oan'</span>, <span class="string">'pan'</span>, <span class="string">'qan'</span>, <span class="string">'ran'</span>, <span class="string">'san'</span>, <span class="string">'tan'</span>, <span class="string">'uan'</span>, <span class="string">'van'</span>, <span class="string">'wan'</span>, <span class="string">'xan'</span>, <span class="string">'yan'</span>, <span class="string">'zan'</span>]</span><br></pre></td></tr></table></figure><ul><li>Notice how split_l has an extra tuple <code>(&#39;can&#39;, &#39;&#39;)</code>, but the output is still the same, so this is okay.</li></ul><h4 id="Note-2-2"><a href="#Note-2-2" class="headerlink" title="Note 2"></a>Note 2</h4><p>If you get something like this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'can'</span>), (<span class="string">'c'</span>, <span class="string">'an'</span>), (<span class="string">'ca'</span>, <span class="string">'n'</span>), (<span class="string">'can'</span>, <span class="string">''</span>)] </span><br><span class="line">replace_l [<span class="string">'aan'</span>, <span class="string">'ban'</span>, <span class="string">'caa'</span>, <span class="string">'cab'</span>, <span class="string">'cac'</span>, <span class="string">'cad'</span>, <span class="string">'cae'</span>, <span class="string">'caf'</span>, <span class="string">'cag'</span>, <span class="string">'cah'</span>, <span class="string">'cai'</span>, <span class="string">'caj'</span>, <span class="string">'cak'</span>, <span class="string">'cal'</span>, <span class="string">'cam'</span>, <span class="string">'cana'</span>, <span class="string">'canb'</span>, <span class="string">'canc'</span>, <span class="string">'cand'</span>, <span class="string">'cane'</span>, <span class="string">'canf'</span>, <span class="string">'cang'</span>, <span class="string">'canh'</span>, <span class="string">'cani'</span>, <span class="string">'canj'</span>, <span class="string">'cank'</span>, <span class="string">'canl'</span>, <span class="string">'canm'</span>, <span class="string">'cann'</span>, <span class="string">'cano'</span>, <span class="string">'canp'</span>, <span class="string">'canq'</span>, <span class="string">'canr'</span>, <span class="string">'cans'</span>, <span class="string">'cant'</span>, <span class="string">'canu'</span>, <span class="string">'canv'</span>, <span class="string">'canw'</span>, <span class="string">'canx'</span>, <span class="string">'cany'</span>, <span class="string">'canz'</span>, <span class="string">'cao'</span>, <span class="string">'cap'</span>, <span class="string">'caq'</span>, <span class="string">'car'</span>, <span class="string">'cas'</span>, <span class="string">'cat'</span>, <span class="string">'cau'</span>, <span class="string">'cav'</span>, <span class="string">'caw'</span>, <span class="string">'cax'</span>, <span class="string">'cay'</span>, <span class="string">'caz'</span>, <span class="string">'cbn'</span>, <span class="string">'ccn'</span>, <span class="string">'cdn'</span>, <span class="string">'cen'</span>, <span class="string">'cfn'</span>, <span class="string">'cgn'</span>, <span class="string">'chn'</span>, <span class="string">'cin'</span>, <span class="string">'cjn'</span>, <span class="string">'ckn'</span>, <span class="string">'cln'</span>, <span class="string">'cmn'</span>, <span class="string">'cnn'</span>, <span class="string">'con'</span>, <span class="string">'cpn'</span>, <span class="string">'cqn'</span>, <span class="string">'crn'</span>, <span class="string">'csn'</span>, <span class="string">'ctn'</span>, <span class="string">'cun'</span>, <span class="string">'cvn'</span>, <span class="string">'cwn'</span>, <span class="string">'cxn'</span>, <span class="string">'cyn'</span>, <span class="string">'czn'</span>, <span class="string">'dan'</span>, <span class="string">'ean'</span>, <span class="string">'fan'</span>, <span class="string">'gan'</span>, <span class="string">'han'</span>, <span class="string">'ian'</span>, <span class="string">'jan'</span>, <span class="string">'kan'</span>, <span class="string">'lan'</span>, <span class="string">'man'</span>, <span class="string">'nan'</span>, <span class="string">'oan'</span>, <span class="string">'pan'</span>, <span class="string">'qan'</span>, <span class="string">'ran'</span>, <span class="string">'san'</span>, <span class="string">'tan'</span>, <span class="string">'uan'</span>, <span class="string">'van'</span>, <span class="string">'wan'</span>, <span class="string">'xan'</span>, <span class="string">'yan'</span>, <span class="string">'zan'</span>]</span><br></pre></td></tr></table></figure></p><ul><li>Notice how there are strings that are 1 letter longer than the original word, such as <code>cana</code>.</li><li>Please check for the case when there is an empty string <code>&#39;&#39;</code>, and if so, do not use that empty string when setting replace_l.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line">print(<span class="string">f"Number of outputs of switch_letter('at') is <span class="subst">&#123;len(switch_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of outputs of switch_letter(&#39;at&#39;) is 1</code></pre><h4 id="Expected-output-3"><a href="#Expected-output-3" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of outputs of switch_letter('at') is 1</span><br></pre></td></tr></table></figure><p><a name="ex-7"></a></p><h3 id="Exercise-7"><a href="#Exercise-7" class="headerlink" title="Exercise 7"></a>Exercise 7</h3><p><strong>Instructions for insert_letter()</strong>: Now implement a function that takes in a word and returns a list with a letter inserted at every offset.</p><p><strong>Step 1:</strong> is the same as in <code>delete_letter()</code></p><p><strong>Step 2:</strong> This can be a list comprehension of the form:<br><code>[f(a,b,c) for a, b in splits if condition for c in string]</code>   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: inserts</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_letter</span><span class="params">(word, verbose=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        inserts: a set of all possible strings with one new letter inserted at every offset</span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    letters = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">    insert_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    insert_l = [L + C + R <span class="keyword">for</span> L, R <span class="keyword">in</span> split_l <span class="keyword">for</span> C <span class="keyword">in</span> letters]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">f"Input word <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \ninsert_l = <span class="subst">&#123;insert_l&#125;</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> insert_l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert_l = insert_letter(<span class="string">'at'</span>, <span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">f"Number of strings output by insert_letter('at') is <span class="subst">&#123;len(insert_l)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Input word at split_l = [(&#39;&#39;, &#39;at&#39;), (&#39;a&#39;, &#39;t&#39;), (&#39;at&#39;, &#39;&#39;)] insert_l = [&#39;aat&#39;, &#39;bat&#39;, &#39;cat&#39;, &#39;dat&#39;, &#39;eat&#39;, &#39;fat&#39;, &#39;gat&#39;, &#39;hat&#39;, &#39;iat&#39;, &#39;jat&#39;, &#39;kat&#39;, &#39;lat&#39;, &#39;mat&#39;, &#39;nat&#39;, &#39;oat&#39;, &#39;pat&#39;, &#39;qat&#39;, &#39;rat&#39;, &#39;sat&#39;, &#39;tat&#39;, &#39;uat&#39;, &#39;vat&#39;, &#39;wat&#39;, &#39;xat&#39;, &#39;yat&#39;, &#39;zat&#39;, &#39;aat&#39;, &#39;abt&#39;, &#39;act&#39;, &#39;adt&#39;, &#39;aet&#39;, &#39;aft&#39;, &#39;agt&#39;, &#39;aht&#39;, &#39;ait&#39;, &#39;ajt&#39;, &#39;akt&#39;, &#39;alt&#39;, &#39;amt&#39;, &#39;ant&#39;, &#39;aot&#39;, &#39;apt&#39;, &#39;aqt&#39;, &#39;art&#39;, &#39;ast&#39;, &#39;att&#39;, &#39;aut&#39;, &#39;avt&#39;, &#39;awt&#39;, &#39;axt&#39;, &#39;ayt&#39;, &#39;azt&#39;, &#39;ata&#39;, &#39;atb&#39;, &#39;atc&#39;, &#39;atd&#39;, &#39;ate&#39;, &#39;atf&#39;, &#39;atg&#39;, &#39;ath&#39;, &#39;ati&#39;, &#39;atj&#39;, &#39;atk&#39;, &#39;atl&#39;, &#39;atm&#39;, &#39;atn&#39;, &#39;ato&#39;, &#39;atp&#39;, &#39;atq&#39;, &#39;atr&#39;, &#39;ats&#39;, &#39;att&#39;, &#39;atu&#39;, &#39;atv&#39;, &#39;atw&#39;, &#39;atx&#39;, &#39;aty&#39;, &#39;atz&#39;]Number of strings output by insert_letter(&#39;at&#39;) is 78</code></pre><h4 id="Expected-output-4"><a href="#Expected-output-4" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'at'</span>), (<span class="string">'a'</span>, <span class="string">'t'</span>), (<span class="string">'at'</span>, <span class="string">''</span>)] </span><br><span class="line">insert_l = [<span class="string">'aat'</span>, <span class="string">'bat'</span>, <span class="string">'cat'</span>, <span class="string">'dat'</span>, <span class="string">'eat'</span>, <span class="string">'fat'</span>, <span class="string">'gat'</span>, <span class="string">'hat'</span>, <span class="string">'iat'</span>, <span class="string">'jat'</span>, <span class="string">'kat'</span>, <span class="string">'lat'</span>, <span class="string">'mat'</span>, <span class="string">'nat'</span>, <span class="string">'oat'</span>, <span class="string">'pat'</span>, <span class="string">'qat'</span>, <span class="string">'rat'</span>, <span class="string">'sat'</span>, <span class="string">'tat'</span>, <span class="string">'uat'</span>, <span class="string">'vat'</span>, <span class="string">'wat'</span>, <span class="string">'xat'</span>, <span class="string">'yat'</span>, <span class="string">'zat'</span>, <span class="string">'aat'</span>, <span class="string">'abt'</span>, <span class="string">'act'</span>, <span class="string">'adt'</span>, <span class="string">'aet'</span>, <span class="string">'aft'</span>, <span class="string">'agt'</span>, <span class="string">'aht'</span>, <span class="string">'ait'</span>, <span class="string">'ajt'</span>, <span class="string">'akt'</span>, <span class="string">'alt'</span>, <span class="string">'amt'</span>, <span class="string">'ant'</span>, <span class="string">'aot'</span>, <span class="string">'apt'</span>, <span class="string">'aqt'</span>, <span class="string">'art'</span>, <span class="string">'ast'</span>, <span class="string">'att'</span>, <span class="string">'aut'</span>, <span class="string">'avt'</span>, <span class="string">'awt'</span>, <span class="string">'axt'</span>, <span class="string">'ayt'</span>, <span class="string">'azt'</span>, <span class="string">'ata'</span>, <span class="string">'atb'</span>, <span class="string">'atc'</span>, <span class="string">'atd'</span>, <span class="string">'ate'</span>, <span class="string">'atf'</span>, <span class="string">'atg'</span>, <span class="string">'ath'</span>, <span class="string">'ati'</span>, <span class="string">'atj'</span>, <span class="string">'atk'</span>, <span class="string">'atl'</span>, <span class="string">'atm'</span>, <span class="string">'atn'</span>, <span class="string">'ato'</span>, <span class="string">'atp'</span>, <span class="string">'atq'</span>, <span class="string">'atr'</span>, <span class="string">'ats'</span>, <span class="string">'att'</span>, <span class="string">'atu'</span>, <span class="string">'atv'</span>, <span class="string">'atw'</span>, <span class="string">'atx'</span>, <span class="string">'aty'</span>, <span class="string">'atz'</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">'at'</span>) <span class="keyword">is</span> <span class="number">78</span></span><br></pre></td></tr></table></figure><h4 id="Note-1-3"><a href="#Note-1-3" class="headerlink" title="Note 1"></a>Note 1</h4><p>If you get a split_l like this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'at'</span>), (<span class="string">'a'</span>, <span class="string">'t'</span>)] </span><br><span class="line">insert_l = [<span class="string">'aat'</span>, <span class="string">'bat'</span>, <span class="string">'cat'</span>, <span class="string">'dat'</span>, <span class="string">'eat'</span>, <span class="string">'fat'</span>, <span class="string">'gat'</span>, <span class="string">'hat'</span>, <span class="string">'iat'</span>, <span class="string">'jat'</span>, <span class="string">'kat'</span>, <span class="string">'lat'</span>, <span class="string">'mat'</span>, <span class="string">'nat'</span>, <span class="string">'oat'</span>, <span class="string">'pat'</span>, <span class="string">'qat'</span>, <span class="string">'rat'</span>, <span class="string">'sat'</span>, <span class="string">'tat'</span>, <span class="string">'uat'</span>, <span class="string">'vat'</span>, <span class="string">'wat'</span>, <span class="string">'xat'</span>, <span class="string">'yat'</span>, <span class="string">'zat'</span>, <span class="string">'aat'</span>, <span class="string">'abt'</span>, <span class="string">'act'</span>, <span class="string">'adt'</span>, <span class="string">'aet'</span>, <span class="string">'aft'</span>, <span class="string">'agt'</span>, <span class="string">'aht'</span>, <span class="string">'ait'</span>, <span class="string">'ajt'</span>, <span class="string">'akt'</span>, <span class="string">'alt'</span>, <span class="string">'amt'</span>, <span class="string">'ant'</span>, <span class="string">'aot'</span>, <span class="string">'apt'</span>, <span class="string">'aqt'</span>, <span class="string">'art'</span>, <span class="string">'ast'</span>, <span class="string">'att'</span>, <span class="string">'aut'</span>, <span class="string">'avt'</span>, <span class="string">'awt'</span>, <span class="string">'axt'</span>, <span class="string">'ayt'</span>, <span class="string">'azt'</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">'at'</span>) <span class="keyword">is</span> <span class="number">52</span></span><br></pre></td></tr></table></figure></p><ul><li>Notice that split_l is missing the extra tuple (‘at’, ‘’).  For insertion, we actually <strong>WANT</strong> this tuple.</li><li>The function is not creating all the desired output strings.</li><li>Check the range that you use for the for loop.</li></ul><h4 id="Note-2-3"><a href="#Note-2-3" class="headerlink" title="Note 2"></a>Note 2</h4><p>If you see this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">''</span>, <span class="string">'at'</span>), (<span class="string">'a'</span>, <span class="string">'t'</span>), (<span class="string">'at'</span>, <span class="string">''</span>)] </span><br><span class="line">insert_l = [<span class="string">'aat'</span>, <span class="string">'bat'</span>, <span class="string">'cat'</span>, <span class="string">'dat'</span>, <span class="string">'eat'</span>, <span class="string">'fat'</span>, <span class="string">'gat'</span>, <span class="string">'hat'</span>, <span class="string">'iat'</span>, <span class="string">'jat'</span>, <span class="string">'kat'</span>, <span class="string">'lat'</span>, <span class="string">'mat'</span>, <span class="string">'nat'</span>, <span class="string">'oat'</span>, <span class="string">'pat'</span>, <span class="string">'qat'</span>, <span class="string">'rat'</span>, <span class="string">'sat'</span>, <span class="string">'tat'</span>, <span class="string">'uat'</span>, <span class="string">'vat'</span>, <span class="string">'wat'</span>, <span class="string">'xat'</span>, <span class="string">'yat'</span>, <span class="string">'zat'</span>, <span class="string">'aat'</span>, <span class="string">'abt'</span>, <span class="string">'act'</span>, <span class="string">'adt'</span>, <span class="string">'aet'</span>, <span class="string">'aft'</span>, <span class="string">'agt'</span>, <span class="string">'aht'</span>, <span class="string">'ait'</span>, <span class="string">'ajt'</span>, <span class="string">'akt'</span>, <span class="string">'alt'</span>, <span class="string">'amt'</span>, <span class="string">'ant'</span>, <span class="string">'aot'</span>, <span class="string">'apt'</span>, <span class="string">'aqt'</span>, <span class="string">'art'</span>, <span class="string">'ast'</span>, <span class="string">'att'</span>, <span class="string">'aut'</span>, <span class="string">'avt'</span>, <span class="string">'awt'</span>, <span class="string">'axt'</span>, <span class="string">'ayt'</span>, <span class="string">'azt'</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">'at'</span>) <span class="keyword">is</span> <span class="number">52</span></span><br></pre></td></tr></table></figure></p><ul><li>Even though you may have fixed the split_l so that it contains the tuple <code>(&#39;at&#39;, &#39;&#39;)</code>, notice that you’re still missing some output strings.<ul><li>Notice that it’s missing strings such as ‘ata’, ‘atb’, ‘atc’ all the way to ‘atz’.</li></ul></li><li>To fix this, make sure that when you set insert_l, you allow the use of the empty string <code>&#39;&#39;</code>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line">print(<span class="string">f"Number of outputs of insert_letter('at') is <span class="subst">&#123;len(insert_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of outputs of insert_letter(&#39;at&#39;) is 78</code></pre><h4 id="Expected-output-5"><a href="#Expected-output-5" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of outputs of insert_letter('at') is 78</span><br></pre></td></tr></table></figure><p><a name="3"></a></p><h1 id="Part-3-Combining-the-edits"><a href="#Part-3-Combining-the-edits" class="headerlink" title="Part 3: Combining the edits"></a>Part 3: Combining the edits</h1><p>Now that you have implemented the string manipulations, you will create two functions that, given a string, will return all the possible single and double edits on that string. These will be <code>edit_one_letter()</code> and <code>edit_two_letters()</code>.</p><p><a name="3-1"></a></p><h2 id="3-1-Edit-one-letter"><a href="#3-1-Edit-one-letter" class="headerlink" title="3.1 Edit one letter"></a>3.1 Edit one letter</h2><p><a name="ex-8"></a></p><h3 id="Exercise-8"><a href="#Exercise-8" class="headerlink" title="Exercise 8"></a>Exercise 8</h3><p><strong>Instructions</strong>: Implement the <code>edit_one_letter</code> function to get all the possible edits that are one edit away from a word. The edits  consist of the replace, insert, delete, and optionally the switch operation. You should use the previous functions you have already implemented to complete this function. The ‘switch’ function  is a less common edit function, so its use will be selected by an “allow_switches” input argument.</p><p>Note that those functions return <em>lists</em> while this function should return a <em>python set</em>. Utilizing a set eliminates any duplicate entries.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Each of the functions returns a list.  You can combine lists using the `+` operator. </li>    <li> To get unique strings (avoid duplicates), you can use the set() function. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: edit_one_letter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edit_one_letter</span><span class="params">(word, allow_switches = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the string/word for which we will generate all possible wordsthat are one edit away.</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    edit_one_set = set()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    edit_one_set = edit_one_set | set(delete_letter(word)) | set(insert_letter(word)) | set(replace_letter(word))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> allow_switches:</span><br><span class="line">        edit_one_set |= set(switch_letter(word))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> edit_one_set</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_word = <span class="string">"at"</span></span><br><span class="line">tmp_edit_one_set = edit_one_letter(tmp_word)</span><br><span class="line"><span class="comment"># turn this into a list to sort it, in order to view it</span></span><br><span class="line">tmp_edit_one_l = sorted(list(tmp_edit_one_set))</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"input word <span class="subst">&#123;tmp_word&#125;</span> \nedit_one_l \n<span class="subst">&#123;tmp_edit_one_l&#125;</span>\n"</span>)</span><br><span class="line">print(<span class="string">f"The type of the returned object should be a set <span class="subst">&#123;type(tmp_edit_one_set)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Number of outputs from edit_one_letter('at') is <span class="subst">&#123;len(edit_one_letter(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>input word at edit_one_l [&#39;a&#39;, &#39;aa&#39;, &#39;aat&#39;, &#39;ab&#39;, &#39;abt&#39;, &#39;ac&#39;, &#39;act&#39;, &#39;ad&#39;, &#39;adt&#39;, &#39;ae&#39;, &#39;aet&#39;, &#39;af&#39;, &#39;aft&#39;, &#39;ag&#39;, &#39;agt&#39;, &#39;ah&#39;, &#39;aht&#39;, &#39;ai&#39;, &#39;ait&#39;, &#39;aj&#39;, &#39;ajt&#39;, &#39;ak&#39;, &#39;akt&#39;, &#39;al&#39;, &#39;alt&#39;, &#39;am&#39;, &#39;amt&#39;, &#39;an&#39;, &#39;ant&#39;, &#39;ao&#39;, &#39;aot&#39;, &#39;ap&#39;, &#39;apt&#39;, &#39;aq&#39;, &#39;aqt&#39;, &#39;ar&#39;, &#39;art&#39;, &#39;as&#39;, &#39;ast&#39;, &#39;ata&#39;, &#39;atb&#39;, &#39;atc&#39;, &#39;atd&#39;, &#39;ate&#39;, &#39;atf&#39;, &#39;atg&#39;, &#39;ath&#39;, &#39;ati&#39;, &#39;atj&#39;, &#39;atk&#39;, &#39;atl&#39;, &#39;atm&#39;, &#39;atn&#39;, &#39;ato&#39;, &#39;atp&#39;, &#39;atq&#39;, &#39;atr&#39;, &#39;ats&#39;, &#39;att&#39;, &#39;atu&#39;, &#39;atv&#39;, &#39;atw&#39;, &#39;atx&#39;, &#39;aty&#39;, &#39;atz&#39;, &#39;au&#39;, &#39;aut&#39;, &#39;av&#39;, &#39;avt&#39;, &#39;aw&#39;, &#39;awt&#39;, &#39;ax&#39;, &#39;axt&#39;, &#39;ay&#39;, &#39;ayt&#39;, &#39;az&#39;, &#39;azt&#39;, &#39;bat&#39;, &#39;bt&#39;, &#39;cat&#39;, &#39;ct&#39;, &#39;dat&#39;, &#39;dt&#39;, &#39;eat&#39;, &#39;et&#39;, &#39;fat&#39;, &#39;ft&#39;, &#39;gat&#39;, &#39;gt&#39;, &#39;hat&#39;, &#39;ht&#39;, &#39;iat&#39;, &#39;it&#39;, &#39;jat&#39;, &#39;jt&#39;, &#39;kat&#39;, &#39;kt&#39;, &#39;lat&#39;, &#39;lt&#39;, &#39;mat&#39;, &#39;mt&#39;, &#39;nat&#39;, &#39;nt&#39;, &#39;oat&#39;, &#39;ot&#39;, &#39;pat&#39;, &#39;pt&#39;, &#39;qat&#39;, &#39;qt&#39;, &#39;rat&#39;, &#39;rt&#39;, &#39;sat&#39;, &#39;st&#39;, &#39;t&#39;, &#39;ta&#39;, &#39;tat&#39;, &#39;tt&#39;, &#39;uat&#39;, &#39;ut&#39;, &#39;vat&#39;, &#39;vt&#39;, &#39;wat&#39;, &#39;wt&#39;, &#39;xat&#39;, &#39;xt&#39;, &#39;yat&#39;, &#39;yt&#39;, &#39;zat&#39;, &#39;zt&#39;]The type of the returned object should be a set &lt;class &#39;set&#39;&gt;Number of outputs from edit_one_letter(&#39;at&#39;) is 129</code></pre><h4 id="Expected-Output-5"><a href="#Expected-Output-5" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input word at </span><br><span class="line">edit_one_l </span><br><span class="line">['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']</span><br><span class="line"></span><br><span class="line">The type of the returned object should be a <span class="built_in">set</span> &lt;class '<span class="built_in">set</span>'&gt;</span><br><span class="line">Number of outputs from edit_one_letter('at') is 129</span><br></pre></td></tr></table></figure><p><a name="3-2"></a></p><h2 id="Part-3-2-Edit-two-letters"><a href="#Part-3-2-Edit-two-letters" class="headerlink" title="Part 3.2 Edit two letters"></a>Part 3.2 Edit two letters</h2><p><a name="ex-9"></a></p><h3 id="Exercise-9"><a href="#Exercise-9" class="headerlink" title="Exercise 9"></a>Exercise 9</h3><p>Now you can generalize this to implement to get two edits on a word. To do so, you would have to get all the possible edits on a single word and then for each modified word, you would have to modify it again. </p><p><strong>Instructions</strong>: Implement the <code>edit_two_letters</code> function that returns a set of words that are two edits away. Note that creating additional edits based on the <code>edit_one_letter</code> function may ‘restore’ some one_edits to zero or one edits. That is allowed here. This accounted for in get_corrections.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>You will likely want to take the union of two sets.</li>    <li>You can either use set.union() or use the '|' (or operator) to union two sets</li>    <li>See the documentation <a href="https://docs.python.org/2/library/sets.html" target="_blank" rel="noopener"> Python sets </a> for examples of using operators or functions of the Python set.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: edit_two_letters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edit_two_letters</span><span class="params">(word, allow_switches = True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        edit_two_set: a set of strings with all possible two edits</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    edit_two_set = set()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    one_letter =  edit_one_letter(word,allow_switches)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> one_letter:</span><br><span class="line">        edit_two_set |= edit_one_letter(word,allow_switches)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> edit_two_set</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tmp_edit_two_set = edit_two_letters(<span class="string">"a"</span>)</span><br><span class="line">tmp_edit_two_l = sorted(list(tmp_edit_two_set))</span><br><span class="line">print(<span class="string">f"Number of strings with edit distance of two: <span class="subst">&#123;len(tmp_edit_two_l)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"First 10 strings <span class="subst">&#123;tmp_edit_two_l[:<span class="number">10</span>]&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Last 10 strings <span class="subst">&#123;tmp_edit_two_l[<span class="number">-10</span>:]&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"The data type of the returned object should be a set <span class="subst">&#123;type(tmp_edit_two_set)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Number of strings that are 2 edit distances from 'at' is <span class="subst">&#123;len(edit_two_letters(<span class="string">'at'</span>))&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Number of strings with edit distance of two: 2654First 10 strings [&#39;&#39;, &#39;a&#39;, &#39;aa&#39;, &#39;aaa&#39;, &#39;aab&#39;, &#39;aac&#39;, &#39;aad&#39;, &#39;aae&#39;, &#39;aaf&#39;, &#39;aag&#39;]Last 10 strings [&#39;zv&#39;, &#39;zva&#39;, &#39;zw&#39;, &#39;zwa&#39;, &#39;zx&#39;, &#39;zxa&#39;, &#39;zy&#39;, &#39;zya&#39;, &#39;zz&#39;, &#39;zza&#39;]The data type of the returned object should be a set &lt;class &#39;set&#39;&gt;Number of strings that are 2 edit distances from &#39;at&#39; is 7154</code></pre><h4 id="Expected-Output-6"><a href="#Expected-Output-6" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Number of strings with edit distance of two: <span class="number">2654</span></span><br><span class="line">First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']</span><br><span class="line">Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']</span><br><span class="line">The data type of the returned object should be a <span class="built_in">set</span> &lt;class '<span class="built_in">set</span>'&gt;</span><br><span class="line">Number of strings that are 2 edit distances from 'at' is 7154</span><br></pre></td></tr></table></figure><p><a name="3-3"></a></p><h2 id="Part-3-3-suggest-spelling-suggestions"><a href="#Part-3-3-suggest-spelling-suggestions" class="headerlink" title="Part 3-3: suggest spelling suggestions"></a>Part 3-3: suggest spelling suggestions</h2><p>Now you will use your <code>edit_two_letters</code> function to get a set of all the possible 2 edits on your word. You will then use those strings to get the most probable word you meant to type aka your typing suggestion.</p><p><a name="ex-10"></a></p><h3 id="Exercise-10"><a href="#Exercise-10" class="headerlink" title="Exercise 10"></a>Exercise 10</h3><p><strong>Instructions</strong>: Implement <code>get_corrections</code>, which returns a list of zero to n possible suggestion tuples of the form (word, probability_of_word). </p><p><strong>Step 1:</strong> Generate suggestions for a supplied word: You’ll use the edit functions you have developed. The ‘suggestion algorithm’ should follow this logic: </p><ul><li>If the word is in the vocabulary, suggest the word. </li><li>Otherwise, if there are suggestions from <code>edit_one_letter</code> that are in the vocabulary, use those. </li><li>Otherwise, if there are suggestions from <code>edit_two_letters</code> that are in the vocabulary, use those. </li><li>Otherwise, suggest the input word.*  </li><li>The idea is that words generated from fewer edits are more likely than words with more edits.</li></ul><p>Note: </p><ul><li>Edits of one or two letters may ‘restore’ strings to either zero or one edit. This algorithm accounts for this by preferentially selecting lower distance edits first.</li></ul><h4 id="Short-circuit"><a href="#Short-circuit" class="headerlink" title="Short circuit"></a>Short circuit</h4><p>In Python, logical operations such as <code>and</code> and <code>or</code> have two useful properties. They can operate on lists and they have <a href="https://docs.python.org/3/library/stdtypes.html" target="_blank" rel="noopener">‘short-circuit’ behavior</a>. Try these:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example of logical operation on lists or sets</span></span><br><span class="line">print( [] <span class="keyword">and</span> [<span class="string">"a"</span>,<span class="string">"b"</span>] )</span><br><span class="line">print( [] <span class="keyword">or</span> [<span class="string">"a"</span>,<span class="string">"b"</span>] )</span><br><span class="line"><span class="comment">#example of Short circuit behavior</span></span><br><span class="line">val1 =  [<span class="string">"Most"</span>,<span class="string">"Likely"</span>] <span class="keyword">or</span> [<span class="string">"Less"</span>,<span class="string">"so"</span>] <span class="keyword">or</span> [<span class="string">"least"</span>,<span class="string">"of"</span>,<span class="string">"all"</span>]  <span class="comment"># selects first, does not evalute remainder</span></span><br><span class="line">print(val1)</span><br><span class="line">val2 =  [] <span class="keyword">or</span> [] <span class="keyword">or</span> [<span class="string">"least"</span>,<span class="string">"of"</span>,<span class="string">"all"</span>] <span class="comment"># continues evaluation until there is a non-empty list</span></span><br><span class="line">print(val2)</span><br></pre></td></tr></table></figure><pre><code>[][&#39;a&#39;, &#39;b&#39;][&#39;Most&#39;, &#39;Likely&#39;][&#39;least&#39;, &#39;of&#39;, &#39;all&#39;]</code></pre><p>The logical <code>or</code> could be used to implement the suggestion algorithm very compactly. Alternately, if/then constructs could be used.</p><p><strong>Step 2</strong>: Create a ‘best_words’ dictionary where the ‘key’ is a suggestion and the ‘value’ is the probability of that word in your vocabulary. If the word is not in the vocabulary, assign it a probability of 0.</p><p><strong>Step 3</strong>: Select the n best suggestions. There may be fewer than n.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>edit_one_letter and edit_two_letters return *python sets*. </li>    <li> Sets have a handy <a href="https://docs.python.org/2/library/sets.html" target="_blank" rel="noopener"> set.intersection </a> feature</li>    <li>To find the keys that have the highest values in a dictionary, you can use the Counter dictionary to create a Counter object from a regular dictionary.  Then you can use Counter.most_common(n) to get the n most common keys.    </li>    <li>To find the intersection of two sets, you can use set.intersection or the & operator.</li>    <li>If you are not as familiar with short circuit syntax (as shown above), feel free to use if else statements instead.</li>    <li>To use an if statement to check of a set is empty, use 'if not x:' syntax </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_corrections</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_corrections</span><span class="params">(word, probs, vocab, n=<span class="number">2</span>, verbose = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        word: a user entered string to check for suggestions</span></span><br><span class="line"><span class="string">        probs: a dictionary that maps each word to its probability in the corpus</span></span><br><span class="line"><span class="string">        vocab: a set containing all the vocabulary</span></span><br><span class="line"><span class="string">        n: number of possible word corrections you want returned in the dictionary</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        n_best: a list of tuples with the most probable n corrected words and their probabilities.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    suggestions = []</span><br><span class="line">    n_best = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">        suggestions = [(word, probs[word])]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        suggestions = [(_, probs[_]) <span class="keyword">for</span> _ <span class="keyword">in</span> edit_one_letter(word) <span class="keyword">if</span> _ <span class="keyword">in</span> vocab] <span class="keyword">or</span>  \</span><br><span class="line">                    [(_, probs[_]) <span class="keyword">for</span> _ <span class="keyword">in</span> edit_two_letter(word) <span class="keyword">if</span> _ <span class="keyword">in</span> vocab] <span class="keyword">or</span> \</span><br><span class="line">                    [(word,<span class="number">0</span>)]</span><br><span class="line">    </span><br><span class="line">    n_best = sorted(suggestions, key = <span class="keyword">lambda</span> x : x[<span class="number">-1</span>], reverse = <span class="keyword">True</span>)[:n]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: print(<span class="string">"suggestions = "</span>, suggestions)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> n_best</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test your implementation - feel free to try other words in my word</span></span><br><span class="line">my_word = <span class="string">'dys'</span> </span><br><span class="line">tmp_corrections = get_corrections(my_word, probs, vocab, <span class="number">2</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, word_prob <span class="keyword">in</span> enumerate(tmp_corrections):</span><br><span class="line">    print(<span class="string">f"word <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;word_prob[<span class="number">0</span>]&#125;</span>, probability <span class="subst">&#123;word_prob[<span class="number">1</span>]:<span class="number">.6</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CODE REVIEW COMMENT: using "tmp_corrections" insteads of "cors". "cors" is not defined</span></span><br><span class="line">print(<span class="string">f"data type of corrections <span class="subst">&#123;type(tmp_corrections)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>suggestions =  [(&#39;dye&#39;, 1.865184466743761e-05), (&#39;days&#39;, 0.0004103405826836274)]word 0: days, probability 0.000410word 1: dye, probability 0.000019data type of corrections &lt;class &#39;list&#39;&gt;</code></pre><h4 id="Expected-Output-7"><a href="#Expected-Output-7" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word <span class="number">0</span>: days, probability <span class="number">0.000410</span></span><br><span class="line">word <span class="number">1</span>: dye, probability <span class="number">0.000019</span></span><br><span class="line">data type of corrections &lt;<span class="class"><span class="keyword">class</span> '<span class="title">list</span>'&gt;</span></span><br></pre></td></tr></table></figure><p><a name="4"></a></p><h1 id="Part-4-Minimum-Edit-distance"><a href="#Part-4-Minimum-Edit-distance" class="headerlink" title="Part 4: Minimum Edit distance"></a>Part 4: Minimum Edit distance</h1><p>Now that you have implemented your auto-correct, how do you evaluate the similarity between two strings? For example: ‘waht’ and ‘what’</p><p>Also how do you efficiently find the shortest path to go from the word, ‘waht’ to the word ‘what’?</p><p>You will implement a dynamic programming system that will tell you the minimum number of edits required to convert a string into another string.</p><p><a name="4-1"></a></p><h3 id="Part-4-1-Dynamic-Programming"><a href="#Part-4-1-Dynamic-Programming" class="headerlink" title="Part 4.1 Dynamic Programming"></a>Part 4.1 Dynamic Programming</h3><p>Dynamic Programming breaks a problem down into subproblems which can be combined to form the final solution. Here, given a string source[0..i] and a string target[0..j], we will compute all the combinations of substrings[i, j] and calculate their edit distance. To do this efficiently, we will use a table to maintain the previously computed substrings and use those to calculate larger substrings.</p><p>You have to create a matrix and update each element in the matrix as follows:  </p><script type="math/tex; mode=display">\text{Initialization}</script><p>\begin{align}<br>D[0,0] &amp;= 0 \\<br>D[i,0] &amp;= D[i-1,0] + del_cost(source[i]) \tag{4}\\<br>D[0,j] &amp;= D[0,j-1] + ins_cost(target[j]) \\<br>\end{align}</p><script type="math/tex; mode=display">\text{Per Cell Operations}</script><p>\begin{align}<br> \\<br>D[i,j] =min<br>\begin{cases}<br>D[i-1,j] + del_cost\\<br>D[i,j-1] + ins_cost\\<br>D[i-1,j-1] + \left\{\begin{matrix}<br>rep_cost; &amp; if src[i]\neq tar[j]\\<br>0 ; &amp; if src[i]=tar[j]<br>\end{matrix}\right.<br>\end{cases}<br>\tag{5}<br>\end{align}</p><p>So converting the source word <strong>play</strong> to the target word <strong>stay</strong>, using an input cost of one, a delete cost of 1, and replace cost of 2 would give you the following table:</p><table style="width:20%">  <tr>    <td> <b> </b>  </td>    <td> <b># </b>  </td>    <td> <b>s </b>  </td>    <td> <b>t </b> </td>     <td> <b>a </b> </td>     <td> <b>y </b> </td>   </tr>   <tr>    <td> <b>  #  </b></td>    <td> 0</td>     <td> 1</td>     <td> 2</td>     <td> 3</td>     <td> 4</td>   </tr>  <tr>    <td> <b>  p  </b></td>    <td> 1</td>  <td> 2</td>     <td> 3</td>     <td> 4</td>    <td> 5</td>  </tr>  <tr>    <td> <b> l </b></td>    <td>2</td>     <td>3</td>     <td>4</td>     <td>5</td>     <td>6</td>  </tr>  <tr>    <td> <b> a </b></td>    <td>3</td>      <td>4</td>      <td>5</td>      <td>4</td>     <td>5</td>   </tr>   <tr>    <td> <b> y </b></td>    <td>4</td>       <td>5</td>      <td>6</td>      <td>5</td>     <td>4</td>   </tr></table><p>The operations used in this algorithm are ‘insert’, ‘delete’, and ‘replace’. These correspond to the functions that you defined earlier: insert_letter(), delete_letter() and replace_letter(). switch_letter() is not used here.</p><p>The diagram below describes how to initialize the table. Each entry in D[i,j] represents the minimum cost of converting string source[0:i] to string target[0:j]. The first column is initialized to represent the cumulative cost of deleting the source characters to convert string “EER” to “”. The first row is initialized to represent the cumulative cost of inserting the target characters to convert from “” to “NEAR”.</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="EditDistInit4.PNG" alt="alternate text" width="width" height="height" style="width:1000px;height:400px;"> Figure 6 Initializing Distance Matrix</div>     <p>Filling in the remainder of the table utilizes the ‘Per Cell Operations’ in the equation (5) above. Note, the diagram below includes in the table some of the 3 sub-calculations shown in light grey. Only ‘min’ of those operations is stored in the table in the <code>min_edit_distance()</code> function.</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="EditDistFill2.PNG" alt="alternate text" width="width" height="height" style="width:800px;height:400px;"> Figure 7 Filling Distance Matrix</div>     <p>Note that the formula for $D[i,j]$ shown in the image is equivalent to:</p><p>\begin{align}<br> \\<br>D[i,j] =min<br>\begin{cases}<br>D[i-1,j] + del_cost\\<br>D[i,j-1] + ins_cost\\<br>D[i-1,j-1] + \left\{\begin{matrix}<br>rep_cost; &amp; if src[i]\neq tar[j]\\<br>0 ; &amp; if src[i]=tar[j]<br>\end{matrix}\right.<br>\end{cases}<br>\tag{5}<br>\end{align}</p><p>The variable <code>sub_cost</code> (for substitution cost) is the same as <code>rep_cost</code>; replacement cost.  We will stick with the term “replace” whenever possible.</p><p>Below are some examples of cells where replacement is used. This also shows the minimum path from the lower right final position where “EER” has been replaced by “NEAR” back to the start. This provides a starting point for the optional ‘backtrace’ algorithm below.</p><div style="width:image width px; font-size:100%; text-align:center;"><img src="EditDistExample1.PNG" alt="alternate text" width="width" height="height" style="width:1200px;height:400px;"> Figure 8 Examples Distance Matrix</div>    <p><a name="ex-11"></a></p><h3 id="Exercise-11"><a href="#Exercise-11" class="headerlink" title="Exercise 11"></a>Exercise 11</h3><p>Again, the word “substitution” appears in the figure, but think of this as “replacement”.</p><p><strong>Instructions</strong>: Implement the function below to get the minimum amount of edits required given a source string and a target string. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>The range(start, stop, step) function excludes 'stop' from its output</li>    <li><a href> words </a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: min_edit_distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min_edit_distance</span><span class="params">(source, target, ins_cost = <span class="number">1</span>, del_cost = <span class="number">1</span>, rep_cost = <span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        source: a string corresponding to the string you are starting with</span></span><br><span class="line"><span class="string">        target: a string corresponding to the string you want to end with</span></span><br><span class="line"><span class="string">        ins_cost: an integer setting the insert cost</span></span><br><span class="line"><span class="string">        del_cost: an integer setting the delete cost</span></span><br><span class="line"><span class="string">        rep_cost: an integer setting the replace cost</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances</span></span><br><span class="line"><span class="string">        med: the minimum edit distance (med) required to convert the source string to the target</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># use deletion and insert cost as  1</span></span><br><span class="line">    m = len(source) </span><br><span class="line">    n = len(target) </span><br><span class="line">    <span class="comment">#initialize cost matrix with zeros and dimensions (m+1,n+1) </span></span><br><span class="line">    D = np.zeros((m+<span class="number">1</span>, n+<span class="number">1</span>), dtype=int) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fill in column 0, from row 1 to row m, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">0</span>,m+<span class="number">1</span>): <span class="comment"># Replace None with the proper range</span></span><br><span class="line">        D[row,<span class="number">0</span>] = row</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Fill in row 0, for all columns from 1 to n, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(<span class="number">0</span>,n+<span class="number">1</span>): <span class="comment"># Replace None with the proper range</span></span><br><span class="line">        D[<span class="number">0</span>,col] = col</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Loop through row 1 to row m, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">1</span>,m+<span class="number">1</span>): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop through column 1 to column n, both inclusive</span></span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> range(<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Intialize r_cost to the 'replace' cost that is passed into this function</span></span><br><span class="line">            r_cost = rep_cost</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check to see if source character at the previous row</span></span><br><span class="line">            <span class="comment"># matches the target character at the previous column, </span></span><br><span class="line">            <span class="keyword">if</span> source[row<span class="number">-1</span>] == target[col<span class="number">-1</span>]:</span><br><span class="line">                <span class="comment"># Update the replacement cost to 0 if source and target are the same</span></span><br><span class="line">                r_cost = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Update the cost at row, col based on previous entries in the cost matrix</span></span><br><span class="line">            <span class="comment"># Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)</span></span><br><span class="line">            D[row,col] = min([ D[row<span class="number">-1</span>,col] + del_cost, D[row, col<span class="number">-1</span>] + ins_cost , D[row<span class="number">-1</span>, col<span class="number">-1</span>] + r_cost])</span><br><span class="line">          </span><br><span class="line">    <span class="comment"># Set the minimum edit distance with the cost found at row m, column n</span></span><br><span class="line">    med = D[m,n]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> D, med</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line"><span class="comment"># testing your implementation </span></span><br><span class="line">source =  <span class="string">'play'</span></span><br><span class="line">target = <span class="string">'stay'</span></span><br><span class="line">matrix, min_edits = min_edit_distance(source, target)</span><br><span class="line">print(<span class="string">"minimum edits: "</span>,min_edits, <span class="string">"\n"</span>)</span><br><span class="line">idx = list(<span class="string">'#'</span> + source)</span><br><span class="line">cols = list(<span class="string">'#'</span> + target)</span><br><span class="line">df = pd.DataFrame(matrix, index=idx, columns= cols)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><pre><code>minimum edits:  4    #  s  t  a  y#  0  1  2  3  4p  1  2  3  4  5l  2  3  4  5  6a  3  4  5  4  5y  4  5  6  5  4</code></pre><p><strong>Expected Results:</strong>  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   <span class="meta">#  s  t  a  y</span></span><br><span class="line">#  <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">p  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span></span><br><span class="line">l  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span></span><br><span class="line">a  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">4</span>  <span class="number">5</span></span><br><span class="line">y  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">5</span>  <span class="number">4</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line"><span class="comment"># testing your implementation </span></span><br><span class="line">source =  <span class="string">'eer'</span></span><br><span class="line">target = <span class="string">'near'</span></span><br><span class="line">matrix, min_edits = min_edit_distance(source, target)</span><br><span class="line">print(<span class="string">"minimum edits: "</span>,min_edits, <span class="string">"\n"</span>)</span><br><span class="line">idx = list(source)</span><br><span class="line">idx.insert(<span class="number">0</span>, <span class="string">'#'</span>)</span><br><span class="line">cols = list(target)</span><br><span class="line">cols.insert(<span class="number">0</span>, <span class="string">'#'</span>)</span><br><span class="line">df = pd.DataFrame(matrix, index=idx, columns= cols)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><pre><code>minimum edits:  3    #  n  e  a  r#  0  1  2  3  4e  1  2  1  2  3e  2  3  2  3  4r  3  4  3  4  3</code></pre><p><strong>Expected Results</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">minimum edits:  <span class="number">3</span> </span><br><span class="line"></span><br><span class="line">   <span class="meta">#  n  e  a  r</span></span><br><span class="line">#  <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">e  <span class="number">1</span>  <span class="number">2</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span></span><br><span class="line">e  <span class="number">2</span>  <span class="number">3</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">r  <span class="number">3</span>  <span class="number">4</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">3</span></span><br></pre></td></tr></table></figure></p><p>We can now test several of our routines at once:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source = <span class="string">"eer"</span></span><br><span class="line">targets = edit_one_letter(source,allow_switches = <span class="keyword">False</span>)  <span class="comment">#disable switches since min_edit_distance does not include them</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> targets:</span><br><span class="line">    _, min_edits = min_edit_distance(source, t,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)  <span class="comment"># set ins, del, sub costs all to one</span></span><br><span class="line">    <span class="keyword">if</span> min_edits != <span class="number">1</span>: print(source, t, min_edits)</span><br></pre></td></tr></table></figure><p><strong>Expected Results:</strong>  (empty)</p><p>The ‘replace()’ routine utilizes all letters a-z one of which returns the original word.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source = <span class="string">"eer"</span></span><br><span class="line">targets = edit_two_letters(source,allow_switches = <span class="keyword">False</span>) <span class="comment">#disable switches since min_edit_distance does not include them</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> targets:</span><br><span class="line">    _, min_edits = min_edit_distance(source, t,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)  <span class="comment"># set ins, del, sub costs all to one</span></span><br><span class="line">    <span class="keyword">if</span> min_edits != <span class="number">2</span> <span class="keyword">and</span> min_edits != <span class="number">1</span>: print(source, t, min_edits)</span><br></pre></td></tr></table></figure><pre><code>eer eer 0</code></pre><p><strong>Expected Results:</strong>  eer eer 0<br>We have to allow single edits here because some two_edits will restore a single edit.</p><h1 id="Submission"><a href="#Submission" class="headerlink" title="Submission"></a>Submission</h1><p>Make sure you submit your assignment before you modify anything below</p><p><a name="5"></a></p><h1 id="Part-5-Optional-Backtrace"><a href="#Part-5-Optional-Backtrace" class="headerlink" title="Part 5: Optional - Backtrace"></a>Part 5: Optional - Backtrace</h1><p>Once you have computed your matrix using minimum edit distance, how would find the shortest path from the top left corner to the bottom right corner? </p><p>Note that you could use backtrace algorithm.  Try to find the shortest path given the matrix that your <code>min_edit_distance</code> function returned.</p><p>You can use these <a href="https://web.stanford.edu/class/cs124/lec/med.pdf" target="_blank" rel="noopener">lecture slides on minimum edit distance</a> by Dan Jurafsky to learn about the algorithm for backtrace.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Experiment with back trace - insert your code here</span></span><br></pre></td></tr></table></figure><h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul><li>Dan Jurafsky - Speech and Language Processing - Textbook</li><li>This auto-correct explanation was first done by Peter Norvig in 2007 </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-1-Auto-Correct&quot;&gt;&lt;a href=&quot;#Assignment-1-Auto-Correct&quot; class=&quot;headerlink&quot; title=&quot;Assignment 1: Auto Correct&quot;&gt;&lt;/a&gt;Assignment
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Parts-of-Speech Tagging</title>
    <link href="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/"/>
    <id>https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/</id>
    <published>2020-07-18T17:39:21.000Z</published>
    <updated>2020-07-18T17:50:42.411Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-2-Parts-of-Speech-Tagging-POS"><a href="#Assignment-2-Parts-of-Speech-Tagging-POS" class="headerlink" title="Assignment 2: Parts-of-Speech Tagging (POS)"></a>Assignment 2: Parts-of-Speech Tagging (POS)</h1><p>Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective…) to each word in an input text.  Tagging is difficult because some words can represent more than one part of speech at different times. They are  <strong>Ambiguous</strong>. Let’s look at the following example: </p><ul><li>The whole team played <strong>well</strong>. [adverb]</li><li>You are doing <strong>well</strong> for yourself. [adjective]</li><li><strong>Well</strong>, this assignment took me forever to complete. [interjection]</li><li>The <strong>well</strong> is dry. [noun]</li><li>Tears were beginning to <strong>well</strong> in her eyes. [verb]</li></ul><p>Distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will: </p><ul><li>Learn how parts-of-speech tagging works</li><li>Compute the transition matrix A in a Hidden Markov Model</li><li>Compute the transition matrix B in a Hidden Markov Model</li><li>Compute the Viterbi algorithm </li><li>Compute the accuracy of your own model </li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#0">0 Data Sources</a></li><li><a href="#1">1 POS Tagging</a><ul><li><a href="#1.1">1.1 Training</a><ul><li><a href="#ex-01">Exercise 01</a></li></ul></li><li><a href="#1.2">1.2 Testing</a><ul><li><a href="#ex-02">Exercise 02</a></li></ul></li></ul></li><li><a href="#2">2 Hidden Markov Models</a><ul><li><a href="#2.1">2.1 Generating Matrices</a><ul><li><a href="#ex-03">Exercise 03</a></li><li><a href="#ex-04">Exercise 04</a></li></ul></li></ul></li><li><a href="#3">3 Viterbi Algorithm</a><ul><li><a href="#3.1">3.1 Initialization</a><ul><li><a href="#ex-05">Exercise 05</a></li></ul></li><li><a href="#3.2">3.2 Viterbi Forward</a><ul><li><a href="#ex-06">Exercise 06</a></li></ul></li><li><a href="#3.3">3.3 Viterbi Backward</a><ul><li><a href="#ex-07">Exercise 07</a></li></ul></li></ul></li><li><a href="#4">4 Predicting on a data set</a><ul><li><a href="#ex-08">Exercise 08</a></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Importing packages and loading in the data set </span></span><br><span class="line"><span class="keyword">from</span> utils_pos <span class="keyword">import</span> get_word_tag, preprocess  </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><p><a name="0"></a></p><h2 id="Part-0-Data-Sources"><a href="#Part-0-Data-Sources" class="headerlink" title="Part 0: Data Sources"></a>Part 0: Data Sources</h2><p>This assignment will use two tagged data sets collected from the <strong>Wall Street Journal (WSJ)</strong>. </p><p><a href="http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html" target="_blank" rel="noopener">Here</a> is an example ‘tag-set’ or Part of Speech designation describing the two or three letter tag and their meaning. </p><ul><li>One data set (<strong>WSJ-2_21.pos</strong>) will be used for <strong>training</strong>.</li><li>The other (<strong>WSJ-24.pos</strong>) for <strong>testing</strong>. </li><li>The tagged training data has been preprocessed to form a vocabulary (<strong>hmm_vocab.txt</strong>). </li><li>The words in the vocabulary are words from the training set that were used two or more times. </li><li>The vocabulary is augmented with a set of ‘unknown word tokens’, described below. </li></ul><p>The training set will be used to create the emission, transmission and tag counts. </p><p>The test set (WSJ-24.pos) is read in to create <code>y</code>. </p><ul><li>This contains both the test text and the true tag. </li><li>The test set has also been preprocessed to remove the tags to form <strong>test_words.txt</strong>. </li><li>This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in <strong>utils_pos.py</strong>. </li><li>This forms the list <code>prep</code>, the preprocessed text used to test our  POS taggers.</li></ul><p>A POS tagger will necessarily encounter words that are not in its datasets. </p><ul><li>To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. </li><li>For example, the suffix ‘ize’ is a hint that the word is a verb, as in ‘final-ize’ or ‘character-ize’. </li><li>A set of unknown-tokens, such as ‘—unk-verb—‘ or ‘—unk-noun—‘ will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.</li></ul><p><img src="DataSources1.png"></p><p>Implementation note: </p><ul><li>For python 3.6 and beyond, dictionaries retain the insertion order. </li><li>Furthermore, their hash-based lookup makes them suitable for rapid membership tests. <ul><li>If _di_ is a dictionary, <code>key in di</code> will return <code>True</code> if _di_ has a key _key_, else <code>False</code>. </li></ul></li></ul><p>The dictionary <code>vocab</code> will utilize these features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the training corpus</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"WSJ_02-21.pos"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    training_corpus = f.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"A few items of the training corpus list"</span>)</span><br><span class="line">print(training_corpus[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>A few items of the training corpus list[&#39;In\tIN\n&#39;, &#39;an\tDT\n&#39;, &#39;Oct.\tNNP\n&#39;, &#39;19\tCD\n&#39;, &#39;review\tNN\n&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read the vocabulary data, split by each line of text, and save the list</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"hmm_vocab.txt"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    voc_l = f.read().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"A few items of the vocabulary list"</span>)</span><br><span class="line">print(voc_l[<span class="number">0</span>:<span class="number">50</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"A few items at the end of the vocabulary list"</span>)</span><br><span class="line">print(voc_l[<span class="number">-50</span>:])</span><br></pre></td></tr></table></figure><pre><code>A few items of the vocabulary list[&#39;!&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;, &#39;&amp;&#39;, &quot;&#39;&quot;, &quot;&#39;&#39;&quot;, &quot;&#39;40s&quot;, &quot;&#39;60s&quot;, &quot;&#39;70s&quot;, &quot;&#39;80s&quot;, &quot;&#39;86&quot;, &quot;&#39;90s&quot;, &quot;&#39;N&quot;, &quot;&#39;S&quot;, &quot;&#39;d&quot;, &quot;&#39;em&quot;, &quot;&#39;ll&quot;, &quot;&#39;m&quot;, &quot;&#39;n&#39;&quot;, &quot;&#39;re&quot;, &quot;&#39;s&quot;, &quot;&#39;til&quot;, &quot;&#39;ve&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;-&#39;, &#39;--&#39;, &#39;--n--&#39;, &#39;--unk--&#39;, &#39;--unk_adj--&#39;, &#39;--unk_adv--&#39;, &#39;--unk_digit--&#39;, &#39;--unk_noun--&#39;, &#39;--unk_punct--&#39;, &#39;--unk_upper--&#39;, &#39;--unk_verb--&#39;, &#39;.&#39;, &#39;...&#39;, &#39;0.01&#39;, &#39;0.0108&#39;, &#39;0.02&#39;, &#39;0.03&#39;, &#39;0.05&#39;, &#39;0.1&#39;, &#39;0.10&#39;, &#39;0.12&#39;, &#39;0.13&#39;, &#39;0.15&#39;]A few items at the end of the vocabulary list[&#39;yards&#39;, &#39;yardstick&#39;, &#39;year&#39;, &#39;year-ago&#39;, &#39;year-before&#39;, &#39;year-earlier&#39;, &#39;year-end&#39;, &#39;year-on-year&#39;, &#39;year-round&#39;, &#39;year-to-date&#39;, &#39;year-to-year&#39;, &#39;yearlong&#39;, &#39;yearly&#39;, &#39;years&#39;, &#39;yeast&#39;, &#39;yelled&#39;, &#39;yelling&#39;, &#39;yellow&#39;, &#39;yen&#39;, &#39;yes&#39;, &#39;yesterday&#39;, &#39;yet&#39;, &#39;yield&#39;, &#39;yielded&#39;, &#39;yielding&#39;, &#39;yields&#39;, &#39;you&#39;, &#39;young&#39;, &#39;younger&#39;, &#39;youngest&#39;, &#39;youngsters&#39;, &#39;your&#39;, &#39;yourself&#39;, &#39;youth&#39;, &#39;youthful&#39;, &#39;yuppie&#39;, &#39;yuppies&#39;, &#39;zero&#39;, &#39;zero-coupon&#39;, &#39;zeroing&#39;, &#39;zeros&#39;, &#39;zinc&#39;, &#39;zip&#39;, &#39;zombie&#39;, &#39;zone&#39;, &#39;zones&#39;, &#39;zoning&#39;, &#39;{&#39;, &#39;}&#39;, &#39;&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocab: dictionary that has the index of the corresponding words</span></span><br><span class="line">vocab = &#123;&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the index of the corresponding words. </span></span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sorted(voc_l)): </span><br><span class="line">    vocab[word] = i       </span><br><span class="line">    </span><br><span class="line">print(<span class="string">"Vocabulary dictionary, key is the word, value is a unique integer"</span>)</span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> vocab.items():</span><br><span class="line">    print(<span class="string">f"<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;v&#125;</span>"</span>)</span><br><span class="line">    cnt += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> cnt &gt; <span class="number">20</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>Vocabulary dictionary, key is the word, value is a unique integer:0!:1#:2$:3%:4&amp;:5&#39;:6&#39;&#39;:7&#39;40s:8&#39;60s:9&#39;70s:10&#39;80s:11&#39;86:12&#39;90s:13&#39;N:14&#39;S:15&#39;d:16&#39;em:17&#39;ll:18&#39;m:19&#39;n&#39;:20</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the test corpus</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"WSJ_24.pos"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    y = f.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"A sample of the test corpus"</span>)</span><br><span class="line">print(y[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>A sample of the test corpus[&#39;The\tDT\n&#39;, &#39;economy\tNN\n&#39;, &quot;&#39;s\tPOS\n&quot;, &#39;temperature\tNN\n&#39;, &#39;will\tMD\n&#39;, &#39;be\tVB\n&#39;, &#39;taken\tVBN\n&#39;, &#39;from\tIN\n&#39;, &#39;several\tJJ\n&#39;, &#39;vantage\tNN\n&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#corpus without tags, preprocessed</span></span><br><span class="line">_, prep = preprocess(vocab, <span class="string">"test.words"</span>)     </span><br><span class="line"></span><br><span class="line">print(<span class="string">'The length of the preprocessed test corpus: '</span>, len(prep))</span><br><span class="line">print(<span class="string">'This is a sample of the test_corpus: '</span>)</span><br><span class="line">print(prep[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>The length of the preprocessed test corpus:  34199This is a sample of the test_corpus: [&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;, &#39;from&#39;, &#39;several&#39;, &#39;--unk--&#39;]</code></pre><p><a name="1"></a></p><h1 id="Part-1-Parts-of-speech-tagging"><a href="#Part-1-Parts-of-speech-tagging" class="headerlink" title="Part 1: Parts-of-speech tagging"></a>Part 1: Parts-of-speech tagging</h1><p><a name="1.1"></a></p><h2 id="Part-1-1-Training"><a href="#Part-1-1-Training" class="headerlink" title="Part 1.1 - Training"></a>Part 1.1 - Training</h2><p>You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. </p><p>In this section, you will find the words that are not ambiguous. </p><ul><li>For example, the word <code>is</code> is a verb and it is not ambiguous. </li><li>In the <code>WSJ</code> corpus, $86$% of the token are unambiguous (meaning they have only one tag) </li><li>About $14\%$ are ambiguous (meaning that they have more than one tag)</li></ul><p><img src="pos.png" style="width:400px;height:250px;"></p><p>Before you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables. </p><h4 id="Transition-counts"><a href="#Transition-counts" class="headerlink" title="Transition counts"></a>Transition counts</h4><ul><li>The first dictionary is the <code>transition_counts</code> dictionary which computes the number of times each tag happened next to another tag. </li></ul><p>This dictionary will be used to compute: </p><script type="math/tex; mode=display">P(t_i |t_{i-1}) \tag{1}</script><p>This is the probability of a tag at position $i$ given the tag at position $i-1$.</p><p>In order for you to compute equation 1, you will create a <code>transition_counts</code> dictionary where </p><ul><li>The keys are <code>(prev_tag, tag)</code></li><li>The values are the number of times those two tags appeared in that order. </li></ul><h4 id="Emission-counts"><a href="#Emission-counts" class="headerlink" title="Emission counts"></a>Emission counts</h4><p>The second dictionary you will compute is the <code>emission_counts</code> dictionary. This dictionary will be used to compute:</p><script type="math/tex; mode=display">P(w_i|t_i)\tag{2}</script><p>In other words, you will use it to compute the probability of a word given its tag. </p><p>In order for you to compute equation 2, you will create an <code>emission_counts</code> dictionary where </p><ul><li>The keys are <code>(tag, word)</code> </li><li>The values are the number of times that pair showed up in your training set. </li></ul><h4 id="Tag-counts"><a href="#Tag-counts" class="headerlink" title="Tag counts"></a>Tag counts</h4><p>The last dictionary you will compute is the <code>tag_counts</code> dictionary. </p><ul><li>The key is the tag </li><li>The value is the number of times each tag appeared.</li></ul><p><a name="ex-01"></a></p><h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Write a program that takes in the <code>training_corpus</code> and returns the three dictionaries mentioned above <code>transition_counts</code>, <code>emission_counts</code>, and <code>tag_counts</code>. </p><ul><li><code>emission_counts</code>: maps (tag, word) to the number of times it happened. </li><li><code>transition_counts</code>: maps (prev_tag, tag) to the number of times it has appeared. </li><li><code>tag_counts</code>: maps (tag) to the number of times it has occured. </li></ul><p>Implementation note: This routine utilises <em>defaultdict</em>, which is a subclass of <em>dict</em>. </p><ul><li>A standard Python dictionary throws a <em>KeyError</em> if you try to access an item with a key that is not currently in the dictionary. </li><li>In contrast, the <em>defaultdict</em> will create an item of the type of the argument, in this case an integer with the default value of 0. </li><li>See <a href="https://docs.python.org/3.3/library/collections.html#defaultdict-objects" target="_blank" rel="noopener">defaultdict</a>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_dictionaries</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dictionaries</span><span class="params">(training_corpus, vocab)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        training_corpus: a corpus where each line has a word followed by its tag.</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary where the keys are the tags and the values are the counts</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the dictionaries using defaultdict</span></span><br><span class="line">    emission_counts = defaultdict(int)</span><br><span class="line">    transition_counts = defaultdict(int)</span><br><span class="line">    tag_counts = defaultdict(int)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "prev_tag" (previous tag) with the start state, denoted by '--s--'</span></span><br><span class="line">    prev_tag = <span class="string">'--s--'</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use 'i' to track the line number in the corpus</span></span><br><span class="line">    i = <span class="number">0</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Each item in the training corpus contains a word and its POS tag</span></span><br><span class="line">    <span class="comment"># Go through each word and its tag in the training corpus</span></span><br><span class="line">    <span class="keyword">for</span> word_tag <span class="keyword">in</span> training_corpus:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the word_tag count</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Every 50,000 words, print the word count</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"word count = <span class="subst">&#123;i&#125;</span>"</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># get the word and tag using the get_word_tag helper function (imported from utils_pos.py)</span></span><br><span class="line">        word, tag = get_word_tag(word_tag,vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the transition count for the previous word and tag</span></span><br><span class="line">        transition_counts[(prev_tag, tag)] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the emission count for the tag and word</span></span><br><span class="line">        emission_counts[(tag, word)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Increment the tag count</span></span><br><span class="line">        tag_counts[tag] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set the previous tag to this tag (for the next iteration of the loop)</span></span><br><span class="line">        prev_tag = tag</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> emission_counts, transition_counts, tag_counts</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)</span><br></pre></td></tr></table></figure><pre><code>word count = 50000word count = 100000word count = 150000word count = 200000word count = 250000word count = 300000word count = 350000word count = 400000word count = 450000word count = 500000word count = 550000word count = 600000word count = 650000word count = 700000word count = 750000word count = 800000word count = 850000word count = 900000word count = 950000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get all the POS states</span></span><br><span class="line">states = sorted(tag_counts.keys())</span><br><span class="line">print(<span class="string">f"Number of POS tags (number of 'states'): <span class="subst">&#123;len(states)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">"View these POS tags (states)"</span>)</span><br><span class="line">print(states)</span><br></pre></td></tr></table></figure><pre><code>Number of POS tags (number of &#39;states&#39;): 46View these POS tags (states)[&#39;#&#39;, &#39;$&#39;, &quot;&#39;&#39;&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;--s--&#39;, &#39;.&#39;, &#39;:&#39;, &#39;CC&#39;, &#39;CD&#39;, &#39;DT&#39;, &#39;EX&#39;, &#39;FW&#39;, &#39;IN&#39;, &#39;JJ&#39;, &#39;JJR&#39;, &#39;JJS&#39;, &#39;LS&#39;, &#39;MD&#39;, &#39;NN&#39;, &#39;NNP&#39;, &#39;NNPS&#39;, &#39;NNS&#39;, &#39;PDT&#39;, &#39;POS&#39;, &#39;PRP&#39;, &#39;PRP$&#39;, &#39;RB&#39;, &#39;RBR&#39;, &#39;RBS&#39;, &#39;RP&#39;, &#39;SYM&#39;, &#39;TO&#39;, &#39;UH&#39;, &#39;VB&#39;, &#39;VBD&#39;, &#39;VBG&#39;, &#39;VBN&#39;, &#39;VBP&#39;, &#39;VBZ&#39;, &#39;WDT&#39;, &#39;WP&#39;, &#39;WP$&#39;, &#39;WRB&#39;, &#39;``&#39;]</code></pre><h5 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Number of POS tags (number of 'states'46</span><br><span class="line">View these states</span><br><span class="line">['#', '$', "''", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']</span><br></pre></td></tr></table></figure><p>The ‘states’ are the Parts-of-speech designations found in the training data. They will also be referred to as ‘tags’ or POS in this assignment. </p><ul><li>“NN” is noun, singular, </li><li>‘NNS’ is noun, plural. </li><li>In addition, there are helpful tags like ‘—s—‘ which indicate a start of a sentence.</li><li>You can get a more complete description at <a href="https://www.clips.uantwerpen.be/pages/mbsp-tags" target="_blank" rel="noopener">Penn Treebank II tag set</a>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"transition examples: "</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> list(transition_counts.items())[:<span class="number">3</span>]:</span><br><span class="line">    print(ex)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"emission examples: "</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> list(emission_counts.items())[<span class="number">200</span>:<span class="number">203</span>]:</span><br><span class="line">    <span class="keyword">print</span> (ex)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"ambiguous word example: "</span>)</span><br><span class="line"><span class="keyword">for</span> tup,cnt <span class="keyword">in</span> emission_counts.items():</span><br><span class="line">    <span class="keyword">if</span> tup[<span class="number">1</span>] == <span class="string">'back'</span>: <span class="keyword">print</span> (tup, cnt)</span><br></pre></td></tr></table></figure><pre><code>transition examples: ((&#39;--s--&#39;, &#39;IN&#39;), 5050)((&#39;IN&#39;, &#39;DT&#39;), 32364)((&#39;DT&#39;, &#39;NNP&#39;), 9044)emission examples: ((&#39;DT&#39;, &#39;any&#39;), 721)((&#39;NN&#39;, &#39;decrease&#39;), 7)((&#39;NN&#39;, &#39;insider-trading&#39;), 5)ambiguous word example: (&#39;RB&#39;, &#39;back&#39;) 304(&#39;VB&#39;, &#39;back&#39;) 20(&#39;RP&#39;, &#39;back&#39;) 84(&#39;JJ&#39;, &#39;back&#39;) 25(&#39;NN&#39;, &#39;back&#39;) 29(&#39;VBP&#39;, &#39;back&#39;) 4</code></pre><h5 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">transition examples: </span><br><span class="line">(('--s--', 'IN'), 5050)</span><br><span class="line">(('IN', 'DT'), 32364)</span><br><span class="line">(('DT', 'NNP'), 9044)</span><br><span class="line"></span><br><span class="line">emission examples: </span><br><span class="line">(('DT', 'any'), 721)</span><br><span class="line">(('NN', 'decrease'), 7)</span><br><span class="line">(('NN', 'insider-trading'), 5)</span><br><span class="line"></span><br><span class="line">ambiguous word example: </span><br><span class="line">('RB', 'back') 304</span><br><span class="line">('VB', 'back') 20</span><br><span class="line">('RP', 'back') 84</span><br><span class="line">('JJ', 'back') 25</span><br><span class="line">('NN', 'back') 29</span><br><span class="line">('VBP', 'back') 4</span><br></pre></td></tr></table></figure><p><a name="1.2"></a></p><h3 id="Part-1-2-Testing"><a href="#Part-1-2-Testing" class="headerlink" title="Part 1.2 - Testing"></a>Part 1.2 - Testing</h3><p>Now you will test the accuracy of your parts-of-speech tagger using your <code>emission_counts</code> dictionary. </p><ul><li>Given your preprocessed test corpus <code>prep</code>, you will assign a parts-of-speech tag to every word in that corpus. </li><li>Using the original tagged test corpus <code>y</code>, you will then compute what percent of the tags you got correct. </li></ul><p><a name="ex-02"></a></p><h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement <code>predict_pos</code> that computes the accuracy of your model. </p><ul><li>This is a warm up exercise. </li><li>To assign a part of speech to a word, assign the most frequent POS for that word in the training set. </li><li>Then evaluate how well this approach works.  Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same.  If so, the prediction was correct!</li><li>Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: predict_pos</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_pos</span><span class="params">(prep, y, emission_counts, vocab, states)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.</span></span><br><span class="line"><span class="string">        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">        states: a sorted list of all possible tags for this assignment</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: Number of times you classified a word correctly</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the number of correct predictions to zero</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the (tag, word) tuples, stored as a set</span></span><br><span class="line">    all_words = set(emission_counts.keys())  <span class="comment"># (tag, word)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of (word, POS) tuples in the corpus 'y'</span></span><br><span class="line">    total = len(y)</span><br><span class="line">    <span class="keyword">for</span> word, y_tup <span class="keyword">in</span> zip(prep, y): </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the (word, POS) string into a list of two items</span></span><br><span class="line">        y_tup_l = y_tup.split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Verify that y_tup contain both word and POS</span></span><br><span class="line">        <span class="keyword">if</span> len(y_tup_l) == <span class="number">2</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the true POS label for this word</span></span><br><span class="line">            true_label = y_tup_l[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># If the y_tup didn't contain word and POS, go to next word</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">        count_final = <span class="number">0</span></span><br><span class="line">        pos_final = <span class="string">''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the word is in the vocabulary...</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> states:</span><br><span class="line"></span><br><span class="line">            <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># define the key as the tuple containing the POS and word</span></span><br><span class="line">                key = (pos, word)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if the (pos, word) key exists in the emission_counts dictionary</span></span><br><span class="line">                <span class="keyword">if</span> key <span class="keyword">in</span> emission_counts: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># get the emission count of the (pos,word) tuple </span></span><br><span class="line">                    count = emission_counts[key]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># keep track of the POS with the largest count</span></span><br><span class="line">                    <span class="keyword">if</span> count &gt; count_final: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final count (largest count)</span></span><br><span class="line">                        count_final = count</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final POS</span></span><br><span class="line">                        pos_final = pos</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the final POS (with the largest count) matches the true POS:</span></span><br><span class="line">            <span class="keyword">if</span> pos_final == true_label: <span class="comment"># complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Update the number of correct predictions</span></span><br><span class="line">                num_correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    accuracy = num_correct / total</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)</span><br><span class="line">print(<span class="string">f"Accuracy of prediction using predict_pos is <span class="subst">&#123;accuracy_predict_pos:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Accuracy of prediction using predict_pos is 0.8889</code></pre><h5 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of prediction <span class="keyword">using</span> predict_pos is <span class="number">0.8889</span></span><br></pre></td></tr></table></figure><p>88.9% is really good for this warm up exercise. With hidden markov models, you should be able to get <strong>95% accuracy.</strong></p><p><a name="2"></a></p><h1 id="Part-2-Hidden-Markov-Models-for-POS"><a href="#Part-2-Hidden-Markov-Models-for-POS" class="headerlink" title="Part 2: Hidden Markov Models for POS"></a>Part 2: Hidden Markov Models for POS</h1><p>Now you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder</p><ul><li>The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization. </li><li>In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. </li><li>By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in Part 1.</li></ul><p>The Markov Model contains a number of states and the probability of transition between those states. </p><ul><li>In this case, the states are the parts-of-speech. </li><li>A Markov Model utilizes a transition matrix, <code>A</code>. </li><li>A Hidden Markov Model adds an observation or emission matrix <code>B</code> which describes the probability of a visible observation when we are in a particular state. </li><li>In this case, the emissions are the words in the corpus</li><li>The state, which is hidden, is the POS tag of that word.</li></ul><p><a name="2.1"></a></p><h2 id="Part-2-1-Generating-Matrices"><a href="#Part-2-1-Generating-Matrices" class="headerlink" title="Part 2.1 Generating Matrices"></a>Part 2.1 Generating Matrices</h2><h3 id="Creating-the-‘A’-transition-probabilities-matrix"><a href="#Creating-the-‘A’-transition-probabilities-matrix" class="headerlink" title="Creating the ‘A’ transition probabilities matrix"></a>Creating the ‘A’ transition probabilities matrix</h3><p>Now that you have your <code>emission_counts</code>, <code>transition_counts</code>, and <code>tag_counts</code>, you will start implementing the Hidden Markov Model. </p><p>This will allow you to quickly construct the </p><ul><li><code>A</code> transition probabilities matrix.</li><li>and the <code>B</code> emission probabilities matrix. </li></ul><p>You will also use some smoothing when computing these matrices. </p><p>Here is an example of what the <code>A</code> transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):</p><div class="table-container"><table><thead><tr><th><strong>A</strong></th><th>…</th><th>RBS</th><th>RP</th><th>SYM</th><th>TO</th><th>UH</th><th>…</th></tr></thead><tbody><tr><td><strong>RBS</strong></td><td>…</td><td>2.217069e-06</td><td>2.217069e-06</td><td>2.217069e-06</td><td>0.008870</td><td>2.217069e-06</td><td>…</td></tr><tr><td><strong>RP</strong></td><td>…</td><td>3.756509e-07</td><td>7.516775e-04</td><td>3.756509e-07</td><td>0.051089</td><td>3.756509e-07</td><td>…</td></tr><tr><td><strong>SYM</strong></td><td>…</td><td>1.722772e-05</td><td>1.722772e-05</td><td>1.722772e-05</td><td>0.000017</td><td>1.722772e-05</td><td>…</td></tr><tr><td><strong>TO</strong></td><td>…</td><td>4.477336e-05</td><td>4.472863e-08</td><td>4.472863e-08</td><td>0.000090</td><td>4.477336e-05</td><td>…</td></tr><tr><td><strong>UH</strong></td><td>…</td><td>1.030439e-05</td><td>1.030439e-05</td><td>1.030439e-05</td><td>0.061837</td><td>3.092348e-02</td><td>…</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table></div><p>Note that the matrix above was computed with smoothing. </p><p>Each cell gives you the probability to go from one part of speech to another. </p><ul><li>In other words, there is a 4.47e-8 chance of going from parts-of-speech <code>TO</code> to <code>RP</code>. </li><li>The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.</li></ul><p>The smoothing was done as follows: </p><script type="math/tex; mode=display">P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_{i}) + \alpha }{C(t_{i-1}) +\alpha * N}\tag{3}</script><ul><li>$N$ is the total number of tags</li><li>$C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in <code>transition_counts</code> dictionary.</li><li>$C(t_{i-1})$ is the count of the previous POS in the <code>tag_counts</code> dictionary.</li><li>$\alpha$ is a smoothing parameter.</li></ul><p><a name="ex-03"></a></p><h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the <code>create_transition_matrix</code> below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix <code>A</code>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_transition_matrix</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_transition_matrix</span><span class="params">(alpha, tag_counts, transition_counts)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: number used for smoothing</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        transition_counts: transition count for the previous word and tag</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        A: matrix of dimension (num_tags,num_tags)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get a sorted list of unique POS tags</span></span><br><span class="line">    all_tags = sorted(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Count the number of unique POS tags</span></span><br><span class="line">    num_tags = len(all_tags)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the transition matrix 'A'</span></span><br><span class="line">    A = np.zeros((num_tags,num_tags))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the unique transition tuples (previous POS, current POS)</span></span><br><span class="line">    trans_keys = set(transition_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Return instances of 'None' with your code) ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row of the transition matrix A</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column of the transition matrix A</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_tags):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the count of the (prev POS, current POS) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># Define the tuple (prev POS, current POS)</span></span><br><span class="line">            <span class="comment"># Get the tag at position i and tag at position j (from the all_tags list)</span></span><br><span class="line">            key = (all_tags[i],all_tags[j])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check if the (prev POS, current POS) tuple </span></span><br><span class="line">            <span class="comment"># exists in the transition counts dictionaory</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> trans_keys: <span class="comment">#complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get count from the transition_counts dictionary </span></span><br><span class="line">                <span class="comment"># for the (prev POS, current POS) tuple</span></span><br><span class="line">                count = transition_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the previous tag (index position i) from tag_counts</span></span><br><span class="line">            count_prev_tag = tag_counts[all_tags[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Apply smoothing using count of the tuple, alpha, </span></span><br><span class="line">            <span class="comment"># count of previous tag, alpha, and number of total tags</span></span><br><span class="line">            A[i,j] = (count + alpha ) / ( count_prev_tag + alpha * num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line">A = create_transition_matrix(alpha, tag_counts, transition_counts)</span><br><span class="line"><span class="comment"># Testing your function</span></span><br><span class="line">print(<span class="string">f"A at row 0, col 0: <span class="subst">&#123;A[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"A at row 3, col 1: <span class="subst">&#123;A[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"View a subset of transition matrix A"</span>)</span><br><span class="line">A_sub = pd.DataFrame(A[<span class="number">30</span>:<span class="number">35</span>,<span class="number">30</span>:<span class="number">35</span>], index=states[<span class="number">30</span>:<span class="number">35</span>], columns = states[<span class="number">30</span>:<span class="number">35</span>] )</span><br><span class="line">print(A_sub)</span><br></pre></td></tr></table></figure><pre><code>A at row 0, col 0: 0.000007040A at row 3, col 1: 0.1691View a subset of transition matrix A              RBS            RP           SYM        TO            UHRBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02</code></pre><h5 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A at row <span class="number">0</span>, col <span class="number">0</span>: <span class="number">0.000007040</span></span><br><span class="line">A at row <span class="number">3</span>, col <span class="number">1</span>: <span class="number">0.1691</span></span><br><span class="line">View a subset of transition matrix A</span><br><span class="line">              RBS            RP           SYM        TO            UH</span><br><span class="line">RBS  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">0.008870</span>  <span class="number">2.217069e-06</span></span><br><span class="line">RP   <span class="number">3.756509e-07</span>  <span class="number">7.516775e-04</span>  <span class="number">3.756509e-07</span>  <span class="number">0.051089</span>  <span class="number">3.756509e-07</span></span><br><span class="line">SYM  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">0.000017</span>  <span class="number">1.722772e-05</span></span><br><span class="line">TO   <span class="number">4.477336e-05</span>  <span class="number">4.472863e-08</span>  <span class="number">4.472863e-08</span>  <span class="number">0.000090</span>  <span class="number">4.477336e-05</span></span><br><span class="line">UH   <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">0.061837</span>  <span class="number">3.092348e-02</span></span><br></pre></td></tr></table></figure><h3 id="Create-the-‘B’-emission-probabilities-matrix"><a href="#Create-the-‘B’-emission-probabilities-matrix" class="headerlink" title="Create the ‘B’ emission probabilities matrix"></a>Create the ‘B’ emission probabilities matrix</h3><p>Now you will create the <code>B</code> transition matrix which computes the emission probability. </p><p>You will use smoothing as defined below: </p><script type="math/tex; mode=display">P(w_i | t_i) = \frac{C(t_i, word_i)+ \alpha}{C(t_{i}) +\alpha * N}\tag{4}</script><ul><li>$C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in <code>emission_counts</code> dictionary).</li><li>$C(t_i)$ is the number of times $tag_i$ was in the training data (stored in <code>tag_counts</code> dictionary).</li><li>$N$ is the number of words in the vocabulary</li><li>$\alpha$ is a smoothing parameter. </li></ul><p>The matrix <code>B</code> is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. </p><p>Here is an example of the matrix, only a subset of tags and words are shown: </p><p style="text-align: center;"> <b>B Emissions Probability Matrix (subset)</b>  </p><div class="table-container"><table><thead><tr><th><strong>B</strong></th><th>…</th><th>725</th><th>adroitly</th><th>engineers</th><th>promoted</th><th>synergy</th><th>…</th></tr></thead><tbody><tr><td><strong>CD</strong></td><td>…</td><td><strong>8.201296e-05</strong></td><td>2.732854e-08</td><td>2.732854e-08</td><td>2.732854e-08</td><td>2.732854e-08</td><td>…</td></tr><tr><td><strong>NN</strong></td><td>…</td><td>7.521128e-09</td><td>7.521128e-09</td><td>7.521128e-09</td><td>7.521128e-09</td><td><strong>2.257091e-05</strong></td><td>…</td></tr><tr><td><strong>NNS</strong></td><td>…</td><td>1.670013e-08</td><td>1.670013e-08</td><td><strong>4.676203e-04</strong></td><td>1.670013e-08</td><td>1.670013e-08</td><td>…</td></tr><tr><td><strong>VB</strong></td><td>…</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>3.779036e-08</td><td>…</td></tr><tr><td><strong>RB</strong></td><td>…</td><td>3.226454e-08</td><td><strong>6.456135e-05</strong></td><td>3.226454e-08</td><td>3.226454e-08</td><td>3.226454e-08</td><td>…</td></tr><tr><td><strong>RP</strong></td><td>…</td><td>3.723317e-07</td><td>3.723317e-07</td><td>3.723317e-07</td><td><strong>3.723317e-07</strong></td><td>3.723317e-07</td><td>…</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table></div><p><a name="ex-04"></a></p><h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement the <code>create_emission_matrix</code> below that computes the <code>B</code> emission probabilities matrix. Your function takes in $\alpha$, the smoothing parameter, <code>tag_counts</code>, which is a dictionary mapping each tag to its respective count, the <code>emission_counts</code> dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix <code>B</code>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_emission_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_emission_matrix</span><span class="params">(alpha, tag_counts, emission_counts, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: tuning parameter used in smoothing </span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        B: a matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get the number of POS tag</span></span><br><span class="line">    num_tags = len(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a list of all POS tags</span></span><br><span class="line">    all_tags = sorted(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the total number of unique words in the vocabulary</span></span><br><span class="line">    num_words = len(vocab)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the emission matrix B with places for</span></span><br><span class="line">    <span class="comment"># tags in the rows and words in the columns</span></span><br><span class="line">    B = np.zeros((num_tags, num_words))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a set of all (POS, word) tuples </span></span><br><span class="line">    <span class="comment"># from the keys of the emission_counts dictionary</span></span><br><span class="line">    emis_keys = set(list(emission_counts.keys()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row (POS tags)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column (words)</span></span><br><span class="line">        <span class="keyword">for</span> j,word <span class="keyword">in</span> enumerate(vocab): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the emission count for the (POS tag, word) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># Define the (POS tag, word) tuple for this row and column</span></span><br><span class="line">            key =  (all_tags[i], word)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the (POS tag, word) tuple exists as a key in emission counts</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> emis_keys: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">                <span class="comment"># Get the count of (POS tag, word) from the emission_counts d</span></span><br><span class="line">                count = emission_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the POS tag</span></span><br><span class="line">            count_tag = tag_counts[all_tags[i]]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Apply smoothing and store the smoothed value </span></span><br><span class="line">            <span class="comment"># into the emission matrix B for this row and column</span></span><br><span class="line">            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> B</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># creating your emission probability matrix. this takes a few minutes to run. </span></span><br><span class="line">B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"View Matrix position at row 0, column 0: <span class="subst">&#123;B[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"View Matrix position at row 3, column 1: <span class="subst">&#123;B[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.9</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Try viewing emissions for a few words in a sample dataframe</span></span><br><span class="line">cidx  = [<span class="string">'725'</span>,<span class="string">'adroitly'</span>,<span class="string">'engineers'</span>, <span class="string">'promoted'</span>, <span class="string">'synergy'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the integer ID for each word</span></span><br><span class="line">cols = [vocab[a] <span class="keyword">for</span> a <span class="keyword">in</span> cidx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose POS tags to show in a sample dataframe</span></span><br><span class="line">rvals =[<span class="string">'CD'</span>,<span class="string">'NN'</span>,<span class="string">'NNS'</span>, <span class="string">'VB'</span>,<span class="string">'RB'</span>,<span class="string">'RP'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># For each POS tag, get the row number from the 'states' list</span></span><br><span class="line">rows = [states.index(a) <span class="keyword">for</span> a <span class="keyword">in</span> rvals]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the emissions for the sample of words, and the sample of POS tags</span></span><br><span class="line">B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )</span><br><span class="line">print(B_sub)</span><br></pre></td></tr></table></figure><pre><code>View Matrix position at row 0, column 0: 0.000006032View Matrix position at row 3, column 1: 0.000000720              725      adroitly     engineers      promoted       synergyCD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07</code></pre><h5 id="Expected-Output-4"><a href="#Expected-Output-4" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">View Matrix position at row <span class="number">0</span>, column <span class="number">0</span>: <span class="number">0.000006032</span></span><br><span class="line">View Matrix position at row <span class="number">3</span>, column <span class="number">1</span>: <span class="number">0.000000720</span></span><br><span class="line">              <span class="number">725</span>      adroitly     engineers      promoted       synergy</span><br><span class="line">CD   <span class="number">8.201296e-05</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span></span><br><span class="line">NN   <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">2.257091e-05</span></span><br><span class="line">NNS  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span>  <span class="number">4.676203e-04</span>  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span></span><br><span class="line">VB   <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span></span><br><span class="line">RB   <span class="number">3.226454e-08</span>  <span class="number">6.456135e-05</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span></span><br><span class="line">RP   <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span></span><br></pre></td></tr></table></figure><p><a name="3"></a></p><h1 id="Part-3-Viterbi-Algorithm-and-Dynamic-Programming"><a href="#Part-3-Viterbi-Algorithm-and-Dynamic-Programming" class="headerlink" title="Part 3: Viterbi Algorithm and Dynamic Programming"></a>Part 3: Viterbi Algorithm and Dynamic Programming</h1><p>In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, <code>A</code> and <code>B</code> to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. </p><ul><li><strong>Initialization</strong> - In this part you initialize the <code>best_paths</code> and <code>best_probabilities</code> matrices that you will be populating in <code>feed_forward</code>.</li><li><strong>Feed forward</strong> - At each step, you calculate the probability of each path happening and the best paths up to that point. </li><li><strong>Feed backward</strong>: This allows you to find the best path with the highest probabilities. </li></ul><p><a name="3.1"></a></p><h2 id="Part-3-1-Initialization"><a href="#Part-3-1-Initialization" class="headerlink" title="Part 3.1:  Initialization"></a>Part 3.1:  Initialization</h2><p>You will start by initializing two matrices of the same dimension. </p><ul><li><p>best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.</p></li><li><p>best_paths: A matrix that helps you trace through the best possible path in the corpus. </p></li></ul><p><a name="ex-05"></a></p><h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions</strong>:<br>Write a program below that initializes the <code>best_probs</code> and the <code>best_paths</code> matrix. </p><p>Both matrices will be initialized to zero except for column zero of <code>best_probs</code>.  </p><ul><li>Column zero of <code>best_probs</code> is initialized with the assumption that the first word of the corpus was preceded by a start token (“—s—“). </li><li>This allows you to reference the <strong>A</strong> matrix for the transition probability</li></ul><p>Here is how to initialize column 0 of <code>best_probs</code>:</p><ul><li>The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\textrm{best_probs}[s_{idx}, i]$.</li><li>This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\mathbf{B}[i, vocab[corpus[0]]]$.</li><li>Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). </li><li><strong>vocab</strong> is a dictionary that returns the unique integer that refers to that particular word.</li></ul><p>Conceptually, it looks like this:<br>$\textrm{best_probs}[s_{idx}, i] = \mathbf{A}[s_{idx}, i] \times \mathbf{B}[i, corpus[0] ]$</p><p>In order to avoid multiplying and storing small values on the computer, we’ll take the log of the product, which becomes the sum of two logs:</p><p>$best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$</p><p>Also, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set $best_probs[i,0] = float(‘-inf’)$ when $A[s_{idx}, i] == 0$</p><p>So the implementation to initialize $best_probs$ looks like this:</p><p>$ if A[s_{idx}, i] &lt;&gt; 0 : best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])$</p><p>$ if A[s_{idx}, i] == 0 : best_probs[i,0] = float(‘-inf’)$</p><p>Please use <a href="https://docs.python.org/3/library/math.html" target="_blank" rel="noopener">math.log</a> to compute the natural logarithm.</p><p>The example below shows the initialization assuming the corpus starts with the phrase “Loss tracks upward”.</p><p><img src="Initialize4.png"></p><p>Represent infinity and negative infinity like this:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">float('inf')</span><br><span class="line">float('-inf')</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: initialize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(states, tag_counts, A, B, corpus, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        states: a list of all possible parts-of-speech</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        A: Transition Matrix of dimension (num_tags, num_tags)</span></span><br><span class="line"><span class="string">        B: Emission Matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">        corpus: a sequence of words whose POS is to be identified in a list </span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        best_probs: matrix of dimension (num_tags, len(corpus)) of floats</span></span><br><span class="line"><span class="string">        best_paths: matrix of dimension (num_tags, len(corpus)) of integers</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the total number of unique POS tags</span></span><br><span class="line">    num_tags = len(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_probs matrix </span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as the columns</span></span><br><span class="line">    best_probs = np.zeros((num_tags, len(corpus)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_paths matrix</span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as columns</span></span><br><span class="line">    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the start token</span></span><br><span class="line">    s_idx = states.index(<span class="string">"--s--"</span>)</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each of the POS tags</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_tags) : <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Handle the special case when the transition from start token to POS tag i is zero</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] == <span class="number">0</span>: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag 'i', column 0, to negative infinity</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = float(<span class="string">"-inf"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># For all other cases when transition from start token to POS tag i is non-zero:</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag 'i', column 0</span></span><br><span class="line">            <span class="comment"># Check the formula in the instructions above</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = math.log(A[s_idx,i])  +  math.log(B[i,vocab[corpus[<span class="number">0</span>]]])</span><br><span class="line">            </span><br><span class="line">                         </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the function</span></span><br><span class="line">print(<span class="string">f"best_probs[0,0]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>"</span>) </span><br><span class="line">print(<span class="string">f"best_paths[2,3]: <span class="subst">&#123;best_paths[<span class="number">2</span>,<span class="number">3</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>best_probs[0,0]: -22.6098best_paths[2,3]: 0.0000</code></pre><h5 id="Expected-Output-5"><a href="#Expected-Output-5" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">0</span>]: <span class="number">-22.6098</span></span><br><span class="line">best_paths[<span class="number">2</span>,<span class="number">3</span>]: <span class="number">0.0000</span></span><br></pre></td></tr></table></figure><p><a name="3.2"></a></p><h2 id="Part-3-2-Viterbi-Forward"><a href="#Part-3-2-Viterbi-Forward" class="headerlink" title="Part 3.2 Viterbi Forward"></a>Part 3.2 Viterbi Forward</h2><p>In this part of the assignment, you will implement the <code>viterbi_forward</code> segment. In other words, you will populate your <code>best_probs</code> and <code>best_paths</code> matrices.</p><ul><li>Walk forward through the corpus.</li><li>For each word, compute a probability for each possible tag. </li><li>Unlike the previous algorithm <code>predict_pos</code> (the ‘warm-up’ exercise), this will include the path up to that (word,tag) combination. </li></ul><p>Here is an example with a three-word corpus “Loss tracks upward”:</p><ul><li>Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. </li><li>In the diagram below, the first word “Loss” is already initialized. </li><li>The algorithm will compute a probability for each of the potential tags in the second and future words. </li></ul><p>Compute the probability that the tag of the second work (‘tracks’) is a verb, 3rd person singular present (VBZ).  </p><ul><li>In the <code>best_probs</code> matrix, go to the column of the second word (‘tracks’), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.</li><li>Examine each of the paths from the tags of the first word (‘Loss’) and choose the most likely path.  </li><li>An example of the calculation for <strong>one</strong> of those paths is the path from (‘Loss’, NN) to (‘tracks’, VBZ).</li><li>The log of the probability of the path up to and including the first word ‘Loss’ having POS tag NN is $-14.32$.  The <code>best_probs</code> matrix contains this value -14.32 in the column for ‘Loss’ and row for ‘NN’.</li><li>Find the probability that NN transitions to VBZ.  To find this probability, go to the <code>A</code> transition matrix, and go to the row for ‘NN’ and the column for ‘VBZ’.  The value is $4.37e-02$, which is circled in the diagram, so add $-14.32 + log(4.37e-02)$. </li><li>Find the log of the probability that the tag VBS would ‘emit’ the word ‘tracks’.  To find this, look at the ‘B’ emission matrix in row ‘VBZ’ and the column for the word ‘tracks’.  The value $4.61e-04$ is circled in the diagram below.  So add $-14.32 + log(4.37e-02) + log(4.61e-04)$.</li><li>The sum of $-14.32 + log(4.37e-02) + log(4.61e-04)$ is $-25.13$. Store $-25.13$ in the <code>best_probs</code> matrix at row ‘VBZ’ and column ‘tracks’ (as seen in the cell that is highlighted in light orange in the diagram).</li><li>All other paths in best_probs are calculated.  Notice that $-25.13$ is greater than all of the other values in column ‘tracks’ of matrix <code>best_probs</code>, and so the most likely path to ‘VBZ’ is from ‘NN’.  ‘NN’ is in row 20 of the <code>best_probs</code> matrix, so $20$ is the most likely path.</li><li>Store the most likely path $20$ in the <code>best_paths</code> table.  This is highlighted in light orange in the diagram below.</li></ul><p>The formula to compute the probability and path for the $i^{th}$ word in the $corpus$, the prior word $i-1$ in the corpus, current POS tag $j$, and previous POS tag $k$ is:</p><p>$\mathrm{prob} = \mathbf{best_prob}_{k, i-1} + \mathrm{log}(\mathbf{A}_{k, j}) + \mathrm{log}(\mathbf{B}_{j, vocab(corpus_{i})})$</p><p>where $corpus_{i}$ is the word in the corpus at index $i$, and $vocab$ is the dictionary that gets the unique integer that represents a given word.</p><p>$\mathrm{path} = k$</p><p>where $k$ is the integer representing the previous POS tag.</p><p><a name="ex-06"></a></p><h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p>Instructions: Implement the <code>viterbi_forward</code> algorithm and store the best_path and best_prob for every possible tag for each word in the matrices <code>best_probs</code> and <code>best_tags</code> using the pseudo code below.</p><p>`for each word in the corpus</p><pre><code>for each POS tag type that this word may be    for POS tag type that the previous word could be        compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.        retain the highest probability computed for the current word        set best_probs to this highest probability        set best_paths to the index &#39;k&#39;, representing the POS tag of the previous word which produced the highest probability `</code></pre><p>Please use <a href="https://docs.python.org/3/library/math.html" target="_blank" rel="noopener">math.log</a> to compute the natural logarithm.</p><p><img src="Forward4.png"></p><h2 id><a href="#" class="headerlink" title></a><details></details></h2><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember that when accessing emission matrix B, the column index is the unique integer ID associated with the word.  It can be accessed by using the 'vocab' dictionary, where the key is the word, and the value is the unique integer ID for that word.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_forward</span><span class="params">(A, B, test_corpus, best_probs, best_paths, vocab)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        A, B: The transiton and emission matrices respectively</span></span><br><span class="line"><span class="string">        test_corpus: a list containing a preprocessed corpus</span></span><br><span class="line"><span class="string">        best_probs: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        best_probs: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the number of unique POS tags (which is the num of rows in best_probs)</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through every word in the corpus starting from word 1</span></span><br><span class="line">    <span class="comment"># Recall that word 0 was initialized in `initialize()`</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(test_corpus)): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print number of words processed, every 5000 words</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">5000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Words processed: &#123;:&gt;8&#125;"</span>.format(i))</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###</span></span><br><span class="line">        <span class="comment"># For each unique POS tag that the current word can be</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_prob for word i to negative infinity</span></span><br><span class="line">            best_prob_i = float(<span class="string">"-inf"</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_path for current word i to None</span></span><br><span class="line">            best_path_i = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># For each POS tag that the previous word can be:</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Calculate the probability = </span></span><br><span class="line">                <span class="comment"># best probs of POS tag k, previous word i-1 + </span></span><br><span class="line">                <span class="comment"># log(prob of transition from POS k to POS j) + </span></span><br><span class="line">                <span class="comment"># log(prob that emission of POS j is word i)</span></span><br><span class="line">                prob = best_probs[k, i<span class="number">-1</span>] + math.log(A[k,j]) + math.log(B[j, vocab[test_corpus[i]]])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if this path's probability is greater than</span></span><br><span class="line">                <span class="comment"># the best probability up to and before this point</span></span><br><span class="line">                <span class="keyword">if</span> prob &gt; best_prob_i: <span class="comment"># complete this line</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Keep track of the best probability</span></span><br><span class="line">                    best_prob_i = prob</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># keep track of the POS tag of the previous word</span></span><br><span class="line">                    <span class="comment"># that is part of the best path.  </span></span><br><span class="line">                    <span class="comment"># Save the index (integer) associated with </span></span><br><span class="line">                    <span class="comment"># that previous word's POS tag</span></span><br><span class="line">                    best_path_i = k</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Save the best probability for the </span></span><br><span class="line">            <span class="comment"># given current word's POS tag</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus</span></span><br><span class="line">            best_probs[j,i] = best_prob_i</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the unique integer ID of the previous POS tag</span></span><br><span class="line">            <span class="comment"># into best_paths matrix, for the POS tag of the current word</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus.</span></span><br><span class="line">            best_paths[j,i] = best_path_i</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure><p>Run the <code>viterbi_forward</code> function to fill in the <code>best_probs</code> and <code>best_paths</code> matrices.</p><p><strong>Note</strong> that this will take a few minutes to run.  There are about 30,000 words to process.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this will take a few minutes to run =&gt; processes ~ 30,000 words</span></span><br><span class="line">best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)</span><br></pre></td></tr></table></figure><pre><code>Words processed:     5000Words processed:    10000Words processed:    15000Words processed:    20000Words processed:    25000Words processed:    30000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test this function </span></span><br><span class="line">print(<span class="string">f"best_probs[0,1]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>"</span>) </span><br><span class="line">print(<span class="string">f"best_probs[0,4]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">4</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>best_probs[0,1]: -24.7822best_probs[0,4]: -49.5601</code></pre><h5 id="Expected-Output-6"><a href="#Expected-Output-6" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">1</span>]: <span class="number">-24.7822</span></span><br><span class="line">best_probs[<span class="number">0</span>,<span class="number">4</span>]: <span class="number">-49.5601</span></span><br></pre></td></tr></table></figure><p><a name="3.3"></a></p><h2 id="Part-3-3-Viterbi-backward"><a href="#Part-3-3-Viterbi-backward" class="headerlink" title="Part 3.3 Viterbi backward"></a>Part 3.3 Viterbi backward</h2><p>Now you will implement the Viterbi backward algorithm.</p><ul><li>The Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the <code>best_paths</code> and the <code>best_probs</code> matrices.</li></ul><p>The example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: “Loss tracks upward”.</p><p>POS tag for ‘upward’ is <code>RB</code></p><ul><li>Select the the most likely POS tag for the last word in the corpus, ‘upward’ in the <code>best_prob</code> table.</li><li>Look for the row in the column for ‘upward’ that has the largest probability.</li><li>Notice that in row 28 of <code>best_probs</code>, the estimated probability is -34.99, which is larger than the other values in the column.  So the most likely POS tag for ‘upward’ is <code>RB</code> an adverb, at row 28 of <code>best_prob</code>. </li><li>The variable <code>z</code> is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus.  In array z, at position 2, store the value 28 to indicate that the word ‘upward’ (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is <code>RB</code>).</li><li>The variable <code>pred</code> contains the POS tags in string form.  So <code>pred</code> at index 2 stores the string <code>RB</code>.</li></ul><p>POS tag for ‘tracks’ is <code>VBZ</code></p><ul><li>The next step is to go backward one word in the corpus (‘tracks’).  Since the most likely POS tag for ‘upward’ is <code>RB</code>, which is uniquely identified by integer ID 28, go to the <code>best_paths</code> matrix in column 2, row 28.  The value stored in <code>best_paths</code>, column 2, row 28 indicates the unique ID of the POS tag of the previous word.  In this case, the value stored here is 40, which is the unique ID for POS tag <code>VBZ</code> (verb, 3rd person singular present).</li><li>So the previous word at index 1 of the corpus (‘tracks’), most likely has the POS tag with unique ID 40, which is <code>VBZ</code>.</li><li>In array <code>z</code>, store the value 40 at position 1, and for array <code>pred</code>, store the string <code>VBZ</code> to indicate that the word ‘tracks’ most likely has POS tag <code>VBZ</code>.</li></ul><p>POS tag for ‘Loss’ is <code>NN</code></p><ul><li>In <code>best_paths</code> at column 1, the unique ID stored at row 40 is 20.  20 is the unique ID for POS tag <code>NN</code>.</li><li>In array <code>z</code> at position 0, store 20.  In array <code>pred</code> at position 0, store <code>NN</code>.</li></ul><p><img src="Backwards5.png"></p><p><a name="ex-07"></a></p><h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Implement the <code>viterbi_backward</code> algorithm, which returns a list of predicted POS tags for each word in the corpus.</p><ul><li>Note that the numbering of the index positions starts at 0 and not 1. </li><li><code>m</code> is the number of words in the corpus.  <ul><li>So the indexing into the corpus goes from <code>0</code> to <code>m - 1</code>.</li><li>Also, the columns in <code>best_probs</code> and <code>best_paths</code> are indexed from <code>0</code> to <code>m - 1</code></li></ul></li></ul><p><strong>In Step 1:</strong><br>Loop through all the rows (POS tags) in the last entry of <code>best_probs</code> and find the row (POS tag) with the maximum value.<br>Convert the unique integer ID to a tag (a string representation) using the dictionary <code>states</code>.  </p><p>Referring to the three-word corpus described above:</p><ul><li><code>z[2] = 28</code>: For the word ‘upward’ at position 2 in the corpus, the POS tag ID is 28.  Store 28 in <code>z</code> at position 2.</li><li>states(28) is ‘RB’: The POS tag ID 28 refers to the POS tag ‘RB’.</li><li><code>pred[2] = &#39;RB&#39;</code>: In array <code>pred</code>, store the POS tag for the word ‘upward’.</li></ul><p><strong>In Step 2:</strong>  </p><ul><li>Starting at the last column of best_paths, use <code>best_probs</code> to find the most likely POS tag for the last word in the corpus.</li><li>Then use <code>best_paths</code> to find the most likely POS tag for the previous word. </li><li>Update the POS tag for each word in <code>z</code> and in <code>preds</code>.</li></ul><p>Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.<br><code>z[1] = best_paths[z[2],2]</code>  </p><p>The small test following the routine prints the last few words of the corpus and their states to aid in debug.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_backward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_backward</span><span class="params">(best_probs, best_paths, corpus, states)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function returns the best path.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Get the number of words in the corpus</span></span><br><span class="line">    <span class="comment"># which is also the number of columns in best_probs, best_paths</span></span><br><span class="line">    m = best_paths.shape[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize array z, same length as the corpus</span></span><br><span class="line">    z = [<span class="keyword">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of unique POS tags</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the best probability for the last word</span></span><br><span class="line">    best_prob_for_last_word = float(<span class="string">'-inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize pred array, same length as corpus</span></span><br><span class="line">    pred = [<span class="keyword">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">    <span class="comment">## Step 1 ##</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each POS tag for the last word (last column of best_probs)</span></span><br><span class="line">    <span class="comment"># in order to find the row (POS tag integer ID) </span></span><br><span class="line">    <span class="comment"># with highest probability for the last word</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_tags): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the probability of POS tag at row k </span></span><br><span class="line">        <span class="comment"># is better than the previosly best probability for the last word:</span></span><br><span class="line">        <span class="keyword">if</span> best_probs[k,<span class="number">-1</span>] &gt; best_prob_for_last_word: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Store the new best probability for the lsat word</span></span><br><span class="line">            best_prob_for_last_word = best_probs[k,<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># Store the unique integer ID of the POS tag</span></span><br><span class="line">            <span class="comment"># which is also the row number in best_probs</span></span><br><span class="line">            z[m - <span class="number">1</span>] = k</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># Convert the last word's predicted POS tag</span></span><br><span class="line">    <span class="comment"># from its unique integer ID into the string representation</span></span><br><span class="line">    <span class="comment"># using the 'states' dictionary</span></span><br><span class="line">    <span class="comment"># store this in the 'pred' array for the last word</span></span><br><span class="line">    pred[m - <span class="number">1</span>] = states[k]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2 ##</span></span><br><span class="line">    <span class="comment"># Find the best POS tags by walking backward through the best_paths</span></span><br><span class="line">    <span class="comment"># From the last word in the corpus to the 0th word in the corpus</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the unique integer ID of</span></span><br><span class="line">        <span class="comment"># the POS tag for the word at position 'i' in the corpus</span></span><br><span class="line">        pos_tag_for_word_i = best_paths[z[i], i]</span><br><span class="line"><span class="comment">#         print(z[i])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># In best_paths, go to the row representing the POS tag of word i</span></span><br><span class="line">        <span class="comment"># and the column representing the word's position in the corpus</span></span><br><span class="line">        <span class="comment"># to retrieve the predicted POS for the word at position i-1 in the corpus</span></span><br><span class="line">        z[i - <span class="number">1</span>] = pos_tag_for_word_i</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the previous word's POS tag in string form</span></span><br><span class="line">        <span class="comment"># Use the 'states' dictionary, </span></span><br><span class="line">        <span class="comment"># where the key is the unique integer ID of the POS tag,</span></span><br><span class="line">        <span class="comment"># and the value is the string representation of that POS tag</span></span><br><span class="line">        pred[i - <span class="number">1</span>] = states[pos_tag_for_word_i]        </span><br><span class="line">     <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run and test your function</span></span><br><span class="line">pred = viterbi_backward(best_probs, best_paths, prep, states)</span><br><span class="line">m=len(pred)</span><br><span class="line">print(<span class="string">'The prediction for pred[-7:m-1] is: \n'</span>, prep[<span class="number">-7</span>:m<span class="number">-1</span>], <span class="string">"\n"</span>, pred[<span class="number">-7</span>:m<span class="number">-1</span>], <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">'The prediction for pred[0:8] is: \n'</span>, pred[<span class="number">0</span>:<span class="number">7</span>], <span class="string">"\n"</span>, prep[<span class="number">0</span>:<span class="number">7</span>])</span><br></pre></td></tr></table></figure><pre><code>The prediction for pred[-7:m-1] is:  [&#39;see&#39;, &#39;them&#39;, &#39;here&#39;, &#39;with&#39;, &#39;us&#39;, &#39;.&#39;]  [&#39;VB&#39;, &#39;PRP&#39;, &#39;RB&#39;, &#39;IN&#39;, &#39;PRP&#39;, &#39;.&#39;] The prediction for pred[0:8] is:  [&#39;DT&#39;, &#39;NN&#39;, &#39;POS&#39;, &#39;NN&#39;, &#39;MD&#39;, &#39;VB&#39;, &#39;VBN&#39;]  [&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;]</code></pre><p><strong>Expected Output:</strong>   </p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The prediction <span class="keyword">for</span> prep[<span class="number">-7</span>:m<span class="number">-1</span>] is:  </span><br><span class="line"> ['see', 'them', 'here', 'with', 'us', '.']  </span><br><span class="line"> ['VB', 'PRP', 'RB', 'IN', 'PRP', '.']   </span><br><span class="line">The prediction <span class="keyword">for</span> pred[<span class="number">0</span>:<span class="number">8</span>] is:    </span><br><span class="line"> ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN']   </span><br><span class="line"> ['The', 'economy', "'s", 'temperature', 'will', 'be', 'taken']</span><br></pre></td></tr></table></figure><p>Now you just have to compare the predicted labels to the true labels to evaluate your model on the accuracy metric!</p><p><a name="4"></a></p><h1 id="Part-4-Predicting-on-a-data-set"><a href="#Part-4-Predicting-on-a-data-set" class="headerlink" title="Part 4: Predicting on a data set"></a>Part 4: Predicting on a data set</h1><p>Compute the accuracy of your prediction by comparing it with the true <code>y</code> labels. </p><ul><li><code>pred</code> is a list of predicted POS tags corresponding to the words of the <code>test_corpus</code>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The third word is:'</span>, prep[<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'Your prediction is:'</span>, pred[<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'Your corresponding label y is: '</span>, y[<span class="number">3</span>])</span><br></pre></td></tr></table></figure><pre><code>The third word is: temperatureYour prediction is: NNYour corresponding label y is:  temperature NN</code></pre><p><a name="ex-08"></a></p><h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p>Implement a function to compute the accuracy of the viterbi algorithm’s POS tag predictions.</p><ul><li>To split y into the word and its tag you can use <code>y.split()</code>. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        pred: a list of the predicted parts-of-speech </span></span><br><span class="line"><span class="string">        y: a list of lines where each word is separated by a '\t' (i.e. word \t tag)</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zip together the prediction and the labels</span></span><br><span class="line">    <span class="keyword">for</span> prediction, y <span class="keyword">in</span> zip(pred, y):</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of 'None' with your code) ###</span></span><br><span class="line">        <span class="comment"># Split the label into the word and the POS tag</span></span><br><span class="line">        word_tag_tuple = y.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check that there is actually a word and a tag</span></span><br><span class="line">        <span class="comment"># no more and no less than 2 items</span></span><br><span class="line">        <span class="keyword">if</span> len(word_tag_tuple) &lt; <span class="number">2</span>: <span class="comment"># complete this line</span></span><br><span class="line">            <span class="keyword">continue</span> </span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># store the word and tag separately</span></span><br><span class="line">        word, tag = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> word_tag_tuple]</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(tag, prediction)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if the POS tag label matches the prediction</span></span><br><span class="line">        <span class="keyword">if</span> tag == prediction: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># count the number of times that the prediction</span></span><br><span class="line">            <span class="comment"># and label match</span></span><br><span class="line">            num_correct += <span class="number">1.0</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># keep track of the total number of examples (that have valid labels)</span></span><br><span class="line">        total += <span class="number">1.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> num_correct/total</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Accuracy of the Viterbi algorithm is <span class="subst">&#123;compute_accuracy(pred, y):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Accuracy of the Viterbi algorithm is 0.9531</code></pre><h5 id="Expected-Output-7"><a href="#Expected-Output-7" class="headerlink" title="Expected Output"></a>Expected Output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of the Viterbi algorithm is <span class="number">0.9531</span></span><br></pre></td></tr></table></figure><p>Congratulations you were able to classify the parts-of-speech with 95% accuracy. </p><h3 id="Key-Points-and-overview"><a href="#Key-Points-and-overview" class="headerlink" title="Key Points and overview"></a>Key Points and overview</h3><p>In this assignment you learned about parts-of-speech tagging. </p><ul><li>In this assignment, you predicted POS tags by walking forward through a corpus and knowing the previous word.</li><li>There are other implementations that use bidirectional POS tagging.</li><li>Bidirectional POS tagging requires knowing the previous word and the next word in the corpus when predicting the current word’s POS tag.</li><li>Bidirectional POS tagging would tell you more about the POS instead of just knowing the previous word. </li><li>Since you have learned to implement the unidirectional approach, you have the foundation to implement other POS taggers used in industry.</li></ul><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul><li><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">“Speech and Language Processing”, Dan Jurafsky and James H. Martin</a></li><li>We would like to thank Melanie Tosik for her help and inspiration</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-2-Parts-of-Speech-Tagging-POS&quot;&gt;&lt;a href=&quot;#Assignment-2-Parts-of-Speech-Tagging-POS&quot; class=&quot;headerlink&quot; title=&quot;Assignment 2
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>ML-Interview-Computer-Vision</title>
    <link href="https://zhangruochi.com/ML-Interview-Computer-Vision/2020/05/29/"/>
    <id>https://zhangruochi.com/ML-Interview-Computer-Vision/2020/05/29/</id>
    <published>2020-05-28T23:41:05.000Z</published>
    <updated>2020-05-29T04:43:09.902Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>为什么输入网络前要对图像做归一化？</p><blockquote><ol><li>把不同的图片映射到同一坐标系，使其具有相同的尺度及相似的特征分布。</li><li>一定程度上消除了过度曝光，质量不佳或者噪声等各种原因对模型权值更新的影响。</li><li>加快gradient更新的收敛速度。</li></ol></blockquote></li><li><p>权重初始化方法有哪些？</p><blockquote><ol><li>Small random numbers (gaussian with zero mean and 1e-2 standard deviation):  Works okay for small networks, but problems with deeper networks.<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%"></center></li><li><strong>Xavier</strong>: 基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0. 初始化方差为:  <code>std = sqrt(node_in)</code>. 参考 <a href="https://zhuanlan.zhihu.com/p/27919794" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27919794</a></li><li><strong>Kaiming</strong>: 在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2. 初始化方差为: <code>std = sqrt(node_in / 2)</code></li></ol></blockquote></li><li><p>说说 FCN 的基本思想.</p><blockquote><p>FCN对图像进行像素级的分类，从而解决了语义级别的图像分割问题。与经典的CNN在卷积层使用全连接层得到固定长度的特征向量进行分类不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷基层的特征图（feature map）进行上采样，使它恢复到输入图像相同的尺寸，从而可以对每一个像素都产生一个预测，同时保留了原始输入图像中的空间信息，最后在上采样的特征图进行像素的分类。</p></blockquote></li><li><p>什么是转置卷积?</p><blockquote><p>事实上，卷积运算还可以通过矩阵乘法来实现.假设我们定义高和宽分别为4的输入X，以及高和宽分别为3的卷积核K, 卷积运算输出高和宽分别为2.</p><script type="math/tex; mode=display">\frac{h(w) - k + 2p}{ s } + 1</script><p>我们将卷积核K改写成含有大量零元素的稀疏矩阵W，即权重矩阵。权重矩阵的形状为(4, 16)， 其中的非零元素来自卷积核K中的元素。将输入X逐行连结，得到⻓度为16的向量。然后将W与向量化的X做矩阵乘法，得到⻓度为4的向量。对其变形后，我们可以得到和上面卷积运算相同的结 果。可⻅，我们在这个例子中使用矩阵乘法实现了卷积运算。<br>现在我们从<strong>矩阵乘法</strong>的⻆度来描述卷积运算。设输入向量为$x$，权重矩阵为$W$，卷积的前向计算 函数的实现可以看作将函数输入乘以权重矩阵，并输出向量$y=Wx$.们知道，反向传播需要依据链式法则。由于$\triangledown_x y = W^T$，卷积的反向传播函数的实现可以看作将函数输入乘以转置后的权重矩阵$W^T$。而转置卷积层正好交换了卷积层的前向计算函数与反向传播函数:转置卷积层的这两个函数可以看作将函数输入向量分别乘以$W^T$ 和 $W$.<br>不难想象，转置卷积层可以用来交换卷积层输入和输出的形状。让我们继续用矩阵乘法描述卷积。设权重矩阵是形状为4 × 16的矩阵，对于⻓度为16的输入向量，卷积前向计算输出⻓度为4的向量。假如输入向量的⻓度为4，转置权重矩阵的形状为16 × 4，那么转置卷积层将输出⻓度为16的向量。在模型设计中，转置卷积层常用于将较小的特征图变换为更大的特征图。在全卷积网络中，当输入是高和宽较小的特征图时，转置卷积层可以用来将高和宽放大到输入图像的尺寸。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%"></center></blockquote></li><li><p>什么是空洞卷积（Dilated convolution）?</p><blockquote><ol><li>从kernel（卷积核）角度：相当于在标准概念的kernel（卷积核）中，相邻点之间添加rate-1个0，然后使用扩张后的kernel（卷积核）与原图进行卷积。如下图rate=2，相当于标准的3<em>3卷积核变为5</em>5卷积核，每一行中间添加2-1个0</li><li>从原图角度：使用标准概念的kernel（卷积核）在原图中每隔rate-1进行像素点卷积采样。如下图rate=2，在原图中每隔rate-1进行卷积。<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%"></center></li></ol></blockquote></li><li><p>解释下 Unet 的 Architecture.</p><blockquote><p>Unet 使用 encoder 和 decoder 的架构，在encoder下采样4次，一共下采样16倍。对称地，其decoder也相应上采样4次，将encoder得到的高级语义特征图恢复到原图片的分辨率。相比于FCN和Deeplab等，UNet共进行了4次上采样，并在同一个stage使用了skip connection，而不是直接在高级语义特征上进行监督和loss反传，这样就保证了最后恢复出来的特征图融合了更多的low-level的feature，也使得不同scale的feature得到了的融合。</p></blockquote></li><li><p>解释下 FPN 网络.</p><blockquote><p>一个自底向上的线路，一个自顶向下的线路，横向连接（lateral connection）。图中放大的区域就是横向连接，这里1 * 1的卷积核的主要作用是减少channel的数量，也就是减少了feature map的个数，并不改变feature map的尺寸大小。</p><ol><li><strong>自底向上</strong>其实就是网络的前向过程。在前向过程中，feature map的大小在经过某些层后会改变，而在经过其他一些层的时候不会改变，作者将不改变feature map大小的层归为一个stage，因此每次抽取的特征都是每个stage的最后一个层输出，这样就能构成特征金字塔。 </li><li><strong>自顶向下</strong>的过程采用上采样（upsampling）进行，而横向连接则是将上采样的结果和自底向上生成的相同大小的feature map进行融合（merge）。在融合之后还会再采用3 * 3的卷积核对每个融合结果进行卷积，目的是消除上采样的混叠效应（aliasing effect）。并假设生成的feature map结果是P2，P3，P4，P5，和原来自底向上的卷积结果C2，C3，C4，C5一一对应。<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%"></center></li></ol></blockquote></li><li><p>什么是Anchors？</p><blockquote><p>参考 <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a></p></blockquote></li><li><p>解释 ROI Pooling 和 ROI Align 的区别.</p><blockquote><p>对于一个region proposal，首先从原图经过全卷积网络到特征图，得到的候选框位置可能存在浮点数，进行取整操作从而出现第一次量化；其次，在ROI Pooling求取每个小网格的位置时也同样存在浮点数取整的情况。这两次量化的结果都使得候选框的位置会出现偏差，在论文里，作者把它总结为“不匹配问题（misalignment）<br>。为了解决ROI Pooling的上述缺点，ROI Align提出改进的方法。ROI Align的思路是：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作</p></blockquote></li><li><p>请解释下two stage object detection 的发展脉络。</p><blockquote><p>参考 <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a><br>讲解 R-CNN, Fast R-CNN, Faster RCNN, Mask R-CNN 的发展轨迹。</p></blockquote></li><li><p>请解释下one stage object detection 的发展脉络。</p><blockquote><p>参考 <a href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/">https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</a><br>讲解YOLO1,YOLO2,YOLO3,YOLO4</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      FCN, Unet, Mask R-CNN, YOLO
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Natural-Language-Processing</title>
    <link href="https://zhangruochi.com/ML-Interview-Natural-Language-Processing/2020/05/28/"/>
    <id>https://zhangruochi.com/ML-Interview-Natural-Language-Processing/2020/05/28/</id>
    <published>2020-05-28T07:14:07.000Z</published>
    <updated>2020-05-28T22:50:59.616Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><ol><li><p>简述 Transformer 提出的背景。</p><blockquote><ol><li>seq2seq 处理长期依赖仍然是一个挑战。</li><li>seq2seq 模型架构的顺序特性阻止了并行化。</li></ol></blockquote></li><li><p>什么是self-attention机制？ 请举例来说明。</p><blockquote><p>参考 <a href="https://mp.weixin.qq.com/s/8Kic83oCoiKzAKe-dvRUvw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/8Kic83oCoiKzAKe-dvRUvw</a></p><ol><li>输入X, 通过3个线性转换把X转换为Q,K,V. 有了K,Q,V 三个特征向量，就可以做attention. </li><li>用每个单词的 query 向量与其自身及其他单词的 key 向量做dot product. 得到权重的分布表示。<br>假设有两个单词，Thinking, Machines. 通过嵌入变换会$X_1$,$X_2$两个向量。分别与$Wq$, $W_k$,$W_v$三个矩阵想做点乘得到，{q1,q2},{k1,k2},{v1,v2} 6个向量。 然后{q1,k1} 做点乘得到得分$score_1$, {q1,k2}做点乘得到$score_2$。对上述 socre 进行规范化，然后 softmax 得到权重 $[w_1,w_2]$. </li><li>对 valuex 向量求加权平均。用权重向量$[w_1,w_2]$ 乘以$[v1,v2]$值得到一个加权后的值. <script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{Q,K^T}{\sqrt{d_k}})V</script><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></li></ol></blockquote></li><li><p>encoder 中的self-attention 与 decoder 中 masked self-attention 有什么区别？</p><blockquote><p>encoder 中，$QK^T$ 会组成一个word2word的attention map. 是一个方阵. 比如说你的输入是一句话 “i have a dream” 总共4个单词， 这里就会形成一张4x4的注意力机制的图.</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center>这里的masked就是要在做language modelling（或者像翻译）的时候，不给模型看到未来的信息。此时的 attention map 是一个下三角矩阵。<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%"></center></blockquote></li><li><p>什么是 Multi-Head Attention？</p><blockquote><p>Multi-Head Attention就是把self-attention 做 N 次，然后把 N 个 heads concatenate 在一起。最后再做一个线性变换。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="50%" height="50%"></center></blockquote></li><li><p>Batch Norm 和 Layer Norm 的区别是什么？</p><blockquote><p>BN并不适用于RNN等动态网络和batchsize较小的时候效果不好。Layer Normalization的提出有效的解决BN的这两个问题。LN和BN不同点是归一化的维度是互相垂直的. N表示样本轴，C 表示通道轴，F 是每个通道的特征数量。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="50%" height="50%"></center></blockquote></li><li><p>什么是transformer 中的 Encoder-Decoder Attention？</p><blockquote><p>在decoder中，Transformer block比编码器中多了个encoder-cecoder attention。在encoder-decoder attention中，Q 来自于 decoder 的上一个输出，K,V 来自于 encoder 的输出.</p></blockquote></li><li><p>什么是Positional Encoding？</p><blockquote><p>参考 <a href="https://zhuanlan.zhihu.com/p/95079337" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/95079337</a><br>Transformer抛弃了RNN，而RNN最大的优点就是在时间序列上对数据的抽象，所以文章中作者提出两种Positional Encoding的方法，将encoding后的数据与embedding数据求和，加入了相对位置信息。</p></blockquote></li><li><p>简述 Transformer 的架构。</p><blockquote><p>Transformer 包含 Encoder Part and Decoder Part. Encoder Part 比较重要的是 Self-Attention. Decoder Part 比较重要的是 Masked Multi-Head Attention，Encoder-Decoder Attention. 同时，encoder 和 decoder 都包含 Feed Forward, Residuals 以及 Layer Norm. </p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="80%" height="80%"></center></blockquote></li></ol><h2 id="ELMo-OpenAI-GPT-BERT"><a href="#ELMo-OpenAI-GPT-BERT" class="headerlink" title="ELMo,OpenAI GPT,BERT"></a>ELMo,OpenAI GPT,BERT</h2><blockquote><p>参考 <a href="https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/">https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/</a></p></blockquote><ol><li><p>解释下 ELMO 的思想.</p><blockquote><p>ELMO 的意思是 embedding from language model. word2vec 最大的问题是在训练好后，词向量在任何context的情况下都是不变的。而实际上我们知道单词的意思随着语境的变化而变化。ELMO 训练了一个双向的 LSTM, 然后将 hidden layer concate 在一起。对于L层的双向lstm语言模型，每个单词一共有2L+1个表征（representations）.最后根据具体的任务，将2L+1个表征加权平均在一起。</p></blockquote></li><li><p>解释下OpenAI Transformer.</p><blockquote><p>OpenAI Pre-training a Transformer Decoder for Language Modeling.</p></blockquote></li><li><p>Bert 是怎样实现 mask 的？</p><blockquote><ul><li>MLM：将完整句子中的部分字mask，预测该mask词</li><li>NSP：为每个训练前的例子选择句子 A 和 B 时，50% 的情况下 B 是真的在 A 后面的下一个句子， 50% 的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句</li></ul></blockquote></li><li><p>在数据中随机mask15%的token，其中80%被换位[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？</p><blockquote><p>Bert随机mask语料中15%的token，然后预测masked token，那么masked token 位置输出的final hidden vectors喂给softmax网络即可得到maskedtoken的预测结果。这样操作存在一个问题，fine-tuning的时候没有[MASK]token，因此存在pre-training和fine-tuning之间的mismatch，为了解决这个问题，采用了下面的策略：</p><ol><li>80%的时间中：将选中的词用[MASK]token来代替，例如<br>my dog is hairy → my dog is [MASK]</li><li>10%的时间中：将选中的词用任意的词来进行代替，例如<br>my dog is hairy → my dog is apple</li><li>10%的时间中：选中的词不发生变化，例如<br>my dog is hairy → my dog is hairy</li></ol></blockquote></li><li><p>为什么BERT有3个嵌入层，它们都是如何实现的？</p><blockquote><ul><li>input_id是语义表达，和传统的w2v一样，方法也一样的lookup</li><li>segment_id是辅助BERT区别句子对中的两个句子的向量表示，从[1,embedding_size]里面lookup</li><li>position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从[511,embedding_size]里面lookup</li></ul></blockquote></li><li><p>Bert的损失函数？</p><blockquote><ol><li>MLM:在 encoder 的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用 softmax 计算mask中每个单词的概率</li><li>NSP:用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量,用 softmax 计算 IsNextSequence 的概率</li><li>MLM+NSP即为最后的损失</li></ol></blockquote></li><li><p>elmo、GPT、bert三者之间有什么区别？</p><blockquote><ol><li>特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。</li><li>单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。</li><li>GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      Natural language processing
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Ensemble</title>
    <link href="https://zhangruochi.com/ML-Interview-Ensemble/2020/05/28/"/>
    <id>https://zhangruochi.com/ML-Interview-Ensemble/2020/05/28/</id>
    <published>2020-05-28T02:07:46.000Z</published>
    <updated>2020-05-28T22:45:39.171Z</updated>
    
    <content type="html"><![CDATA[<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><ol><li><p>集成学习分哪几种，他们有何异同？</p><blockquote><ol><li>Boosting: 采用串行的方式，各个基础分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终的结果。</li><li>Bagging: 采用并行的方式，各个基分类器之间无依赖。其中比较著名的算法之一是基于决策树的随机森林。为了让及分类器之间互相独立，将训练集分成若干子集（当训练样本较少时，子集之间有重叠）。在最终的决策阶段，每个个体单独做出判断，然后通过投票的方式做出最后的集体决策。</li></ol></blockquote></li><li><p>集成学习是如何提高基分类器的性能的？</p><blockquote><p>基分类器的误差，是方差和偏差两种错误之和。偏差源于underfitting，方差源于overfitting. Boosting 方法通过逐步聚焦于基分类器分错的样本，减少集成分类器的偏差。Bagging通过对训练样本进行多次采样，分别训练多个不同的模型，然后做综合，来减少分类器的方差。</p></blockquote></li><li><p>集成学习的有哪些基本步骤？请以 Adaboosting 来举例。</p><blockquote><ol><li>initialize equal weights for all samples<script type="math/tex; mode=display">\alpha_{i} = \frac{1}{N}</script></li><li>Repeat t = 1,…,T<ul><li>learn $f_{t}(x)$ with data weights $\alpha_{i}$</li><li>compute weighted error<script type="math/tex; mode=display">weighted_{error_{t}} = \sum_{i=1}^{m}\alpha_{i}I(y_{i} \neq f_{t}(x_{i}))</script></li><li>compute coefficient <script type="math/tex; mode=display">\hat{w_{t}} = \frac{1}{2}\ln(\frac{1 - weighted_{error_{t}} }{weighted_{error_{t}}})</script><ul><li>$\hat{w_{t}}$ is higher when weighted_error is larger</li></ul></li><li>recomputed weights $\alpha_{i}$<script type="math/tex; mode=display">\alpha_{i} =      \begin{equation}    \left\{     \begin{array}{lr}      \alpha_{i}e^{-\hat{w_{t}}} \quad if \ f_t(x_i) = y_i & \\       \alpha_{i}e^{\hat{w_{t}}}  \quad if \ f_t(x_i) \neq y_i  &     \end{array}     \right.     \end{equation}</script></li><li>Normalize weights $\alpha_{i}$<ul><li>if $x_{i}$ often mistake, weight $\alpha_{i}$ gets very large</li><li>if $x_{i}$ often correct, weight $\alpha_{i}$ gets very small<script type="math/tex; mode=display">\alpha_{i} = \frac{\alpha_{i}}{\sum_{i}^{m}\alpha_{i}}</script></li></ul></li></ul></li><li>In the testing time, the final prediction is:<script type="math/tex; mode=display">\hat{y_{t}} = sign( \sum_1^T \hat{w_{t}} f_t(x) )</script></li></ol></blockquote></li><li><p>常用的基分类器是什么？</p><blockquote><p>常用决策树作为基分类器，主要有以下几方面的原因</p><ol><li>决策树的表达能力和泛化能力，可以通过调节树的层数来方便实现。</li><li>数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树随机性较大，这样”不稳定的学习器”更适合作为基分类器。</li><li>决策树在节点分裂时，随机地选择一个特征子集，从中找出最优分裂特征，很好地引入了随机性。<br>神经网络模型也适合作为基分类器，因为神经网络也是比较”不稳定的”。还可以通过调整神经元的数量，连接方式，网络层数，初始权重引入随机性。</li></ol></blockquote></li><li><p>在随机森林中，可否使用线性分类器或者K-近邻作为基分类器？</p><blockquote><p>随机森林是属于 Bagging类的集成学习。Bagging的主要好处是集成后的分类器的方差，比基分类器的反差小。Bagging 所采用的基分类器，最好是本身不稳定的分类器，这样才能获得更优的性能。线性分类器或者K-近邻都是较为稳定的分类器，本身反差就不大，所以不适合。</p></blockquote></li><li><p>随机森林的随机性体现在哪里？</p><blockquote><ol><li>每棵树的样本是随机抽样得到的</li><li>每课数生长时分裂的属性集合不同</li></ol></blockquote></li><li><p>什么是bias 什么是 variance?</p><blockquote><p>Bias 是 underfitting 造成的。Bias是指由所有采样得到的大小为m的训练数据训练出的所有模型的输出的平均值和真实模型输出之间的偏差。<br>Variance 是 overfitting 造成的。Variance是之由所有采样得到的大小为m的训练数据集训练出的所有模型的输出方差。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></blockquote></li><li><p>GBDT的基本原理是什么？</p><blockquote><p>Gradient Boosting 是 Boosting 中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的的弱分类器以累加的形式结合到现有模型中。</p></blockquote></li><li><p>GBDT 和 Adatboost 的关系和区别是什么？</p><blockquote><p>和AdaBoost一样，Gradient Boosting也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过提升错分数据点的权重来定位模型的不足而Gradient Boosting是通过算梯度（gradient）来定位模型的不足。因此相比AdaBoost, Gradient Boosting可以使用更多种类的目标函数,而当目标函数是均方误差时，计算损失函数的负梯度值在当前模型的值即为残差。当目标函数不是 square loss 时残差并不一定等于负梯度。Adaboost 是 GBDT 的一个特例，GBDT 是 Adaboost的推广。</p><script type="math/tex; mode=display">\left\{ \begin{aligned}& L(y_i, F(x_i)) = \frac{1}{2} * (y_i - F(x_i))^2 \\& - \frac{\partial(y_i, F(x_i))}{\partial F(x_i)} = (y_i - F(x_i))\end{aligned}\right.</script></blockquote></li><li><p>GBDT 为什么要拟合上一次模型的负梯度？</p><blockquote><p>我们要拟合损失函数的负梯度，可以看做拟合一个方向为负梯度方向，步长为单位长度的值，所以拟合的过程相当于我们沿着负梯度方向走了一个步长，具体走多少步（多少步可以理解为训练多少个决策树来拟合该分类器，使得损失函数最低）达到终止的条件，即走到最优点的附近。</p></blockquote></li><li><p>梯度提升和梯度下降的区别和联系？</p><blockquote><p>两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向信息来对当前模型进行更新。在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新。在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中。</p><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center></blockquote></li><li><p>GBDT和局限性有哪些？</p><blockquote><p>GBDT 在高纬度稀疏数据集上，表现不如支持向量机或者神经网络。<br>训练过程需要串行训练。</p></blockquote></li><li><p>XGBoost 与 GBDT 的联系和区别？</p><blockquote><ol><li>原始的 GBDT 算法基于损失函数的负梯度来构造信贷决策树，只是在决策树构建完成构造新的决策树，只是在决策树构建完成后进行剪枝。而 XGBoost在决策树构建阶段就加入了正则项。</li><li>不同的决策树算法采用不同的准则来进行树的构建，比如 IC3 采用信息增益，C4.5 为了克服特征中取值较多的特征而采用信息增益比， CART 采用基尼系数和平方误差。XGBoost 将预测值带入到损失函数中求得损失函数的当前最小值，然后计算出分裂前后损失函数的差值，利用最大化这个差值来作为准则完成树的构建。<br>总的来说，两者的区别和联系可以总结为:<br>a. GBDT是机器学习算法，XGBoost 是其工程实现。<br>b. 在使用 CART 作为基分类器时，XGBoost 显式加入正则项来控制模型的复杂度，有利于防止过拟合。<br>c. GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。<br>d. 传统的 GBDT 采用 CART 作为基分类器，XGBoost 支持多种基分类器。<br>e. 传统的 GBDT 在每轮迭代时使用全部的数据，XGBoost 则采用了与随机森林相似的策略，支持对数据进行采样。<br>f. XGBoost能够自动学习出缺失值的处理策略</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      集成学习,Bagging,Boosting, Bias,Variance
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Deep-Learning</title>
    <link href="https://zhangruochi.com/ML-Interview-Deep-Learning/2020/05/27/"/>
    <id>https://zhangruochi.com/ML-Interview-Deep-Learning/2020/05/27/</id>
    <published>2020-05-27T09:46:36.000Z</published>
    <updated>2020-05-28T23:09:19.497Z</updated>
    
    <content type="html"><![CDATA[<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ol><li><p>写出常用激活函数及其导数</p><blockquote><p>Sigmod </p><script type="math/tex; mode=display">f(z) = \frac{1}{1+exp(-z)}</script><script type="math/tex; mode=display">f\prime(z) = f(z)(1 - f(z))</script><p>Tanh</p><script type="math/tex; mode=display">f(z) = tanh(z) = \frac{e^z - e^{-z}}{ e^z + e^{-z}}</script><script type="math/tex; mode=display">f\prime(z) = 1 - (f(z))^2</script><p>Relu</p><script type="math/tex; mode=display">f(z) = max(0,z)</script><script type="math/tex; mode=display">f\prime(z) = \left\{\begin{aligned}& 1, z > 0 \\& 0, z \leq 0\end{aligned}\right.</script></blockquote></li><li><p>为什么 Sigmoid 和 Tanh 激活函数会导致梯度消失现象？</p><blockquote><p>Sigmoid 函数将输入映射到区间(0,1)，当 z 较大和较小时，f(z) 趋近于 1. 此时的梯度趋近于0. Tanh 实际相当于 Sigmoid 的平移。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></blockquote></li><li><p>ReLU 系列的激活函数相对于Sigmoid 和 Tanh 激活函数的优点是什么？局限性是什么？</p><blockquote><p>优点: 1. 计算简便 2. 有效地解决梯度消失的问题 3. ReLU 单侧抑制提供了网络的稀疏表达能力<br>局限性: 神经元死亡的问题。因为$f(z) = max(0,z)$ 导致负梯度在经过该 ReLU单元时被置为 0，且之后也不被任何数据激活。实际训练时，如果 learning rate 过大，会导致一定比例的 neuron 不可逆死亡，使得整个训练过程失败。Leaky ReLU 可以有效地解决该问题。</p><script type="math/tex; mode=display">f(z) = \left\{\begin{aligned} & z, z > 0 \\& az, z \leq 0\end{aligned}\right.</script></blockquote></li><li><p>平方误差损失函数和交叉熵损失函数分别适用什么场景？</p><blockquote><p>一般来说，平方损失函数适合于连续输出，并且最后一层不含 Sigmoid 或者 Softmax 激活函数的神经网络。交叉熵损失则更适合二分类和多分类场景。</p></blockquote></li></ol><h2 id="神经网络训练技巧"><a href="#神经网络训练技巧" class="headerlink" title="神经网络训练技巧"></a>神经网络训练技巧</h2><ol><li><p>神经网路训练时是否可以将全部参数初始化为0.</p><blockquote><p>同一层的神经元都是同构的，他们拥有相同的输入，如果将参数全部初始化为相同的值，那么无论 forward 还是 backward 都会拥有完全相同的值。因此，我们需要随机地初始化神经网络的参数，以打破这种对称性。</p></blockquote></li><li><p>为什么 Dropout 可以抑制过拟合，它的工作原理是什么？</p><blockquote><p>Dropout作用与每份小批量训练数据，由于其随机丢弃神经元的机制，相当于每次迭代都在训练不同结构的神经网络。类比于Bagging方法，Dropout可被认为是一种实用的大规模神经网络的模型继承算法。对于包含 N 个神经元结点的网络，在 Dropout 的作用下可看做为$2^N$个模型的集成。这$2^N$个模型可认为是原始网络的子网络。应用Dropout包括训练和预测两个阶段，在训练阶段，每个神经元需要增加一个概率系数.</p><script type="math/tex; mode=display">\left\{ \begin{aligned} & r_j^{(l)} \sim Bernoulli(p) \\& \tilde{y}^{(l)} = r^{(l)} * y^{(l)}\end{aligned}\right.</script><p>测试阶段是前向传播过程，每个神经元的参数要预先乘以概率系数p，以恢复在训练时该神经元只有p的概率被用于整个神经网络的前向传播计算</p></blockquote></li><li><p>BatchNorm 的基本动机与原理是什么？ 在卷积网络中如何使用?</p><blockquote><ol><li>神经网络训练的本质是学习数据分布，因此我们常假设训练数据与测试数据是独立同分布的。如果分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有数据进行归一化处理。随着网络训练的进行，每个hidden layer的参数变化使得后一层的输入发生变化，从而每一批训练数据的分布也随之发生变化，使得网络在每次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合风险。</li><li>BatchNorm 是针对每一批数据，在网络的每一层输入之前增加归一化处理，将所有batch数据强制统一在统一的数据分布下。<script type="math/tex; mode=display">\hat{x}^{k} = \frac{x^{(k) - E[x^{(k)}]}}{\sqrt{Var[x^{(k)}]}}</script>其中x^{(k)}为该层第 K 个神经元的原始输入数据，$E[x^{(k)}]$为这一个batch在第k个神经元的均值，$\sqrt{Var[x^{(k)}]}$为这一批数据在第k个神经元的标准差。</li><li>但是均值为 0，方差为1 这个限制太严格了，降低了神经网络的拟合能力。因此加入了两个可学习参数 $\beta$ 和 $\eta$<script type="math/tex; mode=display">y_i = \eta \hat{x}^{k} + \beta</script>在测试阶段，没有batch mean 和 var. 我们使用训练阶段的 running average.</li><li>BatchNorm usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.</li></ol></blockquote></li></ol><h2 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h2><ol><li><p>说说卷及操作的本质。</p><blockquote><ol><li>Sparse Interaction（稀疏交互）： 卷积操作中，每个输出神经元仅仅与前一层特定局部区域的神经元存在连接权重。时间复杂度得到优化，过拟合的情况也得到改善。<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center></li><li>Hierarchical feature representation   ：通常来说，图像，文本，语音等现实世界中的数据都是具有局部的特征结构，我们可以先学习局部的特征，再将局部特征组合起来形成更加复杂的和抽象的特征。这与人类视觉感知物体的共通的。</li><li>Parameter Sharing （参数共享）：给定一个 feature map, 我们使用一个 filter 去扫这个 feature map. filter 里面的参数叫权重，这张图里每个位置都是被同样的 filter 扫描的，所以权重是相同的。参数共享的物理意义是使得卷积层具有平移不变性。例如，在猫的图片上先进行 convolution，再平移l 像素输出，与现将图片平移l 像素再进行卷积操作的输出结果是相等的。</li></ol></blockquote></li><li><p>常用的池化操作有哪些？池化的作用是什么？</p><blockquote><p>Mean Pooling 和 Max pooling. 池化操作除了能显著降低参数数量，还能够保持对平移、伸缩、旋转操作的不变性。Mean Pooling 对背景的保留效果较好，Max pooling 对纹理的提取效果更好。<br>特殊的池化方式有，Global Average Pooling，Spatial Pyramid Pooling(空间金字塔池化). Global Average Pooling 可以将 feature map 转换到特定的维度。SPP 主要考虑多尺度信息，例如计算1x1、2x2、4x4的池化并将结果拼接在一起作为下一层的输入。还可以使得我们构建的网络能够输入任意大小的图片，而不需要提前经过裁剪缩放等预处理操作</p></blockquote></li><li><p>CNN 如何用于文本分类任务？</p><blockquote><p>对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于 N-Gram. CNN 的作用就是能够自动地对 N-gram 特征进行组合和筛选，获得不同抽象层次的语义信息。常用的应用如 char-based model, 把每个char 的 vector concat 在一起，然后使用 conv1d提取特殊的pattern 和 semantic.</p></blockquote></li><li><p>ResNet 的核心理论是什么？</p><blockquote><p>ResNet提出的背景是缓解深层的神经网络中梯度消失的问题。直观来讲，一个 L+1 层的网络不会比 L 层的网络效果差，因为我们简单地设最后一层为一个恒等映射即可。然而实际上深层网络反而会有更大的训练误差，这很大程度上归结于深度神经网络中的梯度消失问题。<br>如下图所示，输入$x$经过两个神经网络变换得到$F(x)$,同时 $x$ 短接到两层之后，最后这个包含两层的神经网络的输出为 $H(x) = F(x) + x$. 这样一来，$F(x)$被设计为只需要拟合x与目标输出H(x)的残差 $H(x) - x$. 如果某一层的效果足够好，那么多加层不会使得模型变差，因为该层的输出短接到了后面的层。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center></blockquote></li><li><p>DenseNet 的核心理论是什么？</p><blockquote><p>既将 $x_0$ 到 $l_1$ 层的所有输出feature map 通过 Channel concat在一起.由于在DenseNet中需要对不同层的feature map进行cat操作,所以需要不同层的feature map保持相同的feature size,这就限制了网络中Down sampling的实现.为了使用Down sampling,作者将DenseNet分为多个Denseblock. 在同一个Denseblock中要求feature size保持相同大小,在不同Denseblock之间设置transition layers实现Down sampling, 在作者的实验中transition layer由BN + Conv(1×1) ＋2×2 average-pooling组成.</p></blockquote></li></ol><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><ol><li><p>处理文本数据时，循环神经网络与前馈神经网络相比有什么特点？</p><blockquote><p>一个长度为T的序列用RNN建模，展开之后可以看作是一个 T 层的前馈神经网络。其中，第$t$层的隐含状态$h_t$ 编码了序列前$t$个输入信息，可以通过当前的输入$x_t$ 和上一层神经网络的状态$h_{t-1}$计算得到. $h_t$和y的计算公式为:</p><script type="math/tex; mode=display">\left\{ \begin{aligned}& net_t = Ux_t + Wh_{t-1} \\& h_t = f(net_t) \\& y = g(Vh_t)\end{aligned}\right.</script><p>其中，$f$ 和 $g$ 为激活函数，U 为输入层到隐藏层的权重矩阵，W 为隐藏层从上一时刻到一下时刻状态转移的权重矩阵。在文本分类中，$f$可以选取Tanh函数或者ReLU函数，$g$可以采用 softmax 函数。相比于CNN, RNN 由于具备对序列信息的刻画能力，往往能够得到更准确的结果。</p></blockquote></li><li><p>循环神经网络为什么会出现梯度消失和梯度爆炸？有哪些改进方案？</p><blockquote><p>RNN 求解采用 BPTT(back propagation through time) 算法实现，实际上是 back propagation 算法的变种。使用 BPTT算法学习的RNN 并不能捕捉长距离的依赖关系，这种现象主要源于神经网络中的梯度消失。因为RNN 的梯度可以写成连乘的形式。详细可参考 <a href="https://zhangruochi.com/BackPropagation-through-time/2019/10/12/">https://zhangruochi.com/BackPropagation-through-time/2019/10/12/</a><br>梯度爆照可以通过梯度裁剪来环节，当梯度大于某个给定值时，对梯度进行收缩。梯度消失可通过 LSTM， GRU 等模型加入门控机制来弥补。</p></blockquote></li><li><p>LSTM 是如何实现长短期记忆功能的？</p><blockquote><p><a href="https://zhangruochi.com/LSTM-Mxnet-Implementation/2019/04/13/">https://zhangruochi.com/LSTM-Mxnet-Implementation/2019/04/13/</a><br>经典的 LSTM，第 t 步的更新公式为：</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{I}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xi} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}_i),\\\boldsymbol{F}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xf} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hf} + \boldsymbol{b}_f),\\\boldsymbol{O}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xo} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{ho} + \boldsymbol{b}_o),\\\tilde{\boldsymbol{C}}_t &= \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xc} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c),\\\boldsymbol{C}_t &= \boldsymbol{F}_t \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t.\end{aligned}</script><p>  与传统的 RNN 相比，LSTM 依然是基于$x_t$和$h_{t-1}$ 来计算$h_t$，只不过对内部的结构进行了更加精心的设计，加入了 input gate $i_t$, forget gate $f_t$, output gate $o_t$. input gate控制当前计算的新状态多大程度更新到当前momery cell 中，forget cell控制前一步的memory cell中的信息有多大程度被遗忘掉，输出门控制当前输出有多程度取决与当前的 memory cell.<br>  当输入的序列中没有重要信息时，LSTM 的遗忘门的值接近于 1，输入门接近于0. 此时过去的记忆会被保留下来，从而实现长期记忆功能。当输入的序列中有重要信息时，LSTM 应当把其存记忆中，此时输入门的值会接近于 1，而遗忘门的值接近于0。经过这样的设计，整个网络更容易学习到序列之间的长期依赖。</p></blockquote></li><li><p>LSTM 里各模块分别适用什么激活函数，可以使用别的激活函数激活吗？</p><blockquote><p>三个门控单元使用Sigmoid作为激活函数,生成候选记忆时，使用tanh作为激活函数。Sigmoid函数的输出在(0, 1)之间，符合门控的物理定义。使用 Tanh函数，是因为其输出在(-1,1)之间，这与大多数场景下特征分布是 0 中心的吻合，此外，Tanh函数在输入为0附近相比Sigmoid函数有更大的梯度，收敛更快。</p></blockquote></li></ol><h2 id="Seq2Seq-模型"><a href="#Seq2Seq-模型" class="headerlink" title="Seq2Seq 模型"></a>Seq2Seq 模型</h2><blockquote><p><a href="https://zhangruochi.com/Attention/2019/12/16/">https://zhangruochi.com/Attention/2019/12/16/</a></p></blockquote><ol><li><p>什么是 Seq2Seq 模型，Seq2Seq 模型有哪些优点？</p><blockquote><p>Seq2Seq模型的核心思想是，通过深度神经网络将输入序列映射为输出序列，这一过程由encoder 与 decoder 两个环节组成。在经典实现中，encoder 和 decoder 都是sequence model. encoder将序列编码成 context vector，decoder 将 context vector 解码成序列。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center></blockquote></li><li><p>Seq2Seq 模型在解码时，有哪些常用的办法？</p><blockquote><p>Seq2Seq 最基础的解码方法是贪心法，即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果，知道遇到结束符。但是贪心算法往往只能得到局部最优解。<br><strong>Beam search</strong> 是贪心算法的改进。改方法会保存beam size 个当前较好的选择，然后解码时每一步根据保存的选择进行下一步的扩展和排序，接着选择前b个进行保存，循环迭代，知道结束后选择最佳的一个座位解码结果。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center></blockquote></li><li><p>Seq2Seq 引入注意力机制是为了解决什么问题？为什么选用了双向循环神经网络模型？</p><blockquote><ol><li>随着输入序列的增长，Seq2Seq的性能发生显著性下降。这是因为编码时输入序列的全部信息压缩到一个 context vector。随着输入序列的增长，句子越前面的词丢失就越严重。Attention机制的引入就是为了解决这个问题。</li><li>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</li><li>机器翻译中，使用双向RNN是因为当前词的状态不仅决定于这个词之前的词，还决定于这个词之后的词。比如 I was a student two years ago.</li></ol></blockquote></li><li><p>如何计算attention score.</p><blockquote><ol><li>利用RNN结构得到encoder中的hidden state $(h_1,h_2,\cdots, h_n)$</li><li>假设当前decoder的hidden state 是$s_{t-1}$, 我们可以计算每一个输入位置j的 hidden state 与当前输出位置的关联性，$e_{ij} = a(s_{t-1}, h_j)$，其中 [公式] 是一种相关性的算符，例如常见的有dot product. 输出位置与所有的输入位置的关联性写成向量形式有 $\vec{e_t} = a(s_{t-1}, h_i), \cdots, a(s_{t-1}, h_T)$</li><li>对$\vec{e_t}$进行softmax操作，然后将其normalize得到attenion score分布$\alpha_{tj}$</li><li>利用 attention score 得到加权的context vector. $\vec{c_t} =\sum_{j=1}^{T}\alpha_{tj} h_j$<br>将加权的context vector 与 decoder 的 $h_t^{dec}$ 拼接在一起。</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      MLP, 神经网络训练技巧, CNN, RNN, Seq2Seq
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Optimization</title>
    <link href="https://zhangruochi.com/ML-Interview-Optimization/2020/05/27/"/>
    <id>https://zhangruochi.com/ML-Interview-Optimization/2020/05/27/</id>
    <published>2020-05-26T23:19:28.000Z</published>
    <updated>2020-05-28T23:04:38.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="监督学习的损失函数"><a href="#监督学习的损失函数" class="headerlink" title="监督学习的损失函数"></a>监督学习的损失函数</h2><ol><li>有监督学习涉及的损失函数有哪些？请列举并简述它们的特点。<blockquote><ol><li>MSE<script type="math/tex; mode=display">L = \sum( Y - f(x))^2</script></li><li>MAE<script type="math/tex; mode=display">L = \sum|Y - f(x)|</script></li><li>Hinge：Hinge损失不仅会惩罚错误的预测，还会惩罚不自信的正确预测。用于支持向量机(SVM)中。<script type="math/tex; mode=display">L_{hinghe}(f,y) = max\{0, 1-f_y\}</script></li><li>Binary Cross Entropy<script type="math/tex; mode=display">L = -y * log(p) - (1-y) * log(1-p)</script></li><li>Cross Entropy <script type="math/tex; mode=display">L(x_i,y_i) = -\sum_{j=1}^{e} y_{ij} * log(p_{ij})</script>where $Y_i$ is one-hot encoded target vector $(y_{i1},\cdots, y_{i2})$.<script type="math/tex; mode=display">y_{ij} = \left\{\begin{aligned}& 1 \quad \text{if i element is in class j} \\ & 0 \quad \text{otherwise}\end{aligned}\right.</script></li><li>Kullback-Leibler Divergence：表示两个概率分布的差异。Variational Auto-Encoder中使用。<script type="math/tex; mode=display">D_{KL}(p||q) = \sum_{i=1}^{N}p(x_i)\dot(log p(x_i) - log q(x_i))</script></li><li>Huber：结合 MSE 和 MAE 的优点<script type="math/tex; mode=display">L = \left\{ \begin{aligned}& \frac{1}{2}(y - f(x))^2, \quad if \ | y - f(x)| \leq \delta, \\ & \delta|y - f(x)| - \frac{1}{2}\delta^2, otherwise\end{aligned}\right.</script></li><li>Dice loss： 两个轮廓的相似度，应用在图像分割领域<script type="math/tex; mode=display">DL(A,B) = 2 \frac{A \cap B }{ |A| + |B|}</script></li></ol></blockquote></li></ol><h2 id="机器学习中的优化问题"><a href="#机器学习中的优化问题" class="headerlink" title="机器学习中的优化问题"></a>机器学习中的优化问题</h2><ol><li><p>机器学习中，哪些是凸优化问题？</p><blockquote><p>凸函数曲面上任意两点连接而成的线段，其上的任意一点都不会处于改函数曲面的下方。一个常用的机器学习模型，逻辑回归，对应的优化问题就是凸优化问题。因为我们可以求得优化函数的 Hessian矩阵是半正定的。</p><script type="math/tex; mode=display">L(\lambda x + (1-\lambda) y) \leq \lambda L(x) + (1 - \lambda)L(y)</script><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></blockquote></li><li><p>当数据量特别大时，经典的梯度下降算法有什么问题？</p><blockquote><p>经典的梯度下降算法在每次模型参数进行更新时，需要遍历所有的训练数据。当 M 很大时，这需要进行很大的计算。为了计算这个问题，随机梯度下降法用单个样本损失来近似所有样本的平均损失。为了降低随机梯度的方差，使得迭代更加稳定，一般使用小批量梯度下降算法。对于小批量下降法的使用，需要注意以下几点：</p><ol><li>不同应用中，每个 batch 的大小通常会不一样。一般选择 2 的幂次可以充分利用矩阵运算。</li><li>为了避免数据的特定顺序给算法收敛带来的影响，一般会在每次遍历数据之前，先对所有数据进行shuffle。</li><li>为了加快收敛速度，同时提高求解精度，通常采用衰减学习速率的方案：一开始算法采用较大的学习速率，当误差曲线进入平台期后，减小学习速率做更精细的调整。</li></ol></blockquote></li><li><p>请给出随机梯度下降算法失效的原因。</p><blockquote><p>随机梯度下降算法放弃了对梯度准确性的追求，每步仅仅采用一个（或少量）样本来估计当前梯度。但是由于每步接受的信息量有限，随机梯度下降法对梯度的估计常常出现偏差，造成目标函数收敛很不稳定，伴有剧烈的波动，有时甚至出现不收敛的情况。对于随机梯度下降法来说，最可怕的不是局部最优点，而是山谷和鞍点。鞍点就是一片平摊的区域，在梯度几乎为零的区域，随机梯度下降法无法计算出梯度的微小变化，导致在来回震荡。</p></blockquote></li><li><p>如何改进随机梯度下降法？（动量和环境感知）</p><blockquote><ol><li>Momentum（动量）：当来到鞍点处，在惯性作用下继续前行，则有机会冲出平坦的陷阱。动量法的收敛速度更快，收敛曲线也更稳定。实际上是对 gradient 做 moving average.<script type="math/tex; mode=display">\begin{aligned}& v_t = \eta v_{t-1} + \gamma g_t \\& \theta_{t+1} = \theta_t - v_t \end{aligned}</script></li></ol></blockquote></li><li><p>AdaGrad 方法</p><blockquote><p>随机梯度下降法对环境的感知是指在参数空间中，根据不同参数的一些经验性判断，自适应地确定参数的学习速率。例如在文本处理中训练 word embedding，有写词频繁出现，有些词极少出现，我们希望极少出现的词更新的步幅大一些。AdaGrad 采用<code>历史梯度平方和</code>来衡量不同参数的稀疏性，取值越小说明越稀疏。 具体的更新公式为:</p><script type="math/tex; mode=display">\theta_{t+1,i} = \theta_{t,i} - \frac{\gamma}{\sqrt{\sum_{k=0}^{t} g_{k,i}^2 + \epsilon}}</script></blockquote></li><li><p>Adam 方法</p><blockquote><p>Adam方法集惯性保持和环境感知两个优点于一身。一方面，Adam 记录梯度的 first moment，即过往梯度与当前梯度的平均，这体现了惯性保持；另一方面，Adam 还记录了梯度的 second moment，即过往梯度平方与当前梯度平方的平均，这类似 AdaGrad方法，体现环境感知能力,为不同参数产生自适应的学习速率。first and second monent 采用exponential decay average，使得时间久远的梯度对当前平均值的贡献呈指数衰减。</p><script type="math/tex; mode=display">\begin{aligned}& m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\& v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2 \\ & \hat{m_t} = \frac{m_t}{1 - \beta^t_1}   \\& \hat{v_t} = \frac{v_t}{1 - \beta^t_2} \\& \theta_{t+1} = \theta_t - \frac{ \gamma \cdot \hat{m_t} }{ \sqrt{\hat{v_t} + \epsilon} }\end{aligned}</script><p>其中$\beta_1$, $\beta_2$ 为衰减系数，$m_t$是 first moment, $v_t$ 是second moment.</p></blockquote></li></ol><p><a href="https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/">https://zhangruochi.com/An-overview-of-gradient-descent-optimization-algorithms/2019/02/23/</a></p><ol><li>L1 正则化与稀疏性原理是什么？<blockquote><p>带正则项和带约束条件是等价的，为了约束w的可能取值空间从而防止过拟合，我们为该最优化问题加上一个约束，就是w的 L2 范数不能大于m</p><script type="math/tex; mode=display">\begin{aligned} & \min sum_{i=1}^N(y_i - w^T x_i)^2 \\ & s.t. ||w||^2_2 \leq m\end{aligned}</script><p>为了求解带约束条件的凸优化问题，写出拉格朗日函数</p><script type="math/tex; mode=display">sum_{i=1}^N(y_i - w^T x_i)^2 + \lambda(||w||^2_2 - m)</script><p>L2正则化相当于为参数定义了一个圆形的解空间(因为必须保证L2范数不能大于m), 而 L1 正则化想当于定义了一个菱形的解空间。L1 的解空间显然更容易与目标函数的等高线在角点碰撞，从而产生稀疏解。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      损失函数，优化算法
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Unsupervised-Learning</title>
    <link href="https://zhangruochi.com/ML-Interview-Unsupervised-Learning/2020/05/26/"/>
    <id>https://zhangruochi.com/ML-Interview-Unsupervised-Learning/2020/05/26/</id>
    <published>2020-05-26T01:14:23.000Z</published>
    <updated>2020-05-28T22:49:19.611Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%"></center><h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><ol><li><p>简述 K-Means算法的具体步骤<br>输入是样本集$D=\{x_1,x_2,…x_m\}$,聚类的簇树k,最大迭代次数N。输出是簇划分$C=\{C_1,C_2,…C_k\}$</p><blockquote><ol><li>数据预处理,如归一化、离群点处理等<br>2.从数据集D中随机选择k个样本作为初始的k个质心向量：$\{\mu_1,\mu_2,…,\mu_k\}$</li><li>对于n=1,2,…,N<ul><li>将簇划分C初始化为 $C_t = \varnothing \;\; t =1,2…k$</li><li>计算样本$x_i$和各个质心向量$\mu_j(j=1,2,…k)$的距离: $d_{ij} = ||x_i - \mu_j||_2^2$，将$x_i$标记最小的为$d_{ij}$所对应的类别$\lambda_i$, 此时更新 $C_{\lambda_i} = C_{\lambda_i} \cup \{x_i\}$</li><li>对于j=1,2,…,k,对 $C_j$中所有的样本点重新计算新的质心$\mu_j = \frac{1}{|C_j|}\sum\limits_{x \in C_j}x$</li><li>如果所有的k个质心向量都没有发生变化，则转到步骤3）</li></ul></li><li>输出簇划分$C=\{C_1,C_2,…C_k\}$</li></ol></blockquote></li><li><p>简述K-Means++与 K-Means的区别</p><blockquote><p>K-Means中k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心。如果仅仅是完全随机的选择，有可能导致算法收敛很慢。K-Means++算法就是对K-Means随机初始化质心的方法的优化。K-Means++的对于初始化质心的优化策略也很简单，如下：</p><ol><li>从输入的数据点集合中随机选择一个点作为第一个聚类中心$u_1$</li><li>对于数据集中的每一个点$x_i$,计算它与已选择的聚类中心中最近聚类中心的距离<script type="math/tex; mode=display">D(x_i) = arg\;min||x_i- \mu_r||_2^2\;\;r=1,2,...k_{selected}</script></li><li>选择一个新的数据点作为新的聚类中心，选择的原则是：$D(x)$较大的点，被选取作为聚类中心的概率较大.</li><li>重复b和c直到选择出k个聚类质心</li><li>利用这k个质心来作为初始化质心去运行标准的K-Means算法</li></ol></blockquote></li><li><p>K-Means均值算法的缺点是什么？</p><blockquote><ol><li>K值的选取不好把握</li><li>对于不是凸的或者球形的数据集比较难收敛</li><li>如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。</li><li>采用迭代方法，得到的结果只是局部最优。</li><li>对噪音和异常点比较的敏感。</li></ol></blockquote></li><li><p>如何选取 K-Means 的 K 值？</p><blockquote><p>K 值的选择一般基于经验和多次试验结果。比如可以采用手肘法，我们可以尝试不同的 K 值，并将不同的 K 值所对应的损失函数画成折线。拐点就是 K 的最佳值。</p></blockquote></li><li><p>什么是 Kernel K- Means ?</p><blockquote><p>还童的欧式距离度量方式，使得 K 均值算法本质上假设了各个数据簇的数据呈现球形或者高维球形，这种分布在实际生活中不常见。面对非凸的数据分布时，引入核函数来进行非线性映射，将输入空间中的数据点映射到高维的特征空间，并在新的特征中空间进行聚类。非线性映射增加了数据点线性可分的概率。</p></blockquote></li></ol><h3 id="DBSCANS-（密度聚类）"><a href="#DBSCANS-（密度聚类）" class="headerlink" title="DBSCANS （密度聚类）"></a>DBSCANS （密度聚类）</h3><blockquote><p><a href="https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/">https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/</a></p></blockquote><h3 id="Birch-层次聚类"><a href="#Birch-层次聚类" class="headerlink" title="Birch (层次聚类)"></a>Birch (层次聚类)</h3><ol><li>什么是层次聚类? 层次聚类的步骤是什么？<blockquote><p>层次聚类不指定具体的簇数，而只关注簇之间的远近，最终会形成一个树形图。<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%"></center><br>根据聚类簇之间距离的计算方法的不同，层次聚类算法可以大致分为：单链接（Single-link）算法，全链接算法（complete-link）或均链接算法（average-link）。单链接算法用两个聚类簇中最近的样本距离作为两个簇之间的距离；而全链接使用计算两个聚类簇中最远的样本距离；均链接算法中两个聚类之间的距离由两个簇中所有的样本共同决定。</p><ol><li>每一个样本点视为一个簇；</li><li>计算各个簇之间的距离，最近的两个簇聚合成一个新簇；</li><li>重复以上过程直至最后只有一簇。</li></ol></blockquote></li></ol><h3 id="Gaussian-Mixed-Model-概率聚类"><a href="#Gaussian-Mixed-Model-概率聚类" class="headerlink" title="Gaussian Mixed Model (概率聚类)"></a>Gaussian Mixed Model (概率聚类)</h3><blockquote><p><a href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/">https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/</a></p></blockquote><ol><li>高斯混合模型的核心思想是什么？它是如何迭代计算的？<blockquote><p>高斯混合模型假设数据可以看作是从多个高斯分布中生成出来的。求解步骤如下:</p><ol><li>E step: 根据当前参数，计算每个点属于各个高斯分布的概率</li><li>M step: 使用上述 E step 求得的概率，计算每个高斯分布的加权平均参数。</li></ol></blockquote></li></ol><h3 id="聚类算法的评估"><a href="#聚类算法的评估" class="headerlink" title="聚类算法的评估"></a>聚类算法的评估</h3><ol><li>以聚类算法为例，假设没有外部标签数据，如何评估两个聚类算法的优劣？<blockquote><p>在无监督的情况下，我们可通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。</p><ol><li>轮廓系数：给定一个点p，改点的轮廓系数定义为<script type="math/tex; mode=display">s(p) = \frac{b(p) - a(p)}{max{a(p), b(p)}}</script>其中，$a(p)$是点$p$与同一簇中其他点$p\prime$之间的平均距离；$b(p)$是点$p$与另一不同簇中的点之间的最小平均距离（如果有n个簇，则只计算和点p最接近的一簇中的点与该点的平均距离). $a(p)$反应的是$p$所属的簇中数据的紧密程度，$b(p)$反应的是该簇与其他临近簇的分离程度。显然，$b(p)$越大，$a(p)$越小，对应的聚类的质量越好。</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      无监督学习，K-Means，DBSCANS，Birch，GMM
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Decomposition</title>
    <link href="https://zhangruochi.com/ML-Interview-Decomposition/2020/05/26/"/>
    <id>https://zhangruochi.com/ML-Interview-Decomposition/2020/05/26/</id>
    <published>2020-05-25T22:56:23.000Z</published>
    <updated>2020-05-28T22:52:58.124Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PCA-最大方差理论"><a href="#PCA-最大方差理论" class="headerlink" title="PCA 最大方差理论"></a>PCA 最大方差理论</h2><ol><li><p>如何定义主成分？从这种定义出发，如何设计目标函数使得降维达到提取主成分的目的？针对这个目标函数，如何对 PCA 问题进行求解？</p><blockquote><p>在信号处理领域，我们认为信号具有较大的方差，噪声具有较小的方差，信号与噪声之比成为信噪比。信噪比越大意味着数据的质量越好。x 投影之后的方差就是协方差矩阵的特征值，最佳投影方向也就是协方差矩阵最大的特征值。至此，<br>PCA 的求解方法为：</p><ol><li>对样本数据进行中心化处理</li><li>求样本的协方差矩阵</li><li>对协方差矩阵进行特征值分解，将特征值从大到小排列</li><li>去特征值前$d$大对应的特征向量$w_1,w_2,…,w_d$,<br>通过以下映射将n维样本映射到$d$维度。<script type="math/tex; mode=display">x_i\prime = \left[\begin{matrix}& w_1^{T}x_i \\& w_2^{T}x_i \\& w_3^{T}x_i \\& \cdots \\& w_d^{T}x_i \end{matrix}\right]</script></li></ol></blockquote></li><li><p>PCA 的缺点是什么？<br>在 PCA 中，算法没有考虑数据的标签（类别），只是把数据映射到一些方差比较大的方向而已。如下图，PCA 算法会把两个类别的数据映射到y轴，使得分类效果特别差。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></li></ol><h2 id="LDA-线性判别分析"><a href="#LDA-线性判别分析" class="headerlink" title="LDA 线性判别分析"></a>LDA 线性判别分析</h2><ol><li><p>对于具有类别标签的数据，映带如何设计目标函数使得降维的过程中不损失类别信息？在这种目标下，应当如何求解？</p><blockquote><p>投影后每类内部方差最小，类间方差最大<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><br>类内散度矩阵$s_w$:</p><script type="math/tex; mode=display">S_w = \Sigma_0 + \Sigma_1 = \sum\limits_{x \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum\limits_{x \in X_1}(x-\mu_1)(x-\mu_1)^T</script><p>类间散度矩阵$s_b$:</p><script type="math/tex; mode=display">S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T</script><p>LDA 的优化目标：</p><script type="math/tex; mode=display">\underbrace{arg\;max}_w\;\;J(w) = \frac{w^TS_bw}{w^TS_ww}</script></blockquote></li><li><p>LDA 算法的步骤是什么？</p><blockquote><ol><li>计算类内散度矩阵$S_w$</li><li>计算类间散度矩阵$S_b$</li><li>计算矩阵$S_w^{-1}S_b$</li><li>计算$S_w^{-1}S_b$的最大的d个特征值和对应的d个特征向量$(w_1,w_2,…w_d)$得到投影矩阵$W$.</li><li>对样本集中的每一个样本特征$x_i$,转化为新的样本$z_i=W^Tx_i$.</li></ol></blockquote></li><li><p>LDA 与 PCA 作为经典的降维算法，如何从应用的角度分析其原理的异同？</p><blockquote><p>从目标出发，PCA 选择的是投影后数据方差最大的方向，由于它是无监督的，因此 PCA 假设方差越大，信息量越多，用主成分来表示原始数据可以去除用于的维度，达到降维。而 LDA选择的是投影后类内方差小、类间方差大的方向。其用到了类别信息，为了找到数据中具有判别性的维度，使得原始数据在这些方向上投影后，不同类 jin尽可能区分开。举例来说，我们想从一段音频中提取人的语音信号，这时可以使用 PCA 先进行降维，过滤掉一些固定频率的北京噪声。但如果我们的需求是从这段音频中区分出声音属于哪个人，那么我们应该使用 LDA 对数据进行降维，使得每个人的语音信号具有区分性。<br><strong>从应用的角度，我们可以掌握一个基本的原则—对无监督的任务使用 PCA 进行降维，对有监督的则应用 LDA</strong></p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      Decomposition, PCA,LDA
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Classicial-Algorithms</title>
    <link href="https://zhangruochi.com/ML-Interview-Classicial-Algorithms/2020/05/25/"/>
    <id>https://zhangruochi.com/ML-Interview-Classicial-Algorithms/2020/05/25/</id>
    <published>2020-05-25T00:28:33.000Z</published>
    <updated>2020-05-28T22:48:42.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h2><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><blockquote><p>参考 <a href="https://zhuanlan.zhihu.com/p/35755150" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35755150</a></p></blockquote><p>SVM的基本型：</p><script type="math/tex; mode=display">min_{w,b} = \frac{1}{2} ||w||^{2}</script><script type="math/tex; mode=display">s.t. \quad y_i(w^T x_i + b) \leq 1, i=1,2,3...m</script><ol><li><p>空间上线性可分的两点，分别向svm的超平面做投影，投影的点在超平面上依然线性可分吗？</p><blockquote><p>一定线性不可分</p></blockquote></li><li><p>硬间隔和软间隔是指什么？</p><blockquote><p>SVM的基本形态是一个硬间隔分类器，它要求所有样本都满足硬间隔约束(即函数间隔要大于1)，所以当数据集有噪声点时，SVM为了把噪声点也划分正确，超平面就会向另外一个类的样本靠拢，这就使得划分超平面的几何间距变小，降低模型的泛化性能。除此之外，当噪声点混入另外一个类时，对于硬间隔分类器而言，这就变成了一个线性不可分的问题，于是就使用核技巧，通过将样本映射到高维特征空间使得样本线性可分，这样得到一个复杂模型，并由此导致过拟合（原样本空间得到的划分超平面会是弯弯曲曲的，它确实可以把所有样本都划分正确，但得到的模型只对训练集有效）。<br>为了解决上述问题，SVM通过引入松弛变量构造了软间隔分类器，它允许分类器对一些样本犯错，允许一些样本不满足硬间隔约束条件，这样做可以避免SVM分类器过拟合，于是也就避免了模型过于复杂，降低了模型对噪声点的敏感性，提升了模型的泛化性能。<br>因为松弛变量是非负的，因此样本的函数间隔可以比1小。函数间隔比1小的样本被叫做离群点，我们放弃了对离群点的精确分类，这对我们的分类器来说是种损失。但是放弃这些点也带来了好处，那就是超平面不必向这些点的方向移动，因而可以得到更大的几何间隔（在低维空间看来，分类边界也更平滑）。显然我们必须权衡这种损失和好处。</p></blockquote></li><li><p>松弛变量和惩罚因子是什么？</p><blockquote><p>松弛变量：松弛变量表示样本离群的程度，松弛变量越大，离群越远，松弛变量为零，则样本没有离群。<br>惩罚因子：惩罚因子表示我们有多重视离群点带来的损失，当C取无穷大时，会迫使超平面将所有的样本都划分正确，这就退化成了硬间隔分类器。</p></blockquote></li><li><p>拉格朗日乘子法是什么？</p><blockquote><p>拉格朗日乘数法是一种优化算法，主要运用于解决优化问题，它的基本思想就是用拉格朗日乘子构造一个新的优化函数将原本的约束优化问题转换成等价的无约束优化问题。</p></blockquote></li><li><p>什么是对偶问题?</p><blockquote><p>常一个优化问题可以从两个角度来考虑，即主问题(primal problem)和对偶问题(dual problem)。在约束最优化问题中，常常利用拉格朗日对偶性将原始问题（主问题）转换成对偶问题，通过解对偶问题来得到原始问题的解。这样做是因为对偶问题的复杂度往往低于主问题。</p></blockquote></li><li><p>什么是 kernel trick?</p><blockquote><p>$x_i$ 和 $x_j$ 在特征空间的內积等于它们在原始的样本空间通过 $k(x_i,x_j)$ 计算的结果。有了这样的函数，我们不必去计算高维甚至无穷维特征空间中的內积。 d<br>SVM 基本式的对偶问题为: </p><script type="math/tex; mode=display">max_{\alpha} \sum_{i=1}^{m}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_i y_j \Phi{x_i}^{T}\Phi_{x_j}</script><script type="math/tex; mode=display">s.t. \sum_{i=1}^{m}\alpha_i y_i = 0</script><script type="math/tex; mode=display">\alpha_i \geq 0, i = 1,2,...,m.</script></blockquote></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><ol><li><p>什么是逻辑回归？</p><blockquote><p>对数几率回归。对逻辑回归的公式进行整理，得到:</p><script type="math/tex; mode=display">log\frac{p}{1-p} = \theta^{T}x</script><script type="math/tex; mode=display">p = P(y=1 | x)</script><p>逻辑回归通过极大似然来得到最佳参数</p><script type="math/tex; mode=display">L(\theta) = \prod_{i:y_{i}=1}p(x_{i})\prod_{i^{\prime}:y_{i^{\prime}}=0}(1-p(x_{i^{\prime}}))</script></blockquote></li><li><p>使用逻辑回归处理多标签的分类问题时，有哪些常用做法？</p><blockquote><ol><li>如果一个样本只对应一个标签，那么可以使用 sofmax regression</li><li>当存在样本属于多个标签的情况，可以训练$i$个分类器，第$i$个分类器用以区分每个样本是否可以归为第i类。可以训练 softmax regression. 设定一个 threshold，判断每个类别的概率是否高于 threshold.</li></ol></blockquote></li></ol><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><ol><li><p>决策树有哪些启发函数？</p><blockquote><p>ID3（最大信息增益） 计算每个特征的信息增益，然后选择信息增益最大的特征来划分样本，完成决策树的增长。<br>C4.5（最大信息增益比）。<br>CART(最大基尼指数)</p></blockquote></li><li><p>信息熵、信息增益、信息增益比、最大基尼系数是什么？</p><blockquote><ol><li><strong>信息熵</strong> 是度量样本集合不确定度（纯度）的最常用的指标。<br>当前样本集合 D 中第 k 类样本所占的比例为 pk ，则 D 的信息熵定义为<script type="math/tex; mode=display">Ent(D) = - \sum_{K=1}^{|y|}p_k log_2^{p_k}</script></li><li><strong>信息增益</strong> 表示得知属性 a 的信息而使得样本集合不确定度减少的程度<br>假设离散属性 a 有 V 个可能的取值 {a1,a2,…,aV}；样本集合中，属性 a 上取值为 av 的样本集合，记为 Dv。<script type="math/tex; mode=display">Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{D^v}{D}Ent(D^v)</script>信息增益率 = 信息增益/IV(a),说明信息增益率是信息增益除了一个属性a的固有值得来的。<script type="math/tex; mode=display">IV(a) = -sum_{v=1}^{v}\frac{D^v}{D}log_2\frac{D^v}{D}</script><strong>Gini</strong>描述的是数据的纯度<script type="math/tex; mode=display">Gini(D) = 1 - sum_{k=1}^{n}(\frac{|C_k|}{|D|})^2</script>特征 A 的 Gini指数定义为:<script type="math/tex; mode=display">Gini(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}Gini(D_i)</script></li></ol></blockquote></li><li><p>ID3,C4.5,CART 各自的优缺点是什么？</p><blockquote><p>ID3倾向于取值较多的特征,因为信息增益放映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性越高，也就是条件熵越小、信息增益越大。<br>C4.5实际上是对 ID3 进行优化，通过引入信息增益比，一定程度上对取值较多的特征进行惩罚、避免 ID3 出现过拟合。<br>CART 与 ID3,C4.5不同，它是一颗二叉树，采用二元分割法，每一步将数据按照特征 A 的取值切成两份，分别进入左右子树。</p></blockquote></li><li><p>Cart 在做 regression 和 classification 的区别是？</p><blockquote><p> 在分类问题中，CART 使用基尼指数（Gini index）作为选择特征（feature）和划分（split）的依据；在回归问题中，CART 使用 mse（mean square error）或者 mae（mean absolute error）作为选择 feature 和 split 的 criteria。<br>在分类问题中，CART的每一片叶子都代表的是一个class；在回归问题中，CART 的每一片叶子表示的是一个预测值，取值是连续的。预测值一般是该片叶子所含训练集元素输出的均值。</p></blockquote></li><li><p>决策树如何进行剪枝？</p><blockquote><p>预剪枝：1. 当树到达一定深度时，停止树的生长；2.当到达当前节点的样本数量小于某个阈值时，停止树的生长；3. 计算每次分裂时测试集的准确度提升，当小于某个阈值时不再继续扩展。<br>后剪枝：后剪枝的方法有很多，比如代价复杂度剪枝、悲观剪枝、最小误差剪枝等。</p></blockquote></li></ol><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><ol><li><p>简述朴素贝叶斯的原理。</p><blockquote><p>朴素贝叶斯采用<code>属性条件独立性</code>的假设，对于给定的待分类观测数据X, 计算在X出现的条件下，各个目标类出现的概率（即后验概率）， 将该后验概率最大的类作为X所属的类。方法是根据已有样本进行贝叶斯估计学习出先验概率$P(Y)$和条件概率$P(X|Y)$，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解后验概率P(Y|X).</p></blockquote></li><li><p>朴素贝叶斯“朴素”在哪里？</p><blockquote><p>利用贝叶斯定理求解联合概率P(XY)时，需要计算条件概率P(X|Y)。在计算P(X|Y)时，朴素贝叶斯做了一个很强的条件独立假设（当Y确定时，X的各个分量取值之间相互独立），即</p><script type="math/tex; mode=display">P(X_1=x_1,X_2=x_2,\cdots,X_j=x_j|Y=y_k) = P(X_1=x_1|Y=y_k) * P(X_2=x_2|Y=y_k),\cdots,P(X_j=x_j|Y=y_k)</script></blockquote></li><li><p>什么是拉普拉斯平滑法?</p><blockquote><p>拉普拉斯平滑法是朴素贝叶斯中处理零概率问题的一种修正方式。在进行分类的时候，可能会出现某个属性在训练集中没有与某个类同时出现过的情况，如果直接基于朴素贝叶斯分类器的表达式进行计算的话就会出现零概率现象。为了避免其他属性所携带的信息被训练集中未出现过的属性值“抹去”，所以才使用拉普拉斯估计器进行修正。具体的方法是：在分子上加1,对于先验概率，在分母上加上训练集中可能的类别数；对于条件概率，则在分母上加上第i个属性可能的取值数</p></blockquote></li><li><p>朴素贝叶斯中有哪些不同的模型？</p><blockquote><p>朴素贝叶斯含有3种模型，分别是<strong>高斯模型</strong>，对连续型数据进行处理；<strong>多项式模型</strong>，对离散型数据进行处理，计算数据的条件概率(使用拉普拉斯估计器进行平滑的一个模型)；<strong>伯努利模型</strong>，伯努利模型的取值特征是布尔型，即出现为ture,不出现为false,在进行文档分类时，就是一个单词有没有在一个文档中出现过。</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      SVM, Logistic Regression, Decision Tree
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Feature-Engineering-and-Evaluation</title>
    <link href="https://zhangruochi.com/ML-Interview-Feature-Engineering-and-Evaluation/2020/05/24/"/>
    <id>https://zhangruochi.com/ML-Interview-Feature-Engineering-and-Evaluation/2020/05/24/</id>
    <published>2020-05-24T08:02:06.000Z</published>
    <updated>2020-05-29T00:18:35.014Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><ol><li><p>为什么需要对数值类型的特征做归一化？</p><blockquote><p>常用的归一化有：Min Max Scaler / Z-Score<br>当特征的 range 不同时，归一化特征可以加快梯度下降收敛的速度。PCA 等算法的假设有数据是均值均值为0,方差为1. </p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></blockquote></li><li><p>应该怎样处理类别特征？</p><blockquote><p>Ordinal Encoding<br>One-hot Encoding<br>Binary Encoding</p></blockquote></li><li><p>什么是特征组合，如何处理高维组合特征？</p><blockquote><p>两个或多个特征组合在一起$(x1,\cdots, xn)$ 等形成组合特征.<br>特征选择，矩阵分解，PCA.</p></blockquote></li><li><p>怎样有效地找到组合特征</p><blockquote><ol><li>决策树从根节点到叶子结点的路径可以看成一种特征组合的方式<br><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><br>Filter, wrapper, embedding 等方法进行特征选择，形成组合特征。</li></ol></blockquote></li><li><p>有哪些文本表示模型，它们各自的优缺点是什么？</p><blockquote><p>Bag of Word, 常用 TF-IDF表示词的权重（term frequency and inverse document frequency）; N-gram. 提取词组; 因为相同的词可能有多种表示，经常会做词干提取word stemming; 主题模型（得到每个主题上词的分布特征）; Word embedding ; Contextual word embeddings</p></blockquote></li><li><p>word2vec是如何工作的</p><blockquote><p>参考 <a href="https://zhangruochi.com/Word-Vectors/2019/12/04/">https://zhangruochi.com/Word-Vectors/2019/12/04/</a><br>CBOW 根据上下文来预测中心词，Skip-gram根据中心词来预测上下文。 CBOW 和 Skip-gram 都是由三层的神经网络组成。输入层为N维 one-hot encoding，隐藏层为 K 维。则输入层和隐藏层的 weight matrix （N*K）就是 embedding vector. word vector 可以由one-hot encoding 与 weight matrix 相乘得到。隐藏层到输出层的weightg matrix 为 （K*N）.输出也是一个N维向量，则可以根据softmax来求每个词的概率，然后应用梯度下降。<br>由于softmax需要对所有词进行遍历，计算量大。此时可以使用negtive sampling 或者 hierarchical softmax.</p></blockquote></li><li><p>LSA（Latent Semantic Analysis） 算法是怎样工作的？</p><blockquote><p>LSA 算法先统计 term-document矩阵（矩阵的每个元素为tf-idf）进行奇异值分解，从而得到term的向量表示和document的向量表示. 其算法的基本流程是：</p><ol><li>分析文档集合，建立词汇-文本矩阵A</li><li>对词汇-文本矩阵进行奇异值分解</li><li>对SVD分解后的矩阵进行降维</li><li>使用降维后的矩阵构建潜在语义空间</li></ol></blockquote></li><li><p>Glove 是怎样工作的？</p><blockquote><ol><li>Construct co-occurrence Matrix</li><li>Construct relationships between word vectors and co-occurrence Matrix<ul><li>Let X denote the word-word co-occurrence matrix, where $X_{ij}$ indicates the number of times word j occur in the context of word i</li><li>$w_{i}$,$\tilde{w_{j}}$ is the word vector</li><li>$b_i,b_j$ is the bias term<script type="math/tex; mode=display">w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} = \log(X_{ij}) \tag{1}</script></li></ul></li><li>Construct loss function: Mean Square Loss<script type="math/tex; mode=display">J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2</script><script type="math/tex; mode=display">f(x)=\begin{equation} \begin{cases} (x/x_{max})^{\alpha}  & \text{if} \ x < x_{max} \\ 1 & \text{otherwise} \end{cases} \end{equation}</script></li></ol></blockquote></li><li><p>LSA, word2vec, 以及 Glove 的区别于联系？</p><blockquote><p>LSA和word2vec作为两大类方法的代表，一个是利用了全局特征的矩阵分解方法，一个是利用局部上下文的方法。GloVe模型就是将这两中特征合并到一起的，即使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。</p></blockquote></li><li><p>图像分类时，训练数据不足如何处理。</p><blockquote><p>数据不足有过拟合风险，或者模型不能收敛。</p><ol><li>可以使用降低过拟合风险的措施。如l1/l2,继承学习,dropout 等</li><li>Data augmentation （旋转、平移、缩放、像素扰动、颜色变换、清晰度、对比度等）</li><li>Fine tuing or transfer learning</li><li>生成对抗模型生成新样本</li><li>对图像进行特征提取，使用传统的机器学习模型。</li></ol></blockquote></li></ol><h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><ol><li><p>准确率的局限性是什么？</p><blockquote><p>当正负数据不平衡时会失去意义</p></blockquote></li><li><p>Precision 和 Recall 怎样权衡？</p><blockquote><p>Precision 是指分类正确的正样本/模型预测的正样本, Recall 是指分类正确的正样本/实际的正样本。P-R 曲线横轴是recall，纵轴是precision。P-R 曲线是将阙值从高到低滑动画出的。<br>使用 P-R 曲线来综合判定两个模型的好坏。 F1 和 ROC 也能反应排序模型的好坏。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center></blockquote></li><li><p>RMSE 的局限是什么？怎样解决？</p><blockquote><p>如果存在个别偏离程度大的异常值，RMSE的效果会很差。</p><ol><li>数据预处理清理 outlier； </li><li>建模考虑异常机制，如异常点检测；</li><li>使用更合适的指标如 MAPE</li></ol></blockquote></li><li><p>什么是 ROC 曲线？</p><blockquote><p>横轴是 FPR（FP/N）, 纵轴是 TPR（TP/P）。绘制 ROC 曲线，需要将模型的输出概率从大到小排序，然后动态地选择阈值。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center></blockquote></li><li><p>AUC如何计算？</p><blockquote><p>AUC 是 ROC 曲线下的面积大小，计算时只用沿着ROC 曲线做积分就行了。AUC取值一般在[0.5,1]之间，越大越好。</p></blockquote></li><li><p>ROC 曲线相比 P-R 曲线有什么特点</p><blockquote><p>当正负样本的分布发生明显变化时，ROC曲线基本不变。因此 ROC 适用的场景更多。如下图是将负样本的数量增加 10 倍之后的结果。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center></blockquote></li><li><p>为什么在一些场景中需要使用余弦相似度而不是欧氏距离。</p><blockquote><p>余弦相似度只关注向量的夹角，并不关心向量的绝对值大小，范围为[-1,1]。比如在度量两个文本的相似度时，以词频和词向量最为特征。文本越长则欧式距离一定越大，但是余弦相似度则可以保持不变。总的来说，关注相对差异，使用余弦相似度。关注数值绝对差异，使用欧式距离。</p></blockquote></li><li><p>如何进行线上 A/B 测试？</p><blockquote><p>用户分桶，在分桶过程中一定要保证独立性和采样的无偏性。</p></blockquote></li><li><p>为什么在进行了离线评估后还要进行线上评估？</p><blockquote><ol><li>离线评估无法完全消除过拟合的影响。</li><li>离线评估无法完全还原线上的工程环境。</li><li>线上系统的某些商业指标无法在离线环境中还原，如用户点击率，留存时长等。</li></ol></blockquote></li><li><p>模型评估时，有哪些主要的验证方法，他们的优缺点是什么？</p><blockquote><ol><li>holdout. 在验证集上计算出的评估指标与原始分组有很大的关系。</li><li>k-fold. 把k次评估的平均值作为最终的评估指标。</li><li>留一法. 每次留下 1 个样本作为验证集。开销大，实际工程中较少使用。</li><li>自助法. 基于自助采样的方法，对于总数为n的样本集合，进行n次有放回的随机采样，得到大小为n的训练集，没有被采样的样本作为测试集。</li></ol></blockquote></li><li><p>超参数有哪些调优方法？</p><blockquote><ol><li>Grid Search。 十分消耗计算资源和时间，一般先使用较广的搜索范围和较大的步长，或者先确定对模型影响最大的参数。</li><li>随机搜索。 业界公认的Random search效果会比Grid search好。 例如前面的场景A有2种选择、B有3种、C有5种、连续值随机采样，那么每次分别在A、B、C中随机取值组合成新的超参数组合来训练。虽然有随机因素，但随机搜索可能出现效果特别差、也可能出现效果特别好，在尝试次数和Grid search相同的情况下一般最值会更大，当然variance也更大但这不影响最终结果。</li><li>贝叶斯优化算法. 是基于数据使用贝叶斯定理估计目标函数的后验分布，然后再根据分布选择下一个采样的超参数组合。它充分利用了前一个采样点的信息，其优化的工作方式是通过对目标函数形状的学习，并找到使结果向全局最大提升的参数</li></ol></blockquote></li><li><p>过拟合、欠拟合具体是指什么现象？</p><blockquote><p>过拟合是指数据拟合过当，模型在训练集上表现好，但是测试集和新数据上表现差。欠拟合是模型在训练集和测试集上都表现不好。</p></blockquote></li><li><p>能否说出集中降低过拟合和欠拟合风险的方法？</p><blockquote><p>降低过拟合：获取更多数据、降低模型复杂度、正则化、集成学习<br>降低欠拟合：添加新特征、增加模型负责度、减少正则化系数</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      特征工程,模型评估相关问题
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Time and Ordering</title>
    <link href="https://zhangruochi.com/Time-and-Ordering/2020/05/09/"/>
    <id>https://zhangruochi.com/Time-and-Ordering/2020/05/09/</id>
    <published>2020-05-08T17:10:01.000Z</published>
    <updated>2020-05-09T05:36:27.341Z</updated>
    
    <content type="html"><![CDATA[<p>分布式系统和传统的单机系统不同，彼此是通过网络而不是”主板”连接、消息通讯是不可靠的。因此如果没有任何同步机制，同一系统的成员之间无法确保时间戳的误差控制在某个范围内。这个基本条件的缺失，会给上层应用的设计带来很多的麻烦。比如，一个业务流程的两个阶段分别在两台机器上处理，而后在第三台机器上将处理记录join起来，就可能因为时间戳的问题引发混乱。如何做好时间同步的协议，成为了分布式系统中的一个基本的问题。</p><p>在系统对时的时候，有两类基本的协议，第一个是外部对时，简单的说，就是整个分布式系统中的所有成员，与外部某个指定的源头进行时间同步，确保与源头的时间的diff在某个误差范围$D$内; 另一种是内部对时，即内部通过广播等各种手段，确保系统内的成员俩俩间的时间误差在一定范围内。从这里可以看出，如果一个集群使用了外部对时，控制误差在$D$以内，那么这个集群内部的时间的误差，也一定能够控制在$2D$的范围内。但反过来不一定，因为有可能整个集群与外部的时间存在很大的整体偏差，尽管在内部彼此的偏差很小。</p><p>那么如何进行时间的同步呢？这里介绍两个经典的协议：Cristian和NTP。</p><h2 id="Cristian"><a href="#Cristian" class="headerlink" title="Cristian"></a>Cristian</h2><p>Cristian的基本过程是这样的，假定现在P进程要从授时服务器S获取时间，那么最朴素的做法就是P向S发送请求，S将自己的时间t返回给P，而后P设置自己的时间为t。这个做法存在一个很关键的问题，就是由于网络的通讯时间是不确定的，P拿到t的时候，已经经过了不确定多久了，无法估计结束后P与S的时间误差范围。因此，我们需要将网络通讯的时间，即RTT(Rount Trip Time)也考虑进来。在这个场景下，RTT指的是P进程发出请求，到得到S的回应消息的时间差，这个时间差是P进程自己可以记录求得的。假定我们知道从 $P \to S$的最小延时是 $min_1$, $S \to P$的最小延时是$min_2$,那么，我们可以推断，真实的时间在$[t+min_2, t+RTT-min_1]$间内，Cristian的做法就将对时结果设置为：$t’=t+\frac{RTT+min_2-min_1}{2}$ 这个中间位置上。那么，其误差就能控制在$\pm \frac{RTT-min_1-min_2}{2}$ 的范围内。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><h2 id="NTP"><a href="#NTP" class="headerlink" title="NTP"></a>NTP</h2><p>另外一个知名的时间同步协议是NTP，全称Network Time Protocol。NTP协议一般在某个大的机构内部署，将机构内的设备组织成树形结构，每个节点都从父节点处获取时间。整个同步过程分为两轮，第一轮父节点记录自己发送返回的时间点$ts_1$，子节点记录自己接收到返回消息的时间 $tr_1$；而后第二轮，子节点记录自己的发送时间$ts_2$；；父节点记录收到请求的时间$tr_2$后将$ts_1$ 和 $tr_2$返回。那么子节点可以计算出自己和父节点之间的时间偏差为: $o=\frac{(tr_{1}-tr_{2}+ts_{2}-ts{1})}{2}$，并以此为依据进行修正(一般需要确保时间不能“倒流”)。那么为什么$o$是这么计算呢？假定子节点与父节点的时间偏差(offset)为$o\prime$、父节点往子节点的通讯时延为$L_1$、子节点往父节点的通讯时延为$L_2$，那么:</p><script type="math/tex; mode=display">\begin{align*} & tr_{1}=ts_{1}+L_{1}+o' \\& tr_{2}=ts_{2}+L_{2}-o' \\\end{align*}</script><p>相减可以得到:</p><script type="math/tex; mode=display">o'= \frac{(tr_1-tr_2+ts_2-ts_1)}{2} + \frac{(L_2 - L1)}{2} = o + \frac{(L_2-L1)}{2}</script><p>因此:</p><script type="math/tex; mode=display">\lvert o'-o \rvert \leqslant \lvert \frac{(L_2-L_1)}{2}\rvert < \frac{(L_{1}+L_{2})}{2} = \frac{RTT}{2}</script><p>由此可知o的这个值也在RTT相关的一个误差范围内，是可估计的。<br>从上面两个协议可以看出，对时的误差是与RTT强相关的。由于消息的传递受制于光速、距离越远时间准确度的保证就越差。对于那些假定了时间误差在某个范围内的分布式协议，在跨越距离很大的时候，我们就必须要将这个误差对系统的影响考虑在内，这将显著增加分布式系统设计的复杂度、或者影响设计出来的系统的吞吐(尤其是有高一致性要求的事务型系统)。</p><p>最后，不论是Cristian还是NTP，都只描述了一次对时如何将时间的偏移(clow skew)控制在一定范围内。由于不同机器的时钟的行进速度(clock drift)是不同的，因此我们需要每隔一段时间，进行一次修正，以消除时钟节奏不同的影响。多久需要做一次同步呢? 这个做一个简单的计算就可以得到。假定系统整体时钟的行进速率与标准时钟的速率小于MDR(Max Drift Rate, 一般由时钟的实现方式决定)，那么系统内俩俩时钟的行进速率差小于2MDR。如果我们要求系统内时间差不能超过M，那就必须以不低于$\delta = \frac{M}{2 \times {MDR}}$的间隔进行时间同步。在现实的系统中，我们需要计算合理的M，以避免系统内出现过多的时间同步消息。</p><p>在上面部分，我们谈到了分布式系统里进程彼此的物理时间是如何进行同步的，并介绍了一些经典的时间同步算法。但静下心来仔细想想，我们希望进行时间同步，很多时候是希望不同的进程，对系统内事件的顺序达成一致。至于是否是使用真实世界的那个时间来排序，往往并不是那么重要。<br>那么，如何在一个分布式系统中，对发生在众多节点上的事件进行定序呢？目前已知的做法包括以下几种：</p><ul><li>使用物理时间同步的方法，确保众多节点的时间偏差在某个范围内。而后记录事件的发生时间及理论误差范围，比如将每个事件的发生时间登记为$(t \pm \Delta)$如果两个事件的时间范围没有overlap，那么就自然的可以排序判断；否则，则需要引入一个新的排序规则(比如以节点id)，对这两个事件约定一个排序。spanner中采用了这种方式。</li><li>采用Lamport Timestamp及其引申算法进行定序，确保事件满足causality consistency的性质，成为后续更高层次的分布式算法设计的基础。本文后面主要将展开这类算法，并引出分布式系统中一些基础概念。这些基础概念是理解分布式共识问题(consensus problem)的基础。</li></ul><h2 id="Lamport"><a href="#Lamport" class="headerlink" title="Lamport"></a>Lamport</h2><p>为明确这个问题，我们首先需要先对事件的序(happen-before)做出一个定义。在Lamport的体系中，事件的先后关系是按照如下原则设定的：</p><ul><li>规则一：如果A、B两个事件都发生在同一个进程内，那么，A、B之间的序自然可以由这个进程给出。假如进程先执行了A后执行了B，那么可以说A在B之前发生，记为$A \prec B$;</li><li>规则二：如果进程x往进程y发送了一条消息M；设在进程x的消息发送事件为A，进程y收到消息的事件为B，则显然我们应当认为A在B之前发生，同样记为$A \prec B$.</li></ul><p>由此引出了Lamport timestamp的算法，这个算法就是一种给事件打上逻辑时间戳、确保其满足causality的基本属性。这个算法的基本过程为：</p><ul><li>每个进程都记录自己的一个当前时间戳，初始的时候，大家都是0</li><li>如果进程内部发生了一个新的事件，那么将当前时间戳记为 $t’=t+1$，并认为事件发生于$t’$时刻</li><li>如果进程A向进程B通讯，则发送消息的时候，进程A的时间戳$t’_A = t_A + 1$并随消息发送到B，B更新自己的时间戳为$t’_B = max(t’_B, t’_A) + 1$.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><h3 id="Concurrent-Events"><a href="#Concurrent-Events" class="headerlink" title="Concurrent Events"></a>Concurrent Events</h3><ul><li>A pair of concurrent events doesn’t have a causal path from one event to another (either way, in the pair)</li><li>Lamport timestamps not guaranteed to be ordered or unequal for concurrent events</li><li>Ok, since concurrent events are not causality related!</li><li>Remember</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">E1 -&gt; E2 =&gt; timestamp(E1) &lt; timestamp (E2), </span><br><span class="line">BUT timestamp(E1) &lt; timestamp (E2) =&gt; &#123;E1 -&gt; E2&#125; OR &#123;E1 and E2 concurrent&#125;</span><br></pre></td></tr></table></figure><h2 id="Vector-timestamps"><a href="#Vector-timestamps" class="headerlink" title="Vector timestamps"></a>Vector timestamps</h2><ul><li>Used in key-value stores like Riak</li><li>Each process uses a vector of integer clocks</li><li>Suppose there are N processes in the group 1…N</li><li>Each vector has N elements</li><li>Process i maintains vector Vi[1…N]</li><li>$j_{th}$ element of vector clock at process $i$, $V_i[j]$, is $i’s$ knowledge of latest events at process $j$</li></ul><p>Incrementing vector clocks</p><ol><li>On an instruction or send event at process $i$, it increments only its $i_{th}$ element of its vector clock.</li><li>Each message carries the send-event’s vector timestamp V_{message}[1…N]</li><li>On receiving a message at process $i$:</li></ol><script type="math/tex; mode=display">\begin{align*} &V_i[i] = V_i[i] + 1 \\& V_i[j] = max(V_{message}[j], V_i[j]) \quad for \ quad j \neq i \\\end{align*}</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center><h3 id="Causality"><a href="#Causality" class="headerlink" title="Causality"></a>Causality</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://lvsizhe.github.io/course/2018/09/time-in-distributed-systems-part1.html" target="_blank" rel="noopener">https://lvsizhe.github.io/course/2018/09/time-in-distributed-systems-part1.html</a></li><li>lecture slide from <a href="https://www.coursera.org/learn/cloud-computing/lecture/dy8wf/2-5-vector-clocks" target="_blank" rel="noopener">https://www.coursera.org/learn/cloud-computing/lecture/dy8wf/2-5-vector-clocks</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;分布式系统和传统的单机系统不同，彼此是通过网络而不是”主板”连接、消息通讯是不可靠的。因此如果没有任何同步机制，同一系统的成员之间无法确保时间戳的误差控制在某个范围内。这个基本条件的缺失，会给上层应用的设计带来很多的麻烦。比如，一个业务流程的两个阶段分别在两台机器上处理，而
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Distributed &amp; Cloud Computing" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Distributed-Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>Programming Language: New Types, Pattern Matching, Tail Recursion</title>
    <link href="https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/05/03/"/>
    <id>https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/05/03/</id>
    <published>2020-05-02T19:29:05.000Z</published>
    <updated>2020-05-03T07:29:53.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Conceptual-Ways-to-Build-New-Types"><a href="#Conceptual-Ways-to-Build-New-Types" class="headerlink" title="Conceptual Ways to Build New Types"></a>Conceptual Ways to Build New Types</h2><p>To create a compound type, there are really only three essential building blocks. Any decent programming language provides these building blocks in some way:</p><ul><li><strong>Each-of</strong>: A compound type t describes values that contain each of values of type t1, t2, …, and tn. Tuples are an example: int * bool describes values that contain an int and a bool. A <strong>Java class</strong> with fields is also an each-of sort of thing.</li><li><strong>One-of</strong>: A compound type t describes values that contain a value of one of the types t1, t2, …, or tn. For a type that contains an int or a bool in ML, we need <code>datatype bindings</code>. In object-oriented languages with classes like Java, one-of types are achieved with <strong>subclassing</strong>, but that is a topic for much later in the course.</li><li><strong>Self-reference</strong>: A compound type t may refer to itself in its definition in order to describe recursive data structures like lists and trees. This is useful in combination with each-of and one-of types. For example, int list describes values that either contain nothing or contain an int and another int list. </li></ul><h2 id="Records-Another-Approach-to-Each-of-Types"><a href="#Records-Another-Approach-to-Each-of-Types" class="headerlink" title="Records: Another Approach to Each-of Types"></a>Records: Another Approach to <strong>Each-of</strong> Types</h2><p>Record types are “each-of” types where each component is a <code>named field</code>.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;foo : <span class="built_in">int</span>, bar : <span class="built_in">int</span>*<span class="built_in">bool</span>, baz : <span class="built_in">bool</span>*<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><p>In ML, we do not have to declare that we want a record type with particular field names and field types — we just write down a record expression and the type-checker gives it the right type.</p><p>Now that we know how to build record values, we need a way to access their pieces. For now, we will use <code>#foo e</code> where <code>foo</code> is a field name. </p><h3 id="The-truth-of-tuple"><a href="#The-truth-of-tuple" class="headerlink" title="The truth of tuple"></a>The truth of tuple</h3><p>In fact, this is how ML actually defines tuples: A tuple is a record. That is, all the syntax for tuples is just a convenient way to write down and use records. The REPL just always uses the tuple syntax where possible, so if you evaluate {2=1+2, 1=3+4} it will print the result as (7,3). Using the tuple syntax is better style, but we did not need to give tuples their own semantics: we can instead use the “another way of writing” rules above and then reuse the semantics for records.</p><p>This is the first of many examples we will see of <code>syntactic sugar</code>. We say, Tuples are just syntactic sugar for records with fields named 1, 2, …, n.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> z = (<span class="number">3</span>,<span class="number">7</span>) : <span class="built_in">int</span> * <span class="built_in">int</span></span><br><span class="line"><span class="keyword">val</span> z = &#123;<span class="number">1</span>=<span class="number">3</span>,<span class="number">3</span>=<span class="number">7</span>&#125; : &#123;<span class="number">1</span>:<span class="built_in">int</span>, <span class="number">3</span>:<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="Datatype-Bindings-Our-Own-One-of-Types"><a href="#Datatype-Bindings-Our-Own-One-of-Types" class="headerlink" title="Datatype Bindings: Our Own One-of Types"></a>Datatype Bindings: Our Own <strong>One-of</strong> Types</h2><p>We now introduce datatype bindings, our third kind of binding after variable bindings and function bindings.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">datatype mytype = TwoInts of int * int</span><br><span class="line">                | Str of string</span><br><span class="line">                | Pizza</span><br></pre></td></tr></table></figure><p>Roughly, this defines a new type where values have an int <em> int or a string or nothing. Any value will also be <code>tagged</code> with information that lets us know which variant it is: These tags, which we will call <em>*constructors</em></em>, are <code>TwoInts</code>, <code>Str</code>, and <code>Pizza</code>.</p><p>More precisely, the example above adds four things to the environment:</p><ul><li>A new type mytype that we can now use just like any other type</li><li>Three constructors TwoInts, Str, and Pizza</li></ul><p>A constructor is two different things. First, it is either a function for creating values of the new type (if the variant has of t for some type t) or it is actually a value of the new type (otherwise). In our example, TwoInts is a function of type int*int -&gt; mytype, Str is a function of type string-&gt;mytype, and Pizza is a value of type mytype. Second, we use constructors in case-expressions as described further below.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> mytype = <span class="type">TwoInts</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span> </span><br><span class="line">                | <span class="type">Str</span> <span class="keyword">of</span> <span class="built_in">string</span> </span><br><span class="line">                | <span class="type">Pizza</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a = <span class="type">Str</span> <span class="string">"hi"</span></span><br><span class="line"><span class="keyword">val</span> b = <span class="type">Str</span></span><br><span class="line"><span class="keyword">val</span> c = <span class="type">Pizza</span></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">TwoInts</span>(<span class="number">1</span>+<span class="number">2</span>,<span class="number">3</span>+<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> e = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">(* val a = Str "hi" : mytype</span></span><br><span class="line"><span class="comment">val b = fn : string -&gt; mytype</span></span><br><span class="line"><span class="comment">val c = Pizza : mytype</span></span><br><span class="line"><span class="comment">val d = TwoInts (3,7) : mytype</span></span><br><span class="line"><span class="comment">val e = Str "hi" : mytype *)</span></span><br></pre></td></tr></table></figure><h2 id="How-ML-Provides-Access-to-Datatype-Values-Case-Expressions"><a href="#How-ML-Provides-Access-to-Datatype-Values-Case-Expressions" class="headerlink" title="How ML Provides Access to Datatype Values: Case Expressions"></a>How ML Provides Access to Datatype Values: Case Expressions</h2><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> f x = <span class="comment">(* f has type mytype -&gt; int*)</span> </span><br><span class="line">    <span class="keyword">case</span> x <span class="keyword">of</span></span><br><span class="line">      <span class="type">Pizza</span> =&gt; <span class="number">3</span></span><br><span class="line">    | <span class="type">TwoInts</span>(i1,i2) =&gt; i1 + i2 </span><br><span class="line">    | <span class="type">Str</span> s =&gt; <span class="type">String</span>.size s</span><br></pre></td></tr></table></figure><p>In one sense, a case-expression is like a more powerful if-then-else expression: Like a conditional expression, it evaluates two of its subexpressions: first the expression between the case and of keywords and second the expression in the first branch that matches. But instead of having two branches (one for true and one for false), we can have one branch for each variant of our datatype (and we will generalize this further below). Like conditional expressions, each branch’s expression must have the same type (int in the example above) because the type-checker cannot know what branch will be used.<br>Each branch has the form <code>p =&gt; e</code> where p is a pattern and e is an expression, and we separate the branches with the | character. Patterns look like expressions, but do not think of them as expressions. Instead they are used to match against the result of evaluating the case’s first expression (the part after case). This is why evaluating a case-expression is called pattern-matching.</p><h3 id="Datatype-Bindings-and-Case-Expressions-So-Far-Precisely"><a href="#Datatype-Bindings-and-Case-Expressions-So-Far-Precisely" class="headerlink" title="Datatype Bindings and Case Expressions So Far, Precisely"></a>Datatype Bindings and Case Expressions So Far, Precisely</h3><p>We can summarize what we know about datatypes and pattern matching so far as follows: The binding</p><blockquote><p>datatype t = C1 of t1 | C2 of t2 | … | Cn of tn</p></blockquote><p>introduces a new type t and each constructor Ci is a function of type ti-&gt;t. One omits the “of ti” for a variant that “carries nothing” and such a constructor just has type t. To “get at the pieces” of a t we use a case expression:</p><blockquote><p>case e of p1 =&gt; e1 | p2 =&gt; e2 | … | pn =&gt; en</p></blockquote><p>A case expression evaluates e to a value v, finds the first pattern pi that matches v, and evaluates ei to produce the result for the whole case expression. So far, patterns have looked like Ci(x1,…,xn) where Ci is a constructor of type t1 <em> … </em> tn -&gt; t (or just Ci if Ci carries nothing). Such a pattern matches a value of the form Ci(v1,…,vn) and binds each xi to vi for evaluating the corresponding ei.</p><h2 id="Type-Synonyms"><a href="#Type-Synonyms" class="headerlink" title="Type Synonyms"></a>Type Synonyms</h2><p>A <strong>type synonym</strong> simply creates another name for an existing type that is entirely interchangeable with the existing type.</p><p>For example, if we write:<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> foo = <span class="built_in">int</span></span><br></pre></td></tr></table></figure></p><p>then we can write foo wherever we write int and vice-versa.</p><p>for more complicated types, it can be convenient to create type synonyms. Here are some examples for types we created above:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> card = suit * rank</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> name_record = &#123; student_num : <span class="built_in">int</span> <span class="built_in">option</span>,</span><br><span class="line">                    first : <span class="built_in">string</span>,</span><br><span class="line">                    middle : <span class="built_in">string</span> <span class="built_in">option</span>,</span><br><span class="line">                    last : <span class="built_in">string</span> &#125;</span><br></pre></td></tr></table></figure><h2 id="Lists-and-Options-are-Datatypes"><a href="#Lists-and-Options-are-Datatypes" class="headerlink" title="Lists and Options are Datatypes"></a>Lists and Options are Datatypes</h2><p>Because datatype definitions can be recursive, we can use them to create our own types for lists. For example, this binding works well for a linked list of integers:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> my_int_list = <span class="type">Empty</span></span><br><span class="line">                        | <span class="type">Cons</span> <span class="keyword">of</span> <span class="built_in">int</span> * my_int_list</span><br></pre></td></tr></table></figure><p>We can use the constructors Empty and Cons to make values of my_int_list and we can use case expressions to use such values:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> one_two_three = <span class="type">Cons</span>(<span class="number">1</span>,<span class="type">Cons</span>(<span class="number">2</span>,<span class="type">Cons</span>(<span class="number">3</span>,<span class="type">Empty</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> append_mylist (xs,ys) = </span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">        <span class="type">Empty</span> =&gt; ys</span><br><span class="line">    | <span class="type">Cons</span>(x,xs’) =&gt; <span class="type">Cons</span>(x, append_mylist(xs’,ys))</span><br></pre></td></tr></table></figure><p>For options, all you need to know is SOME and NONE are constructors, which we use to create values (just like before) and in patterns to access the values. Here is a short example of the latter:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> inc_or_zero intoption = <span class="keyword">case</span> intoption <span class="keyword">of</span></span><br><span class="line">        <span class="type">NONE</span> =&gt; <span class="number">0</span></span><br><span class="line">      | <span class="type">SOME</span> i =&gt; i+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">&lt;!-- <span class="keyword">val</span> inc_or_zero = <span class="keyword">fn</span> : <span class="built_in">int</span> <span class="built_in">option</span> -&gt; <span class="built_in">int</span> --&gt;</span><br></pre></td></tr></table></figure><p>The story for lists is similar with a few convenient syntactic peculiarities: [] really is a constructor that carries nothing and :: really is a constructor that carries two things, but :: is unusual because it is an infix operator (it is placed between its two operands), both when creating things and in patterns:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_list xs = </span><br><span class="line">    <span class="keyword">case</span> xs:</span><br><span class="line">        <span class="literal">[]</span> =&gt; <span class="number">0</span></span><br><span class="line">        | x::xs' =&gt; x + sum_list xs'</span><br></pre></td></tr></table></figure><p>Notice here x and xs’ are nothing but local variables introduced via pattern-matching. We can use any names for the variables we want. </p><h2 id="Pattern-Matching-for-Each-Of-Types-The-Truth-About-Val-Bindings"><a href="#Pattern-Matching-for-Each-Of-Types-The-Truth-About-Val-Bindings" class="headerlink" title="Pattern-Matching for Each-Of Types: The Truth About Val-Bindings"></a>Pattern-Matching for Each-Of Types: The Truth About Val-Bindings</h2><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> full_name (r : &#123;first:<span class="built_in">string</span>,middle:<span class="built_in">string</span>,last:<span class="built_in">string</span>&#125;) = <span class="keyword">case</span> r <span class="keyword">of</span></span><br><span class="line">        &#123;first=x,middle=y,last=z&#125; =&gt; x ^ <span class="string">" "</span> ^ y ^ <span class="string">" "</span> ^z</span><br></pre></td></tr></table></figure><p>However, a case-expression with one branch is poor style — it looks strange because the purpose of such expressions is to distinguish cases, plural. So how should we use pattern-matching for each-of types, when we know that a single pattern will definitely match so we are using pattern-matching just for the convenient <strong>extraction of values</strong>? It turns out you can use patterns in val-bindings too! So this approach is better style:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">fun</span> full_name (r : &#123;first:<span class="built_in">string</span>,middle:<span class="built_in">string</span>,last:<span class="built_in">string</span>&#125;) = </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">val</span> &#123;first=x,middle=y,last=z&#125; = r</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        x ^ <span class="string">" "</span> ^ y ^ <span class="string">" "</span> ^z </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum_triple (triple : <span class="built_in">int</span>*<span class="built_in">int</span>*<span class="built_in">int</span>) = </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">val</span> (x,y,z) = triple</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        x+y+z</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_triple (x,y,z) = x+y+z</span><br></pre></td></tr></table></figure><p>This version of sum_triple should intrigue you: It takes a triple as an argument and uses pattern-matching to bind three variables to the three pieces for use in the function body. But it looks exactly like a function that takes three arguments of type int. Indeed, is the type int<em>int</em>int-&gt;int for three-argument functions or for one argument functions that take triples?<br>It turns out we have been basically lying: There is no such thing as a multi-argument function in ML: <strong>Every function in ML takes exactly one argument!</strong> Every time we write a multi-argument function, we are really writing a one-argument function that takes a tuple as an argument and uses pattern-matching to extract the pieces. This is such a common idiom that it is easy to forget about and it is totally fine to talk about “multi-argument functions” when discussing your ML code with friends. But in terms of the actual language definition, it really is a one-argument function: syntactic sugar for expanding out to the first version of sum_triple with a one-arm case expression.</p><h2 id="Digression-Type-inference"><a href="#Digression-Type-inference" class="headerlink" title="Digression: Type inference"></a>Digression: Type inference</h2><p>In ML, every variable and function has a type (or your program fails to type-check) — type inference only means you do not need to write down the type.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_triple triple = <span class="keyword">case</span> triple <span class="keyword">of</span></span><br><span class="line">    (x,y,z) =&gt; z + y + x</span><br></pre></td></tr></table></figure><p>In fact, type inference sometimes reveals that functions are more general than you might have thought. Consider this code, which does use part of a tuple/record:<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> partial_sum (x,y,z) = x + z</span><br><span class="line"><span class="keyword">fun</span> partial_name &#123;first=x, middle=y, last=z&#125; = x ^ <span class="string">" "</span> ^ z</span><br></pre></td></tr></table></figure></p><p>In both cases, the inferred function types reveal that the type of y can be any type, so we can call partial_sum (3,4,5) or partial_sum (3,false,5). This is okay because the polymorphism indicates that partial_sum has a more gen- eral type. If you can take a type containing ’a, ’b, ’c, etc. and replace each of these type variables consistently to get the type you “want,” then you have a more general type than the one you want.</p><h2 id="Nested-Patterns"><a href="#Nested-Patterns" class="headerlink" title="Nested Patterns"></a>Nested Patterns</h2><p>It turns out the definition of patterns is recursive: anywhere we have been putting a variable in our patterns, we can instead put another pattern. Roughly speaking, the semantics of pattern-matching is that the value being matched must have the same “shape” as the pattern and variables are bound to the “right pieces.” (This is very hand-wavy explanation which is why a precise definition is described below.) For example, the pattern a::(b::(c::d)) would match any list with at least 3 elements and it would bind a to the first element, b to the second, c to the third, and d to the list holding all the other elements (if any). The pattern a::(b::(c::[])) on the other hand, would match only lists with exactly three elements. Another nested patterns is (a,b,c)::d, which matches any non-empty list of triples, binding a to the first component of the head, b to the second component of the head, c to the third component of the head, and d to the tail of the list.</p><p>In general, pattern-matching is about taking a value and a pattern and (1) deciding if the pattern matches the value and (2) if so, binding variables to the right parts of the value. Here are some key parts to the elegant recursive definition of pattern matching:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">BadTriple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> zip3 list_triple = <span class="keyword">case</span> list_triple <span class="keyword">of</span></span><br><span class="line">    (<span class="literal">[]</span>,<span class="literal">[]</span>,<span class="literal">[]</span>) =&gt; <span class="literal">[]</span></span><br><span class="line">        | (hd1::tl1,hd2::tl2,hd3::tl3) =&gt; (hd1,hd2,hd3)::zip3(tl1,tl2,tl3) </span><br><span class="line">        | _ =&gt; <span class="keyword">raise</span> <span class="type">BadTriple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> unzip3 lst =</span><br><span class="line">    <span class="keyword">case</span> lst <span class="keyword">of</span></span><br><span class="line">        <span class="literal">[]</span> =&gt; (<span class="literal">[]</span>,<span class="literal">[]</span>,<span class="literal">[]</span>)</span><br><span class="line">      | (a,b,c)::tl =&gt; <span class="keyword">let</span> <span class="keyword">val</span> (l1,l2,l3) = unzip3 tl</span><br><span class="line">                       <span class="keyword">in</span></span><br><span class="line">                           (a::l1,b::l2,c::l3)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="Exceptions"><a href="#Exceptions" class="headerlink" title="Exceptions"></a>Exceptions</h2><p>ML has a built-in notion of exception. You can raise (also known as throw) an exception with the raise primitive. For example, the hd function in the standard library raises the List.Empty exception when called with []:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> hd xs =</span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line"><span class="literal">[]</span> =&gt; <span class="keyword">raise</span> <span class="type">List</span>.<span class="type">Empty</span> | x::_ =&gt; x</span><br></pre></td></tr></table></figure><p>You can create your own kinds of exceptions with an exception binding. Exceptions can optionally carry values with them, which let the code raising the exception provide more information:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">MyUndesirableCondition</span></span><br><span class="line"><span class="keyword">exception</span> <span class="type">MyOtherException</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span></span><br></pre></td></tr></table></figure><p>Kinds of exceptions are a lot like constructors of a datatype binding. Indeed, they are functions (if they carry values) or values (if they don’t) that create values of type exn rather than the type of a datatype. So Empty, MyUndesirableCondition, and MyOtherException(3,9) are all values of type exn, whereas MyOtherException has type int*int-&gt;exn.</p><h2 id="Tail-Recursion-and-Accumulators"><a href="#Tail-Recursion-and-Accumulators" class="headerlink" title="Tail Recursion and Accumulators"></a>Tail Recursion and Accumulators</h2><p>This topic involves new programming idioms, but no new language constructs. It defines tail recursion, describes how it relates to writing efficient recursive functions in functional languages like ML, and presents how to use accumulators as a technique to make some functions tail recursive.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum1 xs =</span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">        <span class="literal">[]</span> =&gt; <span class="number">0</span></span><br><span class="line">      | i::xs’ =&gt; i + sum1 xs’</span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum2 xs =</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">fun</span> f (xs,acc) =</span><br><span class="line">            <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">                <span class="literal">[]</span> =&gt; acc</span><br><span class="line">              | i::xs’ =&gt; f(xs’,i+acc)</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        f(xs,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Why might sum2 be preferred when it is clearly more complicated? To answer, we need to understand a little bit about how function calls are implemented. Conceptually, there is a <strong>call stack</strong>, which is a stack (the data structure with push and pop operations) with one element for each function call that has been started but has not yet completed. Each element stores things like the value of local variables and what part of the function has not been evaluated yet. When the evaluation of one function body calls another function, a new element is pushed on the call stack and it is popped off when the called function completes.</p><blockquote><p> there is nothing more for the caller to do after the callee returns except return the callee’s result.</p></blockquote><p>This situation is called a tail call (let’s not try to figure out why it’s called this) and functional languages like ML typically promise an essential optimization: When a call is a tail call, the caller’s stack-frame is popped before the call — the callee’s stack-frame just replaces the caller’s. This makes sense: the caller was just going to return the callee’s result anyway. Therefore, calls to sum2 never use more than 1 stack frame.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Conceptual-Ways-to-Build-New-Types&quot;&gt;&lt;a href=&quot;#Conceptual-Ways-to-Build-New-Types&quot; class=&quot;headerlink&quot; title=&quot;Conceptual Ways to Build
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>EDA Summary</title>
    <link href="https://zhangruochi.com/EDA-Summary/2020/04/30/"/>
    <id>https://zhangruochi.com/EDA-Summary/2020/04/30/</id>
    <published>2020-04-30T11:55:33.000Z</published>
    <updated>2020-05-01T00:02:49.967Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-main-goals-of-EDA-are"><a href="#The-main-goals-of-EDA-are" class="headerlink" title="The main goals of EDA are:"></a>The main goals of EDA are:</h2><ul><li>Provide summary level insight into a data set</li><li>Uncover underlying patterns and structure in the data</li><li>Identify outliers, missing data and class balance issues</li><li>Carry out quality control checks</li></ul><h2 id="The-principal-steps-in-the-process-of-EDA-are"><a href="#The-principal-steps-in-the-process-of-EDA-are" class="headerlink" title="The principal steps in the process of EDA are:"></a>The principal steps in the process of EDA are:</h2><ol><li>Summarize the data - Generally done using dataframes in R or Python</li><li>Tell the Story - Summarize the details of what connects the dataset to the business opportunity</li><li>Deal with missing data - Identify the strategy for dealing with missing data</li><li>Investigate - Using data visualization and hypothesis testing delve into the relationship between the dataset and the business opportunity</li><li>Communicate - Communicate the findings from the above steps</li></ol><h2 id="Data-visualization"><a href="#Data-visualization" class="headerlink" title="Data visualization"></a>Data visualization</h2><ol><li>Jupyter notebooks in combination with pandas and simple plots are the basis for modern EDA when using Python as a principal language</li></ol><h3 id="Advantages-of-Jupyter-notebooks"><a href="#Advantages-of-Jupyter-notebooks" class="headerlink" title="Advantages of Jupyter notebooks:"></a>Advantages of Jupyter notebooks:</h3><ul><li>They are portable: then can be used locally on private servers, public cloud, and as part of IBM Watson Studio</li><li>They work with dozens of languages</li><li>They mix markdown with executable code in a way that works naturally with storytelling and investigation</li><li>matplotlib itself and its numerous derivative works like seaborn are the core of the Python data visualization landscape</li><li>pandas and specifically the dataframe class works naturally with Jupyter, matplotlib and downstream modeling frameworks like sklearn</li></ul><h3 id="EDA-and-Data-Visualization-best-practices"><a href="#EDA-and-Data-Visualization-best-practices" class="headerlink" title="EDA and Data Visualization best practices"></a>EDA and Data Visualization best practices</h3><ol><li>The majority of code for any data science project should be contained within text files. This is a software engineering best practice that ensures re-usability, allows for unit testing and works naturally with version control. &gt;In Python the text files can be executable scripts, modules, a full Python package or some combination of these.</li><li>Keep a record of plots and visualization code that you create. It is difficult to remember all of the details of how visualizations were created. Extracting the visualization code to a specific place will ensure that similar plots for future projects will be quick to create.</li><li>Use you plots as a quality assurance tool. Given what you know about the data it can be useful to make an educated guess before you execute the cell or run the script. This habit is surprisingly useful for quality assurance of both data and code.</li></ol><h2 id="Missing-values"><a href="#Missing-values" class="headerlink" title="Missing values"></a>Missing values</h2><ul><li>Dealing with missing data sits at the intersection of EDA and data ingestion in the AI enterprise workflow</li><li>Ignoring missing data may have unintended consequences in terms of model performance that may not be easy to detect</li><li>Removing either complete rows or columns in a feature matrix that contain missing values is called complete case analysis</li><li>Complete case analysis, although commonly used, can lead to undesirable results—the extent to which depends on the category of missingness</li></ul><h3 id="The-categories-of-missingness-are"><a href="#The-categories-of-missingness-are" class="headerlink" title="The categories of missingness are:"></a>The categories of missingness are:</h3><ol><li>Missing completely at random or MCAR:</li></ol><p>When data are MCAR, missing cases are, on average, identical to non-missing cases, with respect to the feature matrix. Complete case analysis will reduce the power of the analysis, but will not affect bias.</p><ol><li>Missing at random or MAR:</li></ol><p>When data are MAR the missing data often have some dependence on measured values, and models can be used to help impute what the likely data would be. For example, in an MLB survey, there may be a gender bias when it comes to completing all of the questions.</p><ol><li>Missing not at random or MNAR:</li></ol><p>In this case the missing data depend on unmeasured or unknown variables. There is no information available to account for the missingness.</p><ul><li>The best case scenario is that the data are MCAR. It should be noted that imputing values under the other two types of missingness can result in an increase in bias.</li><li>In statistics the process of replacing missing data with substituted values is known as imputation.</li><li>It is a common practice to perform multiple imputations.</li><li>The practice of imputing missing values introduces uncertainty into the results of a data science project.</li><li>One way to deal with that additional uncertainty is to try a range of different values for imputation and measure how the results vary between each set of imputations. This technique is known as multiple imputation.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;The-main-goals-of-EDA-are&quot;&gt;&lt;a href=&quot;#The-main-goals-of-EDA-are&quot; class=&quot;headerlink&quot; title=&quot;The main goals of EDA are:&quot;&gt;&lt;/a&gt;The main g
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="EDA" scheme="https://zhangruochi.com/categories/AI-Workflow/EDA/"/>
    
    
  </entry>
  
  <entry>
    <title>Model Training Tricks (2)</title>
    <link href="https://zhangruochi.com/Model-Training-Tricks-2/2020/04/28/"/>
    <id>https://zhangruochi.com/Model-Training-Tricks-2/2020/04/28/</id>
    <published>2020-04-27T20:04:20.000Z</published>
    <updated>2020-04-28T21:09:54.664Z</updated>
    
    <content type="html"><![CDATA[<p>If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a really long time. The reason that deep learning is not straightforward is because your data, memory, and time is limited. If you are running out of memory or time, then the solution is to train a smaller model. If you are not able to train for long enough to overfit, then you are not taking advantage of the capacity of your model.</p><p>So step one is to get to the point that you can overfit. Then, the question is how to reduce that overfitting.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>Many practitioners when faced with an overfitting model start at exactly the wrong end of this diagram. Their starting point is to use a smaller model, or more regularisation. Using a smaller model should be absolutely the last step you take, unless your model is taking up too much time or memory. Reducing the size of your model as reducing the ability of your model to learn subtle relationships in your data.<br>Instead, your first step should be to seek to create more data. That could involve adding more labels to data that you already have in your organisation, finding additional tasks that your model could be asked to solve (or to think of it another way, identifying different kinds of labels that you could model), or creating additional synthetic data via using more or different data augmentation. Thanks to the development of mixup and similar approaches, effective data augmentation is now available for nearly all kinds of data.<br>Once you’ve got as much data as you think you can reasonably get a hold of, and are using it as effectively as possible by taking advantage of all of the labels that you can find, and all of the augmentation that make sense, if you are still overfitting and you should think about using more generalisable architectures. For instance, adding batch normalisation may improve generalisation.<br>If you are still overfitting after doing the best you can at using your data and tuning your architecture, then you can take a look at regularisation. Generally speaking, adding dropout to the last layer or two will do a good job of regularising your model. However, as we learnt from the story of the development of AWD-LSTM, it is often the case that adding dropout of different types throughout your model can help regularise even better. Generally speaking, a larger model with more regularisation is more flexible, and can therefore be more accurate than a smaller model with less regularisation.<br>Only after considering all of these options would be recommend that you try using smaller versions of your architectures.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/fastai/fastbook/blob/master/15_arch_details.ipynb" target="_blank" rel="noopener">https://github.com/fastai/fastbook/blob/master/15_arch_details.ipynb</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a r
      
    
    </summary>
    
    
      <category term="Competition" scheme="https://zhangruochi.com/categories/Competition/"/>
    
      <category term="Tricks" scheme="https://zhangruochi.com/categories/Competition/Tricks/"/>
    
    
  </entry>
  
  <entry>
    <title>Model Training Tricks (1)</title>
    <link href="https://zhangruochi.com/Model-Training-Tricks-1/2020/04/27/"/>
    <id>https://zhangruochi.com/Model-Training-Tricks-1/2020/04/27/</id>
    <published>2020-04-27T08:21:11.000Z</published>
    <updated>2020-04-28T08:04:35.236Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dls</span><span class="params">(bs, size)</span>:</span></span><br><span class="line">    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),</span><br><span class="line">                   get_items=get_image_files,</span><br><span class="line">                   get_y=parent_label,</span><br><span class="line">                   item_tfms=Resize(<span class="number">460</span>),</span><br><span class="line">                   batch_tfms=[*aug_transforms(size=size, min_scale=<span class="number">0.75</span>),</span><br><span class="line">                               Normalize.from_stats(*imagenet_stats)])</span><br><span class="line">    <span class="keyword">return</span> dblock.dataloaders(path, bs=bs)</span><br></pre></td></tr></table></figure><p>Normalization becomes especially important when using pretrained models. The pretrained model only knows how to work with data of the type that it has seen before. If the average pixel was zero in the data it was trained with, but your data has zero as the minimum possible value of a pixel, then the model is going to be seeing something very different to what is intended.</p><p>This means that when you distribute a model, you need to also distribute the statistics used for normalization, since anyone using it for inference, or transfer learning, will need to use the same statistics. By the same token, if you’re using a model that someone else has trained, make sure you find out what normalization statistics they used, and match them.</p><h2 id="Progressive-resizing"><a href="#Progressive-resizing" class="headerlink" title="Progressive resizing"></a>Progressive resizing</h2><blockquote><p>Gradually using larger and larger images as you train</p></blockquote><p>Start training using small images, and end training using large images. By spending most of the epochs training with small images, training completed much faster. By completing training using large images, the final accuracy was much higher. We call this approach progressive resizing.</p><p>Note that for transfer learning, progressive resizing may actually hurt performance. This would happen if your pretrained model was quite <code>similar</code> to your transfer learning task and dataset, and was trained on similar sized images, so the weights don’t need to be changed much. In that case, training on smaller images may damage the pretrained weights. On the other hand, if the transfer learning task is going to be on images that are of different sizes, shapes, or style to those used in the pretraining tasks, progressive resizing will probably help. As always, the answer to “does it help?” is “try it!”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">dls = get_dls(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">learn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(), </span><br><span class="line">                metrics=accuracy)</span><br><span class="line">learn.fit_one_cycle(<span class="number">4</span>, <span class="number">3e-3</span>)</span><br><span class="line"></span><br><span class="line">learn.dls = get_dls(<span class="number">64</span>, <span class="number">224</span>)</span><br><span class="line">learn.fine_tune(<span class="number">5</span>, <span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><h2 id="Test-time-augmentation"><a href="#Test-time-augmentation" class="headerlink" title="Test time augmentation"></a>Test time augmentation</h2><blockquote><p>During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image</p></blockquote><p>Select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we could do this not just for different crops, but for different values across all of our test time augmentation parameters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds,targs = learn.tta()</span><br><span class="line">accuracy(preds, targs).item()</span><br></pre></td></tr></table></figure><h2 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h2><p>Mixup works as follows, for each image:</p><ol><li>Select another image from your dataset at random</li><li>Pick a weight at random</li><li>Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable</li><li>Take a weighted average (with the same weight) of this image’s labels with your image’s labels; this will be your dependent variable</li></ol><p>In pseudo-code, we’re doing (where t is the weight for our weighted average):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image2,target2 = dataset[randint(<span class="number">0</span>,len(dataset)]</span><br><span class="line">t = random_float(<span class="number">0.5</span>,<span class="number">1.0</span>)</span><br><span class="line">new_image = t * image1 + (<span class="number">1</span>-t) * image2</span><br><span class="line">new_target = t * target1 + (<span class="number">1</span>-t) * target2</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>The third image is built by adding 0.3 times the first one and 0.7 times the second. In this example, should the model predict church? gas station? The right answer is 30% church and 70% gas station since that’s what we’ll get if we take the linear combination of the one-hot encoded targets. For instance, if church has for index 2 and gas station as for index 7, the one-hot-encoded representations are</p><blockquote><p>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]<br>[0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0]</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = xresnet50()</span><br><span class="line">learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), </span><br><span class="line">                metrics=accuracy, cbs=Mixup)</span><br><span class="line">learn.fit_one_cycle(<span class="number">5</span>, <span class="number">3e-3</span>)</span><br></pre></td></tr></table></figure><h2 id="Label-smoothing"><a href="#Label-smoothing" class="headerlink" title="Label smoothing"></a>Label smoothing</h2><p>In the theoretical expression of the loss, in classification problems, our targets are one-hot encoded (in practice we tend to avoid doing it to save memory, but what we compute is the same loss as if we had used one-hot encoding). That means the model is trained to return 0 for all categories but one, for which it is trained to return 1. Even 0.999 is not good enough, the model will get gradients and learn to predict activations that are even more confident. This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category even if it’s not too sure, just because it was trained this way. <strong>It can become very harmful if your data is not perfectly labeled.</strong></p><p>This is how label smoothing works in practice: we start with one-hot encoded labels, then replace all zeros by</p><script type="math/tex; mode=display">\frac{\epsilon}{N}</script><p>where $N$ is the number of classes and $\epsilon$ is a parameter (usually 0.1, which would mean we are 10% unsure of our labels). Since you want the labels to add up to 1, replace the 1 by </p><p><script type="math/tex">1-\epsilon + \frac{\epsilon}{N}</script>. </p><p>This way, we don’t encourage the model to predict something overconfident: in our Imagenette example where we have 10 classes, the targets become something like:</p><blockquote><p>[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Normalization&quot;&gt;&lt;a href=&quot;#Normalization&quot; class=&quot;headerlink&quot; title=&quot;Normalization&quot;&gt;&lt;/a&gt;Normalization&lt;/h2&gt;&lt;figure class=&quot;highlight pyth
      
    
    </summary>
    
    
      <category term="Competition" scheme="https://zhangruochi.com/categories/Competition/"/>
    
      <category term="Tricks" scheme="https://zhangruochi.com/categories/Competition/Tricks/"/>
    
    
  </entry>
  
  <entry>
    <title>Empathize Stage</title>
    <link href="https://zhangruochi.com/Empathize-Stage/2020/04/21/"/>
    <id>https://zhangruochi.com/Empathize-Stage/2020/04/21/</id>
    <published>2020-04-21T15:47:10.000Z</published>
    <updated>2020-04-22T05:30:34.864Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Empathize-Process"><a href="#Empathize-Process" class="headerlink" title="Empathize Process"></a>Empathize Process</h2><ol><li>Get as close to the source of data as possible usually by interviewing the people involved</li><li>Identify the business problem</li><li>Obtain all of the relevant the data</li><li>Translate the business problem into a testable hypothesis or hypotheses</li></ol><h2 id="Identifying-the-business-opportunity-Through-the-eyes"><a href="#Identifying-the-business-opportunity-Through-the-eyes" class="headerlink" title="Identifying the business opportunity: Through the eyes"></a>Identifying the business opportunity: Through the eyes</h2><h3 id="Our-story"><a href="#Our-story" class="headerlink" title="Our story"></a>Our story</h3><p><strong>The first stage of any project in a large enterprise is to identify the business opportunity. In the world of design thinking, this begins with the Empathize stage</strong>. During this time, you and your team are gathering as much information as possible to understand the challenges faced by your AAVAIL.</p><p>You are suprised by the fact that you, a data scientist, are being asked to help out with interviews, observations, process mapping, and various design thinking sessions. These techniques as well as many others are used during the empathize stage to gather <strong>as much information as possible</strong> so that a problem may be defined.</p><p>As a data scientist, this process should be used to guide your investigative process. Ultimately, your top priority is to analyze the data coming out of Singapore, understand the problem and fix the situation. <strong>The involved parties are subscribers, data engineers, data scientists, marketing and management</strong>. You are going to need to talk everyone involved in the data generation process. This is why you’re spending time on interviews and observations.</p><p><strong>Asking questions is a critical part of getting the process started</strong>. You will want to be naturally curious gathering details about the product, the subscriber, and the interaction between the two. This information gathering stage provides both a perspective on the situation and it will help you formulate the business question.</p><p>In the short sections below, we provide guidelines for asking questions and beginning with an investigative mindset.</p><h3 id="Articulate-the-business-question"><a href="#Articulate-the-business-question" class="headerlink" title="Articulate the business question"></a>Articulate the business question</h3><p>There are generally many business questions that can be derived from a given situation. It is an important thought exercise to enumerate the possible questions, that way it makes the discussion easier when you work with the involved stakeholders in order to focus and prioritize. In this situation here are some ways of articulating the business case.</p><ul><li>Can we use marketing to reduce the rate of churn?</li><li>Can we salvage the Singapore market with new products?</li><li>Are there factors outside of our influence that caused the situation in Singapore and is it temporary?</li><li>Can we identify the underlying variables in Singapore that are related to churn and can we use the knowledge to remedy the situations?</li><li>The business problem in all of these examples is written shown in terms of the data we have.</li></ul><p>NOTE: This case study can be approached in many different ways and there may not be a clear right or wrong. During the various modules of this course, we will provide guidance when there are multiple paths to choose from.</p><h3 id="Prioritize"><a href="#Prioritize" class="headerlink" title="Prioritize"></a>Prioritize</h3><p>It is logical, but there is a need to prioritize If there are several distinct business objectives. In this case maybe one is related to reducing chrun directly and another is about profitability.</p><p>There are three major contributing factors when it comes to priority.</p><h4 id="Stakeholder-or-domain-expert-opinion"><a href="#Stakeholder-or-domain-expert-opinion" class="headerlink" title="Stakeholder or domain expert opinion"></a>Stakeholder or domain expert opinion</h4><p>In situations where considerable domain expertise is required to effectively prioritize (e.g. Physics, Medicine and Finance) prioritization will likely be driven by the people closest to the domain.</p><h4 id="Feasibility"><a href="#Feasibility" class="headerlink" title="Feasibility"></a>Feasibility</h4><ul><li>Do we have the necessary data to address the business questions?</li><li>Do we have clean enough data to address the business questions?</li><li>Do we have the technology infrastructure to deploy a solution once the data are modeled?</li></ul><h4 id="Impact"><a href="#Impact" class="headerlink" title="Impact"></a>Impact</h4><p>When looking at Impact we’re purely looking at expected dollar contribution and added value from a monetary perspective. When possible, calculating the back-of-the-envelope ROI is a crucial step that you can do. This is an expectation and not real ROI calculation, but it is a guiding principle nonetheless.</p><p>The ROI calculation should be an expected dollar value that you can generate based on all available information you currently have in your organization combined with any domain insight you can collect.</p><p>Measuring the back-of-the-envelope ROI calculation could make use of any of the following:</p><ul><li>Estimates for fully-loaded salaries of employees involved</li><li>Cost per unit item and/or time required to produce</li><li>Number of customers, clients, or users</li><li>Revenue and more</li></ul><h2 id="Scientific-Thinking-for-Business"><a href="#Scientific-Thinking-for-Business" class="headerlink" title="Scientific Thinking for Business"></a>Scientific Thinking for Business</h2><h3 id="Our-Story"><a href="#Our-Story" class="headerlink" title="Our Story"></a>Our Story</h3><p>Data science involves lots of investigation via trial and error. The investigations are based on evidence and this is one of the strongest reasons why data science is considered a “real” science.</p><p>You will be using a scientific process with your work at AAVAIL. This will help you to organize your work as well as be able to clearly explain everything you are doing to the AAVAIL leadership.</p><p>Let’s take a look now at some guidance and best practices for engaging with a <strong>scientific mindset</strong>.</p><h3 id="Science-is-a-process-and-the-route-to-solving-problems-is-not-always-direct"><a href="#Science-is-a-process-and-the-route-to-solving-problems-is-not-always-direct" class="headerlink" title="Science is a process and the route to solving problems is not always direct"></a>Science is a process and the route to solving problems is not always direct</h3><p>A common argument made by statisticians and mathematicians is that data science is not really a science. This is untrue, mainly because data science involves a lot of <strong>investigations</strong> through sometimes chaotic data sets, in search of meaningful patterns that might help in solving particular problems.</p><p>Since data science implies a scientific approach, it is important that all data scientists learn to adopt and use a scientific thought process. <strong>A scientific thought process of observation, developing hypotheses, testing hypotheses, and modifying hypotheses is critical to your success as a data scientist</strong>.</p><p>Pulling in data and jumping right into exploratory data analysis can make your work prone to exactly the types of negative issues that plague data science today. There are a number of well-discussed issues revolving around data science and data science teams not living up to promised potential.</p><p>At the heart of this problem is the process of communicating results to leadership. It should begin with a meaningful and well-articulated business opportunity. If that opportunity is stated too simply, as say, increasing overall revenue then the central talking point for communication is too vague to be meaningful from the data side.</p><blockquote><p>The business scenario needs to be communicated in a couple of ways:</p><ol><li>Stated in a testable way in terms of data</li><li>Stated in a clear way that minimizes the influence of confounding factors</li></ol></blockquote><h3 id="Testable-hypotheses"><a href="#Testable-hypotheses" class="headerlink" title="Testable hypotheses"></a>Testable hypotheses</h3><p>There is no one single best way to articulate a business opportunity as a testable hypothesis. In some cases the statement will be intuitive, but in other cases there will be some back and forth with stakeholders and domain experts.</p><h3 id="Guidelines-for-creating-testable-hypotheses"><a href="#Guidelines-for-creating-testable-hypotheses" class="headerlink" title="Guidelines for creating testable hypotheses"></a>Guidelines for creating testable hypotheses</h3><ol><li>Become a scientist of the business</li></ol><p>Spend a little bit less time learning new algorithms and Python packages and more time learning the levers that make your specific business go up or down and the variables that impact those levers.</p><ol><li>Make an effort to understand how the data are produced</li></ol><p>If it comes down to it, sources of variation can be explicitly accounted for in many types of models. If the data come from a database you should ask about the process by which the data are stored. If the data are compiled by another person then dig into the details and find out about the compiling process as well as the details of what happened before the data arrived on their desk.</p><ol><li>Make yourself part of the business</li></ol><p>Do not under any circumstances become siloed. Proactively get involved with the business unit as a partner, not a support function.</p><ol><li>Think about how to measure success</li></ol><p>When thinking about what course of action might be most appropriate, keep at the forefront of your mind how you will measure business value when said action is complete.</p><p><strong>IMPORTANT</strong>: Data Science is NOT Business Intelligence. BI analysts serve to derive business insights out of data. There is without a doubt some overlap, but the job of a data scientist is to investigate the business opportunity and solve it.</p><p>There is a balancing act to maintain between directly addressing the business need and ensuring that you have thoughtfully studied the problem enough to ensure that you can account for most of the likely contingencies. The scientific method can be of some guidance here.</p><h3 id="Thinking-scientifically-about-the-business-scenario"><a href="#Thinking-scientifically-about-the-business-scenario" class="headerlink" title="Thinking scientifically about the business scenario"></a>Thinking scientifically about the business scenario</h3><p>A major goal of this process is to make the business objectives clear to leadership. Some of these individuals are technical and some are not, so as a good rule-of-thumb get in the habit of articulating the business problem at a level that everyone can understand. Stakeholders and leadership need to know what you are trying to accomplish before you begin work. They also need to be aware from the start what success would look like. Science is an iterative process and many experiments produce results that some might consider a failure. However, experiments that are properly setup will not fail no matter the result–the result may not useful but you have gained valuable information along the way.</p><p>Experiments in this context could refer to an actual scientific experiment (e.g. A/B testing) or it could be more subtle. Let’s say you work for a company that collects tolls in an automated way, and you want to identify the make and model of each car in order to modify pricing models based on predicted vehicle weight. After talking with the stakeholders and the folks who implemented the image storage solution you are ready to begin. The experiment here has to do with how you begin. You may think that there is enough training data to implement a huge multi-class model and just solve most of the problem. If you approach it that way then you are hypothesizing that the solution will work.</p><p>For those of you who have done much image analysis work, you could guess that approach would likely result in a significant loss of time. If we take a step back and think scientifically, we could approach the solution from an evidence driven perspective. Before investing a significant amount of time you may try to see if you can distinguish one make and model from the rest before adding more classes. You may want to first pipe the images through an image segmentation algorithm to identify the make of the car. There are many possible ways to build towards a comprehensive solution, but it is important to determine if either of these piecemeal approaches would have any immediate business value.</p><p>This might be a good time for a reminder about the steps in the scientific method.</p><h3 id="The-Scientific-Method"><a href="#The-Scientific-Method" class="headerlink" title="The Scientific Method"></a>The Scientific Method</h3><p>It is the process by which science is carried out. The general idea is to build on previous knowledge to in order to improve an understanding of a given topic.</p><ol><li>Formulate the question</li><li>Generate a hypothesis to address the question</li><li>Make a prediction</li><li>Conduct an experiment</li><li>Analyze the data and draw a conclusion</li></ol><p>We will continue with an interactive example, but first it is important to note that <strong>Scientific experiments must be repeatable in order to become reliable evidence.</strong></p><h4 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h4><p>The question can be open-ended and generally it summarizes your business opportunity. Let’s say you work for a small business that manufactures sleds and other winter gear and you are not sure which cities to build your next retail locations. You have heard that Utah, Colorado and Vermont are all states that have high rates of snowfall, but it is unclear which one has the highest rate of snowfall.</p><h4 id="Hypothesis"><a href="#Hypothesis" class="headerlink" title="Hypothesis"></a>Hypothesis</h4><p>Because the Rocky mountains are higher in elevation and they are well-known for fresh powder on the ski slopes you hypothesize that both Utah and Colorado have more snow than Vermont.</p><h4 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h4><p>If you were to run a hypothesis test Vermont would have significantly less snow fall than Colorado or Utah</p><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><p>You hit the NOAA weather API to get average annual snowfall by city. We have compiled these data for you in snowfall.csv. You could use a 1-way ANOVA to test the validity of your prediction, but let’s start by looking at the data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First we read in the data</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"../data/snowfall.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Next, subset the data to focus only on the states of interest</span></span><br><span class="line"></span><br><span class="line">mask = [<span class="keyword">True</span> <span class="keyword">if</span> s <span class="keyword">in</span> [<span class="string">'CO'</span>,<span class="string">'UT'</span>,<span class="string">'VT'</span>] <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">'state'</span>].values]</span><br><span class="line">df1 = df[mask]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, create a pivot of the data that focuses only on the relevant summary data</span></span><br><span class="line"></span><br><span class="line">pivot = df1.groupby([<span class="string">'state'</span>])[<span class="string">'snowfall'</span>].describe()</span><br><span class="line">df1_pivot = pd.DataFrame(&#123;<span class="string">'count'</span>: pivot[<span class="string">'count'</span>],</span><br><span class="line">                          <span class="string">'avg_snowfall'</span>: pivot[<span class="string">'mean'</span>],</span><br><span class="line">                          <span class="string">'max_snowfall'</span>: pivot[<span class="string">'max'</span>]&#125;)</span><br><span class="line">print(df1_pivot)</span><br><span class="line"></span><br><span class="line"><span class="comment">#        count  avg_snowfall  max_snowfall</span></span><br><span class="line"><span class="comment"># state</span></span><br><span class="line"><span class="comment"># CO       5.0         37.76          59.6</span></span><br><span class="line"><span class="comment"># UT       2.0         51.65          58.2</span></span><br><span class="line"><span class="comment"># VT       1.0         80.90          80.9</span></span><br></pre></td></tr></table></figure><h4 id="Analyze"><a href="#Analyze" class="headerlink" title="Analyze"></a>Analyze</h4><p>There is not enough data to do a 1-way ANOVA. The experiment is not a failure; it has a few pieces of information.</p><p>There is not enough data<br>There is a small possibility that VT gets more snow on average than either CO or UT<br>Our degree of belief in the conclusion drawn from (2) is very small because of (1)<br>The notion of degree of belief is central to scientific thinking. It is somehow a part of our human nature to believe statements that have little to no supporting evidence. <strong>In science the word belief, with respect to a hypothesis is proportional to the evidence</strong>. With more evidence available, ideally, from repeated experiments, one’s degree of belief should change. Evidence is derived from the process described above and if we have none then we are stuck at the question stage and a proper scientific hypothesiscannot be made.</p><p>The other important side to degree of belief is that it never caps out at 100 percent certainty. Some hypotheses have become laws like Newton’s Law of Gravitation, but most natural phenomena in the world outside of physics cannot be explained as a law.</p><p>A hypothesis is the simplest explanation of a phenomenon. A scientific theory is an in-depth explanation of the observed phenomenon. Do not be mistaken with the word theory, there can be sufficient evidence that your degree of belief all but touches 100%, and is plenty for decision making purposes. A built-in safeguard for scientific thought is that our degree of belief does not reach 100%, which leaves some room to find new evidence that could move the dial in the other direction.</p><p>There are additional factors like external peer review that help ensure the integrity of the scientific method and in the case of implementing a model for a specific business task this could mean assigning reviewers for a pull request or simply asking other qualified individuals to check over your work.</p><h2 id="Gathering-Data"><a href="#Gathering-Data" class="headerlink" title="Gathering Data"></a>Gathering Data</h2><h3 id="Our-Story-1"><a href="#Our-Story-1" class="headerlink" title="Our Story"></a>Our Story</h3><p>Your first step at AAVAIL, just like everywhere else, it to look at the data sources. You soon discover that AAVAIL has data everywhere! There is no shortage of data. It looks like they have managed to store every type of transaction with their subscribers.</p><p>You will need a smart way of managing all of this data. Let’s take a look now at some best practices for managing data in a large enterprise.</p><h3 id="Documenting-your-data"><a href="#Documenting-your-data" class="headerlink" title="Documenting your data"></a>Documenting your data</h3><p>Too often data scientists will find themselves deep in the process of developing a solution, based on the data that was provided to them, before they realize that the data itself is flawed, inaccurate or in some other way non-ideal. Developing the habit of creating a simple document with at least a description of the ideal data needed to test the hypotheses around the business problem may seem like an unnecessary step, but it has potential to both:</p><ul><li>Streamline the modeling process</li><li>Help ensure that all future data come in an improved form</li></ul><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><p>The process of gathering data is often referred to as Extract, Transform, Load (ETL). Data is generally gathered (or extracted) from heterogeneous sources, cleaned (transformed) and loaded into a single place that facilitates analysis. Before the advent of the modern data scientist’s toolkit data was often staged in a database,data lake or a data warehouse. Still today data is frequently staged to facilitate collaboration, but there are tools now that enable more possibilities today than ever before. Jupyter Lab has an extension called data grid that allows it to read delimited files with millions or even billions of rows. Then tools like Dask help you scale your analyses. To ensure that projects are completed in a reasonable amount of time the initial pass at ETL should use a simple format like CSV, then a more complex system can be built out once you have accomplished the Minimum Viable Product (MVP).</p><h3 id="Common-methods-of-gathering-data"><a href="#Common-methods-of-gathering-data" class="headerlink" title="Common methods of gathering data"></a>Common methods of gathering data</h3><h4 id="Plain-text-files"><a href="#Plain-text-files" class="headerlink" title="Plain text files"></a>Plain text files</h4><p>Plain text file can come in many forms and generally the open function is used to bring the data into a Python environment. This is a flexible format, but because no structure is imposed, custom scripts are generally needed to parse these files and these scripts do not always generalize to new files.</p><p>The large majority of data science projects involve a modeling step that requires input data in a tabular numeric format. In order to extract data from a plain text file you may need to identify patterns in the text and use regular expressions (regex) to pull out the relevant information. Python’s built-in regex library is known as re.</p><p>On the other hand if the data you are working with consists of natural language, then there are a number of libraries that can work directly with the text files. The two main libraries are:</p><ul><li>spaCy</li><li>NLTK</li></ul><p>Also, scikit-learn has become a standard tool in the overall workflow when processing these files.</p><ul><li>scikit-learn’s text tutorial</li></ul><p>These tools can be applied to unstructured text to generate things like word counts, and word frequencies. We saw an example of this in the Data science workflow combined with design thinking example.</p><h4 id="Delimited-files"><a href="#Delimited-files" class="headerlink" title="Delimited files"></a>Delimited files</h4><p>One of the most commonly encountered ways of storing structured data is in delimited files, where rows of tabular data are stored in lines of a text file and the columns within each row are separated by a special “delimiter” character such as a comma or a tab character.</p><p>This simple structure helps account for the popularity of these formats, with probably the most widely used being Comma-Separated Values (CSV). CSV files are both human and machine readable, and have minimal overhead in terms of the proportion of the file devoted to defining the structure of the data when compared to most other file formats. As such Pandas comes with methods for both reading and writingCSV files. (Note that these functions can also handle other delimiters like tab or the pipe character “|”, but commas are the default.)</p><p>Spreadsheet programs like Microsoft Excel that are used for analyzing tabular data also read from and write to files in CSV format. The native Excel file format (often with file extensions .xls or .xlsx) can also be considered a delimited file type. Though these files also contain a significant amount of extra information related to things like styling that are separate from the actual data. Nonetheless, since these files are commonly used to save datasets, Pandas also has a method for reading them: pandas.read_excel.</p><p>HINT: A best practice when loading data from plain text or delimited files is to separate the code for parsing into its own script. Because the files are read line by line in a separate Python call, it is more memory efficient and this separation of tasks helps with automation and maintenance.</p><p>It is a common mistake to try to read large files into pandas then use the date frame environment to parse. If your parsing (cleaning) task is simple then use a parser. Here is a simple example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/evn/python</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">simple example of a parser</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment">## specify the files</span></span><br><span class="line">file_in = os.path.join(<span class="string">".."</span>,<span class="string">"data"</span>,<span class="string">"snowfall.csv"</span>)</span><br><span class="line">file_out = os.path.join(<span class="string">".."</span>,<span class="string">"data"</span>,<span class="string">"snowfall_parsed.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## create an outfile handle (needs to be closed)</span></span><br><span class="line">fidout = open(file_out,<span class="string">"w"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## use the csv module to read/write</span></span><br><span class="line">writer = csv.writer(fidout)</span><br><span class="line"></span><br><span class="line"><span class="comment">## generic parsing function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> line[<span class="number">3</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">"HI"</span>,<span class="string">"NC"</span>,<span class="string">"OR"</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> line + [<span class="string">'new_data'</span>]</span><br><span class="line">    </span><br><span class="line"><span class="comment">## for each line in the file read in the first file that you need to reference</span></span><br><span class="line"><span class="keyword">with</span> open(file_in) <span class="keyword">as</span> csvfile:</span><br><span class="line">    reader = csv.reader(csvfile, delimiter=<span class="string">','</span>)</span><br><span class="line">    header_in = reader.__next__()</span><br><span class="line">    writer.writerow(header_in + [<span class="string">"new_column"</span>])</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> reader:</span><br><span class="line">        line = parse_line(line)</span><br><span class="line">        <span class="keyword">if</span> line:</span><br><span class="line">            writer.writerow(line)</span><br><span class="line">   </span><br><span class="line">fidout.close()</span><br><span class="line">print(<span class="string">"done parsing"</span>)</span><br></pre></td></tr></table></figure><p>The highlighted lines show where this parser changes the original data by filtering and adding an additional column.</p><h4 id="JSON-files"><a href="#JSON-files" class="headerlink" title="JSON files"></a>JSON files</h4><p>While delimited files are well suited for housing data in flat tables, datasets with more complex structures require different formats. The JavaScript Object Notation (JSON) file format can accommodate quite complex data hierarchies. Python’s built-in library handles reading/writing JSON files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">data = json.load(open(<span class="string">'some_file.json'</span>))</span><br></pre></td></tr></table></figure><p>In addition, pandas.read_json is also available for loading JSON files.</p><p>At its base a JSON object can be thought of as analogous to a Python dictionary or list of dictionaries. For example a table of data from a JSON file could be read into Python as a list of dictionaries where each dictionary represented a row of the table, and the keys of each dictionary were the column names. This formatting is somewhat inefficient for simple tabular data, with column information explicitly repeated with each row, but is useful when representing more intricate relationships in the data. JSON objects often have a highly nested structure that you can think of as dictionaries within dictionaries within dictionaries.</p><p>For example, modern websites track a great deal of information about users’ interactions with the site and the varied nature of these interactions make a table structure too rigid for recording them. In practice, most sites send JSON objects back and forth between the user’s computer and the website’s server. Many data scientists’ primary source of data are ultimately these JSON objects.</p><h4 id="Relational-databases"><a href="#Relational-databases" class="headerlink" title="Relational databases"></a>Relational databases</h4><p>Relational databases, i.e. those that impose a schema on datasets are a major source of data for data science projects. Database tables can naturally be converted into Python objects like Pandas DataFrames. Reading data into a Python environment requires opening a connection to a database and there are various libraries for managing this connection, depending on the type of database to be accessed. Some Relational DataBase Management System (RDBMS) and their corresponding interface utilities for Python:</p><div class="table-container"><table><thead><tr><th style="text-align:left">RDBMS</th><th style="text-align:left">Python Connector</th></tr></thead><tbody><tr><td style="text-align:left">MySQL</td><td style="text-align:left">MySQL Connector</td></tr><tr><td style="text-align:left">PostgreSQL</td><td style="text-align:left">Psycopg</td></tr><tr><td style="text-align:left">SQLite</td><td style="text-align:left">sqlite3</td></tr><tr><td style="text-align:left">Microsoft SQL</td><td style="text-align:left">pyodbc</td></tr></tbody></table></div><p>Each of these tools are designed with maintaining the integrity of the database in mind, including methods for rolling back updates, and ways to safeguard against SQL Injection vulnerabilities. As such, the process of querying the database and ingesting the results can seem fairly involved. For example, here is a basic flow for getting the contents of a table from a PostgreSQL database using psycopg2.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> psycopg2 <span class="keyword">as</span> pg2</span><br><span class="line">conn = pg2.connect(database=<span class="string">'my_db'</span>, user=<span class="string">'my_username'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a cursor to traverse the database</span></span><br><span class="line">cur = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># cursor object executes a query, but does not automatically return results</span></span><br><span class="line">cur.execute(<span class="string">"SELECT * FROM my_table"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Return all query results</span></span><br><span class="line">results = cur.fetchall()</span><br><span class="line"><span class="comment"># WARNING: If the result set is large, it may overwhelm the memory</span></span><br><span class="line"><span class="comment"># resources on your machine.</span></span><br><span class="line"></span><br><span class="line">cur.close()</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><p>While the steps required to connect, query, and disconnect from a relational database are more involved than when loading in data from a file on your local machine, the table structure from the database schema basically guarantees that the data will fit cleanly into a Pandas DataFrame or NumPy Array.</p><h4 id="NoSQL-databases"><a href="#NoSQL-databases" class="headerlink" title="NoSQL databases"></a>NoSQL databases</h4><p>“NoSQL” is a catch-all term referring to “non SQL” or “non relational”, or more recently “not only SQL”. Usually meaning that the method for housing data does not impose a schema on it (or at least not as tightly constrained as in relational databases). This tradeoff gives greater flexibility in what and how data are stored at the cost of increased traversal times when searching the database. This tradeoff is similar to the one we encountered when working with delimited files like CSVs and with JSON files. When loading or dumping data between a file and a database, CSVs are a good match for tables in a relational database, whereas JSONs are more aligned with NoSQL databases.</p><p>The are many examples of NoSQL databases, each with different use cases, and most of which can be accessed with Python.</p><p>One flexible and popular example is MongoDB. MongoDB is a document-oriented database, where a “document” encapsulates and encodes data in a standard format. In the case of MongoDB, that format is JSON-like. Like the relational databases mentioned above, MongoDB has a client for querying it directly, as well as a connector for querying from within Python. These queries are constructed usingMongoDB’s query syntax.</p><p>The Python connector to MongoDB is PyMongo.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"><span class="comment"># By default a Mongo db running locally is accessible via port 27017</span></span><br><span class="line">client = MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'database_name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Within a db, documents are grouped into "collections" -- roughly equivalent</span></span><br><span class="line"><span class="comment"># to tables in a relational db.</span></span><br><span class="line">coll = db[<span class="string">'collection_name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Return all the documents within the collection</span></span><br><span class="line">docs = coll.find()</span><br></pre></td></tr></table></figure><h4 id="Web-scraping-and-APIs"><a href="#Web-scraping-and-APIs" class="headerlink" title="Web scraping and APIs"></a>Web scraping and APIs</h4><p>Automating the process of downloading content from websites is known as web scraping.</p><p><strong> IMPORTANT </strong></p><blockquote><p>Web scraping can be done in legitimate ways, but just as easily web scraping tools do not stop you from violating a websites terms of service. If a website encourages the sharing of their data then they will create a specific API endpoint that you will use. More often than not the API will require to have an identifying key.</p></blockquote><p>Various tools in Python are available for accessing and parsing webpage data.Requests is a user-friendly library for downloading web pages. It can also be used to retrieve files that are exposed through a URL. For a webpage the data returned from a call using Requests is the HyperText Markup Language (HTML) code that instructs a client such as a web browser how to display a page. This HTML code will often (but not always) contain the data you want to collect from the particular webpage.</p><p>Modern webpages tend to have a great deal of information in their HTML beyond what is shown to the user, so parsing through it all to collect the relevant data can be a daunting task. Fortunately, if a page is readable in your browser, then its HTML must have a coherent structure. Beautiful Soup is a Python library that provides tools for walking through that structure in a systematic and repeatable way. Thus, in the context of web scraping Beautiful Soup can be used to extract the relevant data from the soup of all the other information contained in an HTML file.</p><p>Many websites’ contents are dynamically rendered in such a way that the information displayed on a page never makes it directly into the page’s HTML. In such cases it may not be possible to download the data of interest with a tool like Requests. One option in this scenario is to move to a tool for browser automation, such as Selenium. Selenium’s Python interface is described here.</p><p>Another tool, specifically designed for web scraping in Python, is Scrapy.</p><p>Depending on your website of interest you may have to try several of these tools to successfully collect the relevant data in a scalable way. But a general rule of thumb is that if you can see what you want to collect in your browser, the the website sent it to you, so it should be retrievable.</p><h4 id="Streaming-data"><a href="#Streaming-data" class="headerlink" title="Streaming data"></a>Streaming data</h4><p>In the modern landscape of business data streams are becoming more common. A data stream is a sequence of digitally encoded signals. Data can be streamed for many purposes including storage and further processing (like modeling). Data streams become important when the data of a project or company becomes mature and the AI pipeline is connected to it. As we move into the portions of the AI enterprise workflow that focus on models in production we will be using Apache Spark’s streaming to connect deployed models with streaming data. Data collected from sensors or devices connected via the internet of things are oftent setup to produce streaming data. We will work specifically with these types of data in module 5.</p><h4 id="Apache-Hadoop-File-Share-HDFS"><a href="#Apache-Hadoop-File-Share-HDFS" class="headerlink" title="Apache Hadoop File Share (HDFS)"></a>Apache Hadoop File Share (HDFS)</h4><p>Apache Hadoop File Share (HDFS) is the core of Apache Hadoop , an open source system that is designed to use arrays of commodity hardware to store and manage very large datasets.</p><p>HDFS is the storage component of the system. Large datasets are divided into blocks, and those blocks are distributed and stored across the nodes in an HDFS cluster. Any code that is created to analyze the datasets stored in a Hadoop cluster is executed locally for each block of data, and in parallel. This parallel analysis of data blocks means that Hadoop can process very large data sets rapidly.</p><p>The Hadoop framework itself is written mostly in Java. However, any language, including Python, may be used to analyze the data stored in a Hadoop cluster. The Apache Foundation provides a number of other packages that may be installed alongside Hadoop to add additional relational database functionality and improve scalability.</p><p>IMPORTANT: Apache Hadoop is a de facto standard in many large enterprises today. It is often used with Apache Spark and a NoSQL database engine to provide data storage and management of data pipelines used by machine learning models.</p><h4 id="Other-sources-of-data-formats"><a href="#Other-sources-of-data-formats" class="headerlink" title="Other sources of data formats"></a>Other sources of data formats</h4><div class="table-container"><table><thead><tr><th style="text-align:left">Format</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">HDF5</td><td style="text-align:left">There is a is a hierarchical format HDF5 used to store complex scientific data. The format is useful for storing and sharing large amounts of data.</td></tr><tr><td style="text-align:left">NumPy’s *.npy and *.nzp formats</td><td style="text-align:left">NumPy has its own binary format (NPY) and the NPZ format is an extension of it that allows multiple arrays and compression.</td></tr></tbody></table></div><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this module you should have learned:</p><ul><li>Stakeholder or domain expert opinion, feasibility and impact are three of the most important factors when prioritizing business opportunities</li><li>The practice of articulating a business opportunity, with the data in mind, as a testable hypothesis helps keep the overall project linked to the business needs</li><li>The notion of degree of belief is important when making statements both in science and in business. No statement has 100% degree of belief, it is some percentage of 100% that is a reflection of accumulated evidence</li><li>The scientific method helps formalize a process for rationalizing business decisions through experimentation and evidence</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Empathize-Process&quot;&gt;&lt;a href=&quot;#Empathize-Process&quot; class=&quot;headerlink&quot; title=&quot;Empathize Process&quot;&gt;&lt;/a&gt;Empathize Process&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Get 
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Process Model" scheme="https://zhangruochi.com/categories/AI-Workflow/Process-Model/"/>
    
    
  </entry>
  
  <entry>
    <title>Decision Thinking and Data Science Process</title>
    <link href="https://zhangruochi.com/Decision-Thinking-and-Data-Science-Process/2020/04/21/"/>
    <id>https://zhangruochi.com/Decision-Thinking-and-Data-Science-Process/2020/04/21/</id>
    <published>2020-04-21T09:18:18.000Z</published>
    <updated>2020-04-21T21:40:13.008Z</updated>
    
    <content type="html"><![CDATA[<p>Between design thinking and the above mentioned process models there are several 1:1 relationships between stages. The other relationships are generally straightforward to delineate. The design thinking process is consists of five stages and it has the distinct advantage of being applied outside of data science.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Design Thinking</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Process Models</div></center><p>It is the details that keep you flowing from one stage to the next, while iterating in ways that are business driven that makes the contents of this course new. Let’s use a simple example to illustrate the basic process.</p><blockquote><p>A friend of yours just opened a new Sherlock Holmes themed café. Her café is state-of-the-art complete with monitors built into the tables. The business is off to a good start, but she has gotten some feedback that the games could use improvement. She knows that good games keep the customers around a little longer. The games are a way to keep customers entertained while they drink coffee and buy food items. She has some games already, but wants your help to create a few more games to keep customers both informed and entertained.</p></blockquote><p>Being a data scientist you would not just sit down and create a game—you are, of course, going to create based on your initial investigation of the business scenario.</p><h3 id="Empathize"><a href="#Empathize" class="headerlink" title="Empathize"></a>Empathize</h3><p><strong>In this stage time is dedicated to understanding the business opportunities.</strong></p><p>In this setting the frequency and duration of customer visits are going to be related to overall sales. The initial business opportunity here is How do you ensure new games drive revenue?. There are many other business opportunities, like what is the optimal menu for the customer-base and do seasonal variations of offerings help the business?, but lets focus on the initial one for this example. As part of this stage you would talk with your friend, her employees and some customers to do your best to fully understand the experience of the customer. The important thing here is to spend time on-site simulating the experience of a customer to obtain as genuine an understanding of the problem as possible. You may realize that most customers are there to work or most of them are just passing through. This domain knowledge is useful when making decisions like which new types of new games to create. After you have gathered your information and studied it you will generally articulate the business scenario using a scientific thought process—this means a statement that can be tested. The business opportunity should be stated in a way that minimizes the presence of confounding factors.</p><p>There are logical follow-up questions to ask to fully understand the problem, but the next two stages are the more appropriate places to get into these details. Now that you understand the problem it is time to gather the data.</p><p><strong>HINT</strong>: This is the stage where we gather all of the data and we make note of what would be ideal data.</p><ol><li><p>The data here are mostly sales and customer profiles. There are two important aspects of the data that would be ideal:</p></li><li><p>The data are at a transaction level (each purchase and its associated data are recorded)<br>We can associate game usage with transactions.</p></li></ol><p>Fortunately for us this is a modern cafe so customers order and play games through the same interface. Additionally, they are incentivized to login to the system and generate a customer profile. In this stage we go through the process of gathering the raw data. This may involve querying a database, gathering files, web-scraping and other mechanisms. It is important to gather <strong>all of the relevant data</strong> in this stage, because access and quality of the data may force you to modify the business question. It is very difficult to assess the quality of data when it is not in hand. If possible effort should be made to collect even marginally related data.</p><p>Lets assume that your initial investigation led you to understand that games that used quotations from the books in an interactive way were the most effective. So you have come up with the idea to develop a game that is built on a chatbot that has been trained to talk like Sherlock. This would involve Natural Language Processing (NLP) and we would need a corpus. As a start you might download The Adventures of Sherlock Holmes, by Arthur Conan Doyle from Project Gutenberg.</p><p><strong>HINT</strong>: This is a live coding example and we suggest that you open a Jupyter notebook either locally or within Watson Studio so that you may annotate and expand on the example freely.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">text = requests.get(<span class="string">'https://www.gutenberg.org/files/1661/1661-0.txt'</span>).text</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"sherlock-holmes.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> text_file:</span><br><span class="line">    text_file.write(text)</span><br></pre></td></tr></table></figure><h3 id="Define"><a href="#Define" class="headerlink" title="Define"></a>Define</h3><p><strong>This is the data wrangling stage</strong></p><p>Given the data, an understanding of the business scenario and your gathered domain knowledge you will next perform your data cleaning and preliminary exploratory data analysis. To get to the point of preliminary investigation into the findings from the empathize stage it is frequently the case that we need to clean our data.</p><p>This could involve parsing JSON, manipulating SQL queries, reading CSV, cleaning a corpus of text, sifting through images, and so much more. One common goal of this part of the process is the creation of one or more Pandas dataframes or NumPy arrays that will be used for initial exploratory data analysis (EDA).</p><blockquote><p>Exploratory data analysis (EDA) is the process of analyzing data sets to create summaries and visualizations of the data. These summaries and visualizations are then used to guide the use of the data for solving business challenges.</p></blockquote><p><strong>HINT</strong>: This is the stage where we perform the initial EDA</p><p>Sometimes we need to perform a little EDA in order to determine how to best clean the data so these two steps are not necessarily distinct from each other. Visualization, basic hypothesis testing and simple feature engineering are among the most important tasks for EDA at this stage. An minimal example of a EDA plot is one where we look at the average number of words per sentence for the name mentions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## read in the book        </span></span><br><span class="line">text = open(<span class="string">'sherlock-holmes.txt'</span>, <span class="string">'r'</span>).read()</span><br><span class="line"></span><br><span class="line"><span class="comment">## do some basic parsing and cleaning of sentences</span></span><br><span class="line">stop_pattern = <span class="string">'\.|\?|\!'</span></span><br><span class="line">sentences = re.split(stop_pattern, text)</span><br><span class="line">sentences = [re.sub(<span class="string">"\r|\n"</span>,<span class="string">" "</span>,s.lower()) <span class="keyword">for</span> s <span class="keyword">in</span> sentences][<span class="number">3</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">## extract a few features and create a pandas df</span></span><br><span class="line">has_sherlock =  [<span class="keyword">True</span> <span class="keyword">if</span> re.search(<span class="string">"sherlock|holmes"</span>,s) <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">has_watson = [<span class="keyword">True</span> <span class="keyword">if</span> re.search(<span class="string">"john|watson"</span>,s) <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'text'</span>:sentences,<span class="string">'has_sherlock'</span>:has_sherlock,<span class="string">'has_watson'</span>:has_watson&#125;)</span><br><span class="line">df[<span class="string">'num_words'</span>] = df[<span class="string">'text'</span>].apply(<span class="keyword">lambda</span> x: len(x.split(<span class="string">" "</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">## make eda plot</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line"></span><br><span class="line">data1 = df[df[<span class="string">'has_sherlock'</span>]==<span class="keyword">True</span>]</span><br><span class="line">data2 = df[df[<span class="string">'has_watson'</span>]==<span class="keyword">True</span>]</span><br><span class="line"></span><br><span class="line">data = [df[df[col]==<span class="keyword">True</span>][<span class="string">'num_words'</span>].values <span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">'has_sherlock'</span>,<span class="string">'has_watson'</span>]]</span><br><span class="line"></span><br><span class="line">pos = [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">ax1.violinplot(data, pos, points=<span class="number">40</span>, widths=<span class="number">0.5</span>,showextrema=<span class="keyword">True</span>, showmedians=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">'Sherlock'</span>, <span class="string">'Watson'</span>]</span><br><span class="line">ax1.set_xticks(np.arange(<span class="number">1</span>, len(labels) + <span class="number">1</span>))</span><br><span class="line">ax1.set_xticklabels(labels)</span><br><span class="line">ax1.set_xlim(<span class="number">0.25</span>, len(labels) + <span class="number">0.75</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">'Feature'</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">'# Words'</span>)</span><br><span class="line">ax1.set_title(<span class="string">"Words per sentence"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="Ideate"><a href="#Ideate" class="headerlink" title="Ideate"></a>Ideate</h3><p><strong>This is the stage where we modify our data and our features</strong></p><p>Now that you have clean data the data processing must continue until you are ready to input your data into a model. This stage contains all of the possible data manipulations you might perform before modeling. Perhaps the data need to be log transformed, standardized, reduced in dimensionality, kernel transformed, engineered to contain more features or transformed in some other way.</p><p>For our text data we would likely want to dig into the sentences themselves to make sure they fit the desired use case. If we were building a chatbot to engage with in a very Holmes manner then we would likely want to remove any sentences that were not said by Mr. Holmes, but his name was mentioned. If we were building a predictive model to determine which story a phrase would most likely have been generated, we would need to create a new column in our data frame representing the books themselves.</p><p>When working with text data many models that we might consider prefer a numeric representation of the data. This may be occurrences, frequencies, or another transformation of the original data. It is in this stage that these types of transformations are readied or carried out. For example here we import the necessary transformers for usage in the next stage.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract the data to be used in the model from the df</span></span><br><span class="line">labels = np.zeros(df.shape[<span class="number">0</span>])</span><br><span class="line">labels[(df[<span class="string">'has_sherlock'</span>] == <span class="keyword">True</span>)] = <span class="number">1</span></span><br><span class="line">labels[(df[<span class="string">'has_watson'</span>] == <span class="keyword">True</span>)] = <span class="number">2</span></span><br><span class="line">df[<span class="string">'labels'</span>] = labels</span><br><span class="line">df = df[df[<span class="string">'labels'</span>]!=<span class="number">0</span>]</span><br><span class="line">X = df[<span class="string">'text'</span>].values</span><br><span class="line">y = df[<span class="string">'labels'</span>].values</span><br></pre></td></tr></table></figure><p>There are a lot of ways to prepare data for different models. In some case you will not know the best transformation or series of transformations until you have run the different models and made a comparison. The concept of pipelines is extremely useful for iterating over different permutations of transformers and models. The following topics will be covered in detail during Module 3.</p><ul><li>Unsupervised learning</li><li>Feature engineering</li><li>Dimension Reduction</li><li>Simulation</li><li>Missing value imputation</li><li>Outlier detection</li></ul><p><strong>HINT</strong>: This is the stage where we enumerate the advantages and disadvantages of the possible modeling solutions</p><p>Once the transformations are carried or staged as part of some pipeline it is a valuable exercise to document what you know about the process so far. The form that this most commonly takes is a table of possible modeling strategies complete with the advantages and disadvantages of each.</p><h3 id="Prototype"><a href="#Prototype" class="headerlink" title="Prototype"></a>Prototype</h3><p><strong>This is the modeling stage</strong></p><p>The data have been cleaned, processed and staged (ideally in a pipeline) for modeling. The modeling (classic statistics and machine learning) is the bread and butter of data science. This is the stage where most data scientists want to spend the majority of their time. It is where you will interface with the most intriguing aspects of this discipline.</p><p>To illustrate the process to the end shown below is a Support Vector Machine with Stochastic gradient decent as a model. The process involves the use of a train-test split and a pipeline because we want you to be exposed from the very beginning of this course with best practices. Given this example we also see that there can be considerable overlap between the ideate and prototype stages. The overlap exists because transformations of data are generally specific to models–as you will explore which model fits the situation best you will be modifying the transformations of your data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment">## carry out the train test split</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line">text_clf = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, CountVectorizer()),</span><br><span class="line">    (<span class="string">'tfidf'</span>, TfidfTransformer()),</span><br><span class="line">    (<span class="string">'clf'</span>, SGDClassifier(loss=<span class="string">'hinge'</span>, penalty=<span class="string">'l2'</span>,</span><br><span class="line">                        alpha=<span class="number">1e-3</span>, random_state=<span class="number">42</span>,</span><br><span class="line">                        max_iter=<span class="number">5</span>, tol=<span class="keyword">None</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">## train a model</span></span><br><span class="line">text_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p><strong>This is the production, testing and feedback loop stage</strong></p><p>The model works and there are evaluation metrics to provide insight into how well it works. However, the process does not end here. Perhaps the model runs, but it is not yet in production or maybe you want to try different models and/or transformers. Once in production you might want to run some tests to determine if it will handle load or if it will scale well as the data grows. A working model with an impressive f-score does not mean it will be effective in practice. This stage is dedicated to all of the considerations that come after the initial modeling is carried out.</p><p>It is also the stage where you will determine how best to iterate. Design thinking like data science is an iterative process. Our model performed very well (see below), possibly because Dr. Holmes and Dr. Watson are described in very different ways in the stories, but it could be something else.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment">## evaluate the model performance</span></span><br><span class="line">predicted = text_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(metrics.classification_report(y_test, predicted,</span><br><span class="line">      target_names=[<span class="string">'sherlock'</span>,<span class="string">'watson'</span>]))</span><br></pre></td></tr></table></figure><p>As a scientist you always want to remain skeptical about your findings until you have multiple ways to corroborate them. You will also want to always be aware of the overall goal of why you are doing the work you are doing. This example is an interesting metaphor for what can happen as a data scientist. It is possible to go down a path that may only marginally be related to the central business question. Developing a game here is not unlike using a new model for deep-learning or incorporating a new technology into your workflow—it may be fun and it may to some degree help the business case, but you need to always ask yourself is this the best way for me or my team to address the business problem? The questions your ask here are going to guide how best to iterate on the entire workflow.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Between design thinking and the above mentioned process models there are several 1:1 relationships between stages. The other relationship
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Process Model" scheme="https://zhangruochi.com/categories/AI-Workflow/Process-Model/"/>
    
    
  </entry>
  
  <entry>
    <title>Develop a blockchain application from scratch in Python</title>
    <link href="https://zhangruochi.com/Develop-a-blockchain-application-from-scratch-in-Python/2020/04/21/"/>
    <id>https://zhangruochi.com/Develop-a-blockchain-application-from-scratch-in-Python/2020/04/21/</id>
    <published>2020-04-20T16:37:03.000Z</published>
    <updated>2020-04-21T06:02:15.358Z</updated>
    
    <content type="html"><![CDATA[<p>Blockchain is a way of storing digital data. The data can literally be anything. For Bitcoin, it’s the transactions (logs of transfers of Bitcoin from one account to another), but it can even be files; it doesn’t matter. The data is stored in the form of blocks, which are linked (or chained) together using cryptographic hashes — hence the name “blockchain.”</p><p>All of the magic lies in the way this data is stored and added to the blockchain. A blockchain is essentially a linked list that contains ordered data, with a few constraints such as:</p><ul><li>Blocks can’t be modified once added; in other words, it is append only.</li><li>There are specific rules for appending data to it.</li><li>Its architecture is distributed.</li></ul><p>Enforcing these constraints yields the following benefits:</p><ul><li>Immutability and durability of data</li><li>No single point of control or failure</li><li>A verifiable audit trail of the order in which data was added</li></ul><h3 id="Store-transactions-into-blocks"><a href="#Store-transactions-into-blocks" class="headerlink" title="Store transactions into blocks"></a>Store transactions into blocks</h3><p>We’ll be storing data in our blockchain in a format that’s widely used: JSON. Here’s what a post stored in blockchain will look like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  <span class="string">"author"</span>: <span class="string">"some_author_name"</span>, </span><br><span class="line">  <span class="string">"content"</span>: <span class="string">"Some thoughts that author wants to share"</span>, </span><br><span class="line">  <span class="string">"timestamp"</span>: <span class="string">"The time at which the content was created"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The generic term “data” is often replaced on the internet by the term “transactions.” So, just to avoid confusion and maintain consistency, we’ll be using the term “transaction” to refer to data in our example application.</p><p>The transactions are packed into blocks. A block can contain one or many transactions. The blocks containing the transactions are generated frequently and added to the blockchain. Because there can be multiple blocks, each block should have a unique ID.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, index, transactions, timestamp)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Block` class.</span></span><br><span class="line"><span class="string">        :param index: Unique ID of the block.</span></span><br><span class="line"><span class="string">        :param transactions: List of transactions.</span></span><br><span class="line"><span class="string">        :param timestamp: Time of generation of the block.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.index = index </span><br><span class="line">        self.transactions = transactions </span><br><span class="line">        self.timestamp = timestamp</span><br></pre></td></tr></table></figure><h3 id="Add-digital-fingerprints-to-the-blocks"><a href="#Add-digital-fingerprints-to-the-blocks" class="headerlink" title="Add digital fingerprints to the blocks"></a>Add digital fingerprints to the blocks</h3><p>We’d like to prevent any kind of tampering in the data stored inside the block, and detection is the first step to that. To detect if the data in the block has been tampered with, you can use cryptographic hash functions.</p><p>A hash function is a function that takes data of any size and produces data of a fixed size from it (a hash), which is generally used to identify the input. The characteristics of an ideal hash function are:</p><ul><li>It should be easy to compute.</li><li>It should be deterministic, meaning the same data will always result in the same hash.</li><li>It should be uniformly random, meaning even a single bit change in the data should change the hash significantly.</li></ul><p>The consequence of this is:</p><ul><li>It is virtually impossible to guess the input data given the hash. (The only way is to try all possible input combinations.)</li><li>If you know both the input and the hash, you can simply pass the input through the hash function to verify the provided hash.</li></ul><p>This asymmetry of efforts that’s required to figure out the hash from an input (easy) vs. figuring out the input from a hash (almost impossible) is what blockchain leverages to obtain the desired characteristics.</p><p>We’ll store the hash of the block in a field inside our Block object, and it will act like a digital fingerprint (or signature) of data contained in it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns the hash of the block instance by first converting it</span></span><br><span class="line"><span class="string">    into JSON string.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br></pre></td></tr></table></figure><p><strong>Note</strong>: In most cryptocurrencies, even the individual transactions in the block are hashed and then stored to form a hash tree (also known as a merkle tree). The root of the tree usually represents the hash of the block. It’s not a necessary requirement for the functioning of the blockchain, so we’re omitting it to keep things simple.</p><h3 id="Chain-the-blocks"><a href="#Chain-the-blocks" class="headerlink" title="Chain the blocks"></a>Chain the blocks</h3><p>Okay, we’ve now set up the blocks. The blockchain is supposed to be a collection of blocks. We can store all the blocks in the Python list (the equivalent of an array). But this is not sufficient, because what if someone intentionally replaces an old block with a new block in the collection? Creating a new block with altered transactions, computing the hash, and replacing it with any older block is no big deal in our current implementation.</p><p>We need a way to make sure that any change in the previous blocks invalidates the entire chain. The Bitcoin way to do this is to create dependency among consecutive blocks by chaining them with the hash of the block immediately previous to them. By chaining here, we mean to include the hash of the previous block in the current block in a new field called previous_hash.</p><p>Okay, if every block is linked to the previous block through the previous_hash field, what about the very first block? That block is called the genesis block and it can be generated either manually or through some unique logic. Let’s add the previous_hash field to the Block class and implement the initial structure of our Blockchain class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    def__init__(self, index, transactions, timestamp, previous_hash):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Block` class.</span></span><br><span class="line"><span class="string">        :param index:         Unique ID of the block.</span></span><br><span class="line"><span class="string">        :param transactions:  List of transactions.</span></span><br><span class="line"><span class="string">        :param timestamp:     Time of generation of the block.</span></span><br><span class="line"><span class="string">        :param previous_hash: Hash of the previous block in the chain which this block is part of.                                        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.index = index</span><br><span class="line">        self.transactions = transactions</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line">        self.previous_hash = previous_hash <span class="comment"># Adding the previous hash field</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Returns the hash of the block instance by first converting it</span></span><br><span class="line"><span class="string">        into JSON string.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>) <span class="comment"># The string equivalent also considers the previous_hash field now</span></span><br><span class="line">        <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Blockchain` class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_genesis_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function to generate genesis block and appends it to</span></span><br><span class="line"><span class="string">        the chain. The block has index 0, previous_hash as 0, and</span></span><br><span class="line"><span class="string">        a valid hash.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        genesis_block = Block(<span class="number">0</span>, [], time.time(), <span class="string">"0"</span>)</span><br><span class="line">        genesis_block.hash = genesis_block.compute_hash()</span><br><span class="line">        self.chain.append(genesis_block)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">last_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A quick pythonic way to retrieve the most recent block in the chain. Note that</span></span><br><span class="line"><span class="string">        the chain will always consist of at least one block (i.e., genesis block)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.chain[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><p>Now, if the content of any of the previous blocks changes:</p><ul><li>The hash of that previous block would change.</li><li>This will lead to a mismatch with the previous_hash field in the next block.</li><li>Since the input data to compute the hash of any block also consists of the previous_hash field, the hash of the next block will also change.</li></ul><p>Ultimately, the entire chain following the replaced block is invalidated, and the only way to fix it is to recompute the entire chain.</p><h3 id="Implement-a-proof-of-work-algorithm"><a href="#Implement-a-proof-of-work-algorithm" class="headerlink" title="Implement a proof of work algorithm"></a>Implement a proof of work algorithm</h3><p>There is one problem, though. If we change the previous block, the hashes of all the blocks that follow can be re-computed quite easily to create a different valid blockchain. To prevent this, we can exploit the asymmetry in efforts of hash functions that we discussed earlier to make the task of calculating the hash difficult and random. Here’s how we do this: Instead of accepting any hash for the block, we add some constraint to it. Let’s add a constraint that our hash should start with “n leading zeroes” where n can be any positive integer.</p><p>We know that unless we change the data of the block, the hash is not going to change, and of course we don’t want to change existing data. So what do we do? Simple! We’ll add some dummy data that we can change. Let’s introduce a new field in our block called nonce. A nonce is a number that we can keep on changing until we get a hash that satisfies our constraint. The nonce satisfying the constraint serves as proof that some computation has been performed. This technique is a simplified version of the Hashcash algorithm used in Bitcoin. The number of zeroes specified in the constraint determines the difficulty of our proof of work algorithm (the greater the number of zeroes, the harder it is to figure out the nonce).</p><p>Also, due to the asymmetry, proof of work is difficult to compute but very easy to verify once you figure out the nonce (you just have to run the hash function again):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="comment"># difficulty of PoW algorithm</span></span><br><span class="line">    difficulty = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd..</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">proof_of_work</span><span class="params">(self, block)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Function that tries different values of the nonce to get a hash</span></span><br><span class="line"><span class="string">        that satisfies our difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        computed_hash = block.compute_hash()</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> computed_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty):</span><br><span class="line">            block.nonce += <span class="number">1</span></span><br><span class="line">            computed_hash = block.compute_hash()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> computed_hash</span><br></pre></td></tr></table></figure><p>Notice that there is no specific logic to figuring out the nonce quickly; it’s just brute force. The only definite improvement that you can make is to use hardware chips that are specially designed to compute the hash function in a smaller number of CPU instructions.</p><h3 id="Add-blocks-to-the-chain"><a href="#Add-blocks-to-the-chain" class="headerlink" title="Add blocks to the chain"></a>Add blocks to the chain</h3><p>To add a block to the chain, we’ll first have to verify that:</p><ul><li>The data has not been tampered with (the proof of work provided is correct).</li><li>The order of transactions is preserved (the previous_hash field of the block to be added points to the hash of the latest block in our chain).</li></ul><p>Let’s see the code for adding blocks into the chain:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd..</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_block</span><span class="params">(self, block, proof)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that adds the block to the chain after verification.</span></span><br><span class="line"><span class="string">        Verification includes:</span></span><br><span class="line"><span class="string">        * Checking if the proof is valid.</span></span><br><span class="line"><span class="string">        * The previous_hash referred in the block and the hash of a latest block</span></span><br><span class="line"><span class="string">          in the chain match.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        previous_hash = self.last_block.hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> previous_hash != block.previous_hash:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Blockchain.is_valid_proof(block, proof):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        block.hash = proof</span><br><span class="line">        self.chain.append(block)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_valid_proof</span><span class="params">(self, block, block_hash)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Check if block_hash is valid hash of block and satisfies</span></span><br><span class="line"><span class="string">        the difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (block_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty) <span class="keyword">and</span></span><br><span class="line">                block_hash == block.compute_hash())</span><br></pre></td></tr></table></figure><h3 id="Mining"><a href="#Mining" class="headerlink" title="Mining"></a>Mining</h3><p>The transactions will be initially stored as a pool of unconfirmed transactions. The process of putting the unconfirmed transactions in a block and computing proof of work is known as the mining of blocks. Once the nonce satisfying our constraints is figured out, we can say that a block has been mined and it can be put into the blockchain.</p><p>In most of the cryptocurrencies (including Bitcoin), miners may be awarded some cryptocurrency as a reward for spending their computing power to compute a proof of work. Here’s what our mining function looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions = [] <span class="comment"># data yet to get into blockchain</span></span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_new_transaction</span><span class="params">(self, transaction)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions.append(transaction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mine</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        This function serves as an interface to add the pending</span></span><br><span class="line"><span class="string">        transactions to the blockchain by adding them to the block</span></span><br><span class="line"><span class="string">        and figuring out proof of work.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.unconfirmed_transactions:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        last_block = self.last_block</span><br><span class="line"></span><br><span class="line">        new_block = Block(index=last_block.index + <span class="number">1</span>,</span><br><span class="line">                          transactions=self.unconfirmed_transactions,</span><br><span class="line">                          timestamp=time.time(),</span><br><span class="line">                          previous_hash=last_block.hash)</span><br><span class="line"></span><br><span class="line">        proof = self.proof_of_work(new_block)</span><br><span class="line">        self.add_block(new_block, proof)</span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        <span class="keyword">return</span> new_block.index</span><br></pre></td></tr></table></figure><h3 id="Combined-Code"><a href="#Combined-Code" class="headerlink" title="Combined Code"></a>Combined Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, index, transactions, timestamp, previous_hash)</span>:</span></span><br><span class="line">        self.index = index</span><br><span class="line">        self.transactions = transactions</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line">        self.previous_hash = previous_hash</span><br><span class="line">        self.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that return the hash of the block contents.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="comment"># difficulty of our PoW algorithm</span></span><br><span class="line">    difficulty = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_genesis_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function to generate genesis block and appends it to</span></span><br><span class="line"><span class="string">        the chain. The block has index 0, previous_hash as 0, and</span></span><br><span class="line"><span class="string">        a valid hash.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        genesis_block = Block(<span class="number">0</span>, [], time.time(), <span class="string">"0"</span>)</span><br><span class="line">        genesis_block.hash = genesis_block.compute_hash()</span><br><span class="line">        self.chain.append(genesis_block)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">last_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.chain[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_block</span><span class="params">(self, block, proof)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that adds the block to the chain after verification.</span></span><br><span class="line"><span class="string">        Verification includes:</span></span><br><span class="line"><span class="string">        * Checking if the proof is valid.</span></span><br><span class="line"><span class="string">        * The previous_hash referred in the block and the hash of latest block</span></span><br><span class="line"><span class="string">          in the chain match.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        previous_hash = self.last_block.hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> previous_hash != block.previous_hash:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.is_valid_proof(block, proof):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        block.hash = proof</span><br><span class="line">        self.chain.append(block)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_valid_proof</span><span class="params">(self, block, block_hash)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Check if block_hash is valid hash of block and satisfies</span></span><br><span class="line"><span class="string">        the difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (block_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty) <span class="keyword">and</span></span><br><span class="line">                block_hash == block.compute_hash())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">proof_of_work</span><span class="params">(self, block)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Function that tries different values of nonce to get a hash</span></span><br><span class="line"><span class="string">        that satisfies our difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        computed_hash = block.compute_hash()</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> computed_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty):</span><br><span class="line">            block.nonce += <span class="number">1</span></span><br><span class="line">            computed_hash = block.compute_hash()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> computed_hash</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_new_transaction</span><span class="params">(self, transaction)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions.append(transaction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mine</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        This function serves as an interface to add the pending</span></span><br><span class="line"><span class="string">        transactions to the blockchain by adding them to the block</span></span><br><span class="line"><span class="string">        and figuring out Proof Of Work.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.unconfirmed_transactions:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        new_block = Block(index=last_block.index + <span class="number">1</span>,</span><br><span class="line">                          transactions=self.unconfirmed_transactions,</span><br><span class="line">                          timestamp=time.time(),</span><br><span class="line">                          previous_hash=self.last_block.hash)</span><br><span class="line"></span><br><span class="line">        proof = self.proof_of_work(new_block)</span><br><span class="line">        self.add_block(new_block, proof)</span><br><span class="line"></span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        <span class="keyword">return</span> new_block.index</span><br></pre></td></tr></table></figure><h3 id="Create-interfaces"><a href="#Create-interfaces" class="headerlink" title="Create interfaces"></a>Create interfaces</h3><p>Okay, now it’s time to create interfaces for our blockchain node to interact with the application we’re going to build. We’ll be using a popular Python microframework called Flask to create a REST API that interacts with and invokes various operations in our blockchain node. If you’ve worked with any web framework before, the code below shouldn’t be difficult to follow along.</p><p>These REST endpoints can be used to play around with our blockchain by creating some transactions and then mining them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize flask application</span></span><br><span class="line">app =  Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a blockchain object.</span></span><br><span class="line">blockchain = Blockchain()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### We need an endpoint for our application to submit a new transaction. This will be used by our application to add new data (posts) to the blockchain:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Flask's way of declaring end-points</span></span><br><span class="line"><span class="meta">@app.route('/new_transaction', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_transaction</span><span class="params">()</span>:</span></span><br><span class="line">    tx_data = request.get_json()</span><br><span class="line">    required_fields = [<span class="string">"author"</span>, <span class="string">"content"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> field <span class="keyword">in</span> required_fields:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> tx_data.get(field):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"Invalid transaction data"</span>, <span class="number">404</span></span><br><span class="line"></span><br><span class="line">    tx_data[<span class="string">"timestamp"</span>] = time.time()</span><br><span class="line"></span><br><span class="line">    blockchain.add_new_transaction(tx_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Success"</span>, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### Here’s an endpoint to return the node’s copy of the chain. Our application will be using this endpoint to query all of the data to display:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/chain', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_chain</span><span class="params">()</span>:</span></span><br><span class="line">    chain_data = []</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blockchain.chain:</span><br><span class="line">        chain_data.append(block.__dict__)</span><br><span class="line">    <span class="keyword">return</span> json.dumps(&#123;<span class="string">"length"</span>: len(chain_data),</span><br><span class="line">                       <span class="string">"chain"</span>: chain_data&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here’s an endpoint to request the node to mine the unconfirmed transactions (if any). We’ll be using it to initiate a command to mine from our application itself:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/mine', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mine_unconfirmed_transactions</span><span class="params">()</span>:</span></span><br><span class="line">    result = blockchain.mine()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> result:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"No transactions to mine"</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Block #&#123;&#125; is mined."</span>.format(result)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/pending_tx')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pending_tx</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(blockchain.unconfirmed_transactions)</span><br></pre></td></tr></table></figure><h3 id="Establish-consensus-and-decentralization"><a href="#Establish-consensus-and-decentralization" class="headerlink" title="Establish consensus and decentralization"></a>Establish consensus and decentralization</h3><p>Up to this point, the blockchain that we’ve implemented is meant to run on a single computer. Even though we’re linking block with hashes and applying the proof of work constraint, we still can’t trust a single entity (in our case, a single machine). We need the data to be distributed, we need multiple nodes maintaining the blockchain. So, to transition from a single node to a peer-to-peer network, let’s first create a mechanism to let a new node become aware of other peers in the network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Contains the host addresses of other participating members of the network</span></span><br><span class="line">peers = set()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Endpoint to add new peers to the network</span></span><br><span class="line"><span class="meta">@app.route('/register_node', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_new_peers</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># The host address to the peer node </span></span><br><span class="line">    node_address = request.get_json()[<span class="string">"node_address"</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node_address:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Invalid data"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add the node to the peer list</span></span><br><span class="line">    peers.add(node_address)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the blockchain to the newly registered node so that it can sync</span></span><br><span class="line">    <span class="keyword">return</span> get_chain()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/register_with', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_with_existing_node</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Internally calls the `register_node` endpoint to</span></span><br><span class="line"><span class="string">    register current node with the remote node specified in the</span></span><br><span class="line"><span class="string">    request, and sync the blockchain as well with the remote node.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    node_address = request.get_json()[<span class="string">"node_address"</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node_address:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Invalid data"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    data = &#123;<span class="string">"node_address"</span>: request.host_url&#125;</span><br><span class="line">    headers = &#123;<span class="string">'Content-Type'</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make a request to register with remote node and obtain information</span></span><br><span class="line">    response = requests.post(node_address + <span class="string">"/register_node"</span>,</span><br><span class="line">                             data=json.dumps(data), headers=headers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">global</span> blockchain</span><br><span class="line">        <span class="keyword">global</span> peers</span><br><span class="line">        <span class="comment"># update chain and the peers</span></span><br><span class="line">        chain_dump = response.json()[<span class="string">'chain'</span>]</span><br><span class="line">        blockchain = create_chain_from_dump(chain_dump)</span><br><span class="line">        peers.update(response.json()[<span class="string">'peers'</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Registration successful"</span>, <span class="number">200</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># if something goes wrong, pass it on to the API response</span></span><br><span class="line">        <span class="keyword">return</span> response.content, response.status_code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_chain_from_dump</span><span class="params">(chain_dump)</span>:</span></span><br><span class="line">    blockchain = Blockchain()</span><br><span class="line">    <span class="keyword">for</span> idx, block_data <span class="keyword">in</span> enumerate(chain_dump):</span><br><span class="line">        block = Block(block_data[<span class="string">"index"</span>],</span><br><span class="line">                      block_data[<span class="string">"transactions"</span>],</span><br><span class="line">                      block_data[<span class="string">"timestamp"</span>],</span><br><span class="line">                      block_data[<span class="string">"previous_hash"</span>])</span><br><span class="line">        proof = block_data[<span class="string">'hash'</span>]</span><br><span class="line">        <span class="keyword">if</span> idx &gt; <span class="number">0</span>:</span><br><span class="line">            added = blockchain.add_block(block, proof)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> added:</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">"The chain dump is tampered!!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># the block is a genesis block, no verification needed</span></span><br><span class="line">            blockchain.chain.append(block)</span><br><span class="line">    <span class="keyword">return</span> blockchain</span><br></pre></td></tr></table></figure><p>A new node participating in the network can invoke the register_with_existing_node method (via the /register_with endpoint) to register with existing nodes in the network. This will help with the following:</p><ul><li>Asking the remote node to add a new peer to its list of known peers.</li><li>Initializing the blockchain of the new node with that of the remote node.</li><li>Resyncing the blockchain with the network if the node goes off-grid.</li></ul><p>However, there’s a problem with multiple nodes. Due to intentional manipulation or unintentional reasons (like network latency), the copy of chains of a few nodes can differ. In that case, the nodes need to agree upon some version of the chain to maintain the integrity of the entire system. In other words, we need to achieve consensus.</p><p>A simple consensus algorithm could be to agree upon the longest valid chain when the chains of different participating nodes in the network appear to diverge. The rationale behind this approach is that the longest chain is a good estimate of the most amount of work done (remember proof of work is difficult to compute):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span></span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class">    <span class="title">previous</span> <span class="title">code</span> <span class="title">continued</span>...</span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class">    <span class="title">def</span> <span class="title">check_chain_validity</span><span class="params">(cls, chain)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A helper method to check if the entire blockchain is valid.            </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = <span class="keyword">True</span></span><br><span class="line">        previous_hash = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate through every block</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> chain:</span><br><span class="line">            block_hash = block.hash</span><br><span class="line">            <span class="comment"># remove the hash field to recompute the hash again</span></span><br><span class="line">            <span class="comment"># using `compute_hash` method.</span></span><br><span class="line">            delattr(block, <span class="string">"hash"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cls.is_valid_proof(block, block.hash) <span class="keyword">or</span> \</span><br><span class="line">                    previous_hash != block.previous_hash:</span><br><span class="line">                result = <span class="keyword">False</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            block.hash, previous_hash = block_hash, block_hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consensus</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Our simple consensus algorithm. If a longer valid chain is</span></span><br><span class="line"><span class="string">    found, our chain is replaced with it.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">global</span> blockchain</span><br><span class="line"></span><br><span class="line">    longest_chain = <span class="keyword">None</span></span><br><span class="line">    current_len = len(blockchain.chain)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> peers:</span><br><span class="line">        response = requests.get(<span class="string">'&#123;&#125;/chain'</span>.format(node))</span><br><span class="line">        length = response.json()[<span class="string">'length'</span>]</span><br><span class="line">        chain = response.json()[<span class="string">'chain'</span>]</span><br><span class="line">        <span class="keyword">if</span> length &gt; current_len <span class="keyword">and</span> blockchain.check_chain_validity(chain):</span><br><span class="line">              <span class="comment"># Longer valid chain found!</span></span><br><span class="line">            current_len = length</span><br><span class="line">            longest_chain = chain</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> longest_chain:</span><br><span class="line">        blockchain = longest_chain</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>Next, we need to develop a way for any node to announce to the network that it has mined a block so that everyone can update their blockchain and move on to mine other transactions. Other nodes can simply verify the proof of work and add the mined block to their respective chains (remember that verification is easy once the nonce is known):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># endpoint to add a block mined by someone else to</span></span><br><span class="line"><span class="comment"># the node's chain. The node first verifies the block</span></span><br><span class="line"><span class="comment"># and then adds it to the chain.</span></span><br><span class="line"><span class="meta">@app.route('/add_block', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verify_and_add_block</span><span class="params">()</span>:</span></span><br><span class="line">    block_data = request.get_json()</span><br><span class="line">    block = Block(block_data[<span class="string">"index"</span>],</span><br><span class="line">                  block_data[<span class="string">"transactions"</span>],</span><br><span class="line">                  block_data[<span class="string">"timestamp"</span>],</span><br><span class="line">                  block_data[<span class="string">"previous_hash"</span>])</span><br><span class="line"></span><br><span class="line">    proof = block_data[<span class="string">'hash'</span>]</span><br><span class="line">    added = blockchain.add_block(block, proof)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> added:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"The block was discarded by the node"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Block added to the chain"</span>, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">announce_new_block</span><span class="params">(block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A function to announce to the network once a block has been mined.</span></span><br><span class="line"><span class="string">    Other blocks can simply verify the proof of work and add it to their</span></span><br><span class="line"><span class="string">    respective chains.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> peer <span class="keyword">in</span> peers:</span><br><span class="line">        url = <span class="string">"&#123;&#125;add_block"</span>.format(peer)</span><br><span class="line">        requests.post(url, data=json.dumps(block.__dict__, sort_keys=<span class="keyword">True</span>))</span><br></pre></td></tr></table></figure><p>The announce_new_block method should be called after every block is mined by the node so that peers can add it to their chains.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/mine', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mine_unconfirmed_transactions</span><span class="params">()</span>:</span></span><br><span class="line">    result = blockchain.mine()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> result:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"No transactions to mine"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Making sure we have the longest chain before announcing to the network</span></span><br><span class="line">        chain_length = len(blockchain.chain)</span><br><span class="line">        consensus()</span><br><span class="line">        <span class="keyword">if</span> chain_length == len(blockchain.chain):</span><br><span class="line">            <span class="comment"># announce the recently mined block to the network</span></span><br><span class="line">            announce_new_block(blockchain.last_block)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Block #&#123;&#125; is mined."</span>.format(blockchain.last_block.index)</span><br></pre></td></tr></table></figure><h3 id="Build-the-application"><a href="#Build-the-application" class="headerlink" title="Build the application"></a>Build the application</h3><p>Now, it’s time to start working on the interface of our application. We’ve used Jinja2 templating to render the web pages and some CSS to make things look nice.</p><p>Our application needs to connect to a node in the blockchain network to fetch the data and also to submit new data. There can also be multiple nodes, as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> render_template, redirect, request</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> app <span class="keyword">import</span> app</span><br><span class="line"></span><br><span class="line"><span class="comment"># Node in the blockchain network that our application will communicate with</span></span><br><span class="line"><span class="comment"># to fetch and add data.</span></span><br><span class="line">CONNECTED_NODE_ADDRESS = <span class="string">"http://127.0.0.1:8000"</span></span><br><span class="line"></span><br><span class="line">posts = []</span><br></pre></td></tr></table></figure><p>The fetch_posts function gets the data from the node’s /chain endpoint, parses the data, and stores it locally.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_posts</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function to fetch the chain from a blockchain node, parse the</span></span><br><span class="line"><span class="string">    data, and store it locally.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    get_chain_address = <span class="string">"&#123;&#125;/chain"</span>.format(CONNECTED_NODE_ADDRESS)</span><br><span class="line">    response = requests.get(get_chain_address)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        content = []</span><br><span class="line">        chain = json.loads(response.content)</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> chain[<span class="string">"chain"</span>]:</span><br><span class="line">            <span class="keyword">for</span> tx <span class="keyword">in</span> block[<span class="string">"transactions"</span>]:</span><br><span class="line">                tx[<span class="string">"index"</span>] = block[<span class="string">"index"</span>]</span><br><span class="line">                tx[<span class="string">"hash"</span>] = block[<span class="string">"previous_hash"</span>]</span><br><span class="line">                content.append(tx)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">global</span> posts</span><br><span class="line">        posts = sorted(content,</span><br><span class="line">                       key=<span class="keyword">lambda</span> k: k[<span class="string">'timestamp'</span>],</span><br><span class="line">                       reverse=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>The application has an HTML form to take user input and then makes a POST request to a connected node to add the transaction into the unconfirmed transactions pool. The transaction is then mined by the network, and then finally fetched once we refresh our web page:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/submit', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submit_textarea</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Endpoint to create a new transaction via our application</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    post_content = request.form[<span class="string">"content"</span>]</span><br><span class="line">    author = request.form[<span class="string">"author"</span>]</span><br><span class="line"></span><br><span class="line">    post_object = &#123;</span><br><span class="line">        <span class="string">'author'</span>: author,</span><br><span class="line">        <span class="string">'content'</span>: post_content,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Submit a transaction</span></span><br><span class="line">    new_tx_address = <span class="string">"&#123;&#125;/new_transaction"</span>.format(CONNECTED_NODE_ADDRESS)</span><br><span class="line"></span><br><span class="line">    requests.post(new_tx_address,</span><br><span class="line">                  json=post_object,</span><br><span class="line">                  headers=&#123;<span class="string">'Content-type'</span>: <span class="string">'application/json'</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return to the homepage</span></span><br><span class="line">    <span class="keyword">return</span> redirect(<span class="string">'/'</span>)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://developer.ibm.com/tutorials/develop-a-blockchain-application-from-scratch-in-python/" target="_blank" rel="noopener">https://developer.ibm.com/tutorials/develop-a-blockchain-application-from-scratch-in-python/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Blockchain is a way of storing digital data. The data can literally be anything. For Bitcoin, it’s the transactions (logs of transfers of
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Blockchain" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Blockchain/"/>
    
    
  </entry>
  
</feed>
