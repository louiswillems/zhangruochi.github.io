<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="BLOG">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="Assignment 2 - Semi-gradient TD with a Neural NetworkWelcome to Course 3 Programming Assignment 2. In the previous assignment, you implemented semi-gradient TD with State Aggregation for solving a pol">
<meta property="og:type" content="article">
<meta property="og:title" content="Semi Gradient TD with a Neural Network">
<meta property="og:url" content="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Assignment 2 - Semi-gradient TD with a Neural NetworkWelcome to Course 3 Programming Assignment 2. In the previous assignment, you implemented semi-gradient TD with State Aggregation for solving a pol">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/nn_structure.png">
<meta property="og:image" content="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/output_39_2.png">
<meta property="og:image" content="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/nn_5000_episodes.png">
<meta property="og:image" content="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/nn_vs_tc.png">
<meta property="og:updated_time" content="2020-10-16T07:04:08.748Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Semi Gradient TD with a Neural Network">
<meta name="twitter:description" content="Assignment 2 - Semi-gradient TD with a Neural NetworkWelcome to Course 3 Programming Assignment 2. In the previous assignment, you implemented semi-gradient TD with State Aggregation for solving a pol">
<meta name="twitter:image" content="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/nn_structure.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/">





  <title>Semi Gradient TD with a Neural Network | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Semi Gradient TD with a Neural Network</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-14T17:18:55+08:00">
                2020-10-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Reinforcement Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Assignment-2-Semi-gradient-TD-with-a-Neural-Network"><a href="#Assignment-2-Semi-gradient-TD-with-a-Neural-Network" class="headerlink" title="Assignment 2 - Semi-gradient TD with a Neural Network"></a>Assignment 2 - Semi-gradient TD with a Neural Network</h1><p>Welcome to Course 3 Programming Assignment 2. In the previous assignment, you implemented semi-gradient TD with State Aggregation for solving a <strong>policy evaluation task</strong>. In this assignment, you will implement <strong>semi-gradient TD with a simple Neural Network</strong> and use it for the same policy evaluation problem. </p>
<p>You will implement an agent to evaluate a fixed policy on the 500-State Randomwalk. As you may remember from the previous assignment, the 500-state Randomwalk includes 500 states. Each episode begins with the agent at the center and terminates when the agent goes far left beyond state 1 or far right beyond state 500. At each time step, the agent selects to move either left or right with equal probability. The environment determines how much the agent moves in the selected direction.</p>
<p><strong>In this assignment, you will:</strong></p>
<ul>
<li>Implement stochastic gradient descent method for state-value prediction.</li>
<li>Implement semi-gradient TD with a neural network as the function approximator and Adam algorithm.</li>
<li>Compare performance of semi-gradient TD with a neural network and semi-gradient TD with tile-coding.</li>
</ul>
<h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>We import the following libraries that are required for this assignment:</p>
<ul>
<li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li>
<li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> : Library for plotting graphs in Python.</li>
<li><a href="http://www.jmlr.org/papers/v10/tanner09a.html" target="_blank" rel="noopener">RL-Glue</a> : Library for reinforcement learning experiments.</li>
<li><a href="https://tqdm.github.io/" target="_blank" rel="noopener">tqdm</a> : A package to display progress bar when running experiments.</li>
<li>BaseOptimizer : An abstract class that specifies the optimizer API for Agent.</li>
<li>plot_script : Custom script to plot results.</li>
<li>RandomWalkEnvironment : The Randomwalk environment script from Course 3 Assignment 1.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="comment"># DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os, shutil</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> environment <span class="keyword">import</span> BaseEnvironment</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> optimizer <span class="keyword">import</span> BaseOptimizer</span><br><span class="line"><span class="keyword">import</span> plot_script</span><br><span class="line"><span class="keyword">from</span> randomwalk_environment <span class="keyword">import</span> RandomWalkEnvironment</span><br></pre></td></tr></table></figure>
<h2 id="Section-1-Create-semi-gradient-TD-with-a-Neural-Network"><a href="#Section-1-Create-semi-gradient-TD-with-a-Neural-Network" class="headerlink" title="Section 1: Create semi-gradient TD with a Neural Network"></a>Section 1: Create semi-gradient TD with a Neural Network</h2><p>In this section, you will implement an Agent that learns with semi-gradient TD with a neural network. You will use a neural network with one hidden layer. The input of the neural network is the one-hot encoding of the state number. We use the one-hot encoding of the state number instead of the state number itself because we do not want to build the prior knowledge that integer number inputs close to each other have similar values. The hidden layer contains 100 rectifier linear units (ReLUs) which pass their input if it is bigger than one and return 0 otherwise. ReLU gates are commonly used in neural networks due to their nice properties such as the sparsity of the activation and having non-vanishing gradients. The output of the neural network is the estimated state value. It is a linear function of the hidden units as is commonly the case when estimating the value of a continuous target using neural networks.</p>
<p>The neural network looks like this:<br><img src="nn_structure.png" alt></p>
<p>For a given input, $s$, value of $s$ is computed by:</p>
<script type="math/tex; mode=display">
\begin{align} 
\psi &= sW^{[0]} + b^{[0]} \\
x &= \textit{max}(0, \psi) \\
v &= xW^{[1]} + b^{[1]}
\end{align}</script><p>where $W^{[0]}$, $b^{[0]}$, $W^{[1]}$, $b^{[1]}$  are the parameters of the network and will be learned when training the agent.</p>
<h2 id="1-1-Implement-helper-methods"><a href="#1-1-Implement-helper-methods" class="headerlink" title="1-1: Implement helper methods"></a>1-1: Implement helper methods</h2><p>Before implementing the agent, you first implement some helper functions which you will later use in agent’s main methods. </p>
<h3 id="Implement-get-value"><a href="#Implement-get-value" class="headerlink" title="Implement get_value()"></a>Implement <code>get_value()</code></h3><p>First, you will implement get_value() method which feeds an input $s$ into the neural network and returns the output of the network $v$ according to the equations above. To implement get_value(), take into account the following notes:</p>
<ul>
<li><code>get_value()</code> gets the one-hot encoded state number denoted by s as an input. </li>
<li><p><code>get_value()</code> receives the weights of the neural network as input, denoted by weights and structured as an array of dictionaries. Each dictionary corresponds to weights from one layer of the neural network to the next. Each dictionary includes $W$ and $b$. The shape of the elements in weights are as follows:</p>
<ul>
<li>weights[0][“W”]: num_states $\times$ num_hidden_units</li>
<li>weights[0][“b”]: 1 $\times$ num_hidden_units</li>
<li>weights[1][“W”]: num_hidden_units $\times$ 1</li>
<li>weights[1][“b”]: 1 $\times$ 1</li>
</ul>
</li>
<li><p>The input of the neural network is a sparse vector. To make computation faster, we take advantage of input sparsity. To do so, we provided a helper method <code>my_matmul()</code>. <strong>Make sure that you use <code>my_matmul()</code> for all matrix multiplications except for element-wise multiplications in this notebook.</strong></p>
</li>
<li>The max operator used for computing $x$ is element-wise. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_matmul</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given matrices x1 and x2, return the multiplication of them</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    result = np.zeros((x1.shape[<span class="number">0</span>], x2.shape[<span class="number">1</span>]))</span><br><span class="line">    x1_non_zero_indices = x1.nonzero()</span><br><span class="line">    <span class="keyword">if</span> x1.shape[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">and</span> len(x1_non_zero_indices[<span class="number">1</span>]) == <span class="number">1</span>:</span><br><span class="line">        result = x2[x1_non_zero_indices[<span class="number">1</span>], :]</span><br><span class="line">    <span class="keyword">elif</span> x1.shape[<span class="number">1</span>] == <span class="number">1</span> <span class="keyword">and</span> len(x1_non_zero_indices[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        result[x1_non_zero_indices[<span class="number">0</span>], :] = x2 * x1[x1_non_zero_indices[<span class="number">0</span>], <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = np.matmul(x1, x2)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_value</span><span class="params">(s, weights)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute value of input s given the weights of a neural network</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### Compute the ouput of the neural network, v, for input s</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    hidden = my_matmul(s, weights[<span class="number">0</span>][<span class="string">"W"</span>]) + weights[<span class="number">0</span>][<span class="string">"b"</span>]</span><br><span class="line">    x = np.maximum(hidden, np.zeros_like(hidden))</span><br><span class="line">    v = my_matmul(x, weights[<span class="number">1</span>][<span class="string">"W"</span>]) + weights[<span class="number">1</span>][<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>get_value()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 10 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line">s = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">"asserts/get_value_weights.npz"</span>)</span><br><span class="line">weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W0"</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b0"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W1"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">estimated_value = get_value(s, weights)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Estimated value: &#123;&#125;"</span>.format(estimated_value))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(estimated_value, [[<span class="number">-0.21915705</span>]]))</span><br></pre></td></tr></table></figure>
<pre><code>Estimated value: [[-0.21915705]]
</code></pre><p><strong>Expected output</strong>:</p>
<pre><code>Estimated value: [[-0.21915705]]
</code></pre><h3 id="Implement-get-gradient"><a href="#Implement-get-gradient" class="headerlink" title="Implement get_gradient()"></a>Implement <code>get_gradient()</code></h3><p>You will also implement <code>get_gradient()</code> method which computes the gradient of the value function for a given input, using backpropagation. You will later use this function to update the value function. </p>
<p>As you know, we compute the value of a state $s$ according to: </p>
<script type="math/tex; mode=display">
\begin{align} 
\psi &= sW^{[0]} + b^{[0]} \\
x &= \textit{max}(0, \psi) \\
v &= xW^{[1]} + b^{[1]}
\end{align}</script><p>To update the weights of the neural network ($W^{[0]}$, $b^{[0]}$, $W^{[1]}$, $b^{[1]}$), we compute the gradient of $v$ with respect to the weights according to:</p>
<script type="math/tex; mode=display">
\begin{align} 
\frac{\partial v}{\partial W^{[0]}} &= s^T(W^{[1]T} \odot I_{x>0}) \\
\frac{\partial v}{\partial b^{[0]}} &= W^{[1]T} \odot I_{x>0} \\
\frac{\partial v}{\partial W^{[1]}} &= x^T \\
\frac{\partial v}{\partial b^{[1]}} &= 1
\end{align}</script><p>where $\odot$ denotes element-wise matrix multiplication and $I_{x&gt;0}$ is the gradient of the ReLU activation function which is an indicator whose $i$th element is 1 if $x[i]&gt;0$ and 0 otherwise.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gradient</span><span class="params">(s, weights)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given inputs s and weights, return the gradient of v with respect to the weights</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Compute the gradient of the value function with respect to W0, b0, W1, b1 for input s</span></span><br><span class="line">    <span class="comment"># grads[0]["W"] = ?</span></span><br><span class="line">    <span class="comment"># grads[0]["b"] = ?</span></span><br><span class="line">    <span class="comment"># grads[1]["W"] = ?</span></span><br><span class="line">    <span class="comment"># grads[1]["b"] = ?</span></span><br><span class="line">    <span class="comment"># Note that grads[0]["W"], grads[0]["b"], grads[1]["W"], and grads[1]["b"] should have the same shape as </span></span><br><span class="line">    <span class="comment"># weights[0]["W"], weights[0]["b"], weights[1]["W"], and weights[1]["b"] respectively</span></span><br><span class="line">    <span class="comment"># Note that to compute the gradients, you need to compute the activation of the hidden layer (x)</span></span><br><span class="line"></span><br><span class="line">    grads = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights))]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    hidden = my_matmul(s, weights[<span class="number">0</span>][<span class="string">"W"</span>]) + weights[<span class="number">0</span>][<span class="string">"b"</span>]</span><br><span class="line">    x = np.maximum(hidden, np.zeros_like(hidden))</span><br><span class="line">    </span><br><span class="line">    grads[<span class="number">0</span>][<span class="string">"W"</span>] = my_matmul(np.transpose(s),np.maximum(np.transpose(weights[<span class="number">1</span>][<span class="string">"W"</span>]),<span class="number">0</span>))</span><br><span class="line">    grads[<span class="number">0</span>][<span class="string">"b"</span>] = np.maximum(np.transpose(weights[<span class="number">1</span>][<span class="string">"W"</span>]),<span class="number">0</span>)</span><br><span class="line">    grads[<span class="number">1</span>][<span class="string">"W"</span>] = np.transpose(x)</span><br><span class="line">    grads[<span class="number">1</span>][<span class="string">"b"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>get_gradient()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line">s = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">"asserts/get_gradient_weights.npz"</span>)</span><br><span class="line">weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W0"</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b0"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W1"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">grads = get_gradient(s, weights)</span><br><span class="line"></span><br><span class="line">grads_answer = np.load(<span class="string">"asserts/get_gradient_grads.npz"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">0</span>][<span class="string">"W"</span>], grads_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">0</span>][<span class="string">"b"</span>], grads_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">1</span>][<span class="string">"W"</span>], grads_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">1</span>][<span class="string">"b"</span>], grads_answer[<span class="string">"b1"</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<pre><code>grads[0][&quot;W&quot;]
 [[0.         0.        ]
 [0.         0.        ]
 [0.         0.        ]
 [0.76103773 0.12167502]
 [0.         0.        ]] 

grads[0][&quot;b&quot;]
 [[0.76103773 0.12167502]] 

grads[1][&quot;W&quot;]
 [[0.69198983]
 [0.82403662]] 

grads[1][&quot;b&quot;]
 [[1.]] 
</code></pre><h3 id="Implement-stochastic-gradient-descent-method-for-state-value-prediction"><a href="#Implement-stochastic-gradient-descent-method-for-state-value-prediction" class="headerlink" title="Implement stochastic gradient descent method for state-value prediction"></a>Implement stochastic gradient descent method for state-value prediction</h3><p>In this section, you will implement stochastic gradient descent (SGD) method for state_value prediction. Here is the basic SGD update for state-value prediction with TD:</p>
<script type="math/tex; mode=display">\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha \delta_t \nabla \hat{v}(S_t,\mathbf{w_{t}})</script><p>At each time step, we update the weights in the direction  $g_t = \delta_t \nabla \hat{v}(S_t,\mathbf{w_t})$ using a fixed step-size $\alpha$. $\delta_t = R_{t+1} + \gamma \hat{v}(S_{t+1},\mathbf{w_{t}}) - \hat{v}(S_t,\mathbf{w_t})$ is the TD-error. $\nabla \hat{v}(S_t,\mathbf{w_{t}})$ is the gradient of the value function with respect to the weights.</p>
<p>The following cell includes the SGD class. You will complete the <code>update_weight()</code> method of SGD assuming that the weights and update g are provided.</p>
<p><strong>As you know, in this assignment, we structured the weights as an array of dictionaries. Note that the updates $g_t$, in the case of TD, is $\delta_t \nabla \hat{v}(S_t,\mathbf{w_t})$. As a result, $g_t$ has the same structure as $\nabla \hat{v}(S_t,\mathbf{w_t})$ which is also an array of dictionaries.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SGD</span><span class="params">(BaseOptimizer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimizer_init</span><span class="params">(self, optimizer_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the optimizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the stochastic gradient descent method.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume optimizer_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            step_size: float</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.step_size = optimizer_info.get(<span class="string">"step_size"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weights</span><span class="params">(self, weights, g)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Given weights and update g, return updated weights</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights)):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> weights[i].keys():</span><br><span class="line">                </span><br><span class="line">                <span class="comment">### update weights</span></span><br><span class="line">                <span class="comment"># weights[i][param] = None</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                weights[i][param] += self.step_size * g[i][param]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>update_weights()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">"asserts/update_weights_weights.npz"</span>)</span><br><span class="line">weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W0"</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b0"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"W"</span>] = weights_data[<span class="string">"W1"</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">"b"</span>] = weights_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">g_data = np.load(<span class="string">"asserts/update_weights_g.npz"</span>)</span><br><span class="line">g = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">g[<span class="number">0</span>][<span class="string">"W"</span>] = g_data[<span class="string">"W0"</span>]</span><br><span class="line">g[<span class="number">0</span>][<span class="string">"b"</span>] = g_data[<span class="string">"b0"</span>]</span><br><span class="line">g[<span class="number">1</span>][<span class="string">"W"</span>] = g_data[<span class="string">"W1"</span>]</span><br><span class="line">g[<span class="number">1</span>][<span class="string">"b"</span>] = g_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">test_sgd = SGD()</span><br><span class="line">optimizer_info = &#123;<span class="string">"step_size"</span>: <span class="number">0.3</span>&#125;</span><br><span class="line">test_sgd.optimizer_init(optimizer_info)</span><br><span class="line">updated_weights = test_sgd.update_weights(weights, g)</span><br><span class="line"></span><br><span class="line"><span class="comment"># updated weights asserts</span></span><br><span class="line">updated_weights_answer = np.load(<span class="string">"asserts/update_weights_updated_weights.npz"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">"W"</span>], updated_weights_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">"b"</span>], updated_weights_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">"W"</span>], updated_weights_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">"b"</span>], updated_weights_answer[<span class="string">"b1"</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<pre><code>updated_weights[0][&quot;W&quot;]
 [[ 1.17899492  0.53656321]
 [ 0.58008221  1.47666572]
 [ 1.01909411 -1.10248056]
 [ 0.72490408  0.06828853]
 [-0.20609725  0.69034095]] 

updated_weights[0][&quot;b&quot;]
 [[-0.18484533  0.92844539]] 

updated_weights[1][&quot;W&quot;]
 [[0.70488257]
 [0.58150878]] 

updated_weights[1][&quot;b&quot;]
 [[0.88467086]] 
</code></pre><h3 id="Adam-Algorithm"><a href="#Adam-Algorithm" class="headerlink" title="Adam Algorithm"></a>Adam Algorithm</h3><p>In this assignment, instead of using SGD for updating the weights, we use a more advanced algorithm called Adam. The Adam algorithm improves the SGD update with two concepts: adaptive vector step-sizes and momentum. It keeps estimates of the mean and second moment of the updates, denoted by $\mathbf{m}$ and $\mathbf{v}$ respectively:</p>
<script type="math/tex; mode=display">\mathbf{m_t} = \beta_m \mathbf{m_{t-1}} + (1 - \beta_m)g_t \\
\mathbf{v_t} = \beta_v \mathbf{v_{t-1}} + (1 - \beta_v)g^2_t</script><p>Given that $\mathbf{m}$ and $\mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $\mathbf{\hat{m}}$ and $\mathbf{\hat{v}}$ as:</p>
<script type="math/tex; mode=display">\mathbf{\hat{m_t}} = \frac{\mathbf{m_t}}{1 - \beta_m^t} \\
\mathbf{\hat{v_t}} = \frac{\mathbf{v_t}}{1 - \beta_v^t}</script><p>The weights are then updated as follows:</p>
<script type="math/tex; mode=display">\mathbf{w_t} = \mathbf{w_{t-1}} + \frac{\alpha}{\sqrt{\mathbf{\hat{v_t}}}+\epsilon} \mathbf{\hat{m_t}}</script><p>When implementing the agent you will use the Adam algorithm instead of SGD because it is more efficient. We have already provided you the implementation of the Adam algorithm in the cell below. You will use it when implementing your agent. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(BaseOptimizer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimizer_init</span><span class="params">(self, optimizer_info)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the optimizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume optimizer_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: integer,</span></span><br><span class="line"><span class="string">            num_hidden_layer: integer,</span></span><br><span class="line"><span class="string">            num_hidden_units: integer,</span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            self.beta_m: float</span></span><br><span class="line"><span class="string">            self.beta_v: float</span></span><br><span class="line"><span class="string">            self.epsilon: float</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        self.num_states = optimizer_info.get(<span class="string">"num_states"</span>)</span><br><span class="line">        self.num_hidden_layer = optimizer_info.get(<span class="string">"num_hidden_layer"</span>)</span><br><span class="line">        self.num_hidden_units = optimizer_info.get(<span class="string">"num_hidden_units"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Specify Adam algorithm's hyper parameters</span></span><br><span class="line">        self.step_size = optimizer_info.get(<span class="string">"step_size"</span>)</span><br><span class="line">        self.beta_m = optimizer_info.get(<span class="string">"beta_m"</span>)</span><br><span class="line">        self.beta_v = optimizer_info.get(<span class="string">"beta_v"</span>)</span><br><span class="line">        self.epsilon = optimizer_info.get(<span class="string">"epsilon"</span>)</span><br><span class="line"></span><br><span class="line">        self.layer_size = np.array([self.num_states, self.num_hidden_units, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize Adam algorithm's m and v</span></span><br><span class="line">        self.m = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        self.v = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize self.m[i]["W"], self.m[i]["b"], self.v[i]["W"], self.v[i]["b"] to zero</span></span><br><span class="line">            self.m[i][<span class="string">"W"</span>] = np.zeros((self.layer_size[i], self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.m[i][<span class="string">"b"</span>] = np.zeros((<span class="number">1</span>, self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">"W"</span>] = np.zeros((self.layer_size[i], self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">"b"</span>] = np.zeros((<span class="number">1</span>, self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat</span></span><br><span class="line">        self.beta_m_product = self.beta_m</span><br><span class="line">        self.beta_v_product = self.beta_v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weights</span><span class="params">(self, weights, g)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Given weights and update g, return updated weights</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights)):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment">### update self.m and self.v</span></span><br><span class="line">                self.m[i][param] = self.beta_m * self.m[i][param] + (<span class="number">1</span> - self.beta_m) * g[i][param]</span><br><span class="line">                self.v[i][param] = self.beta_v * self.v[i][param] + (<span class="number">1</span> - self.beta_v) * (g[i][param] * g[i][param])</span><br><span class="line"></span><br><span class="line">                <span class="comment">### compute m_hat and v_hat</span></span><br><span class="line">                m_hat = self.m[i][param] / (<span class="number">1</span> - self.beta_m_product)</span><br><span class="line">                v_hat = self.v[i][param] / (<span class="number">1</span> - self.beta_v_product)</span><br><span class="line"></span><br><span class="line">                <span class="comment">### update weights</span></span><br><span class="line">                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)</span><br><span class="line">                </span><br><span class="line">        <span class="comment">### update self.beta_m_product and self.beta_v_product</span></span><br><span class="line">        self.beta_m_product *= self.beta_m</span><br><span class="line">        self.beta_v_product *= self.beta_v</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<h2 id="1-2-Implement-Agent-Methods"><a href="#1-2-Implement-Agent-Methods" class="headerlink" title="1-2: Implement Agent Methods"></a>1-2: Implement Agent Methods</h2><p>In this section, you will implement <code>agent_init()</code>, <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code>.</p>
<p>In <code>agent_init()</code>, you will:</p>
<ul>
<li>specify the neural network structure by filling self.layer_size with the size of the input layer, hidden layer, and output layer.</li>
<li>initialize the network’s parameters. We show the parameters as an array of dictionaries, self.weights, where each dictionary corresponds to weights from one layer to the next. Each dictionary includes $W$ and $b$. </li>
</ul>
<script type="math/tex; mode=display">\sqrt{ \frac{2}{ input \, of \, each \, node } }</script><p>This initialization heuristic is commonly used when using ReLU gates and helps keep the output of a neuron from getting too big or too small. To initialize the network’s parameters, use <strong>self.rand_generator.normal()</strong> which draws random samples from a normal distribution. The parameters of self.rand_generator.normal are mean of the distribution, standard deviation of the distribution, and output shape in the form of tuple of integers.</p>
<p>In <code>agent_start()</code>, you will:</p>
<ul>
<li>specify self.last_state and self.last_action.</li>
</ul>
<p>In <code>agent_step()</code> and <code>agent_end()</code>, you will:</p>
<ul>
<li>compute the TD error using $v(S_t)$ and $v(S_{t+1})$. To compute the value function for $S_t$ and $S_{t+1}$, you will get their one-hot encoding using <code>one_hot()</code> method that we provided below. You feed the one-hot encoded state number to the neural networks using <code>get_value()</code> method that you implemented above. Note that <code>one_hot()</code> method returns the one-hot encoding of a state as a numpy array of shape (1, num_states).</li>
<li>retrieve the gradients using <code>get_gradient()</code> function that you implemented.</li>
<li>use Adam_algorithm that we provided to update the neural network’s parameters, self.weights.</li>
<li>use <code>agent_policy()</code> method to select actions with. (only in <code>agent_step()</code>)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(state, num_states)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given num_state and a state, return the one-hot encoding of the state</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Create the one-hot encoding of state</span></span><br><span class="line">    <span class="comment"># one_hot_vector is a numpy array of shape (1, num_states)</span></span><br><span class="line">    </span><br><span class="line">    one_hot_vector = np.zeros((<span class="number">1</span>, num_states))</span><br><span class="line">    one_hot_vector[<span class="number">0</span>, int((state - <span class="number">1</span>))] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot_vector</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span><span class="params">(BaseAgent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.name = <span class="string">"td_agent"</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span><span class="params">(self, agent_info=&#123;&#125;)</span>:</span></span><br><span class="line">        <span class="string">"""Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the semi-gradient TD with a Neural Network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: integer,</span></span><br><span class="line"><span class="string">            num_hidden_layer: integer,</span></span><br><span class="line"><span class="string">            num_hidden_units: integer,</span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            discount_factor: float,</span></span><br><span class="line"><span class="string">            self.beta_m: float</span></span><br><span class="line"><span class="string">            self.beta_v: float</span></span><br><span class="line"><span class="string">            self.epsilon: float</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Set random seed for weights initialization for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set random seed for policy for each run</span></span><br><span class="line">        self.policy_rand_generator = np.random.RandomState(agent_info.get(<span class="string">"seed"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set attributes according to agent_info</span></span><br><span class="line">        self.num_states = agent_info.get(<span class="string">"num_states"</span>)</span><br><span class="line">        self.num_hidden_layer = agent_info.get(<span class="string">"num_hidden_layer"</span>)</span><br><span class="line">        self.num_hidden_units = agent_info.get(<span class="string">"num_hidden_units"</span>)</span><br><span class="line">        self.discount_factor = agent_info.get(<span class="string">"discount_factor"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Define the neural network's structure</span></span><br><span class="line">        <span class="comment"># Specify self.layer_size which shows the number of nodes in each layer</span></span><br><span class="line">        <span class="comment"># self.layer_size = np.array([None, None, None])</span></span><br><span class="line">        <span class="comment"># Hint: Checkout the NN diagram at the beginning of the notebook</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.layer_size = np.array([self.num_states, self.num_hidden_units, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the neural network's parameter</span></span><br><span class="line">        self.weights = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Initialize self.weights[i]["W"] and self.weights[i]["b"] using self.rand_generator.normal()</span></span><br><span class="line">            <span class="comment"># Note that The parameters of self.rand_generator.normal are mean of the distribution, </span></span><br><span class="line">            <span class="comment"># standard deviation of the distribution, and output shape in the form of tuple of integers.</span></span><br><span class="line">            <span class="comment"># To specify output shape, use self.layer_size.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line">            <span class="comment"># your code here</span></span><br><span class="line">            self.weights[i][<span class="string">"W"</span>] = self.rand_generator.normal(loc=<span class="number">0</span>, scale= np.sqrt(<span class="number">2.0</span> / self.layer_size[i]) ,size = (self.layer_size[i],self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.weights[i][<span class="string">"b"</span>] = self.rand_generator.normal(loc=<span class="number">0</span>, scale= np.sqrt(<span class="number">2.0</span> / self.layer_size[i]) ,size = (<span class="number">1</span>,self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Specify the optimizer</span></span><br><span class="line">        self.optimizer = Adam()</span><br><span class="line">        self.optimizer.optimizer_init(&#123;</span><br><span class="line">            <span class="string">"num_states"</span>: agent_info[<span class="string">"num_states"</span>],</span><br><span class="line">            <span class="string">"num_hidden_layer"</span>: agent_info[<span class="string">"num_hidden_layer"</span>],</span><br><span class="line">            <span class="string">"num_hidden_units"</span>: agent_info[<span class="string">"num_hidden_units"</span>],</span><br><span class="line">            <span class="string">"step_size"</span>: agent_info[<span class="string">"step_size"</span>],</span><br><span class="line">            <span class="string">"beta_m"</span>: agent_info[<span class="string">"beta_m"</span>],</span><br><span class="line">            <span class="string">"beta_v"</span>: agent_info[<span class="string">"beta_v"</span>],</span><br><span class="line">            <span class="string">"epsilon"</span>: agent_info[<span class="string">"epsilon"</span>],</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">        self.last_state = <span class="keyword">None</span></span><br><span class="line">        self.last_action = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_policy</span><span class="params">(self, state)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Set chosen_action as 0 or 1 with equal probability. </span></span><br><span class="line">        chosen_action = self.policy_rand_generator.choice([<span class="number">0</span>,<span class="number">1</span>])    </span><br><span class="line">        <span class="keyword">return</span> chosen_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">"""The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment's evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first action the agent takes.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### select action given state (using self.agent_policy()), and save current state and action</span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_action = self.agent_policy(state)</span><br><span class="line">        self.last_state = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span><span class="params">(self, reward, state)</span>:</span></span><br><span class="line">        <span class="string">"""A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment's step based, where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action the agent is taking.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        last_state_vec = one_hot(self.last_state, self.num_states)</span><br><span class="line">        last_value = get_value(last_state_vec, self.weights)</span><br><span class="line"></span><br><span class="line">        state_vec = one_hot(state, self.num_states)</span><br><span class="line">        value = get_value(state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Compute TD error</span></span><br><span class="line">        <span class="comment"># delta = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        delta = reward + self.discount_factor * value - last_value</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Retrieve gradients</span></span><br><span class="line">        <span class="comment"># grads = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        grads = get_gradient(last_state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Compute g (1 line)</span></span><br><span class="line">        g = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment"># g[i][param] = None</span></span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                g[i][param] = grads[i][param] * delta</span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update the weights using self.optimizer</span></span><br><span class="line">        <span class="comment"># self.weights = None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.optimizer.update_weights(self.weights, g)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update self.last_state and self.last_action</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_action = self.agent_policy(state)</span><br><span class="line">        self.last_state = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span><span class="params">(self, reward)</span>:</span></span><br><span class="line">        <span class="string">"""Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        last_state_vec = one_hot(self.last_state, self.num_states)</span><br><span class="line">        last_value = get_value(last_state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### compute TD error</span></span><br><span class="line">        <span class="comment"># delta = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        delta = reward - last_value</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Retrieve gradients</span></span><br><span class="line">        <span class="comment"># grads = None</span></span><br><span class="line">        grads = get_gradient(last_state_vec, self.weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Compute g</span></span><br><span class="line">        g = [dict() <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment"># g[i][param] = None</span></span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                g[i][param] = grads[i][param] * delta</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update the weights using self.optimizer</span></span><br><span class="line">        <span class="comment"># self.weights = None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.optimizer.update_weights(self.weights, g)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span><span class="params">(self, message)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> message == <span class="string">'get state value'</span>:</span><br><span class="line">            state_value = np.zeros(self.num_states)</span><br><span class="line">            <span class="keyword">for</span> state <span class="keyword">in</span> range(<span class="number">1</span>, self.num_states + <span class="number">1</span>):</span><br><span class="line">                s = one_hot(state, self.num_states)</span><br><span class="line">                state_value[state - <span class="number">1</span>] = get_value(s, self.weights)</span><br><span class="line">            <span class="keyword">return</span> state_value</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>agent_init()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.25</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"layer_size: &#123;&#125;"</span>.format(test_agent.layer_size))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.layer_size, np.array([agent_info[<span class="string">"num_states"</span>], </span><br><span class="line">                                                    agent_info[<span class="string">"num_hidden_units"</span>], </span><br><span class="line">                                                    <span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>].shape == (agent_info[<span class="string">"num_states"</span>], agent_info[<span class="string">"num_hidden_units"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>].shape == (<span class="number">1</span>, agent_info[<span class="string">"num_hidden_units"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>].shape == (agent_info[<span class="string">"num_hidden_units"</span>], <span class="number">1</span>))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>].shape == (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">agent_weight_answer = np.load(<span class="string">"asserts/agent_init_weights_1.npz"</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>], agent_weight_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>], agent_weight_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>], agent_weight_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>], agent_weight_answer[<span class="string">"b1"</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>layer_size: [5 2 1]
</code></pre><p><strong>Expected output</strong>:</p>
<pre><code>layer_size: [5 2 1]
weights[0][&quot;W&quot;] shape: (5, 2)
weights[0][&quot;b&quot;] shape: (1, 2)
weights[1][&quot;W&quot;] shape: (2, 1)
weights[1][&quot;b&quot;] shape: (1, 1) 

weights[0][&quot;W&quot;]
 [[ 1.11568467  0.25308164]
 [ 0.61900825  1.4172653 ]
 [ 1.18114738 -0.6180848 ]
 [ 0.60088868 -0.0957267 ]
 [-0.06528133  0.25968529]] 

weights[0][&quot;b&quot;]
 [[0.09110115 0.91976332]] 

weights[1][&quot;W&quot;]
 [[0.76103773]
 [0.12167502]] 

weights[1][&quot;b&quot;]
 [[0.44386323]]
</code></pre><p>Run the following code to test your implementation of the <code>agent_start()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">10</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose state = 250</span></span><br><span class="line">state = <span class="number">250</span></span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line">test_agent.agent_start(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_state == <span class="number">250</span>)</span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_action == <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<pre><code>Agent state: 250
Agent selected action: 1
</code></pre><p>Run the following code to test your implementation of the <code>agent_step()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load initial weights</span></span><br><span class="line">agent_initial_weight = np.load(<span class="string">"asserts/agent_step_initial_weights.npz"</span>)</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>] = agent_initial_weight[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>] = agent_initial_weight[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>] = agent_initial_weight[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>] = agent_initial_weight[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load m and v for the optimizer</span></span><br><span class="line">m_data = np.load(<span class="string">"asserts/agent_step_initial_m.npz"</span>)</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">"W"</span>] = m_data[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">"b"</span>] = m_data[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">"W"</span>] = m_data[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">"b"</span>] = m_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">v_data = np.load(<span class="string">"asserts/agent_step_initial_v.npz"</span>)</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">"W"</span>] = v_data[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">"b"</span>] = v_data[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">"W"</span>] = v_data[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">"b"</span>] = v_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 3</span></span><br><span class="line">start_state = <span class="number">3</span></span><br><span class="line">test_agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and the next state observed was State 1</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">next_state = <span class="number">1</span></span><br><span class="line">test_agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line">agent_updated_weight_answer = np.load(<span class="string">"asserts/agent_step_updated_weights.npz"</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>], agent_updated_weight_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>], agent_updated_weight_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>], agent_updated_weight_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>], agent_updated_weight_answer[<span class="string">"b1"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_state == <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_action == <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<pre><code>updated_weights[0][&quot;W&quot;]
 [[ 1.10893459  0.30763738]
 [ 0.63690565  1.14778865]
 [ 1.23397791 -0.48152743]
 [ 0.72792093 -0.15829832]
 [ 0.15021996  0.39822163]] 

updated_weights[0][&quot;b&quot;]
 [[0.29798822 0.96254535]] 

updated_weights[1][&quot;W&quot;]
 [[0.76628754]
 [0.11486511]] 

updated_weights[1][&quot;b&quot;]
 [[0.58530057]] 

Agent last state: 1
Agent last action: 1 
</code></pre><p>Run the following code to test your implementation of the <code>agent_end()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">"num_states"</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load initial weights</span></span><br><span class="line">agent_initial_weight = np.load(<span class="string">"asserts/agent_end_initial_weights.npz"</span>)</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>] = agent_initial_weight[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>] = agent_initial_weight[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>] = agent_initial_weight[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>] = agent_initial_weight[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load m and v for the optimizer</span></span><br><span class="line">m_data = np.load(<span class="string">"asserts/agent_step_initial_m.npz"</span>)</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">"W"</span>] = m_data[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">"b"</span>] = m_data[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">"W"</span>] = m_data[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">"b"</span>] = m_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line">v_data = np.load(<span class="string">"asserts/agent_step_initial_v.npz"</span>)</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">"W"</span>] = v_data[<span class="string">"W0"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">"b"</span>] = v_data[<span class="string">"b0"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">"W"</span>] = v_data[<span class="string">"W1"</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">"b"</span>] = v_data[<span class="string">"b1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 4</span></span><br><span class="line">start_state = <span class="number">4</span></span><br><span class="line">test_agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and reached the terminal state</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">test_agent.agent_end(reward)</span><br><span class="line"></span><br><span class="line"><span class="comment"># updated weights asserts</span></span><br><span class="line">agent_updated_weight_answer = np.load(<span class="string">"asserts/agent_end_updated_weights.npz"</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"W"</span>], agent_updated_weight_answer[<span class="string">"W0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">"b"</span>], agent_updated_weight_answer[<span class="string">"b0"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"W"</span>], agent_updated_weight_answer[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">"b"</span>], agent_updated_weight_answer[<span class="string">"b1"</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Expected output:</strong></p>
<pre><code>updated_weights[0][&quot;W&quot;]
 [[ 1.10893459  0.30763738]
 [ 0.63690565  1.14778865]
 [ 1.17531054 -0.51043162]
 [ 0.75062903 -0.13736817]
 [ 0.15021996  0.39822163]] 

updated_weights[0][&quot;b&quot;]
 [[0.30846523 0.95937346]] 

updated_weights[1][&quot;W&quot;]
 [[0.68861703]
 [0.15986364]] 

updated_weights[1][&quot;b&quot;]
 [[0.586074]] 
</code></pre><h2 id="Section-2-Run-Experiment"><a href="#Section-2-Run-Experiment" class="headerlink" title="Section 2 - Run Experiment"></a>Section 2 - Run Experiment</h2><p>Now that you implemented the agent, we can run the experiment. Similar to Course 3 Programming Assignment 1, we will plot the learned state value function and the learning curve of the TD agent. To plot the learning curve, we use Root Mean Squared Value Error (RMSVE). </p>
<h2 id="2-1-Run-Experiment-for-Semi-gradient-TD-with-a-Neural-Network"><a href="#2-1-Run-Experiment-for-Semi-gradient-TD-with-a-Neural-Network" class="headerlink" title="2-1: Run Experiment for Semi-gradient TD with a Neural Network"></a>2-1: Run Experiment for Semi-gradient TD with a Neural Network</h2><p>We have already provided you the experiment/plot code, so you can go ahead and run the two cells below.</p>
<p>Note that running the cell below will take <strong>approximately 12 minutes</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">true_state_val = np.load(<span class="string">'data/true_V.npy'</span>)    </span><br><span class="line">state_distribution = np.load(<span class="string">'data/state_distribution.npy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_RMSVE</span><span class="params">(learned_state_val)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span>(len(true_state_val) == len(learned_state_val) == len(state_distribution))</span><br><span class="line">    MSVE = np.sum(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))</span><br><span class="line">    RMSVE = np.sqrt(MSVE)</span><br><span class="line">    <span class="keyword">return</span> RMSVE</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define function to run experiment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(environment, agent, environment_parameters, agent_parameters, experiment_parameters)</span>:</span></span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># save rmsve at the end of each episode</span></span><br><span class="line">    agent_rmsve = np.zeros((experiment_parameters[<span class="string">"num_runs"</span>], </span><br><span class="line">                            int(experiment_parameters[<span class="string">"num_episodes"</span>]/experiment_parameters[<span class="string">"episode_eval_frequency"</span>]) + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save learned state value at the end of each run</span></span><br><span class="line">    agent_state_val = np.zeros((experiment_parameters[<span class="string">"num_runs"</span>], </span><br><span class="line">                                environment_parameters[<span class="string">"num_states"</span>]))</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">"num_states"</span>: environment_parameters[<span class="string">"num_states"</span>],</span><br><span class="line">                <span class="string">"start_state"</span>: environment_parameters[<span class="string">"start_state"</span>],</span><br><span class="line">                <span class="string">"left_terminal_state"</span>: environment_parameters[<span class="string">"left_terminal_state"</span>],</span><br><span class="line">                <span class="string">"right_terminal_state"</span>: environment_parameters[<span class="string">"right_terminal_state"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    agent_info = &#123;<span class="string">"num_states"</span>: environment_parameters[<span class="string">"num_states"</span>],</span><br><span class="line">                  <span class="string">"num_hidden_layer"</span>: agent_parameters[<span class="string">"num_hidden_layer"</span>],</span><br><span class="line">                  <span class="string">"num_hidden_units"</span>: agent_parameters[<span class="string">"num_hidden_units"</span>],</span><br><span class="line">                  <span class="string">"step_size"</span>: agent_parameters[<span class="string">"step_size"</span>],</span><br><span class="line">                  <span class="string">"discount_factor"</span>: environment_parameters[<span class="string">"discount_factor"</span>],</span><br><span class="line">                  <span class="string">"beta_m"</span>: agent_parameters[<span class="string">"beta_m"</span>],</span><br><span class="line">                  <span class="string">"beta_v"</span>: agent_parameters[<span class="string">"beta_v"</span>],</span><br><span class="line">                  <span class="string">"epsilon"</span>: agent_parameters[<span class="string">"epsilon"</span>]</span><br><span class="line">                 &#125;</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'Setting - Neural Network with 100 hidden units'</span>)</span><br><span class="line">    os.system(<span class="string">'sleep 1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># one agent setting</span></span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_runs"</span>]+<span class="number">1</span>)):</span><br><span class="line">        env_info[<span class="string">"seed"</span>] = run</span><br><span class="line">        agent_info[<span class="string">"seed"</span>] = run</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute initial RMSVE before training</span></span><br><span class="line">        current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">        agent_rmsve[run<span class="number">-1</span>, <span class="number">0</span>] = calc_RMSVE(current_V)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">1</span>, experiment_parameters[<span class="string">"num_episodes"</span>]+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># run episode</span></span><br><span class="line">            rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> episode % experiment_parameters[<span class="string">"episode_eval_frequency"</span>] == <span class="number">0</span>:</span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line">                agent_rmsve[run<span class="number">-1</span>, int(episode/experiment_parameters[<span class="string">"episode_eval_frequency"</span>])] = calc_RMSVE(current_V)</span><br><span class="line">            <span class="keyword">elif</span> episode == experiment_parameters[<span class="string">"num_episodes"</span>]: <span class="comment"># if last episode</span></span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">"get state value"</span>)</span><br><span class="line"></span><br><span class="line">        agent_state_val[run<span class="number">-1</span>, :] = current_V</span><br><span class="line"></span><br><span class="line">    save_name = <span class="string">"&#123;&#125;"</span>.format(rl_glue.agent.name).replace(<span class="string">'.'</span>,<span class="string">''</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'results'</span>):</span><br><span class="line">                os.makedirs(<span class="string">'results'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save avg. state value</span></span><br><span class="line">    np.save(<span class="string">"results/V_&#123;&#125;"</span>.format(save_name), agent_state_val)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># save avg. rmsve</span></span><br><span class="line">    np.savez(<span class="string">"results/RMSVE_&#123;&#125;"</span>.format(save_name), rmsve = agent_rmsve,</span><br><span class="line">                                                   eval_freq = experiment_parameters[<span class="string">"episode_eval_frequency"</span>],</span><br><span class="line">                                                   num_episodes = experiment_parameters[<span class="string">"num_episodes"</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_runs"</span> : <span class="number">20</span>,</span><br><span class="line">    <span class="string">"num_episodes"</span> : <span class="number">1000</span>,</span><br><span class="line">    <span class="string">"episode_eval_frequency"</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episode</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">"num_states"</span> : <span class="number">500</span>,</span><br><span class="line">    <span class="string">"start_state"</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">"left_terminal_state"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"right_terminal_state"</span> : <span class="number">501</span>,</span><br><span class="line">    <span class="string">"discount_factor"</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">"num_hidden_layer"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"num_hidden_units"</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">"step_size"</span>: <span class="number">0.001</span>,</span><br><span class="line">    <span class="string">"beta_m"</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">"beta_v"</span>: <span class="number">0.999</span>,</span><br><span class="line">    <span class="string">"epsilon"</span>: <span class="number">0.0001</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = RandomWalkEnvironment</span><br><span class="line">current_agent = TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># run experiment</span></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot result</span></span><br><span class="line">plot_script.plot_result([<span class="string">"td_agent"</span>])</span><br><span class="line"></span><br><span class="line">shutil.make_archive(<span class="string">'results'</span>, <span class="string">'zip'</span>, <span class="string">'results'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Setting - Neural Network with 100 hidden units
</code></pre><p><img src="output_39_2.png" alt="png"></p>
<pre><code>&#39;/home/jovyan/work/release/TD-NN/results.zip&#39;
</code></pre><p>You plotted the learning curve for 1000 episodes. As you can see the RMSVE is still decreasing. Here we provide the pre-computed result for 5000 episodes and 20 runs so that you can see the performance of semi-gradient TD with a neural network after being trained for a long time.</p>
<p><img src="nn_5000_episodes.png" alt></p>
<p>Does semi-gradient TD with a neural network find a good approximation within 5000 episodes? </p>
<p>As you may remember from the previous assignment, semi-gradient TD with 10-state aggregation converged within 100 episodes. Why is TD with a neural network slower?</p>
<p>Would it be faster if we decrease the number of hidden units? Or what about if we increase the number of hidden units?</p>
<h2 id="2-2-Compare-Performance-of-Semi-gradient-TD-with-a-Neural-Network-and-Semi-gradient-TD-with-Tile-coding"><a href="#2-2-Compare-Performance-of-Semi-gradient-TD-with-a-Neural-Network-and-Semi-gradient-TD-with-Tile-coding" class="headerlink" title="2-2: Compare Performance of Semi-gradient TD with a Neural Network and Semi-gradient TD with Tile-coding"></a>2-2: Compare Performance of Semi-gradient TD with a Neural Network and Semi-gradient TD with Tile-coding</h2><p>In this section, we compare the performance of semi-gradient TD with a Neural Network and semi-gradient TD with tile-coding. Tile-coding is a kind of coarse coding that uses multiple overlapping partitions of the state space to produce features. For tile-coding, we used 50 tilings each with 6 tiles. We set the step-size for semi-gradient TD with tile-coding to $\frac{0.1}{tilings}$. See the figure below for the comparison between semi-gradient TD with tile-coding and semi-gradient TD with a neural network and Adam algorithm. This result is for 5000 episodes and 20 runs:</p>
<p><img src="nn_vs_tc.png" alt></p>
<p>How are the results?</p>
<p>Semi-gradient TD with tile-coding is much faster than semi-gradient TD with a neural network. Why?</p>
<p>Which method has a lower RMSVE at the end of 5000 episodes?</p>
<h3 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up!"></a>Wrapping up!</h3><p>You have successfully implemented Course 3 Programming Assignment 2.</p>
<p>You have implemented <strong>semi-gradient TD with a Neural Network and Adam algorithm</strong> in 500-state Random Walk. </p>
<p>You also compared semi-gradient TD with a neural network and semi-gradient TD with tile-coding. </p>
<p>From the experiments and lectures, you should be more familiar with some of the strengths and weaknesses of using neural networks as the function approximator for an RL agent. On one hand, neural networks are powerful function approximators capable of representing a wide class of functions. They are also capable of producing features without exclusively relying on hand-crafted mechanisms. On the other hand, compared to a linear function approximator with tile-coding, neural networks can be less sample efficient. When implementing your own Reinforcement Learning agents, you may consider these strengths and weaknesses to choose the proper function approximator for your problems.</p>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/TD-with-State-Aggregation/2020/10/13/" rel="next" title="TD with State Aggregation ">
                <i class="fa fa-chevron-left"></i> TD with State Aggregation 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Function-Approximation-and-Control/2020/10/15/" rel="prev" title="Function Approximation and Control">
                Function Approximation and Control <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">273</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Assignment-2-Semi-gradient-TD-with-a-Neural-Network"><span class="nav-text">Assignment 2 - Semi-gradient TD with a Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Packages"><span class="nav-text">Packages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-1-Create-semi-gradient-TD-with-a-Neural-Network"><span class="nav-text">Section 1: Create semi-gradient TD with a Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Implement-helper-methods"><span class="nav-text">1-1: Implement helper methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Implement-get-value"><span class="nav-text">Implement get_value()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Implement-get-gradient"><span class="nav-text">Implement get_gradient()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Implement-stochastic-gradient-descent-method-for-state-value-prediction"><span class="nav-text">Implement stochastic gradient descent method for state-value prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-Algorithm"><span class="nav-text">Adam Algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Implement-Agent-Methods"><span class="nav-text">1-2: Implement Agent Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Section-2-Run-Experiment"><span class="nav-text">Section 2 - Run Experiment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Run-Experiment-for-Semi-gradient-TD-with-a-Neural-Network"><span class="nav-text">2-1: Run Experiment for Semi-gradient TD with a Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Compare-Performance-of-Semi-gradient-TD-with-a-Neural-Network-and-Semi-gradient-TD-with-Tile-coding"><span class="nav-text">2-2: Compare Performance of Semi-gradient TD with a Neural Network and Semi-gradient TD with Tile-coding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Wrapping-up"><span class="nav-text">Wrapping up!</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
