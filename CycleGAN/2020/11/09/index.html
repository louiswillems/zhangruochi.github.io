<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="PfeH4jmhwL">
<meta name="google-site-verification" content="A749_BVo91Gbd5oqBRsAAzolnmY_5JCET--CVn3ZQQA">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Didot:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Project,Generative Adversarial Network,">





  <link rel="alternate" href="/atom.xml" title="RUOCHI.AI" type="application/atom+xml">






<meta name="description" content="CycleGANGoalsIn this notebook, you will write a generative model based on the paper Unpaired Image-to-Image Translationusing Cycle-Consistent Adversarial Networks by Zhu et al. 2017, commonly referred">
<meta name="keywords" content="Project,Generative Adversarial Network">
<meta property="og:type" content="article">
<meta property="og:title" content="CycleGAN">
<meta property="og:url" content="https://zhangruochi.com/CycleGAN/2020/11/09/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="CycleGANGoalsIn this notebook, you will write a generative model based on the paper Unpaired Image-to-Image Translationusing Cycle-Consistent Adversarial Networks by Zhu et al. 2017, commonly referred">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://drive.google.com/uc?id=1K7fCr6lXzxn9LCUvKl0ozXthz2I9FZSv">
<meta property="og:image" content="https://drive.google.com/uc?id=1T-oic1q-wi-Hj4yeWb4r8sl_9laq58wm&">
<meta property="og:image" content="https://drive.google.com/uc?id=19cO4sawwlNNbnyh-jZCTWFSRd6mfrary">
<meta property="og:image" content="https://drive.google.com/uc?id=1bg8w_LGMLcVJEMdA4kyUr5UqxxtySTDq">
<meta property="og:updated_time" content="2020-11-09T10:10:56.251Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CycleGAN">
<meta name="twitter:description" content="CycleGANGoalsIn this notebook, you will write a generative model based on the paper Unpaired Image-to-Image Translationusing Cycle-Consistent Adversarial Networks by Zhu et al. 2017, commonly referred">
<meta name="twitter:image" content="https://drive.google.com/uc?id=1K7fCr6lXzxn9LCUvKl0ozXthz2I9FZSv">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangruochi.com/CycleGAN/2020/11/09/">





  <title>CycleGAN | RUOCHI.AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">RUOCHI.AI</span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            projects
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/CycleGAN/2020/11/09/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">CycleGAN</h2>
        

        <div class="post-meta">
          
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-09T18:10:31+08:00">
                2020-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index">
                    <span itemprop="name">Artificial Intelligence</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/CycleGAN/2020/11/09/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/CycleGAN/2020/11/09/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h1><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><p>In this notebook, you will write a generative model based on the paper <a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener"><em>Unpaired Image-to-Image Translation<br>using Cycle-Consistent Adversarial Networks</em></a> by Zhu et al. 2017, commonly referred to as CycleGAN.</p>
<p>You will be training a model that can convert horses into zebras, and vice versa. Once again, the emphasis of the assignment will be on the loss functions. In order for you to see good outputs more quickly, you’ll be training your model starting from a pre-trained checkpoint. You are also welcome to train it from scratch on your own, if you so choose.</p>
<!-- You will take the segmentations that you generated in the previous assignment and produce photorealistic images. -->
<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ol>
<li>Implement the loss functions of a CycleGAN model.</li>
<li>Observe the two GANs used in CycleGAN.</li>
</ol>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>You will start by importing libraries, defining a visualization function, and getting the pre-trained CycleGAN checkpoint.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_tensor_images</span><span class="params">(image_tensor, num_images=<span class="number">25</span>, size=<span class="params">(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class="line"><span class="string">    size per image, plots and prints the images in an uniform grid.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_tensor = (image_tensor + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    image_shifted = image_tensor</span><br><span class="line">    image_unflat = image_shifted.detach().cpu().view(<span class="number">-1</span>, *size)</span><br><span class="line">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class="number">5</span>)</span><br><span class="line">    plt.imshow(image_grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).squeeze())</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inspired by https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/datasets.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transform=None, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.files_A = sorted(glob.glob(os.path.join(root, <span class="string">'%sA'</span> % mode) + <span class="string">'/*.*'</span>))</span><br><span class="line">        self.files_B = sorted(glob.glob(os.path.join(root, <span class="string">'%sB'</span> % mode) + <span class="string">'/*.*'</span>))</span><br><span class="line">        <span class="keyword">if</span> len(self.files_A) &gt; len(self.files_B):</span><br><span class="line">            self.files_A, self.files_B = self.files_B, self.files_A</span><br><span class="line">        self.new_perm()</span><br><span class="line">        <span class="keyword">assert</span> len(self.files_A) &gt; <span class="number">0</span>, <span class="string">"Make sure you downloaded the horse2zebra images!"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">new_perm</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))</span><br><span class="line">        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))</span><br><span class="line">        <span class="keyword">if</span> item_A.shape[<span class="number">0</span>] != <span class="number">3</span>: </span><br><span class="line">            item_A = item_A.repeat(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> item_B.shape[<span class="number">0</span>] != <span class="number">3</span>: </span><br><span class="line">            item_B = item_B.repeat(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> index == len(self) - <span class="number">1</span>:</span><br><span class="line">            self.new_perm()</span><br><span class="line">        <span class="comment"># Old versions of PyTorch didn't support normalization for different-channeled images</span></span><br><span class="line">        <span class="keyword">return</span> (item_A - <span class="number">0.5</span>) * <span class="number">2</span>, (item_B - <span class="number">0.5</span>) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> min(len(self.files_A), len(self.files_B))</span><br></pre></td></tr></table></figure>
<h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>The code for a CycleGAN generator is much like Pix2Pix’s U-Net with the addition of the residual block between the encoding (contracting) and decoding (expanding) blocks.</p>
<p><img src="https://drive.google.com/uc?id=1K7fCr6lXzxn9LCUvKl0ozXthz2I9FZSv" alt="Diagram of a CycleGAN generator: composed of encoding blocks, residual blocks, then decoding blocks"><br><em>Diagram of a CycleGAN generator: composed of encoding blocks, residual blocks, and then decoding blocks.</em></p>
<h4 id="Residual-Block"><a href="#Residual-Block" class="headerlink" title="Residual Block"></a>Residual Block</h4><p>Perhaps the most notable architectural difference between the U-Net you used for Pix2Pix and the architecture you’re using for CycleGAN are the residual blocks. In CycleGAN, after the expanding blocks, there are convolutional layers where the output is ultimately added to the original input so that the network can change as little as possible on the image. You can think of this transformation as a kind of skip connection, where instead of being concatenated as new channels before the convolution which combines them, it’s added directly to the output of the convolution. In the visualization below, you can imagine the stripes being generated by the convolutions and then added to the original image of the horse to transform it into a zebra. These skip connections also allow the network to be deeper, because they help with vanishing gradients issues that come when a neural network gets too deep and the gradients multiply in backpropagation to become very small; instead, these skip connections enable more gradient flow. A deeper network is often able to learn more complex features.</p>
<p><img src="https://drive.google.com/uc?id=1T-oic1q-wi-Hj4yeWb4r8sl_9laq58wm&amp;" alt="Residual block explanation: shows horse going through convolutions leading to stripes, added to the original horse image to get a zebra"></p>
<p><em>Example of a residual block.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ResidualBlock Class:</span></span><br><span class="line"><span class="string">    Performs two convolutions and an instance normalization, the input is added</span></span><br><span class="line"><span class="string">    to this output to form the residual block output.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, padding_mode=<span class="string">'reflect'</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(input_channels, input_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, padding_mode=<span class="string">'reflect'</span>)</span><br><span class="line">        self.instancenorm = nn.InstanceNorm2d(input_channels)</span><br><span class="line">        self.activation = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ResidualBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes a residual block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        original_x = x.clone()</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.instancenorm(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.instancenorm(x)</span><br><span class="line">        <span class="keyword">return</span> original_x + x</span><br></pre></td></tr></table></figure>
<h4 id="Contracting-and-Expanding-Blocks"><a href="#Contracting-and-Expanding-Blocks" class="headerlink" title="Contracting and Expanding Blocks"></a>Contracting and Expanding Blocks</h4><p>The rest of the generator code will otherwise be much like the code you wrote for the last assignment: Pix2Pix’s U-Net. The primary changes are the use of instance norm instead of batch norm (which you may recall from StyleGAN), no dropout, and a stride-2 convolution instead of max pooling. Feel free to investigate the code if you’re interested!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContractingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ContractingBlock Class</span></span><br><span class="line"><span class="string">    Performs a convolution followed by a max pool operation and an optional instance norm.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, use_bn=True, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>)</span>:</span></span><br><span class="line">        super(ContractingBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, input_channels * <span class="number">2</span>, kernel_size=kernel_size, padding=<span class="number">1</span>, stride=<span class="number">2</span>, padding_mode=<span class="string">'reflect'</span>)</span><br><span class="line">        self.activation = nn.ReLU() <span class="keyword">if</span> activation == <span class="string">'relu'</span> <span class="keyword">else</span> nn.LeakyReLU(<span class="number">0.2</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            self.instancenorm = nn.InstanceNorm2d(input_channels * <span class="number">2</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ContractingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes a contracting block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.instancenorm(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpandingBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ExpandingBlock Class:</span></span><br><span class="line"><span class="string">    Performs a convolutional transpose operation in order to upsample, </span></span><br><span class="line"><span class="string">        with an optional instance norm</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, use_bn=True)</span>:</span></span><br><span class="line">        super(ExpandingBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.ConvTranspose2d(input_channels, input_channels // <span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_bn:</span><br><span class="line">            self.instancenorm = nn.InstanceNorm2d(input_channels // <span class="number">2</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        self.activation = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of ExpandingBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, completes an expanding block and returns the transformed tensor.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">            skip_con_x: the image tensor from the contracting path (from the opposing block of x)</span></span><br><span class="line"><span class="string">                    for the skip connection</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.instancenorm(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureMapBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    FeatureMapBlock Class</span></span><br><span class="line"><span class="string">    The final layer of a Generator - </span></span><br><span class="line"><span class="string">    maps each the output to the desired number of output channels</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels)</span>:</span></span><br><span class="line">        super(FeatureMapBlock, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=<span class="number">7</span>, padding=<span class="number">3</span>, padding_mode=<span class="string">'reflect'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of FeatureMapBlock: </span></span><br><span class="line"><span class="string">        Given an image tensor, returns it mapped to the desired number of channels.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="CycleGAN-Generator"><a href="#CycleGAN-Generator" class="headerlink" title="CycleGAN Generator"></a>CycleGAN Generator</h4><p>Finally, you can put all the blocks together to create your CycleGAN generator.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Generator Class</span></span><br><span class="line"><span class="string">    A series of 2 contracting blocks, 9 residual blocks, and 2 expanding blocks to </span></span><br><span class="line"><span class="string">    transform an input image into an image from the other class, with an upfeature</span></span><br><span class="line"><span class="string">    layer at the start and a downfeature layer at the end.</span></span><br><span class="line"><span class="string">    Values:</span></span><br><span class="line"><span class="string">        input_channels: the number of channels to expect from a given input</span></span><br><span class="line"><span class="string">        output_channels: the number of channels to expect for a given output</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, output_channels, hidden_channels=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        res_mult = <span class="number">4</span></span><br><span class="line">        self.res0 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res1 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res2 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res3 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res4 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res5 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res6 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res7 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.res8 = ResidualBlock(hidden_channels * res_mult)</span><br><span class="line">        self.expand2 = ExpandingBlock(hidden_channels * <span class="number">4</span>)</span><br><span class="line">        self.expand3 = ExpandingBlock(hidden_channels * <span class="number">2</span>)</span><br><span class="line">        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)</span><br><span class="line">        self.tanh = torch.nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Function for completing a forward pass of Generator: </span></span><br><span class="line"><span class="string">        Given an image tensor, passes it through the U-Net with residual blocks</span></span><br><span class="line"><span class="string">        and returns the output.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            x: image tensor of shape (batch size, channels, height, width)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.res0(x2)</span><br><span class="line">        x4 = self.res1(x3)</span><br><span class="line">        x5 = self.res2(x4)</span><br><span class="line">        x6 = self.res3(x5)</span><br><span class="line">        x7 = self.res4(x6)</span><br><span class="line">        x8 = self.res5(x7)</span><br><span class="line">        x9 = self.res6(x8)</span><br><span class="line">        x10 = self.res7(x9)</span><br><span class="line">        x11 = self.res8(x10)</span><br><span class="line">        x12 = self.expand2(x11)</span><br><span class="line">        x13 = self.expand3(x12)</span><br><span class="line">        xn = self.downfeature(x13)</span><br><span class="line">        <span class="keyword">return</span> self.tanh(xn)</span><br></pre></td></tr></table></figure>
<h2 id="PatchGAN-Discriminator"><a href="#PatchGAN-Discriminator" class="headerlink" title="PatchGAN Discriminator"></a>PatchGAN Discriminator</h2><p>Next, you will define the discriminator—a PatchGAN. It will be very similar to what you saw in Pix2Pix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Discriminator Class</span></span><br><span class="line"><span class="string">    Structured like the contracting path of the U-Net, the discriminator will</span></span><br><span class="line"><span class="string">    output a matrix of values classifying corresponding portions of the image as real or fake. </span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        input_channels: the number of image input channels</span></span><br><span class="line"><span class="string">        hidden_channels: the initial number of discriminator convolutional filters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_channels, hidden_channels=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)</span><br><span class="line">        self.contract1 = ContractingBlock(hidden_channels, use_bn=<span class="keyword">False</span>, kernel_size=<span class="number">4</span>, activation=<span class="string">'lrelu'</span>)</span><br><span class="line">        self.contract2 = ContractingBlock(hidden_channels * <span class="number">2</span>, kernel_size=<span class="number">4</span>, activation=<span class="string">'lrelu'</span>)</span><br><span class="line">        self.contract3 = ContractingBlock(hidden_channels * <span class="number">4</span>, kernel_size=<span class="number">4</span>, activation=<span class="string">'lrelu'</span>)</span><br><span class="line">        self.final = nn.Conv2d(hidden_channels * <span class="number">8</span>, <span class="number">1</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x0 = self.upfeature(x)</span><br><span class="line">        x1 = self.contract1(x0)</span><br><span class="line">        x2 = self.contract2(x1)</span><br><span class="line">        x3 = self.contract3(x2)</span><br><span class="line">        xn = self.final(x3)</span><br><span class="line">        <span class="keyword">return</span> xn</span><br></pre></td></tr></table></figure>
<h2 id="Training-Preparation"><a href="#Training-Preparation" class="headerlink" title="Training Preparation"></a>Training Preparation</h2><!-- You'll be using the same U-Net as in the previous assignment, but you'll write another discriminator and change the loss to make it a GAN. -->
<p>Now you can put everything together for training! You will start by defining your parameters:</p>
<ul>
<li>adv_criterion: an adversarial loss function to keep track of how well the GAN is fooling the discriminator and how well the discriminator is catching the GAN</li>
<li>recon_criterion: a loss function that rewards similar images to the ground truth, which “reconstruct” the image</li>
<li>n_epochs: the number of times you iterate through the entire dataset when training</li>
<li>dim_A: the number of channels of the images in pile A</li>
<li>dim_B: the number of channels of the images in pile B (note that in the visualization this is currently treated as equivalent to dim_A)</li>
<li>display_step: how often to display/visualize the images</li>
<li>batch_size: the number of images per forward/backward pass</li>
<li>lr: the learning rate</li>
<li>target_shape: the size of the input and output images (in pixels)</li>
<li>load_shape: the size for the dataset to load the images at before randomly cropping them to target_shape as a simple data augmentation</li>
<li>device: the device type</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">adv_criterion = nn.MSELoss() </span><br><span class="line">recon_criterion = nn.L1Loss() </span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">20</span></span><br><span class="line">dim_A = <span class="number">3</span></span><br><span class="line">dim_B = <span class="number">3</span></span><br><span class="line">display_step = <span class="number">200</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">load_shape = <span class="number">286</span></span><br><span class="line">target_shape = <span class="number">256</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br></pre></td></tr></table></figure>
<p>You will then load the images of the dataset while introducing some data augmentation (e.g. crops and random horizontal flips). </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(load_shape),</span><br><span class="line">    transforms.RandomCrop(target_shape),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = ImageDataset(<span class="string">"horse2zebra"</span>, transform=transform)</span><br></pre></td></tr></table></figure>
<p>Next, you can initialize your generators and discriminators, as well as their optimizers. For CycleGAN, you will have two generators and two discriminators since there are two GANs:</p>
<ul>
<li>Generator for horse to zebra (<code>gen_AB</code>)</li>
<li>Generator for zebra to horse (<code>gen_BA</code>)</li>
<li>Discriminator for horse (<code>disc_A</code>)</li>
<li>Discriminator for zebra (<code>disc_B</code>)</li>
</ul>
<p>You will also load your pre-trained model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">gen_AB = Generator(dim_A, dim_B).to(device)</span><br><span class="line">gen_BA = Generator(dim_B, dim_A).to(device)</span><br><span class="line">gen_opt = torch.optim.Adam(list(gen_AB.parameters()) + list(gen_BA.parameters()), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">disc_A = Discriminator(dim_A).to(device)</span><br><span class="line">disc_A_opt = torch.optim.Adam(disc_A.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">disc_B = Discriminator(dim_B).to(device)</span><br><span class="line">disc_B_opt = torch.optim.Adam(disc_B.parameters(), lr=lr, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d) <span class="keyword">or</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.normal_(m.weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        torch.nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feel free to change pretrained to False if you're training the model from scratch</span></span><br><span class="line">pretrained = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">if</span> pretrained:</span><br><span class="line">    pre_dict = torch.load(<span class="string">'cycleGAN_100000.pth'</span>)</span><br><span class="line">    gen_AB.load_state_dict(pre_dict[<span class="string">'gen_AB'</span>])</span><br><span class="line">    gen_BA.load_state_dict(pre_dict[<span class="string">'gen_BA'</span>])</span><br><span class="line">    gen_opt.load_state_dict(pre_dict[<span class="string">'gen_opt'</span>])</span><br><span class="line">    disc_A.load_state_dict(pre_dict[<span class="string">'disc_A'</span>])</span><br><span class="line">    disc_A_opt.load_state_dict(pre_dict[<span class="string">'disc_A_opt'</span>])</span><br><span class="line">    disc_B.load_state_dict(pre_dict[<span class="string">'disc_B'</span>])</span><br><span class="line">    disc_B_opt.load_state_dict(pre_dict[<span class="string">'disc_B_opt'</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    gen_AB = gen_AB.apply(weights_init)</span><br><span class="line">    gen_BA = gen_BA.apply(weights_init)</span><br><span class="line">    disc_A = disc_A.apply(weights_init)</span><br><span class="line">    disc_B = disc_B.apply(weights_init)</span><br></pre></td></tr></table></figure>
<h2 id="Discriminator-Loss"><a href="#Discriminator-Loss" class="headerlink" title="Discriminator Loss"></a>Discriminator Loss</h2><p>First, you’re going to be implementing the discriminator loss. This is the same as in previous assignments, so it should be a breeze :) Don’t forget to detach your generator!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_disc_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_disc_loss</span><span class="params">(real_X, fake_X, disc_X, adv_criterion)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the discriminator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_X: the real images from pile X</span></span><br><span class="line"><span class="string">        fake_X: the generated images of class X</span></span><br><span class="line"><span class="string">        disc_X: the discriminator for class X; takes images and returns real/fake class X</span></span><br><span class="line"><span class="string">            prediction matrices</span></span><br><span class="line"><span class="string">        adv_criterion: the adversarial loss function; takes the discriminator </span></span><br><span class="line"><span class="string">            predictions and the target labels and returns a adversarial </span></span><br><span class="line"><span class="string">            loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    fake_X.detach_()</span><br><span class="line">    fake_pred = disc_X(fake_X)</span><br><span class="line">    fake_loss = adv_criterion(fake_pred,torch.zeros_like(fake_pred))</span><br><span class="line">    real_pred = disc_X(real_X)</span><br><span class="line">    real_loss = adv_criterion(real_pred,torch.ones_like(real_pred))</span><br><span class="line">    disc_loss = (fake_loss + real_loss ) / <span class="number">2.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> disc_loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_disc_X = <span class="keyword">lambda</span> x: x * <span class="number">97</span></span><br><span class="line">test_real_X = torch.tensor(<span class="number">83.</span>)</span><br><span class="line">test_fake_X = torch.tensor(<span class="number">89.</span>)</span><br><span class="line">test_adv_criterion = <span class="keyword">lambda</span> x, y: x * <span class="number">79</span> + y * <span class="number">73</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs((get_disc_loss(test_real_X, test_fake_X, test_disc_X, test_adv_criterion)) - <span class="number">659054.5000</span>) &lt; <span class="number">1e-6</span></span><br><span class="line">test_disc_X = <span class="keyword">lambda</span> x: x.mean(<span class="number">0</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">test_adv_criterion = torch.nn.BCEWithLogitsLoss()</span><br><span class="line">test_input = torch.ones(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># If this runs, it's a pass - checks that the shapes are treated correctly</span></span><br><span class="line">get_disc_loss(test_input, test_input, test_disc_X, test_adv_criterion)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h2 id="Generator-Loss"><a href="#Generator-Loss" class="headerlink" title="Generator Loss"></a>Generator Loss</h2><p>While there are some changes to the CycleGAN architecture from Pix2Pix, the most important distinguishing feature of CycleGAN is its generator loss. You will be implementing that here!</p>
<h4 id="Adversarial-Loss"><a href="#Adversarial-Loss" class="headerlink" title="Adversarial Loss"></a>Adversarial Loss</h4><p>The first component of the generator’s loss you’re going to implement is its adversarial loss—this once again is pretty similar to the GAN loss that you’ve implemented in the past. The important thing to note is that the criterion now is based on least squares loss, rather than binary cross entropy loss or W-loss.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_adversarial_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_adversarial_loss</span><span class="params">(real_X, disc_Y, gen_XY, adv_criterion)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the adversarial loss of the generator given inputs</span></span><br><span class="line"><span class="string">    (and the generated images for testing purposes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_X: the real images from pile X</span></span><br><span class="line"><span class="string">        disc_Y: the discriminator for class Y; takes images and returns real/fake class Y</span></span><br><span class="line"><span class="string">            prediction matrices</span></span><br><span class="line"><span class="string">        gen_XY: the generator for class X to Y; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class Y</span></span><br><span class="line"><span class="string">        adv_criterion: the adversarial loss function; takes the discriminator </span></span><br><span class="line"><span class="string">                  predictions and the target labels and returns a adversarial </span></span><br><span class="line"><span class="string">                  loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    fake_Y = gen_XY(real_X)</span><br><span class="line">    pred_Y = disc_Y(fake_Y)</span><br><span class="line">    adversarial_loss = adv_criterion(pred_Y,torch.ones_like(pred_Y))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> adversarial_loss, fake_Y</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_disc_Y = <span class="keyword">lambda</span> x: x * <span class="number">97</span></span><br><span class="line">test_real_X = torch.tensor(<span class="number">83.</span>)</span><br><span class="line">test_gen_XY = <span class="keyword">lambda</span> x: x * <span class="number">89</span></span><br><span class="line">test_adv_criterion = <span class="keyword">lambda</span> x, y: x * <span class="number">79</span> + y * <span class="number">73</span></span><br><span class="line">test_res = get_gen_adversarial_loss(test_real_X, test_disc_Y, test_gen_XY, test_adv_criterion)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">0</span>] - <span class="number">56606652</span>) &lt; <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">1</span>] - <span class="number">7387</span>) &lt; <span class="number">1e-6</span></span><br><span class="line">test_disc_Y = <span class="keyword">lambda</span> x: x.mean(<span class="number">0</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">test_adv_criterion = torch.nn.BCEWithLogitsLoss()</span><br><span class="line">test_input = torch.ones(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># If this runs, it's a pass - checks that the shapes are treated correctly</span></span><br><span class="line">get_gen_adversarial_loss(test_input, test_disc_Y, test_gen_XY, test_adv_criterion)</span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h4 id="Identity-Loss"><a href="#Identity-Loss" class="headerlink" title="Identity Loss"></a>Identity Loss</h4><p>Here you get to see some of the superbly new material! You’ll want to measure the change in an image when you pass the generator an example from the target domain instead of the input domain it’s expecting. The output should be the same as the input since it is already of the target domain class. For example, if you put a horse through a zebra -&gt; horse generator, you’d expect the output to be the same horse because nothing needed to be transformed. It’s already a horse! You don’t want your generator to be transforming it into any other thing, so you want to encourage this behavior. In encouraging this identity mapping, the authors of CycleGAN found that for some tasks, this helped properly preserve the colors of an image, even when the expected input (here, a zebra) was put in. This was particularly useful for the photos &lt;-&gt; paintings mapping and, while an optional aesthetic component, you might find it useful for your applications down the line.</p>
<p><img src="https://drive.google.com/uc?id=19cO4sawwlNNbnyh-jZCTWFSRd6mfrary" alt="Diagram showing a real horse image going through a zebra -&gt; horse generator and the ideal output being the same input image"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_identity_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_identity_loss</span><span class="params">(real_X, gen_YX, identity_criterion)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the identity loss of the generator given inputs</span></span><br><span class="line"><span class="string">    (and the generated images for testing purposes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_X: the real images from pile X</span></span><br><span class="line"><span class="string">        gen_YX: the generator for class Y to X; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class X</span></span><br><span class="line"><span class="string">        identity_criterion: the identity loss function; takes the real images from X and</span></span><br><span class="line"><span class="string">                        those images put through a Y-&gt;X generator and returns the identity </span></span><br><span class="line"><span class="string">                        loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    identity_X = gen_YX(real_X)</span><br><span class="line">    identity_loss = identity_criterion(real_X,identity_X)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> identity_loss, identity_X</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_real_X = torch.tensor(<span class="number">83.</span>)</span><br><span class="line">test_gen_YX = <span class="keyword">lambda</span> x: x * <span class="number">89</span></span><br><span class="line">test_identity_criterion = <span class="keyword">lambda</span> x, y: (x + y) * <span class="number">73</span></span><br><span class="line">test_res = get_identity_loss(test_real_X, test_gen_YX, test_identity_criterion)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">0</span>] - <span class="number">545310</span>) &lt; <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">1</span>] - <span class="number">7387</span>) &lt; <span class="number">1e-6</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h4 id="Cycle-Consistency-Loss"><a href="#Cycle-Consistency-Loss" class="headerlink" title="Cycle Consistency Loss"></a>Cycle Consistency Loss</h4><p>Now, you can implement the final generator loss and the part that puts the “cycle” in CycleGAN: cycle consistency loss. This is used to ensure that when you put an image through one generator, that if it is then transformed back into the input class using the opposite generator, the image is the same as the original input image.</p>
<p><img src="https://drive.google.com/uc?id=1bg8w_LGMLcVJEMdA4kyUr5UqxxtySTDq" alt="Diagram showing a real zebra image being transformed into a horse and then back into a zebra. The output zebra should be the same as the input zebra."></p>
<p>Since you’ve already generated a fake image for the adversarial part, you can pass that fake image back to produce a full cycle—this loss will encourage the cycle to preserve as much information as possible.</p>
<p><em>Fun fact: Cycle consistency is a broader concept that’s used outside of CycleGAN a lot too! It’s helped with data augmentation and has been used on text translation too, e.g. French -&gt; English -&gt; French should get the same phrase back.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_cycle_consistency_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cycle_consistency_loss</span><span class="params">(real_X, fake_Y, gen_YX, cycle_criterion)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the cycle consistency loss of the generator given inputs</span></span><br><span class="line"><span class="string">    (and the generated images for testing purposes).</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_X: the real images from pile X</span></span><br><span class="line"><span class="string">        fake_Y: the generated images of class Y</span></span><br><span class="line"><span class="string">        gen_YX: the generator for class Y to X; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class X</span></span><br><span class="line"><span class="string">        cycle_criterion: the cycle consistency loss function; takes the real images from X and</span></span><br><span class="line"><span class="string">                        those images put through a X-&gt;Y generator and then Y-&gt;X generator</span></span><br><span class="line"><span class="string">                        and returns the cycle consistency loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    cycle_X = gen_YX(fake_Y)</span><br><span class="line">    cycle_loss = cycle_criterion(real_X,cycle_X)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> cycle_loss, cycle_X</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_real_X = torch.tensor(<span class="number">83.</span>)</span><br><span class="line">test_fake_Y = torch.tensor(<span class="number">97.</span>)</span><br><span class="line">test_gen_YX = <span class="keyword">lambda</span> x: x * <span class="number">89</span></span><br><span class="line">test_cycle_criterion = <span class="keyword">lambda</span> x, y: (x + y) * <span class="number">73</span></span><br><span class="line">test_res = get_cycle_consistency_loss(test_real_X, test_fake_Y, test_gen_YX, test_cycle_criterion)</span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">1</span>] - <span class="number">8633</span>) &lt; <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">assert</span> torch.abs(test_res[<span class="number">0</span>] - <span class="number">636268</span>) &lt; <span class="number">1e-6</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h4 id="Generator-Loss-Total"><a href="#Generator-Loss-Total" class="headerlink" title="Generator Loss (Total)"></a>Generator Loss (Total)</h4><p>Finally, you can put it all together! There are many components, so be careful as you go through this section.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_gen_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gen_loss</span><span class="params">(real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, identity_criterion, cycle_criterion, lambda_identity=<span class="number">0.1</span>, lambda_cycle=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the loss of the generator given inputs.</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        real_A: the real images from pile A</span></span><br><span class="line"><span class="string">        real_B: the real images from pile B</span></span><br><span class="line"><span class="string">        gen_AB: the generator for class A to B; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class B</span></span><br><span class="line"><span class="string">        gen_BA: the generator for class B to A; takes images and returns the images </span></span><br><span class="line"><span class="string">            transformed to class A</span></span><br><span class="line"><span class="string">        disc_A: the discriminator for class A; takes images and returns real/fake class A</span></span><br><span class="line"><span class="string">            prediction matrices</span></span><br><span class="line"><span class="string">        disc_B: the discriminator for class B; takes images and returns real/fake class B</span></span><br><span class="line"><span class="string">            prediction matrices</span></span><br><span class="line"><span class="string">        adv_criterion: the adversarial loss function; takes the discriminator </span></span><br><span class="line"><span class="string">            predictions and the true labels and returns a adversarial </span></span><br><span class="line"><span class="string">            loss (which you aim to minimize)</span></span><br><span class="line"><span class="string">        identity_criterion: the reconstruction loss function used for identity loss</span></span><br><span class="line"><span class="string">            and cycle consistency loss; takes two sets of images and returns</span></span><br><span class="line"><span class="string">            their pixel differences (which you aim to minimize)</span></span><br><span class="line"><span class="string">        cycle_criterion: the cycle consistency loss function; takes the real images from X and</span></span><br><span class="line"><span class="string">            those images put through a X-&gt;Y generator and then Y-&gt;X generator</span></span><br><span class="line"><span class="string">            and returns the cycle consistency loss (which you aim to minimize).</span></span><br><span class="line"><span class="string">            Note that in practice, cycle_criterion == identity_criterion == L1 loss</span></span><br><span class="line"><span class="string">        lambda_identity: the weight of the identity loss</span></span><br><span class="line"><span class="string">        lambda_cycle: the weight of the cycle-consistency loss</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Hint 1: Make sure you include both directions - you can think of the generators as collaborating</span></span><br><span class="line">    <span class="comment"># Hint 2: Don't forget to use the lambdas for the identity loss and cycle loss!</span></span><br><span class="line">    <span class="comment">#### START CODE HERE ####</span></span><br><span class="line">    <span class="comment"># Adversarial Loss -- get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion)</span></span><br><span class="line">    adv_loss,fake_B = get_gen_adversarial_loss(real_A, disc_B, gen_AB, adv_criterion)</span><br><span class="line">    adv_loss_2,fake_A = get_gen_adversarial_loss(real_B, disc_A, gen_BA, adv_criterion)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Identity Loss -- get_identity_loss(real_X, gen_YX, identity_criterion)</span></span><br><span class="line">    ide_loss,identity_A = get_identity_loss(real_A, gen_BA, identity_criterion)</span><br><span class="line">    ide_loss_2,identity_B = get_identity_loss(real_B, gen_AB, identity_criterion)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cycle-consistency Loss -- get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion)</span></span><br><span class="line">    cyc_loss,cycle_A = get_cycle_consistency_loss(real_A, fake_B, gen_BA, cycle_criterion)</span><br><span class="line">    cyc_loss_2,cycle_B = get_cycle_consistency_loss(real_B, fake_A, gen_AB, cycle_criterion)</span><br><span class="line">    <span class="comment"># Total loss</span></span><br><span class="line">    </span><br><span class="line">    gen_loss = adv_loss + adv_loss_2 + lambda_identity * (ide_loss + ide_loss_2) +  lambda_cycle * (cyc_loss + cyc_loss_2)</span><br><span class="line">    <span class="comment">#### END CODE HERE ####</span></span><br><span class="line">    <span class="keyword">return</span> gen_loss, fake_A, fake_B</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST</span></span><br><span class="line">test_real_A = torch.tensor(<span class="number">97</span>)</span><br><span class="line">test_real_B = torch.tensor(<span class="number">89</span>)</span><br><span class="line">test_gen_AB = <span class="keyword">lambda</span> x: x * <span class="number">83</span></span><br><span class="line">test_gen_BA = <span class="keyword">lambda</span> x: x * <span class="number">79</span></span><br><span class="line">test_disc_A = <span class="keyword">lambda</span> x: x * <span class="number">47</span></span><br><span class="line">test_disc_B = <span class="keyword">lambda</span> x: x * <span class="number">43</span></span><br><span class="line">test_adv_criterion = <span class="keyword">lambda</span> x, y: x * <span class="number">73</span> + y * <span class="number">71</span></span><br><span class="line">test_recon_criterion = <span class="keyword">lambda</span> x, y: (x + y) * <span class="number">61</span></span><br><span class="line">test_lambda_identity = <span class="number">59</span></span><br><span class="line">test_lambda_cycle = <span class="number">53</span></span><br><span class="line">test_res = get_gen_loss(</span><br><span class="line">    test_real_A, </span><br><span class="line">    test_real_B, </span><br><span class="line">    test_gen_AB, </span><br><span class="line">    test_gen_BA, </span><br><span class="line">    test_disc_A,</span><br><span class="line">    test_disc_B,</span><br><span class="line">    test_adv_criterion, </span><br><span class="line">    test_recon_criterion, </span><br><span class="line">    test_recon_criterion, </span><br><span class="line">    test_lambda_identity, </span><br><span class="line">    test_lambda_cycle)</span><br><span class="line"><span class="keyword">assert</span> test_res[<span class="number">0</span>].item() == <span class="number">4047804560</span></span><br><span class="line"><span class="keyword">assert</span> test_res[<span class="number">1</span>].item() == <span class="number">7031</span></span><br><span class="line"><span class="keyword">assert</span> test_res[<span class="number">2</span>].item() == <span class="number">8051</span></span><br><span class="line">print(<span class="string">"Success!"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Success!
</code></pre><h2 id="CycleGAN-Training"><a href="#CycleGAN-Training" class="headerlink" title="CycleGAN Training"></a>CycleGAN Training</h2><p>Lastly, you can train the model and see some of your zebras, horses, and some that might not quite look like either! Note that this training will take a long time, so feel free to use the pre-trained checkpoint as an example of what a pretty-good CycleGAN does.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> color</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(save_model=False)</span>:</span></span><br><span class="line">    mean_generator_loss = <span class="number">0</span></span><br><span class="line">    mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    cur_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="comment"># Dataloader returns the batches</span></span><br><span class="line">        <span class="comment"># for image, _ in tqdm(dataloader):</span></span><br><span class="line">        <span class="keyword">for</span> real_A, real_B <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">            <span class="comment"># image_width = image.shape[3]</span></span><br><span class="line">            real_A = nn.functional.interpolate(real_A, size=target_shape)</span><br><span class="line">            real_B = nn.functional.interpolate(real_B, size=target_shape)</span><br><span class="line">            cur_batch_size = len(real_A)</span><br><span class="line">            real_A = real_A.to(device)</span><br><span class="line">            real_B = real_B.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update discriminator A ###</span></span><br><span class="line">            disc_A_opt.zero_grad() <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake_A = gen_BA(real_B)</span><br><span class="line">            disc_A_loss = get_disc_loss(real_A, fake_A, disc_A, adv_criterion)</span><br><span class="line">            disc_A_loss.backward(retain_graph=<span class="keyword">True</span>) <span class="comment"># Update gradients</span></span><br><span class="line">            disc_A_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update discriminator B ###</span></span><br><span class="line">            disc_B_opt.zero_grad() <span class="comment"># Zero out the gradient before backpropagation</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake_B = gen_AB(real_A)</span><br><span class="line">            disc_B_loss = get_disc_loss(real_B, fake_B, disc_B, adv_criterion)</span><br><span class="line">            disc_B_loss.backward(retain_graph=<span class="keyword">True</span>) <span class="comment"># Update gradients</span></span><br><span class="line">            disc_B_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">### Update generator ###</span></span><br><span class="line">            gen_opt.zero_grad()</span><br><span class="line">            gen_loss, fake_A, fake_B = get_gen_loss(</span><br><span class="line">                real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, recon_criterion, recon_criterion</span><br><span class="line">            )</span><br><span class="line">            gen_loss.backward() <span class="comment"># Update gradients</span></span><br><span class="line">            gen_opt.step() <span class="comment"># Update optimizer</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Keep track of the average discriminator loss</span></span><br><span class="line">            mean_discriminator_loss += disc_A_loss.item() / display_step</span><br><span class="line">            <span class="comment"># Keep track of the average generator loss</span></span><br><span class="line">            mean_generator_loss += gen_loss.item() / display_step</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Visualization code ###</span></span><br><span class="line">            <span class="keyword">if</span> cur_step % display_step == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">f"Epoch <span class="subst">&#123;epoch&#125;</span>: Step <span class="subst">&#123;cur_step&#125;</span>: Generator (U-Net) loss: <span class="subst">&#123;mean_generator_loss&#125;</span>, Discriminator loss: <span class="subst">&#123;mean_discriminator_loss&#125;</span>"</span>)</span><br><span class="line">                show_tensor_images(torch.cat([real_A, real_B]), size=(dim_A, target_shape, target_shape))</span><br><span class="line">                show_tensor_images(torch.cat([fake_B, fake_A]), size=(dim_B, target_shape, target_shape))</span><br><span class="line">                mean_generator_loss = <span class="number">0</span></span><br><span class="line">                mean_discriminator_loss = <span class="number">0</span></span><br><span class="line">                <span class="comment"># You can change save_model to True if you'd like to save the model</span></span><br><span class="line">                <span class="keyword">if</span> save_model:</span><br><span class="line">                    torch.save(&#123;</span><br><span class="line">                        <span class="string">'gen_AB'</span>: gen_AB.state_dict(),</span><br><span class="line">                        <span class="string">'gen_BA'</span>: gen_BA.state_dict(),</span><br><span class="line">                        <span class="string">'gen_opt'</span>: gen_opt.state_dict(),</span><br><span class="line">                        <span class="string">'disc_A'</span>: disc_A.state_dict(),</span><br><span class="line">                        <span class="string">'disc_A_opt'</span>: disc_A_opt.state_dict(),</span><br><span class="line">                        <span class="string">'disc_B'</span>: disc_B.state_dict(),</span><br><span class="line">                        <span class="string">'disc_B_opt'</span>: disc_B_opt.state_dict()</span><br><span class="line">                    &#125;, <span class="string">f"cycleGAN_<span class="subst">&#123;cur_step&#125;</span>.pth"</span>)</span><br><span class="line">            cur_step += <span class="number">1</span></span><br><span class="line">train()</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Project/" rel="tag"># Project</a>
          
            <a href="/tags/Generative-Adversarial-Network/" rel="tag"># Generative Adversarial Network</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Pix2Pix/2020/11/09/" rel="next" title="Pix2Pix">
                <i class="fa fa-chevron-left"></i> Pix2Pix
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/CS224W-Colab-1/2021/01/22/" rel="prev" title="CS224W - Colab 1">
                CS224W - Colab 1 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

    


  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Ruochi Zhang">
            
              <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">273</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangruochi" target="_blank" title="GitHub rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zrc720@gmail.com" target="_blank" title="E-Mail rel=" external nofollow"">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friend links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.healthinformaticslab.org" title="HILab" target="_blank" rel="external nofollow">HILab</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.shihaizhou.com" title="Rose" target="_blank" rel="external nofollow">Rose</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/cherish_CX/" title="Chunxia" target="_blank" rel="external nofollow">Chunxia</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CycleGAN"><span class="nav-text">CycleGAN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Goals"><span class="nav-text">Goals</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Objectives"><span class="nav-text">Learning Objectives</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Getting-Started"><span class="nav-text">Getting Started</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generator"><span class="nav-text">Generator</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Residual-Block"><span class="nav-text">Residual Block</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Contracting-and-Expanding-Blocks"><span class="nav-text">Contracting and Expanding Blocks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CycleGAN-Generator"><span class="nav-text">CycleGAN Generator</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#PatchGAN-Discriminator"><span class="nav-text">PatchGAN Discriminator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Preparation"><span class="nav-text">Training Preparation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Discriminator-Loss"><span class="nav-text">Discriminator Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generator-Loss"><span class="nav-text">Generator Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Adversarial-Loss"><span class="nav-text">Adversarial Loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Identity-Loss"><span class="nav-text">Identity Loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cycle-Consistency-Loss"><span class="nav-text">Cycle Consistency Loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Generator-Loss-Total"><span class="nav-text">Generator Loss (Total)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CycleGAN-Training"><span class="nav-text">CycleGAN Training</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
  
  
</div>








        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
        appKey: 'GL6JvT9DgGxqYrY5Vj6bXVuv',
        lang: 'en',
        placeholder: 'Thank you for your reply',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
